

# *第三章*:彗星中的模型评估

在接受一个数据科学模型之前，我们需要评估它，以确定它是否可以投入生产。模型评估是评估已训练模型是否按预期执行的过程。通常，我们在不同于模型训练数据集的数据集上执行模型评估。

在这一章中，你将回顾模型评估背后的基本概念，比如数据分割，如何选择评估的度量标准，以及错误分析背后的基本概念。此外，您将看到不同数据科学任务(分类、回归和聚类)的主要模型评估技术。

最后，您将通过深化一些您已经知道的概念，例如实验、面板和报告，以及引入新的概念，包括超参数调整、模型注册和查询，来学习如何在 Comet 中执行模型评估。

在本章中，你还将实现一个实际的例子。

本章组织如下:

*   引入模型评估
*   探索模型评估技术
*   使用 Comet 进行模型评估

在回顾模型评估背后的概念之前，让我们安装运行本章包含的代码和实验所需的所有 Python 包。

# 技术要求

我们将使用 Python 3.8 运行本章中的所有实验和代码。可以从官网([https://www.python.org/downloads/](https://www.python.org/downloads/))下载，选择 3.8 版本。

本章中描述的示例使用了以下 Python 包:

*   `comet-ml 3.23.0`
*   `matplotlib 3.4.3`
*   `numpy 1.19.5`
*   `pandas 1.3.4`
*   `scikit-learn 1.0`

我们已经在 [*第一章*](B17530_01_ePub.xhtml#_idTextAnchor015) 、*慧星概述*中描述了前五个包以及如何安装。因此，请参考后面的安装细节。

既然您已经安装了本章中需要的所有软件，让我们从回顾一些关于模型评估的基本概念开始，向如何使用 Comet 进行模型评估的方向前进。

# 引入模型评估

**模型评估**是对一个或多个数据科学模型的性能进行评估的过程，以决定哪一个是解决给定任务的最佳模型。模型评估是一个迭代的任务，因为我们一遍又一遍地运行它，直到我们得到一个满意的模型。

模型评估取决于我们想要解决的任务。一般来说，有两种类型的任务:

*   **监督学习**——你用一些有标签的数据训练一个模型，你用其他有标签的数据测试模型，然后你试着预测看不见的和没有标签的数据的目标值。在这种情况下，模型评估很简单，因为在测试阶段，您可以将模型产生的输出与标记的测试数据进行比较。
*   **无监督学习**——你确实没有任何被标记的数据，但是你试图在一些标准的基础上预测输出，比如数据相似度。在这种情况下，模型评估相当复杂，因为您没有任何测试数据来进行比较。

在监督学习的情况下，模型评估涉及测试数据和模型预测值之间的承诺误差的比较。我们可以根据特定的任务计算不同的指标，我们将在接下来的章节中看到。在无监督学习的情况下，模型评估不是一项简单的任务，因为我们没有用于比较的参考数据集。然而，在这种情况下，我们仍然可以计算一些指标，我们将在下面的章节中看到。

如果我们假设我们已经有了一组要测试的模型，模型评估由以下两个步骤组成:

*   **数据拆分**
*   **选择指标**

让我们从第一步——数据分割开始，分别研究每一步。

## 数据拆分

让我们假设我们想要建立一个应用程序，用户上传图片，系统识别其中的汽车。为了训练分类模型，我们需要一个图片数据集。我们可以从 web 上收集它们，将它们分成训练集和测试集(例如，70%的图像用于训练集，剩余的 30%用于测试集)，并用训练集训练一个神经网络。然后，我们对测试集执行模型评估，从而计算准确性。让我们假设我们获得了 90%的正确率。但是当我们在真实场景中运行我们的系统时，我们获得了一个糟糕的性能，比如只有 50%的准确率。我们的模式出了什么问题？

在前面的示例中，我们将原始数据集分为两部分，即训练集和测试集，我们使用训练集来训练模型，使用测试集来评估模型性能。然而，在大数据时代，这种将数据以 70-30 的比例划分为训练集和测试集的做法现在已经过时，因为测试集有足够的样本来评估性能的微小改进就足够了。此外，最佳实践是使用三个数据集:

*   **训练集**–用于训练模型的数据集。
*   **Dev(开发)set**–用于调整超参数、选择特性以及在我们的模型上执行其他决策任务的数据集。
*   **测试集**–用于执行模型评估的数据集。对测试集的评估结果不影响模型的选择。

在实践中，我们使用开发和测试集来执行模型评估，但是当我们利用在开发集上计算的性能来改进模型时，我们只使用测试集来评估模型的最终结果。在实践中，测试集代表真实的场景；因此，我们不应该从原始数据集中提取它，而是从我们将使用我们的系统的真实世界中提取它。在前面的汽车识别示例中，我们应该直接从应用程序中提取测试集。在开始的时候，我们会有几张图片，所以我们的测试集会很小。然后，随着用户上传图片，我们的测试集将增加，因此我们可以做出更准确的评估。

所有三个数据集应该具有相同的分布。**数据分布**显示数据可以假设的所有可能值。尽管我们知道现实世界中有许多种数据分布，如正态分布、贝塔分布和伽玛分布，但数据并不采用其中的任何一种。例如，参考前面的汽车识别示例，如果开发集包含带有*赛车*的图片，而测试集包含带有*老爷车*的图片，则两个数据集遵循不同的分布。

有许多技术可以检验两个变量是否遵循相同的分布。然而，他们需要独立测试两个数据集的每一列。在本节中，我们提出一个策略来检查整个数据集是否遵循相同的分布，而不仅仅是单个列。

我们可以使用下图所示的策略。

![Figure 3.1 – A possible strategy to check whether two datasets have the same distribution

](image/B17530_03_001.jpg)

图 3.1–检查两个数据集是否具有相同分布的可能策略

假设我们有两个数据集，即**数据集 A** 和**数据集 B** (例如，开发和测试集):

1.  首先，我们向两个数据集添加一个新列，并将其命名为`target`。对于数据集 A，我们将所有记录的目标设置为 1；对于数据集 B，我们将所有记录的目标值设置为 0。
2.  我们将两个丰富的数据集组合起来，以获得单个数据集。我们也洗牌。
3.  我们将获得的数据集分成两部分——训练集和测试集。
4.  我们用训练集训练一个分类模型，并用测试集评估它。
5.  我们计算模型的精确度。如果我们获得一个较低的精度值，这意味着两个原始数据集非常相似；因此，模型无法正确识别记录是属于第一个数据集还是第二个数据集。如果准确率高，说明两个数据集差别很大。

请注意，在前面的策略中，我们已经将计算两个数据集是否遵循相同分布的问题转化为分类问题。

如果开发和测试数据集不遵循相同的分布，我们有一种称为**协变量移位**的情况。在这个案例中，我们会遇到以下问题:

*   **过拟合**—模型在开发集上表现良好，但在测试集上表现不佳。
*   **复杂性**–测试集可能比开发集更复杂；因此，该模型不适合管理问题的复杂性。
*   **多样性**–简单地说，测试集不同于开发集，因此模型解决的问题与我们在现实中遇到的问题不同。

既然您已经了解了如何将数据分割成训练、开发和测试集，以及协变量转移的一般问题，我们可以进入下一步，选择指标。

## 选择指标

让我们假设你实现了四个模型解决了图片中汽车识别的同一个问题。现在，您希望选择最佳模型并将其投入生产。您可能决定为所有模型计算不同的度量(例如，精度和召回率)，然后选择具有最佳度量的模型。但是，可能会出现这样的情况:一个模型在一个指标上比其他模型表现得更好，而另一个模型在另一个指标上比其他模型表现得更好，如下表所示:

![ Figure 3.2 – Precision and recall for the four models of the car recognition example

](image/B17530_03_Table1.jpg)

图 3.2-汽车识别示例的四种模型的精确度和召回率

我们注意到**模型 1** 具有最好的精度值，而**模型 4** 具有最好的召回值。哪一个是最好的模型？根据计算的指标，前面的例子表明没有绝对的最佳模型。

要解决前面的问题，您应该将您计算的所有指标组合起来定义一个指标。您的最佳模型将是对您定义的指标具有最佳价值的模型。您可以定义自己的指标，这取决于您的特定任务。例如，您可以编写以下度量:

![](image/Formula_B17530_03_001.png)

在前面的公式中，![](image/Formula_B17530_03_002.png)是组合度量，![](image/Formula_B17530_03_003.png)和![](image/Formula_B17530_03_004.png)分别是精度和召回率，![](image/Formula_B17530_03_005.png)和![](image/Formula_B17530_03_006.png)分别是分配给![](image/Formula_B17530_03_007.png)和![](image/Formula_B17530_03_008.png)的权重。例如，如果对于您的任务来说，精度比召回更重要，您可以设置![](image/Formula_B17530_03_009.png)和![](image/Formula_B17530_03_010.png)。

在上例中，选择最佳模型很简单，如下表所示:

![Figure 3.3 – Precision, Recall, and Combined Metric for the car recognition example

](image/B17530_03_Table2.jpg)

图 3.3-汽车识别示例的精确度、召回率和综合指标

上表显示**型号 2** 为最佳型号。

一旦您定义了您的组合指标，您应该在 dev 集合上优化它。可能发生的情况是，您的组合指标并不是衡量模型性能的最佳指标。通常，当至少出现下列情况之一时，就会出现这种情况:

*   **过拟合**–开发集和测试集之间的性能完全不同。
*   **改变**–测试集已经改变。
*   **瞄准**–组合的度量标准测量项目需要优化的内容之外的其他内容。

如果至少有一个的情况发生，建议改变你的综合指标。

既然您已经学习了如何选择最佳的度量来评估您的模型，我们可以进入下一部分，探索模型评估技术。

# 探索模型评估技术

根据我们想要解决的问题，有不同的模型评估技术。在这一节中，我们将考虑三种类型的问题:回归、分类和聚类。

前两个问题属于有监督学习的范畴，第三种方法属于无监督学习的范畴。

在本节中，您将回顾在前面提到的问题中用于模型评估的主要指标。我们将用 Python 实现一个实际例子来说明如何计算每个指标。为了回顾主要的评估指标，我们将只使用两个数据集:训练集和测试集。

关于监督学习，还有一种额外的技术来执行模型评估。这种技术被称为**交叉验证**。交叉验证背后的基本思想是将一个原始数据集分成几个子集。该模型训练所有子集，除了一个。训练阶段完成后，将在剩余的子集上测试模型。对于数据集的所有可能子集，这是一个迭代过程。我们会在 [*第八章*](B17530_08_ePub.xhtml#_idTextAnchor169) 、 *Comet for Machine Learning* 中详细讨论交叉验证以及 Comet 是如何支持的。

本节组织如下:

*   加载和准备数据集
*   回归
*   分类
*   使聚集

让我们从第一步开始，加载和准备数据集。

## 加载和准备数据集

我们将使用`ggplot2`在麻省理工学院许可下提供的`Diamonds`数据集([https://ggplot2.tidyverse.org/reference/diamonds.xhtml](https://ggplot2.tidyverse.org/reference/diamonds.xhtml))，并在 Kaggle 上以 CSV 文件([https://www.kaggle.com/shivam2503/diamonds](https://www.kaggle.com/shivam2503/diamonds))的形式提供:

1.  首先，我们加载数据集作为一个`pandas`数据帧:

    ```
    import pandas as pd
    df = pd.read_csv('source/diamonds.csv')
    ```

数据集包含 53，940 行和 11 列。下图显示了数据集的前 10 行:

![Figure 3.4 – The first 10 rows of the diamonds dataset

](image/B17530_03_004.jpg)

图 3.4–钻石数据集的前 10 行

请注意， `diamonds`数据集包含一些分类列，包括**切割**、**颜色**和**净度**，以及一些数字列(其他)。

1.  我们删除第一列`Unnamed: 0`，如下所示:

    ```
    df = df.drop(["Unnamed: 0"], axis=1)
    ```

我们使用由`pandas`和`axis=1`提供的`drop()`方法来表示我们想要删除列。

1.  我们定义了两个实用函数来转换我们的数据；我们将使用第一个方法将分类列转换为数值列，使用第二个方法对数值列进行缩放。我们将第一个函数定义如下:

    ```
    from sklearn.preprocessing import LabelEncoder
    def encode_labels(data):
         categories = (data.dtypes =="object")
         cat_cols = list(categories[categories].index)
          categories = (X.dtypes =="object")
        feature_label_encoder_dict = {}
        for col in cat_cols:
            feature_label_encoder_dict[col] = LabelEncoder()
            X[col] = feature_label_encoder_dict[col].fit_transform(X[col])
    ```

函数接收数据帧作为输入。首先，我们导入`LabelEncoder`类，它允许我们将分类值转换成数值。然后，我们选择所有的分类列，并将它们存储在`categories`变量中。接下来，我们为每个类别列构建一个`LabelEncoder()`对象，并将其存储在名为`feature_label_encoder_dict`的字典中。最后，对于每个分类列，我们拟合并转换构建的`feature_label_encoder_dict`对象。

1.  现在，我们定义`scale_numerical()`函数，如下:

    ```
    from sklearn.preprocessing import StandardScaler
    def scale_numerical(data):
        scaler = StandardScaler()
        data[data.columns] = scaler.fit_transform(data[data.columns])
    ```

该函数接收数据帧作为输入，并通过一个`StandardScaler()`对象缩放所有数字列。

既然我们已经准备好了数据，我们可以进入下一步，评估回归的度量。

## 回归

回归分析是一种受监督的机器学习，它试图在一个或多个输入变量`X`的基础上预测一个连续的目标变量`Y`。为了评估一个回归任务，我们可以计算许多指标。

作为一个回归任务示例，我们希望建立一个模型，根据其他特征预测钻石的价格。在计算这些指标之前，我们按如下方式构建训练集和测试集:

1.  我们将数据集分成输入特征(`X`)和目标变量(`Y` ):

    ```
    X = df.drop("price", axis = 1)
    y = df["price"]
    ```

2.  我们对标签进行编码，并对数字列进行缩放:

    ```
    encode_labels(X)
    scale_numerical(X)
    ```

请注意，我们已经使用了之前定义的函数。

1.  我们通过`scikit-learn` :

    ```
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
    ```

    提供的`train_test_split()`函数将数据集分成训练集和测试集

我们为测试集保留了 20%的样本，为训练集保留了剩余的 80%的样本。此外，我们将`random_state`设置为`42`以使实验具有可重复性。

1.  我们创建一个新的线性回归模型，用训练集拟合它，并计算测试集上的预测值:

    ```
    model = LinearRegression()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    ```

在这个例子中，我们不关心模型优化；因此，我们使用默认模型。

既然我们已经提取了预测值，我们可以计算用于评估回归模型的主要指标。在本节中，我们计算三个最常用的指标:平均绝对误差、均方根误差和 R 平方:

*   **平均绝对误差(MAE)**–实际值与预测值之差的平均值。这衡量了预测与实际输出之间的差距。MAE 越低，模型越好。在前面的例子中，我们可以如下计算 MAE:

    ```
    from sklearn.metrics import mean_absolute_error
    MAE = mean_absolute_error(y_test,y_pred)
    ```

*   **均方根误差(RMSE)**—**均方误差** ( **MSE** )的平方根。MSE 类似于 MAE。唯一的区别是 MSE 计算实际值和预测值之间的差的平方的平均值。RMSE 是评估回归模型最常用的指标，因为它可以让您了解模型预测线周围的数据集中程度。

在前面的例子中，我们可以如下计算 RMSE:

```
from sklearn.metrics import mean_squared_error
import numpy as np
RMSE = np.sqrt(mean_squared_error(y_test, y_pred))
```

*   **R 的平方**或**决定系数**—`Y`中可以用`X`解释的方差的比例。换句话说，R 的平方描述了数据符合回归模型的程度。r 的平方是一个介于 0 和 1 之间的数。例如，R 的平方= 0.80 意味着 80%的数据符合模型。一般来说，R 的平方值越高，模型越好。然而，这并不总是正确的；因此，您应该始终将 R 平方与其他指标结合起来。在前面的例子中，我们可以如下计算 R 的平方:

    ```
    from sklearn.metrics import r2_score
    R2 = r2_score(y_test, y_pred)
    ```

既然您已经看到了用于评估回归模型的最流行的指标，我们可以分析用于分类的最常见的指标。

## 分类

分类是监督学习的一种类型，它试图在一个或多个输入变量`X`的基础上预测目标类别标签`Y`。如果类别标签的数量是两个，我们有二进制分类；否则，如果标签的数量大于两个，我们有多类分类。在这一章中，我们只考虑二元分类，但是所描述的一般概念也可以扩展到多类分类。

作为分类任务示例，我们希望建立一个模型，根据其他特征预测钻石的切工。钻石数据集包含五种类型的钻石切工(**理想的**、**优质的**、**非常好的**、**良好的**和**一般的**)；因此，问题是多类分类。为简单起见，我们将多类分类问题转化为二元分类问题。

在计算二元分类的指标之前，我们准备数据集，如下所示:

1.  我们将切割分成两类，`Gold`和`Silver`，如下面的代码所示:

    ```
    def set_target(x):
        golden_set = ['Ideal', 'Premium', 'Very Good']
        if x in golden_set:
            return 'Gold'
        return 'Silver'
    df['target'] = df['cut'].apply(lambda x: set_target(x))
    df.drop("cut", axis = 1,inplace=True)
    ```

我们定义一个名为`set_target()`的函数，它接收名为`x`的变量作为输入，检查它是否属于`golder_set`，如果是`true`则返回`'Gold'`，否则返回`'Silver'`。然后，我们在原始数据帧中创建一个新列，称为`target`，它包含`set_target()`函数的输出。我们还删除了原来的`cut`列，不再需要它了。

1.  现在，我们构建训练和测试集，如下:

    ```
    X = df.drop("target", axis = 1)
    y = df["target"]
    ```

作为输入特性`X`，我们考虑除了`target`之外的所有列，它与`y`输出特性相关联。

1.  我们对输入特征进行编码和缩放:

    ```
    encode_labels(X)
    scale_numerical(X)
    ```

因为我们之前已经定义了`encode_labels()`和`scale_numerical()`函数，所以这个操作非常简单。

1.  我们还对`target`标签进行编码:

    ```
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y)
    ```

2.  我们将数据集分为训练集和测试集，如下面的代码所示:

    ```
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
    ```

我们使用`scikit-learn`提供的`train_test_split()`函数。

1.  我们建立分类模型并训练它。在这个例子中，我们考虑一个`RandomForestClassifier` :

    ```
    model = RandomForestClassifier()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    ```

我们用训练集拟合模型，并预测测试集的值。与回归的情况类似，我们不关心模型优化，因为在这一部分，我们的目标是显示评估指标。

现在我们已经建立了模型，我们可以计算最流行的分类指标-混淆矩阵、精确度、召回率、准确度、F1 分数和 ROC 曲线:

*   **混淆矩阵**——一个两行两列的表格，如下图所示:

![Figure 3.5 – The confusion matrix

](image/B17530_03_005.jpg)

图 3.5–混淆矩阵

该表显示了预测值(行)和实际值(列)。表格的每个单元格对应正确或错误分类的数量；

*   **真正(TP)**—实际值为正，预测值为正。在这种情况下，模型表现良好。
*   **真负(TN)**—实际值为负，预测值为负。在这种情况下，模型表现良好。
*   **假阳性(FP)**—实际值为负，但预测值为正。在这种情况下，模型出错了。
*   **假阴性(FN)**—实际值为正，但预测值为负。在这种情况下，模型出错了。

1.  在`scikit-learn`中，我们可以如下计算混淆矩阵:

    ```
    from sklearn.metrics import confusion_matrix
    [tp,fp], [fn,tn] = confusion_matrix(y_test, y_pred)
    ```

2.  我们使用`confusion_matrix()`函数，它接收测试集和预测值作为输入，并返回 TP、FP、FN 和 TN。`scikit-learn`还提供了直接绘制混淆矩阵的函数，如下面这段代码所示:

    ```
    from sklearn.metrics import plot_confusion_matrix
    import matplotlib.pyplot as plt
    plot_confusion_matrix(model, X_test, y_test, cmap='GnBu')
    plt.show()
    ```

函数接收模型和测试集作为输入。作为一个附加参数，我们可以传递颜色图(在我们的例子中是`'GnBu'`)。下图显示了我们示例中的`plot_confusion_matrix()`函数的输出:

![Figure 3.6 – The output of the plot_confusion_matrix() function

](image/B17530_03_006.jpg)

图 3.6–plot _ confusion _ matrix()函数的输出

矩阵显示了表中每个单元格的记录数。例如，第一个单元格表示有 9，300 个真阳性。该表格还根据颜色渐变给单元格着色。

1.  在前面测量的基础上，我们可以定义其他指标:
    *   **精度**——在所有的正面预测(TP 和 FP)中，数一数有多少是真正正面的(TP)。在`scikit-learn`中，我们可以这样计算:

        ```
        from sklearn.metrics import precision_score
        precision = precision_score(y_test, y_pred)
        ```

我们使用`precision_score()`函数，它接收测试集和预测值作为输入，并返回一个对应于精度的数字。

*   **回忆**——在所有真正的阳性病例(TP 和 FN)中，统计有多少是预测阳性(TP)。在`scikit-learn`中，我们可以这样计算:

    ```
    from sklearn.metrics import recall_score
    recall = recall_score(y_test, y_pred)
    ```

我们使用的`recall_score()`函数，它接收测试集和预测值作为输入，并返回一个对应于召回的数字。

*   **准确率**——在所有情况(TP + TN + FP + FN)中，统计有多少被正确预测。在`scikit-learn`中，我们可以这样计算:

    ```
    from sklearn.metrics import accuracy_score
    accuracy = accuracy_score(y_test, y_pred)
    ```

我们使用`accuracy_score()`函数，它接收测试集和预测值作为输入，并返回一个对应于精确度的数字。许多数据科学家使用准确性作为单一指标来测试他们模型的有效性。

*   **F1-得分**-精确和回忆的谐音。在`scikit-learn`中，我们可以这样计算:

    ```
    from sklearn.metrics import f1_score
    f1 = f1_score(y_test, y_pred)
    ```

我们使用`f1_score()`函数，它接收测试集和预测值作为输入，并返回一个对应于 F1 分数的数字。

*   **受试者工作特性曲线** ( **ROC 曲线**)——显示**真阳性率**(召回或 **TPR** )对**假阳性率** ( **FRP** )的曲线。我们可以认为 FPR 是所有负值(FP 和 TN)中被错误分类为正值(FP)的预测数。我们绘制了不同阈值的 ROC 曲线——阈值= 1，TPR = 1，FPR = 1。在`scikit-learn`中，我们可以绘制 ROC 曲线如下:

    ```
    from sklearn.metrics import roc_curve,roc_auc_score
    y_pred_proba = model.predict_proba(X_test)[::,1]
    fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)
    auc = roc_auc_score(y_test, y_pred_proba)
    plt.plot(fpr,tpr,label='auc=%.3f' % auc, color='#084081')
    axis_ranges = [0,1]
    plt.plot(axis_ranges, axis_ranges, linestyle='--', color='k', scalex=False, scaley=False)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend()
    plt.grid()
    plt.show()
    ```

首先，我们通过`predict_proba()`方法计算每一类的预测概率。然后，我们通过`roc_curve()`函数计算 ROC 曲线，该函数接收`predict_proba()`方法的输出作为输入。我们还通过`roc_auc_score()`函数计算 AUC 分数。曲线下的**面积** ( **AUC** )分数衡量分类器区分目标类别的能力。AUC 越高，模型在区分阳性和阴性目标方面的表现越好。下图显示了前面代码的输出:

![Figure 3.7 – The ROC curve

](image/B17530_03_007.jpg)

图 3.7-ROC 曲线

请注意，在我们的示例中，该模型表现得相当好，因为 ROC 曲线倾向于向左变平。虚线代表 0.5 的 AUC 分数，这是随机猜测模型所固有的。

既然我们已经回顾了用于评估分类模型的最流行的指标，我们可以分析用于聚类的最常见的指标。

## 聚类

聚类分析是一种无监督的机器学习，没有训练集。聚类用于根据相似性标准(如距离)对记录进行分组。聚类分析模型将数据集作为输入，并将对应于关联分类的标签列表作为输出返回。

评估分类模型的性能并不容易，因为您应该验证每个记录都被分配了正确的分类。换句话说，您应该验证每条记录与属于其分类的记录的相似程度要比与属于其他分类的记录的相似程度高得多。

在计算指标之前，我们将准备`diamonds`数据集，如下所示:

1.  为了简单起见，我们将只考虑数据集的两列，`price`和`carat`，如下面的代码所示:

    ```
    X = df[['price', 'carat']]
    ```

2.  然后，我们绘制数据集，如下:

    ```
    plt.scatter(X['price'],X['carat'])
    plt.xlabel('Price')
    plt.ylabel('Carat')
    plt.grid()
    plt.show()
    ```

下图显示了结果图:

![Figure 3.8 – A scatter plot showing Carat against Price

](image/B17530_03_008.jpg)

图 3.8–显示克拉与价格的散点图

1.  我们使用 K-means 模型将我们的数据分为两类，如下:

    ```
    from sklearn.cluster import KMeans
    model = KMeans(n_clusters=2)
    labels = model.fit_predict(X)
    ```

我们使用由`scikit-learn`提供的`KMeans()`类，然后我们调用 `fit_predict()`方法来计算集群。

1.  我们将聚类的结果绘制如下:

    ```
    from matplotlib.colors import ListedColormap
    cmap = ListedColormap(['#40B7AD', '#084081'])
    plt.scatter(X['price'],X['carat'], c=labels, cmap=cmap)
    plt.xlabel('Price')
    plt.ylabel('Carat')
    plt.grid()
    plt.show()
    ```

我们给散点图的每个点分配一种颜色，对应于相关的标签。下图显示了结果图:

![Figure 3.9 – The dataset after clustering

](image/B17530_03_009.jpg)

图 3.9–聚类后的数据集

现在我们已经对数据集进行了聚类，我们可以对模型进行评估了。有两种类型的评估:

*   **基于监督的评估**(或**外在方法**)假设有一个基础真值来执行评估。其思想是将基本事实与聚类算法的结果进行比较，以计算得分。许多指标都属于这一类别，例如同质性得分**和软糖得分**。因为计算一个地面真值是非常困难的，在这一章中，我们将不会关注这种类型的评估。
*   **基于无监督的评估**(或**固有方法**)计算聚类的分离程度以及聚类的紧密程度。

关于内在方法，我们可以计算许多指标，包括以下指标:

*   **轮廓系数**或**轮廓分数**测量一个物体与它的簇相比与其他物体的相似程度。该系数的范围从–1 到 1。值等于 1 意味着对象被正确分类，而值为–1 意味着模型将对象插入了错误的聚类中。在`scikit-learn`中，我们可以这样计算:

    ```
    from sklearn.metrics import silhouette_score
    score = silhouette_score(X, labels)
    ```

该函数接收`X`数据集和标签作为输入，并返回代表轮廓分数的数字。在前面的例子中，分数是 0.708。

*   **肘法**——一种估计 k 个簇的最佳数量的可视化方法。我们使用逐渐增加的聚类数多次运行该算法，然后我们绘制样本到其最近聚类中心(SSE)的距离平方和，作为聚类数的函数。在 Python 中，我们可以如下应用肘方法:

    ```
    sse = {}
    for i in range(2,10):
        model = KMeans(n_clusters=i)
        sse[i] = model.inertia_
    ```

我们定义了一个具有不同簇数量值的循环。在拟合模型之后，我们为每次迭代计算 SSE ( `model.inertia_`)。我们将结果绘制如下:

```
plt.plot(sse.keys(), sse.values())
plt.grid()
plt.xlabel('Number of Clusters')
plt.ylabel('SSE')
plt.show()
```

前面的代码生成了下图:

![Figure 3.10 – The elbow method

](image/B17530_03_010.jpg)

图 3.10–弯管法

我们可以将曲线开始快速下降的点确定为正确的聚类数。在我们的例子中，集群的最佳数量可能是两个。

既然我们已经回顾了模型评估的所有主要技术，我们准备进入下一部分，使用 Comet 进行模型评估。

# 利用彗星进行模型评估

Comet 提供了以下特性来处理模型评估:

*   **Log**——用于在 Comet 中存储指标、资产和对象
*   **仪表板**–用于比较实验结果
*   **注册表**–用于跟踪和存储您的模型
*   **报告**–用于显示结果

下图显示了如何结合 Comet 提供的特性来比较不同的模型，然后选择最佳的一个用于生产:

![Figure 3.11 – How to use Comet for model evaluation

](image/B17530_03_011.jpg)

图 3.11–如何使用 Comet 进行模型评估

假设您想要比较 *N* 个模型，然后选择最佳模型进行部署。你建立你的实验，然后在 Comet 中跟踪它们。通过 Comet Dashboard，您可以通过构建面板、图表、表格和其他类似的对象来比较模型。您还可以将您的模型存储在 Comet 注册表中。您甚至可以从 Comet 平台导出显示比较结果的报告。一旦您选择了最佳模型，您就可以从注册表中将其导出并用于生产。

为了展示我们如何使用 Comet 进行模型评估，您将使用之前定义的 diamonds 数据集，而 you 将实现四个分类模型—**随机森林**、**决策树**、**高斯朴素贝叶斯**和**K-最近邻**。我们将使用这些模型的基本版本，而不优化它们，因为本章的目的是展示如何执行模型评估。关于如何优化分类模型的更多细节，可以参考 [*第八章*](B17530_08_ePub.xhtml#_idTextAnchor169) ，*机器学习的慧星*。您将使用前一节*分类*中描述的数据集的清理版本，我们按如下方式构建:

```
options = ['Ideal', 'Premium']
```

```
df2 = df[df['cut'].isin(options)]
```

```
X = df2.drop("cut", axis = 1)
```

```
y = df2["cut"]
```

我们假设`df`包含了`diamonds`数据集。我们只为`cut`选择了两个可能的值来处理二元分类。然后，我们构建了输入(`X`)和目标(`y`)变量。我们还假设我们已经对标签进行了编码，并对数值进行了缩放，如前一节所述。

我们现在可以继续分析 Comet 提供的每个特性，从第一个日志开始。

## 彗星日志

一个 **Comet 日志**是一个对象，它在 Comet 中存储一个度量、一个参数或一个对象。我们已经在 [*第一章*](B17530_01_ePub.xhtml#_idTextAnchor015)*慧星*概述 [*第二章*](B17530_02_ePub.xhtml#_idTextAnchor043)*慧星*探索性数据分析中描述了慧星日志背后的基本概念。因此，你可以参考那些章节来了解基本概念。在这一节中，我们将回顾对于模型评估最有用的 Comet 日志。

我们将使用由`experiment`类提供的以下方法:

*   `log_metrics()`
*   `log_curve()`
*   `log_confusion_matrix()`

假设我们已经将数据集分为训练集和测试集，如前一节所述:

1.  首先，我们定义一个名为`compute_metrics()`的辅助函数，它计算我们实验的所有评估指标:

    ```
    from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
    def compute_metrics(y_pred, y_true):
    metrics = {}
        metrics['precision'] = precision_score(y_true, y_pred)
         metrics['recall'] = recall_score(y_true, y_pred)
         metrics['f1-score'] = f1_score(y_true, y_pred)
         metrics['accuracy'] =  accuracy_score(y_true, y_pred)
         return metrics
    ```

函数返回一个名为`metrics`的字典，其中存储了所有计算出的指标。我们计算精确度、召回率、F1 分数和准确度。在*选择指标*部分的讨论之后，我们将只使用一个指标来执行不同模型之间的比较:准确性。但是，为了完整起见，我们还计算了其他指标。

1.  现在，我们定义另一个辅助函数，它运行单个实验。该函数接收模型类和模型名称作为输入，然后用训练集训练模型，最后，它计算指标，以及混淆矩阵和 ROC 曲线。下面的代码实现了所描述的功能:

    ```
    from sklearn.metrics import roc_curve
    def run_experiment(ModelClass, name):
        experiment = Experiment()
        experiment.set_name(name)
        experiment.add_tag(name)

        model = ModelClass()
        with experiment.train():    
            model.fit(X_train, y_train)
            y_pred = model.predict(X_train)
            metrics = compute_metrics(y_pred, y_train)
            experiment.log_metrics(metrics)
            experiment.log_confusion_matrix(y_train, y_pred)

        with experiment.validate():
            y_pred = model.predict(X_test)
            metrics = compute_metrics(y_pred, y_test)
            experiment.log_metrics(metrics)
            experiment.log_confusion_matrix(y_test, y_pred)
            fpr, tpr, _ = roc_curve(y_test, y_pred)
            experiment.log_curve(name, fpr, tpr)
    ```

首先，我们创建一个新的`experiment`对象来允许与 Comet 通信。要配置`experiment`参数，包括工作区和 API 键，可以参考 [*第一章*](B17530_01_ePub.xhtml#_idTextAnchor015) ，*慧星概述*。我们还将实验名称设置为作为输入传递的名称，以使实验可以从 Comet 仪表板上识别出来。我们使用`Experiment`类的`set_name()`方法来执行这个操作。此外，我们通过`add_tag()`方法给实验添加了一个新的标签。然后，我们通过`model = ModelClass()`语句创建我们的模型。注意`ModelClass()`是一个变量，您将在调用函数时设置它。

一旦我们创建了模型，我们就可以训练它。我们使用`with experiment.train()`语句让 Comet 知道所有记录的对象都属于训练阶段。我们通过`compute_metrics()`函数计算指标，然后通过`Experiment`类提供的`log_metrics()`方法将它们记录到 Comet 中。此外，我们通过的`log_confusion_matrix()`方法记录混淆矩阵。一旦训练完成，我们就可以测试模型了。类似于训练阶段，我们使用`with experiment.validate()`语句让 Comet 知道所有记录的对象都属于训练阶段。我们在测试集上调用模型提供的`predict()`方法，并且我们计算所有的度量，就像在训练阶段已经做的那样。另外，在这个阶段，我们通过`roc_curve()`函数计算 ROC 曲线，并通过`log_curve()`方法将其记录到 Comet 中，该方法由`Experiment`类提供。

1.  最后，我们可以运行实验。我们测试四个分类器，如下:

    ```
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier

    run_experiment(RandomForestClassifier, 'RandomForest')
    run_experiment(DecisionTreeClassifier, 'DecisionTreeClassifier')
    run_experiment(GaussianNB, 'GaussianNB')
    run_experiment(KNeighborsClassifier, 'KNeighborsClassifier')
    ```

简单地说，我们为每个我们想要测试的分类器调用`run_experiment()`函数。在示例中，我们测试了随机森林、决策树、高斯朴素贝叶斯和 K-最近邻。

`log_metrics()`方法还允许我们记录时期和步骤。一个**历元**是一个超参数，仅适用于某些类型的算法，例如基于梯度下降的算法。一个时期对应于算法将对训练数据集工作的次数。

将历元设置为 1 意味着训练集的每个样本在训练阶段只有一次更新算法的机会。一个**步骤**(或批次)定义在更新模型参数之前到使用的样本数量。我们可以将步长设置为等于训练集大小或一个更小的数字。

1.  为了处理历元，我们可以修改`run_experiment()`函数，如下:

    ```
    def run_experiment(ModelClass, name, n_epochs):
        ...    
        with experiment.train():
            for i in range(n_epochs):
                model = ModelClass(max_iter=n_epochs)
                model.fit(X_train, y_train)
                y_pred = model.predict(X_train)
                metrics = compute_metrics(y_pred, y_train)
                experiment.log_metrics(metrics, epoch = i)
                experiment.log_confusion_matrix(y_train, y_pred, epoch=i)
    ...
    ```

我们向函数添加另一个参数，即历元的数量(`n_epochs`)。然后，在训练阶段，我们在多个时期上构建一个循环，并为每个时期构建一个模型。前面的函数仅适用于支持历元数的模型，如`SGD`分类器:

```
from sklearn.linear_model import SGDClassifier
run_experiment_with_epoch(SGDClassifier, 'SGD',1000)
```

我们构建了一个分类器，并将时期的数量设置为 1000。

类似于历元数，我们可以设置步数。例如，我们可以决定用不同大小的数据集来训练我们的模型。因此，我们可以修改`run_experiment()`函数，如下所示:

```
import numpy as np
def run_experiment(ModelClass, name):
    step_size = len(X_train)
    min_steps = 20
    ...    
    with experiment.train():
        for i in np.arange(min_steps, step_size+1, step = 5000):
            model = ModelClass()
            X_t = X_train[0:i]
            y_t = y_train[0:i]
            model.fit(X_t, y_t)
            y_pred = model.predict(X_t)
            metrics = compute_metrics(y_pred, y_t)
            experiment.log_metrics(metrics, step = i)
            experiment.log_confusion_matrix(y_t, y_pred, step=i)
    ...
```

我们将步长设置为训练集的长度，并且一步中的最小样本数等于 20。然后，我们循环不同的步骤并训练不同的模型，每个模型都有越来越多的样本。

现在 you 已经记录了所有需要的指标，我们可以在 Comet 仪表板中分析它们。

## 彗星仪表盘

**Comet Dashboard** 是 Comet online 网站，里面储存了你所有的实验。让我们假设您已经运行了前面描述的所有四个实验。在**面板**或**实验**选项卡下，您会看到它们，如下图所示:

![Figure 3.12 – The four experiments shown in Comet

](image/B17530_03_012.jpg)

图 3.12–Comet 中显示的四个实验

我们可以容易地识别每个实验，因为我们已经为每个模型设置了实验名称。

我们可以在实验中进行以下类型的比较:

*   排序
*   通过表格进行原始比较
*   过滤
*   分组

让我们分别看一下每种类型的比较，从第一种开始，排序。

我们可以通过特定的参数或指标来安排实验:

1.  我们选择订购按钮，如下图所示:

![Figure 3.13 – The ordering button in Comet Dashboard

](image/B17530_03_013.jpg)

图 3.13–Comet 仪表板中的订购按钮

1.  一个新的弹出窗口打开，显示订购实验的默认标准。默认有两个标准。我们可以添加任意多的标准。在这个例子中，我们只设置了一个标准。我们可以通过点击 **X** 按钮删除一个默认标准。
2.  然后，我们可以通过打开下拉菜单并选择 **validate_accuracy** 来更改剩余的标准，如下图所示:

![Figure 3.14 – How to select the ordering criterion in the drop-down menu

](image/B17530_03_014.jpg)

图 3.14–如何在下拉菜单中选择排序标准

我们可以也可以选择是按升序(上箭头)还是降序(下箭头)排列实验。在我们的例子中，我们选择降序。现在订购了四款，如下图所示:

![Figure 3.15 – The four experiments after sorting by validate_accuracy

](image/B17530_03_015.jpg)

图 3.15–通过 validate_accuracy 排序后的四个实验

由于这个简单的操作，我们根据我们的评估标准知道哪个是最好的模型。

现在，我们可以通过表格对实验进行粗略的比较。我们可以看到每个评估指标的详细信息。

1.  我们点击**实验**选项卡，看到所有实验的所有信息，如下图所示:

![Figure 3.16 – A portion of the Experiments tab

](image/B17530_03_016.jpg)

图 3.16–实验选项卡的一部分

图只显示了每个实验的一些细节，比如**标签**、**服务器结束时间**、**文件名**、**持续时间**。然而，Comet 为用户提供了所有记录的参数和指标，在我们的例子中有 15 列，如上图顶部的**列**按钮所示。

1.  如果我们点击**列**按钮，我们可以选择我们想要查看的列，如下图所示:

![Figure 3.17 – The pop-up window to customize columns

](image/B17530_03_017.jpg)

图 3.17–自定义列的弹出窗口

1.  我们选择以下各列—**验证 _ 精度**、**验证 _ 精度**、**验证 _ 召回**和**验证 _ f1-分数**。因此，**实验**选项卡显示一个表格，如下图所示:

![Figure 3.18 – The Experiments tab after selecting some columns

](image/B17530_03_018.jpg)

图 3.18–选择一些列后的实验选项卡

我们注意到随机森林模型达到了 0.957 的精度，其次是决策树模型，精度为 0.942，然后是另外两个模型。通过彗星仪表盘，对比实验非常简单。

现在，我们可以根据一些标准对实验进行分组:

1.  我们可以通过按钮点击**分组，如下图所示:**

![Figure 3.19 – The Group by button in the Experiments tab

](image/B17530_03_019.jpg)

图 3.19–实验选项卡中的分组按钮

1.  一个下拉菜单打开。在下拉菜单中，我们可以选择分组标准——例如，我们可以选择**持续时间**。因此，Comet 按照持续时间对所有实验进行分组，如下图所示:

![Figure 3.20 – The result of grouping by duration

](image/B17530_03_020.jpg)

图 3.20–按持续时间分组的结果

有三组——1 秒、4 秒和 10 秒。在 4 秒组中，有两个模型，而在其他组中，只有一个模型。

最后，我们可以通过一些标准来筛选实验:

1.  我们可以点击**滤镜**按钮，如下图所示:

![Figure 3.21 – The Filters button in the Experiments tab

](image/B17530_03_021.jpg)

图 3.21–实验选项卡中的过滤器按钮

1.  将打开一个弹出窗口。我们可以点击**添加过滤器**按钮| **validate_accuracy** | **大于**|**0.92**|**Done**|**Close**。因此，Comet Dashboard 只显示满足过滤器的实验，如下图所示:

![Figure 3.22 – The Experiments tab after applying the validate_accuracy greater than 0.92 filter

](image/B17530_03_022.jpg)

图 3.22–应用 validate_accuracy 大于 0.92 过滤器后的实验选项卡

请注意，随机森林、决策树和 K-最近邻满足应用的过滤器。

对于所有描述的操作，您可以从弹出的窗口中应用任意多的标准。

既然您已经学习了如何在 Comet 中比较实验，我们可以进入下一步，模型注册。

## 注册表

一个**彗星注册表**或**模型注册表**是彗星平台中可用的位置，存储所有注册的模型。在 Comet 中注册模型至少有两个好处。首先，您可以跟踪项目的所有阶段，其次，您可以将注册表用作安全存储。

要使模型在模型注册表中可用，首先，您需要在实验过程中通过使用由`Experiment`类提供的以下方法之一来注册它:

*   `log_model(name, file_name)`–将模型记录为工件，然后手动将其添加到注册表中。
*   `register_model(MODEL_NAME)`–注册整个实验并将其添加到注册表中

为了记录模型，我们可以修改先前定义的`run_experiment()`函数，如下所示:

1.  首先，我们定义了一个辅助函数，它将模型保存在本地文件系统和 Comet 中:

    ```
    import pickle 
    def save_file_to_comet(obj, obj_name, file_name, experiment):
        with open(file_name, 'wb') as file:  
            pickle.dump(obj, file)
        file.close()
        experiment.log_model(obj_name, file_name)
    ```

该函数接收对象(`obj`)作为输入，保存其名称、输出文件名和 Comet `experiment`对象。我们可以通过`pickle`库提供的`dump()`函数，将模型保存为`pickle`文件。然后，我们通过`Experiment`类的`log_model()`方法记录保存的模型。

1.  然后，我们可以在`run_experiment()`函数中使用这个函数，如下:

    ```
    def run_experiment(ModelClass, name):
       ...
       with experiment.train():
            ...
            file_name = 'model.pkl'
            save_file_to_comet(model, name, file_name, experiment)
              ...
    ```

2.  您还应该记住保存用于构建训练和测试集的定标器和编码器。您可以简单地通过使用之前定义的`save_file_to_comet()`函数来完成。例如，您可以将`scaler and encoder` 对象作为附加参数传递给`run_experiment()`函数，然后调用`save_file_to_comet()`函数来将它们保存在 Comet 中，如下面这段代码中的所示:

    ```
    def run_experiment(ModelClass, name, feature_label_encoder_dict, scaler,label_encoder):
            ...
            if feature_label_encoder_dict:
            for k,v in feature_label_encoder_dict.items():
                obj_name = f"{k}FeatureLabelEncoder"
                file_name = f"{obj_name}.pkl"
                save_file_to_comet(feature_label_encoder_dict[k], obj_name, file_name, experiment)

        if label_encoder:
            obj_name = "labelEncoder"
            file_name = f"{obj_name}.pkl"
            save_file_to_comet(label_encoder, obj_name, file_name, experiment)

        if scaler:
            obj_name = "scaler"
            file_name = f"{obj_name}.pkl"
            save_file_to_comet(scaler, obj_name, file_name, experiment)
    ```

我们分别保存每个对象。对于每个对象，我们在本地文件系统中定义其名称(`obj_name`)和输出文件名(`file_name`)。

一旦我们运行了所有实验，我们就可以访问保存的模型，如下所示:

1.  在每个实验的**实验**标签下(例如随机森林)，我们选择**资产**和**工件**菜单项，如下图所示:

![Figure 3.23 – The models directory under the Assets and Artifacts menu items

](image/B17530_03_023.jpg)

图 3.23–资产和工件菜单项下的模型目录

该图显示了所有记录的对象，重点是`models`目录。如果我们愿意，可以下载模型。

1.  在屏幕的右边，有一个叫做**注册**的按钮。我们点击它，将模型添加到注册表中。将打开以下窗口:

![Figure 3.24 – The popup to register a new model

](image/B17530_03_024.jpg)

图 3.24–注册新模型的弹出窗口

因为它是我们项目的第一个模型，我们需要将它注册为一个新模型——例如，我们可以称它为`Diamonds classification`。如果我们已经注册了以前的模型，我们可以将新模型添加到现有的注册表中。

1.  我们可以为其他实验重复相同的过程，但是当我们需要将`experiment`模型添加到注册表中时，我们将它保存到现有的模型中。要添加它，我们需要增加模型版本——例如 1.0.1。在本例中，我们只保存精度大于 0.92 的实验。
2.  我们现在可以从主 Comet 仪表板访问注册表。我们需要退出当前的项目，如下图所示:

![Figure 3.25 – A view of the Model Registry

](image/B17530_03_025.jpg)

图 3.25–模型注册中心的视图

1.  点击**查看模型**按钮。我们有三种型号，与注册型号相对应。我们可以选择最佳模型用于生产(在我们的例子中为 1.0.0，对应于随机森林)，如下图所示:

![Figure 3.26 – How to set the stage as an experiment

](image/B17530_03_026.jpg)

图 3.26–如何将阶段设置为实验

1.  我们单击左边靠近版本的箭头，然后我们将阶段设置为**生产**。最后，我们点击**更新**按钮。如果您不记得哪一个是最佳实验，您可以单击实验键并检索原始实验。

既然你已经学会了如何在 Comet 中保存你的模型，我们可以进入下一步，一个报告。

## 报告

Comet 报告是一个包含实验、面板和文本的交互式文档。你已经在 [*第二章*](B17530_02_ePub.xhtml#_idTextAnchor043) ，*慧星*探索性数据分析中学习了慧星报表背后的基本概念。在本节中，您将学习如何为模型评估构建报告。该报告将包含以下面板:

*   精确度、召回率、F1 分数和精确度图表
*   ROC 曲线

让我们从第一个面板开始，精度、召回、F1 分数和精度图表。假设我们已经使用`step`参数记录了相关的指标，如前几节所述:

1.  首先，我们创建一个新的报告，如第二章[](B17530_02_ePub.xhtml#_idTextAnchor043)*，*慧星*中的探索性数据分析所述。我们把它的名字设为`Diamonds Report`。报告仪表板显示为空内容。*
**   然后，我们通过单击屏幕左上方的项目名称返回到主控制面板。*   我们访问**面板**部分，在这里我们可以看到一些记录的指标，如下图所示:*

*![Figure 3.27 – The Panels menu item in the Comet Dashboard

](image/image_27.jpg)

图 3.27–Comet 仪表板中的面板菜单项

该窗口包含四个折线图，分别是训练准确度、训练 F1 分数、训练精确度和训练回忆，都与该步骤相对应。

1.  我们可以通过点击**添加** | **新面板**来添加任意多的面板。在我们的例子中，我们添加了一个新的条形图，具有验证准确性。一旦**新面板**被选中，我们选择**条形图** | **添加** | **Y 轴** | **验证 _ 精度** | **完成**。
2.  现在，我们通过点击**保存视图**按钮| **创建新视图**来保存当前视图。我们将视图名称设置为`metrics`。
3.  最后，我们可以通过点击**Add**|**Add to Report**|**diamonds-Report**将面板添加到报告中。Comet 仪表板现在显示报告。我们可以点击**保存**按钮进行保存。

现在我们已经将基本面板添加到您的报告中，我们可以进入下一步，ROC 曲线。因为 Comet 没有为 ROC 曲线提供默认面板，所以我们的想法是构建一个自定义面板来显示它:

1.  首先，我们打开 Comet online SDK，选择 Python 语言。你已经在 [*第二章*](B17530_02_ePub.xhtml#_idTextAnchor043) ，【慧星】探索性数据分析中学会了如何构建自定义面板；因此，您可以参考它来获得关于如何访问 Comet online SDK 的更多细节。
2.  我们导入所有需要的库，包括`matplotlib`，我们将使用它来绘制一个图形:

    ```
    from comet_ml import API, ui
    import matplotlib.pyplot as plt
    ```

3.  我们检索所有的`experiment`键，如下:

    ```
    api = API()
    experiment_keys = api.get_panel_experiment_keys()
    ```

我们构建一个`API()`对象，并通过`get_panel_experiment_keys()`方法获得所有的实验密钥。

1.  对于每个实验的，我们提取记录的曲线，并绘制它们:

    ```
    colors = iter(['#40B7AD', '#A1CDB3','#508DED','#454372'])
    for experiment_key in experiment_keys:
        for curve in api.get_experiment_curves(experiment_key):
            curve_json = api.get_experiment_asset(experiment_key,curve['assetId'], return_type='json')
            plt.plot(curve_json['x'],curve_json['y'],color=next(colors), label=curve_json['name'])
    ```

我们使用`get_experiment_curve()`方法访问所有记录的曲线，使用`get_experiment_asset()`方法检索每条曲线作为 JSON，其中包含 TPR ( `x`)和 FPR ( `y`)。我们通过`matplotlib`提供的`plot()`函数绘制曲线。

1.  我们设置了剧情布局，如下:

    ```
    plt.plot([0, 1], [0, 1], color="navy", linestyle="--")
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.legend()
    ```

首先，我们绘制直线`y = x`，然后我们通过`xlim()`和`ylim()`函数设置轴的范围。最后，我们通过`xlabel()`和`ylabel()`定义轴标题，以及图例。

1.  我们使用中的`display()`函数在面板中显示图形:

    ```
    ui.display(plt)
    ```

2.  现在，我们将面板保存为`ROC curve`。下图显示了生成的图形:

![Figure 3.28 – The custom panel showing the ROC curve for all the experiments

](image/B17530_03_028.jpg)

图 3.28–显示所有实验的 ROC 曲线的自定义面板

1.  最后，我们准备好将定制的面板添加到我们的报告中。我们打开报告，然后我们点击**添加面板** | **工作区** | **ROC 曲线** | **添加** | **完成**。

下图显示了最终报告:

![Figure 3.29 – The final report

](image/image_29.jpg)

图 3.29–最终报告

报告是交互式的；因此，我们可以通过在**实验**选项卡中选择所涉及的实验来动态修改它。

# 总结

我们刚刚完成了在 Comet 中执行模型评估的旅程！

在本章中，我们描述了一些关于模型评估的一般概念，以及评估回归、分类和聚类的主要技术。我们还说明了模型评估在数据科学项目中的重要性；模型评估允许我们定义一些度量标准来为生产选择最佳模型。

在本章的第三部分，您了解了 Comet 提供了哪些特性来执行模型评估，以及如何通过一个实际的例子来使用它们。我们深化了日志和报告的概念，您已经知道了，并举例说明了两个新概念，Comet 仪表板和模型注册中心。

在本章中，您学习了使用 Comet 来运行模型评估是多么容易，因为 Comet 提供了非常直观的特性，可以组合这些特性来构建精彩的报告，以及如何跟踪生产中的最佳模型。

现在，您已经学习了如何在 Comet 中执行模型评估，我们可以继续探索 Comet 用于数据科学的旅程。在下一章，我们将学习一些关于 Comet 中的工作区、项目、实验和模型的高级概念。

# 延伸阅读

*   Bonaccorso，G. (2018)。*掌握机器学习算法:专家技术来实现流行的机器学习算法，并微调您的模型*。帕克特出版有限公司
*   Ng，A. (2017)。*机器学习向往*:【https://www.deeplearning.ai/machine-learning-yearning/】T2。要下载这本书，你需要到页面底部，插入你的电子邮件。*