

# *第九章*:用于自然语言处理的彗星

**自然语言处理** ( **NLP** )是人工智能的一个子领域，旨在让计算机能够理解文本和口语形式的人类自然语言。您可以使用 NLP 应用程序来构建虚拟助手，如 Alexa 或 Siri、情感分析器、文档翻译器、聊天机器人和文档分类器。在本章中，您将回顾 NLP 背后的主要概念，包括基本的 NLP 管道以及如何将文本转换为数据结构。

在过去的一年中，已经实现了不同的开源工具和库来执行 NLP，包括 spaCy、spaCy 和**自然语言工具包** ( **NLTK** )。在本章中，您将回顾 Spark NLP 库，并了解如何将其与 Comet 集成。我们将重点讨论如何对文本执行 NLP，尽管您也可以将 NLP 应用于音频文档。

训练一个好的 NLP 模型是非常耗时的，因为它通常需要大量的文本作为输入。出于这个原因，你可以在网上找到许多预先训练的模型，例如由非常受欢迎的网站 Hugging Face 提供的模型，该网站被大约 5000 个组织使用，包括谷歌、脸书、微软和亚马逊网络服务。在本章中，您将回顾预训练模型最受欢迎的中枢。

在本章的最后一部分，您将实现一个使用 Spark NLP 库的实际用例，并在 Comet 中跟踪结果。

本章组织如下:

*   介绍基本的自然语言处理概念
*   探索 Spark NLP 包
*   为 Spark NLP 设置环境
*   使用 NLP，从项目设置到报告构建

在开始回顾基本的 NLP 概念之前，让我们安装运行本章中描述的例子所需的软件。

# 技术要求

我们将使用 Python 3.8 运行本章中的所有实验和代码。可以从官网下载，[https://www.python.org/downloads/](https://www.python.org/downloads/)，选择 3.8 版本。

本章中描述的示例使用了以下 Python 包:

*   `comet-ml 3.23.0`
*   `pandas 1.3.4`
*   `scikit-learn 1.0`

我们已经在 [*第一章*](B17530_01_ePub.xhtml#_idTextAnchor015) 、*慧星概述*中描述了这些包以及如何安装它们，所以请参考后面的内容以了解更多关于安装的细节。

此外，运行示例将需要其他特定的需求，这些需求将在本章的*为 Spark NLP* 部分设置环境中描述。

现在您已经安装了本章中需要的所有软件，让我们看看如何使用 Comet 进行 NLP，从回顾一些基本概念开始。

# 介绍基本的自然语言处理概念

NLP 是人工智能的一个子领域，旨在分析和合成人类语言和语音。你可以将这些模型用于不同的目的，比如翻译、聊天机器人、垃圾邮件过滤器、语法纠正软件和语音助手。NLP 在过去几年变得非常流行，这要归功于大量文本的传播，这些文本可以被分析以构建非常特定于领域的工具。

该部分的组织结构如下:

*   探索 NLP 工作流
*   分类自然语言处理系统
*   探索 NLP 挑战
*   回顾最受欢迎的模型中心

让我们从第一步开始，探索 NLP 工作流程。

## 探索自然语言处理工作流程

下图显示了最简单的 NLP 工作流程:

![Figure 9.1 – The simplest NLP workflow

](image/B17530_09_001.jpg)

图 9.1–最简单的 NLP 工作流程

工作流程包括以下步骤:

*   **文本预处理**–这一步包括为进一步分析准备文本。通常，在这个阶段，文本被分成句子，停用词被删除。然后，句子被分割成记号，最后被规范化。
*   **句法分析**——这一步涉及句子解析和**词性** ( **PoS** )标注。
*   **语义分析**——这一步涉及**命名实体识别** ( **NER** )和概念识别。
*   **特征工程**——在这一步，你提取特征来建立一个机器学习模型或者基于规则的系统，它执行一个特定的 NLP 任务，比如文本分类或者翻译。
*   **模型开发**——这一步涉及模型的开发，可以是机器学习，也可以是基于规则的模型。通常，您可以使用机器学习模型来完成分类任务，而您可以使用基于规则的模型来从文本中提取一些模式。
*   **模型评估**–这是计算开发模型性能的最后一步。

下面的表显示了一个工作流程示例，包括句子分割、停用词移除、标记化、词性标注和 NER，适用于以下文本:

昨天我去了罗马。我参观了罗马圆形大剧场。

![Figure 9.2 – A part of the NLP workflow for the text “Yesterday I went to Rome. I visited the Colosseum."

](image/B17530_09_Table1.jpg)

图 9.2–文本“昨天我去了罗马”的 NLP 工作流的一部分。我参观了斗兽场。”

文本包含两个句子，由中间点(句号)分隔。停用词删除任务只是从每个句子中删除最后一个句号。标记化将每个句子拆分成标记，而词性标注识别每个标记的词性。最后，NER 承认两个实体:*罗马*和*斗兽场*。

既然我们已经回顾了基本的 NLP 工作流程，我们可以进入下一步，对 NLP 系统进行分类。

## 自然语言处理系统分类

一般来说，你可以将 NLP 系统分为两大类:

*   **功能库**–功能库是旨在实现某些特定 NLP 任务的功能集合。通常，这些函数是相互独立的，例如，如果您执行一个词性标注任务，relative 函数也会执行上游必须执行的操作，如句子拆分和标记化。这些库适合于研究目的，但是它们的性能不如注释库，因为它们不提供不同任务之间的任何组合。
*   **注释库**–由于文档注释模型的概念，注释库确保每个 NLP 任务与之前和之后的任务同步。因此，如果您必须执行词性标注任务，您首先必须单独执行为词性标注准备文本的所有任务。

在这一章中，我们将关注 Spark NLP，它是一个注释库。

现在我们已经回顾了 NLP 系统的基本分类，我们可以进入下一步，探索 NLP 的挑战。

## 探索自然语言处理的挑战

尽管 NLP 在最近几年有了巨大的发展，但由于大量文本的传播，许多挑战仍然存在，包括:

*   **语境词**–一个词可能有多个意思，这取决于它插入的语境。虽然人类可以很容易地识别特定的含义，但这个过程对于机器来说并不那么微不足道。
*   **同义词**–不同的单词可能有相同的意思。这个问题是对前一个问题的补充，但它对机器有相同的影响，机器可能会遇到将相同的意思与不同的单词关联起来的困难。
*   **讽刺和挖苦**——通常，一个词要么是肯定的，要么是否定的。然而，当你使用讽刺和挖苦时，你颠倒了一个词的原始极性；因此，一个否定词用来表达一个肯定的概念，反之亦然。对于一台机器来说，识别讽刺和挖苦可能非常困难，因为它将每个单词都与一个极性相关联。
*   **歧义**——这方面指的是一个句子有多种解释的可能性。歧义有不同的层次，包括词汇、语义和句法歧义。
*   **文本或语音中的错误**–可能会出现一个单词写得不正确，因为它包含一个打字错误或拼写错误。对于机器来说，识别这种错误可能很困难。
*   口语和俚语-识别口语中的口语和俚语对机器来说可能是有问题的，尤其是当要分析的文档以音频形式出现时。
*   特定领域语言(Domain-specific language)–一些预先训练的模型在通用文本上工作得很好，但它们需要适应特定的领域。在这种情况下，可能很难找到文本来训练模型。

既然我们已经回顾了 NLP 的一些流行挑战，我们可以进入下一步，概述最流行的 NLP 模型中心。

## 回顾最受欢迎的模特中心

在大量数据上训练 NLP模型既耗时又耗资源。出于这个原因，随着时间的推移，社区、公司和研究人员联合起来，开发出了可以随时使用的预训练模型。您可以直接使用这些模型，或者您可以将它们应用于特定的领域。有几个网站，或**模型中心**，收集预先训练好的模型，可以免费下载和使用。

上述最受欢迎的模型中枢包括:

*   模型动物园
*   TensorFlow Hub
*   拥抱脸

让我们简单描述一下每个 hub，从第一个开始，Model Zoo。

### 模型动物园

**模型动物园**是深度学习模型的模型中枢，可在以下链接获得:[https://modelzoo.co/](https://modelzoo.co/)。模型是按类别和框架组织的。类别包括计算机视觉、NLP、生成式模型、强化学习、无监督学习以及音频和语音。

框架包括 TensorFlow、Caffe、Caffe2、PyTorch 和 Keras。对于每个型号，都有详细的说明，显示如何安装和使用它。

### TensorFlow Hub

**TensorFlow Hub** 包含使用 TensorFlow 的预训练模型，与不同类别相关，包括文本和音频的 NLP，以及计算机视觉任务。对于每个模型，您可以找到关于如何使用它的描述和代码片段。

tensor flow hub可在以下链接获得:[https://tfhub.dev/](https://tfhub.dev/)。

### 拥抱脸

**抱抱脸**是一个 Python 库，也提供了一个模型和数据集 hub，可在以下链接获得:[https://huggingface.co/models](https://huggingface.co/models)。您可以找到用于 NLP、图像分类、强化学习等的预训练模型。

要使用特定的预训练模型，您需要安装拥抱面部库并从库中加载模型。

彗星与抱脸完全融合。关于如何使用 Comet with Hugging Face 的更多详细信息，可以阅读 Comet 官方文档，可从以下链接获得:[https://www . Comet . ml/docs/v2/guides/getting-started/tutorials/NLP-data/](https://www.comet.ml/docs/v2/guides/getting-started/tutorials/nlp-data/)。

现在我们已经回顾了一些流行的 NLP 模型中心，我们可以进入下一步，回顾 Spark NLP 包。

# 探索 Spark NLP 包

Spark NLP 是由 John Snow Labs 发布的 NLP 开源库。它支持不同的编程语言，包括 Python、Java 和 Scala。Spark NLP 广泛用于生产，因为它与 Apache Spark(一种用于大规模分析的多语言引擎)进行了本机集成。

Spark NLP 提供了 50 多种功能，包括标记化、NER 和情感分析。

在本节中，您将研究以下几个方面:

*   介绍 Spark NLP 包
*   将 Spark NLP 与 Comet 集成

先从第一点开始，介绍 Spark NLP 包。

## 介绍 Spark NLP 包

Spark NLP 是一个开放的源代码库，构建在 Apache Spark 和 Spark ML(一个在 Apache Spark 之上实现的机器学习库)之上。Spark NLP 库提供了几乎所有的 NLP 任务，包括标记化、词干化、词汇化、词性标注、情感分析、拼写检查和 NER。已实施任务的完整列表可在 Spark NLP 官方文档中找到，可通过以下链接获得:【https://nlp.johnsnowlabs.com/docs/en/quickstart】T2。

Spark NLP 定义了以下主要概念:

*   注解者
*   管道

让我们分别研究每个概念，从第一个概念开始，即注释器。

### 注解者

一个**注释者**要么是 Spark ML **估计者**要么是**转换者**。在 Spark NLP 中，估计器是一种算法，你可以在给定的数据帧上训练它产生一个模型，称为转换器。转换器是一种经过训练的算法，可以将一个输入数据帧转换成一个输出数据帧。输入和输出数据帧之间的区别在于，输入数据帧包含特征，而输出数据帧包含预测。下图显示了估计器和变压器之间的关系:

![Figure 9.3 – The relationship between estimators and transformers in Spark ML

](image/B17530_09_003.jpg)

图 9.3-Spark ML 中估计器和变压器的关系

Spark NLP 提供了两种类型的标注器:

*   **一个注释器方法**，它扩展了 Spark ML 估计器来实现`fit()`方法
*   **一个注释器模型**，它扩展了 Spark ML transformer 来实现`transform()`方法

Spark NLP 还提供了许多预训练的注释器模型，这些模型已经被其他人训练过了，可以使用了。你可以在 NLP 模型中心网站上查看所有经过预训练的模型的完整列表，可以通过以下链接获得:【https://nlp.johnsnowlabs.com/models。所有预训练的模型还提供了`pretrained()`方法，允许您选择要加载的特定模型，如下面的代码所示:

```
lemmatizer = LemmatizerModel.pretrained('lemma_antbnc', 'en')
```

前面的代码加载了预训练的`lemma_antbnc`模型。

所有的注释器，无论是注释器方法还是注释器模型，都实现以下两种方法:

*   `setInputCols(column_names)`:该注释器所需的输入列名列表
*   `setOutputCol(column_name)`:包含当前注释器结果的输出列的名称

既然我们已经讨论了注释器背后的基本概念，我们可以继续讨论由 Spark NLP 定义的下一个概念，管道。

### 管道

流水线是有序的级序列，每个级要么是转换器要么是估计器。每个阶段都通过添加新注释来更新数据帧，如下图所示:

![Figure 9.4 – A simple NLP pipeline

](image/B17530_09_004.jpg)

图 9.4–一个简单的 NLP 管道

上图实现了一个简单的管道，由以下三个阶段组成:

*   **文档组装器**–这个注释器是每个 Spark NLP 管道的初始阶段。它在数据帧上创建第一个注释，由管道中的下一个阶段读取。这个注释器的输出列被称为一个**文档**。
*   **句子检测器**–这个注释器将文档分割成句子，并向数据帧添加一个新的注释，称为**句子**。
*   **标记器**–这个注释器标记每个句子，并向数据帧添加一个新的注释，称为**标记**。

与预训练模型类似，Spark NLP 也提供了预训练的流水线，比如`BasicPipeline`，其中实现了句子拆分、标记化、词条化、词干化、和词性标注。

有关管道的更多信息，您可以阅读 Spark NLP 官方文档，可通过以下链接获得:[https://nlp.johnsnowlabs.com/docs/en/concepts](https://nlp.johnsnowlabs.com/docs/en/concepts)。

现在我们已经回顾了 Spark NLP 背后的基本概念，我们准备学习如何将 Spark NLP 与 Comet 集成。

## 将 Spark NLP 与 Comet 整合

您可以在 Comet 中直接监控您的 NLP 实验，无论是在训练过程中还是在实验结束时的。要将 Spark NLP 与 Comet 集成，您应该创建一个`CometLogger`对象，如下所示:

```
from sparknlp.logging.comet import CometLogger 
```

```
logger = CometLogger()
```

`CometLogger`类提供了以下主要方法:

*   `log_pipeline_parameters()`:在 Comet 中记录管道各阶段的参数
*   `log_visualization()`:将 Spark NLP Display 提供的 NER 可视化上传到 Comet
*   `log_metrics()`:在 Comet 中记录评估指标
*   `log_parameters()`:在 Comet 中记录多个参数
*   `log_completed_run()`:管道培训流程完成后，提交培训指标的结果
*   `log_asset()`:上传资产到 Comet
*   `monitor()`:监控模型的训练，并在管道训练过程运行时向 Comet 提交日志

有关方法的完整列表，您可以阅读文档，可从以下链接获得:[https://NLP . johnsnowlabs . com/API/python/modules/spark NLP/logging/comet . XHTML](https://nlp.johnsnowlabs.com/api/python/modules/sparknlp/logging/comet.xhtml)。

您可以通过`logger.experiment`字段访问 Comet 中定义的`experiment`类。

您只能对一些注释器方法对象使用`monitor()`和`log_completed_run()`方法，比如一个`MultiClassifierDLApproach`对象。当您将管道配置为使用受支持的注释器之一时，您应该启用日志，并设置保存日志的路径，如下面的代码所示:

```
from sparknlp.annotator import MultiClassifierDLApproach() 
```

```
PATH=/path/to/my/directory 
```

```
model = MultiClassifierDLApproach() 
```

```
      # add other configuration parameters 
```

```
.setEnableOutputLogs(True) \
```

```
      .setOutputLogsPath(PATH) 
```

我们使用了`setEnableOutputLogs(True)`方法来启用日志，使用`setOutputLogsPath()`方法来设置保存日志的路径。`CometLogger`将在训练过程中读取日志的输出，如下面这段代码所示:

```
logger.monitor(PATH, model)
```

或者，`CometLogger`可以在训练过程结束时读取日志:

```
logger.log_completed_run('/Path/To/Log/File.log')
```

下图显示了由`monitor()`方法产生的 Comet 输出示例:

![Figure 9.5 – An example of the output of the monitor() method

](image/B17530_09_005.jpg)

图 9.5–monitor()方法的输出示例

每张图显示了在训练阶段单个指标如何演变。

我们刚刚回顾了 Spark NLP 背后的主要概念，以及如何将其与 Comet 集成。现在，是时候练习了。让我们从设置使用 Spark NLP 的环境开始。

# 为 Spark NLP 设置环境

由于 Spark NLP 在 Apache Spark 上运行，你需要先设置 Apache Spark，让 Spark NLP 正常工作。Apache Spark 需要 Java 才能正常运行；因此，您还需要安装 Java。或者，您可以安装另一种编程语言 Scala。

要安装 Spark NLP，您应该安装以下框架和库:

*   计算机编程语言
*   Java 语言(一种计算机语言，尤用于创建网站)
*   Scala (optional)
*   阿帕奇火花
*   PySpark 和 Spark NLP

我们已经按照*技术需求*部分描述的步骤安装了 Python，所以我们可以从第二步开始安装软件，Java。

## 安装 Java

Spark NLP 构建在 Apache Spark 的顶层，可以安装在任何支持 Java 的操作系统上。Apache Spark 需要 **Java 8** 才能正常工作:

1.  要验证 Java 是否已经安装在您的计算机上，以及它的当前版本，您可以打开一个终端并运行以下命令:

    ```
    Java –version
    ```

如果已经安装了 Java，该命令应该返回类似于以下输出的内容:

```
openjdk version "1.8.0_322"
OpenJDK Runtime Environment (build 1.8.0_322-bre_2022_02_28_15_01-b00)
OpenJDK 64-Bit Server VM (build 25.322-b00, mixed mode)
```

version 关键字后的第二个数字表示当前的 Java 版本，在本例中是 8。

1.  如果没有安装 Java，可以从 Oracle 官方网站或者其他开源网站下载，比如 OpenJDK。在 Ubuntu 中，可以安装`openjdk-8`如下:

    ```
    sudo apt-get install openjdk-8-jre
    ```

在 macOS 中，您可以运行以下命令，前提是您已经安装了`brew`:

```
brew install openjdk@8
```

在所有情况下，包括 Windows，你都可以从以下链接下载 Java 8，[https://java.com/en/download/](https://java.com/en/download/)，然后按照向导进行操作。

1.  如果 Java 已经安装了，但是您没有版本 8，您可以按照*步骤 2* 中描述的过程安装它，然后您将`JAVA_HOME`环境变量设置为 Java 8 目录的路径。
2.  再次运行*步骤 1* 以确保 Java 8 安装正确。

一旦安装了 Java，就可以进入下一步，安装 Scala。

## 安装 Scala(可选)

Scala 是一种编程语言，用于数据处理、分布式计算和 T2 网络开发。你可以在 Scala 的官方网站上找到更多关于 Scala 的信息，链接如下:【https://www.scala-lang.org/。

Apache Spark 需要 Scala 2.12 或 2.13 才能正常工作:

1.  你可以按照以下链接描述的步骤安装 Scala 2 . 12 . 15:[https://www.scala-lang.org/download/2.12.15.xhtml](https://www.scala-lang.org/download/2.12.15.xhtml)。
2.  为了验证您已经正确安装了 Scala，您可以运行下面的命令:

    ```
    scala -version
    ```

该命令应该返回版本 2.12。

一旦安装了 Scala，就可以安装 Apache Spark 了。

## 安装 Apache Spark

你可以从官方网站下载 Apache Spark，在以下链接可以找到:[https://spark.apache.org/downloads.xhtml](https://spark.apache.org/downloads.xhtml)。

1.  对于本章中描述的示例，我们将使用 3.1.2 版，可通过以下链接[https://archive.apache.org/dist/spark/spark-3.1.2/](https://archive.apache.org/dist/spark/spark-3.1.2/)获得。
2.  一旦你下载了这个包，你可以把它解压并放在你的文件系统中任何你想要的地方。然后，您应该将包含在`spark`目录中的`bin`目录的路径添加到`PATH`环境变量中。在 Unix 系统中，您可以通过将下面一行代码添加到您的`.bashrc`或`.zprofile`(或其他类似的文件，取决于您的 shell)文件中来导出`PATH`变量:

    ```
    export PATH=$PATH:/path/to/spark/bin
    ```

您应该修改的确切文件名取决于您使用的特定 shell。

1.  您还需要导出带有 spark 目录路径的`SPARK_HOME`环境变量。在 Unix 系统中，您可以像下面这样导出`SPARK_HOME`变量:

    ```
    export SPARK_HOME="/path/to/spark"
    ```

您应该将前一行代码添加到您的`.bashrc`文件(或类似的文件)中。

注意

您可能需要退出并再次进入您的终端以使更改生效。

1.  要测试您是否正确安装了 Apache Spark，您可以运行下面的命令:

    ```
    spark-shell
    ```

您应该会看到类似下面的输出:

```
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ '/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.1.2
      /_/

Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 1.8.0_322)
Type in expressions to have them evaluated.
Type :help for more information.
scala>
```

1.  要退出 Apache Spark shell，可以使用 *Ctrl* + *C* 。

现在我们已经安装了 Apache Spark，我们可以进入最后一步，安装 PySpark 和 Spark NLP。

## 安装 PySpark 和 Spark NLP

要安装 PySpark，您可以运行下面的命令:

```
pip install pyspark
```

要安装 Spark NLP，您可以运行下面的命令:

```
pip install spark-nlp
```

可能发生的情况是，某些预训练的管道模型仅适用于特定版本的 Spark NLP。因此，您可以创建一个虚拟环境来包含特定版本的 Spark NLP。您可以按如下方式进行操作:

1.  首先，创建一个新的虚拟环境:

    ```
    python3 -m venv /path/to/your/virtual/environment
    ```

2.  然后，你激活它:

    ```
    cd /path/to/your/virtual/environment
    source bin/activate
    ```

3.  现在，你可以安装 Spark NLP 的具体版本，如下:

    ```
    pip install spark-nlp==x.y.z
    ```

注意

在新的虚拟环境中，您还应该安装运行软件所需的所有库。例如，如果您使用 Jupyter 创建您的代码，您应该在您的虚拟环境中安装并运行它。

现在我们已经安装了运行 Spark NLP 所需的所有软件，让我们继续看一个实际的例子。

# 使用 NLP，从项目设置到报告构建

在本节中，您将实现一个实际的例子，对从 Twitter 中提取的一些文本进行情感分析。这个例子比较了一个预训练模型和一个定制模型，并在 Comet 中显示了结果。

本节中描述的示例的完整代码可从以下链接获得:[https://github . com/packt publishing/Comet-for-Data-Science/tree/main/09](https://github.com/PacktPublishing/Comet-for-Data-Science/tree/main/09)。

我们将重点关注以下几个方面:

*   配置环境
*   加载数据集
*   实现预训练管道
*   Comet 中的日志记录结果
*   使用自定义管道
*   构建最终报告

让我们从第一点开始，配置环境。

## 配置环境

环境配置包括以下两个步骤:

1.  我们初始化 Spark NLP。我们导入并启动 Spark NLP 库如下:

    ```
    import sparknlp
    spark = sparknlp.start()
    ```

注意

如果您用 Jupyter 编写代码，前面的命令可能会失败，因为您的内核无法找到 Apache Spark 目录。要解决这个问题，在调用`sparknlp.start()`之前，使用`findspark` Python 库，如下所示:

`import findspark`

`findspark.init()`

1.  我们初始化彗星。我们导入并初始化 Comet 库，如下:

    ```
    import comet_ml
    from sparknlp.logging.comet import CometLogger
    comet_ml.init()
    logger = CometLogger()
    logger.experiment.set_name('PretrainedModel')
    ```

我们应该有一个`.comet.config`文件，它包含我们的 Comet 凭证，位于与工作目录相同的目录中，如 [*第 1 章*](B17530_01_ePub.xhtml#_idTextAnchor015) ，*Comet*概述中所述。

现在我们已经配置了环境，我们可以进入下一步，加载数据集。

## 加载数据集

作为一个用例，您将使用`Disneyland Reviews`数据集，可在 Kaggle 上通过以下链接获得，[https://www . ka ggle . com/datasets/arushchillar/disneyland-reviews](https://www.kaggle.com/datasets/arushchillar/disneyland-reviews)，并以 CC0: Public Domain license 发布。该数据集包含游客在 Tripadvisor 上发布的对迪士尼乐园三个分店(巴黎、加州和香港)的 42，656 条评论。对于每个评论，它还提供评级，范围从 1(完全不满意)到 5(满意)。我们将评级分为两类:如果评级大于 2，则为正面，否则为负面。为了简单起见，我们假设正面评级也对应于文本评论中的正面情绪，而负面评级也对应于文本评论中的负面情绪。

下表显示了数据集的前两行:

![Figure 9.6 – Extracts from the Disneyland Reviews dataset

](image/B17530_09_Table2.jpg)

图 9.6-迪士尼乐园评论数据集摘录

数据集有许多列；从它们中，我们将使用下面的:`Review_Text`，它包含文本，和`Rating`，它包含评级，我们将使用它作为一个情感。

我们执行以下步骤将数据集作为`pyspark`数据帧加载:

1.  首先，我们加载数据集:

    ```
    from pyspark.sql.functions import when, col
    df=spark.read.format("csv").option("header", "true").load("source/DisneylandReviews.csv")
    ```

我们使用 Spark NLP 库提供的`format()`函数来加载包，以及选项头，设置为`true`来读取 CSV 头。

1.  然后，我们将行分组如下:

    ```
     df = df.withColumn("sentiment", when(col("Rating") > 2, "positive").otherwise("negative"))
    ```

我们将`Rating`值大于`2`的行标记为正，将`Rating`值小于或等于`2`的行标记为负。

我们将使用的预训练模型需要前面的操作。

现在我们已经加载了数据集，我们可以继续下一步，实现预训练的管道。

## 实施预训练管道

我们将实现的第一个流水线使用 Spark NLP 提供的名为`ViveknSentimentModel`的预训练模型。你可以在下面的链接找到更多关于预训练模型以及如何使用它的信息:[https://NLP . johnsnowlabs . com/2021/11/22/情操 _vivekn_en.xhtml](https://nlp.johnsnowlabs.com/2021/11/22/sentiment_vivekn_en.xhtml) 。

1.  首先，我们导入所有需要的包:

    ```
    from sparknlp.base import *
    from sparknlp.annotator import *
    from pyspark.ml import Pipeline
    ```

2.  现在，我们用以下五个阶段实现 Spark 流水线:

    ```
    document = DocumentAssembler() \
        .setInputCol("Review_Text") \
        .setOutputCol("document")
    token = Tokenizer() \
        .setInputCols(["document"]) \
        .setOutputCol("token")

    normalizer = Normalizer() \
        .setInputCols(["token"]) \
        .setOutputCol("normal")

    vivekn =  ViveknSentimentModel.pretrained() \
        .setInputCols(["document", "normal"]) \
        .setOutputCol("result_sentiment") \

    finisher = Finisher() \
        .setInputCols(["result_sentiment"]) \
        .setOutputCols("final_sentiment")
    ```

流水线阶段包括文档汇编器、标记器、规格化器、`Vivekn`情感模型的预训练模型和完成器。

1.  我们创建管道:

    ```
    pipeline = Pipeline().setStages([document, token, normalizer, vivekn, finisher])
    ```

2.  现在，我们用文本:

    ```
    X = df.select('Review_Text').toDF('Review_Text')
    pipelineModel = pipeline.fit(X)
    ```

    来拟合管道

我们构建了一个名为`X`的辅助变量，它只包含文本。

1.  最后，我们计算结果:

    ```
    result = pipelineModel.transform(X)
    ```

现在我们已经训练了管道，我们准备在 Comet 中记录结果。

## 彗星中的记录结果

在 Comet 中，我们现在可以记录评估结果:

1.  为了执行这个操作，我们需要将管道产生的结果与原始的情感标签进行映射，如下面这段代码所示:

    ```
    df_tot = df.join(result, on=["Review_Text"])
    ```

我们构建了一个新的 spark 数据帧，称为`df_tot`，其结构如下:

![Figure 9.7 – The merged DataFrame, with both a true sentiment (sentiment) and a predicted sentiment (final_sentiment)

](image/B17530_09_Table3.1.jpg)![Figure 9.7 – The merged DataFrame, with both a true sentiment (sentiment) and a predicted sentiment (final_sentiment)

](image/B17530_09_Table3.2.jpg)

图 9.7–合并的数据框架，包含真实情绪(情绪)和预测情绪(最终情绪)

1.  为了在 Comet 中记录结果，我们需要将 Spark 数据帧转换成 pandas 数据帧，如下面的代码所示:

    ```
    pandas_df = df_tot.toPandas()
    ```

2.  我们需要将包含在`final_sentiment`列中的列表转换成字符串:

    ```
    pandas_df['predicted_sentiment'] = [','.join(map(str, l)) for l in pandas_df['final_sentiment']]
    ```

我们构建了一个新的列，名为`predicted_sentiment`，它存储了`final_sentiment`的清理版本。我们将使用`predicted_sentiment`进行进一步分析。

1.  我们使用 scikit-learn 提供的`classification_report()`函数来计算精确度、召回率和 F1 分数:

    ```
    from sklearn.metrics import classification_report
    report = classification_report(pandas_df['sentiment'], pandas_df['predicted_sentiment'], output_dict=True, labels=['positive', 'negative'])
    ```

2.  我们在 Comet:

    ```
     for key, value in report.items():
        if key!='accuracy':
            logger.log_metrics(value,prefix=key)
        else:
            logger.log_metrics({"accuracy": value})
    ```

    中记录了的结果

运行实验后，您应该在 Comet 中看到结果，如下图所示:

![Figure 9.8 – The evaluation metrics of the pretrained model in Comet

](image/B17530_09_008.jpg)

图 9.8–Comet 中预训练模型的评估指标

现在我们已经在 Comet 中记录了第一个管道的结果，我们可以继续第二个管道了。

## 使用定制管道

定制管道基于`ViveknSentimentApproach`类训练一个定制模型。除了模型之外，管道包含与前面的示例相同的阶段:

```
vivekn_custom = ViveknSentimentApproach() \
```

```
    .setInputCols(["document", "normal"]) \
```

```
    .setSentimentCol("sentiment") \
```

```
    .setOutputCol("result_sentiment")
```

在前一个示例中，模型已经训练好了，这一次您需要训练它。

要执行培训，您可以执行以下步骤:

1.  首先，我们将数据集分成训练集和测试集，如下面这段代码所述:

    ```
    (training_set, test_set) = df.randomSplit([0.8, 0.2])
    X_train = training_set.select('Review_Text', 'sentiment').toDF('Review_Text', 'sentiment')
    X_test = test_set.select('Review_Text', 'sentiment').toDF('Review_Tex', 'sentiment')
    ```

我们为训练集保留了 80%的样本，为测试集保留了剩余的 20%。

1.  然后，我们在训练集上训练管道:

    ```
    pipelineModel = pipeline.fit(X_train)
    ```

2.  最后，我们计算测试集上的预测:

    ```
    result = pipelineModel.transform(X_test)
    ```

一旦您训练了模型并计算了测试集的预测，您就可以在 Comet 中记录结果，就像您在前面的例子中所做的那样。由于`ViveknSentimentApproach`没有实现`setOutputLogsPath()`方法，我们无法在 Comet 中监控训练过程。

既然我们已经构建了定制模型并评估了它的性能，我们可以继续最后一步，构建最终报告。

## 建筑最终报告

现在，您已经准备好构建最终报告了。在本例中，我们将构建一个简单的报告，对两个模型进行比较。作为进一步的练习，你可以通过应用在 [*第五章*](B17530_05_ePub.xhtml#_idTextAnchor105) ，*在彗星*中建立一个叙事来改进它们。您将创建一个报告，其中包含两个模型中计算的性能指标之间的比较。

我们执行以下两个步骤:

*   构建面板
*   构建报告

让我们从第一步开始，构建面板。

### 构建面板

我们实现了以下面板:

*   一个**条形图**，用于宏观层面的性能指标，另一个用于微观层面的性能指标
*   一个**平行坐标图**用于比较不同的绩效指标，针对每个目标类别进行计算(正面和负面情绪)
*   一个**项目总结**，它以表格的形式包含了两个实验的所有性能指标

要在宏观级别构建度量的第一个条形图，可以遵循以下步骤:

1.  在 Comet 项目主仪表盘中，选择**面板**选项卡，然后**添加** | **新面板** | **内置** | **条形图** | **添加**。
2.  在 **Y 轴**输入框中，选择精度。
3.  点击**添加字段**按钮，选择**宏 avg_f1_score** 。
4.  重复前两步，添加**宏平均值 _ 精度**和**宏平均值 _ 召回**。
5.  点击**完成**按钮。您应该会看到类似下图所示的图表:

![Figure 9.9 – The bar chart panel for the macro performance metrics

](image/B17530_09_009.jpg)

图 9.9–宏观性能指标的条形图面板

要构建平行坐标图，您可以执行以下步骤:

1.  从慧星项目主仪表盘中，选择**面板**选项卡，然后**添加** | **新面板** | **内置** | **平行坐标图** | **添加**。
2.  在**目标变量**输入框中，选择精度。
3.  在 **Y 轴**输入框中，您可以添加任意数量的指标。在我们的示例中，我们添加了以下度量:**正 _ 精度**、**正 _ 召回**、**负 _ 精度**和**负 _ 召回**。
4.  点击**完成**按钮。您应该会看到类似下图所示的图表:

![Figure 9.10 – The parallel coordinates chart

](image/B17530_09_010.jpg)

图 9.10-平行坐标图

1.  要构建**项目摘要**面板，选择**面板**选项卡，然后**添加** | **新面板** | **特色** | **项目摘要** | **添加** | **完成**。生成的面板应该类似于下图所示的面板:

![Figure 9.11 – The Project Summary panel

](image/B17530_09_011.jpg)

图 9.11–项目总结面板

既然我们已经构建了面板，我们可以将它们添加到报告中。因此，我们可以进入最后一步，构建报告。

### 构建报告

要创建报告，在 Comet 仪表板中，执行以下步骤:

1.  点击**面板的**选项卡，然后选择**添加** | **添加到报告** | **新建报告**。
2.  将出现一个新报告，其中包含以前创建的面板。
3.  通过选择**编辑布局**按钮，然后用鼠标调整每个图表的布局，可以自定义面板中每个图表的大小。
4.  您可以使用标题、描述等来自定义您的报告。
5.  最后，点击**保存**按钮。

下图显示了部分最终报告的快照:

![Figure 9.12 – A snapshot of the final report

](image/B17530_09_012.jpg)

图 9.12–最终报告的快照

你可以在[https://www . comet . ml/packt/spark-NLP/reports/analysis-of-dysneyland-sensation-using-two-models](https://www.comet.ml/packt/spark-nlp/reports/analysis-of-dysneyland-sentiment-using-two-models)看到的完整报告，而 Comet 中的完整项目可在以下链接获得:[https://www.comet.ml/packt/spark-nlp/](https://www.comet.ml/packt/spark-nlp/)。

# 总结

我们刚刚完成了在 Spark NLP 中构建 NLP 模型并在 Comet 中跟踪它的旅程！

在本章中，我们描述了一些关于 NLP 的一般概念，包括基本的 NLP 工作流程，如何对 NLP 工具进行分类，以及主要的 NLP 挑战。此外，您已经看到了 Spark NLP 包的主要结构，以及如何设置环境使其工作。我们还举例说明了一些重要的概念，比如注释器和管道。

在本章的最后一部分，您实现了一个实际的用例，向您展示了如何在 Comet 中跟踪一个 NLP 实验，以及如何用实验结果构建一个报告。

在下一章中，我们将回顾与深度学习相关的基本概念以及如何在 Comet 中执行它。

# 延伸阅读

*   a .凯迪亚和 m .拉苏(2020 年)。*Python 自然语言处理实践:探索分析和处理文本的工具和技术，以期构建真实世界的自然语言处理应用*。帕克特出版有限公司
*   托马斯，A. (2020)。使用 Spark NLP 的自然语言处理:学习大规模理解文本。奥赖利媒体公司。