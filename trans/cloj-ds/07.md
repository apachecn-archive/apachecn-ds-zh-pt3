

# 七、推荐系统

|   | 喜欢这种东西的人会发现这正是他们喜欢的东西 |   |
|   | - *归功于亚伯拉罕·林肯* |

在前一章中，我们使用 k-means 算法对文本文档进行了聚类。这要求我们对要聚类的文本文档之间的相似性进行度量。在这一章中，我们将研究推荐系统，我们将使用相似性的概念来推荐我们认为用户可能喜欢的项目。

我们还看到了高维数据带来的挑战——所谓的**维数灾难**。虽然这不是推荐系统特有的问题，但本章将展示各种解决其影响的技术。特别是，我们将研究用主成分分析和奇异值分解建立最重要维度的方法，以及用 Bloom filters 和 MinHash 压缩非常高维集合的概率方法。此外——因为确定项目之间的相似性需要进行许多成对比较——我们将学习如何使用区分位置的散列法高效地预计算具有最可能相似性的组。

最后，我们将介绍 Spark，一个分布式计算框架，以及一个名为 Sparkling 的相关 Clojure 库。我们将展示如何使用 Spark 的机器学习库 MLlib 来构建分布式推荐系统。

但是首先，我们将在本章开始讨论推荐系统的基本类型，并在 Clojure 中实现一个最简单的系统。然后，我们将演示如何使用上一章介绍的 Mahout 来创建各种不同类型的推荐器。

# 下载代码和数据

在这一章中，我们将利用来自 https://movielens.org/[网站的电影推荐数据](https://movielens.org/)。该网站由位于双城的明尼苏达大学计算机科学与工程系的研究实验室 GroupLens 运营。

数据集在 https://grouplens.org/datasets/movielens/已经有了几种不同的大小。在这一章中，我们将使用“MovieLens 100k”，这是 1，000 名用户对 1，700 部电影的 100，000 个评级的集合。随着数据于 1998 年发布，它开始显示其年龄，但它提供了一个适度的数据集，我们可以在其上展示推荐系统的原理。本章将为您提供处理最近发布的“MovieLens 20M”数据所需的工具:138，000 名用户对 27，000 部电影的 2000 万次评分。

### 注意

本章的代码可从 Packt Publishing 的网站或[https://github . com/clojuredadascience/ch7-recommender-systems](https://github.com/clojuredatascience/ch7-recommender-systems)获得。

像往常一样，提供了一个 shell 脚本，将数据下载并解压缩到本章的`data`目录中。您可以在同一个代码目录中使用以下命令运行它:

```

script/download-data.sh

```

一旦您运行了这个脚本，或者手动下载了一个解包的数据，您应该会看到各种以字母“u”开头的文件。我们在本章中最常用的评级数据在`ua.base`文件中。`ua.base`、`ua.test`、`ub.base`和`ub.test`文件包含执行交叉验证的数据子集。我们还将使用`u.item`文件，它包含电影本身的信息。



# 检查数据

评级文件是制表符分隔的，包含字段的用户 ID、项目 ID、评级和时间戳。用户 ID 链接到`u.user`文件中的一行，该行提供基本的人口统计信息，如年龄、性别和职业:

```
(defn ex-7-1 []

  (->> (io/resource "ua.base")

       (io/reader)

       (line-seq)

       (first)))

;; "1\t1\t5\t874965758"
```

该字符串显示了文件中的一行——一个由制表符分隔的行，包含用户 ID、商品 ID、评级(1-5)和时间戳(显示评级时间)。评级是从 1 到 5 的整数，时间戳是从 1970 年 1 月 1 日开始的秒数。项目 ID 链接到`u.item`文件中的一行。

我们还希望加载`u.item`文件，这样我们就可以确定被评级的项目的名称(以及作为回报被预测的项目)。以下示例显示了数据是如何存储在`u.item`文件中的:

```
(defn ex-7-2 []

  (->> (io/resource "u.item")

       (io/reader)

       (line-seq)

       (first)))

;; "1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0"
```

前两个字段分别是项目 ID 和名称。本章中没有用到的后续字段是上映日期、电影在 IMDB 上的 URL 以及一系列指示电影类型的标志。



# 解析数据

为了方便起见，数据将全部放入主存中，我们将定义几个函数将评级加载到 Clojure 数据结构中。`line->rating`函数获取一行，将它分割成可以找到制表符的字段，将每个字段转换成`long`数据类型，然后使用`zipmap`将序列转换成带有提供的键的映射:

```
(defn to-long [s]

  (Long/parseLong s))

(defn line->rating [line]

  (->> (s/split line #"\t")

       (map to-long)

       (zipmap [:user :item :rating])))

(defn load-ratings [file]

  (with-open [rdr (io/reader (io/resource file))]

    (->> (line-seq rdr)

         (map line->rating)

         (into []))))

(defn ex-7-3 []

  (->> (load-ratings "ua.base")

       (first)))

;; {:rating 5, :item 1, :user 1}
```

让我们编写一个函数来解析`u.items`文件，这样我们就知道电影的名字了:

```
(defn line->item-tuple [line]

  (let [[id name] (s/split line #"\|")]

    (vector (to-long id) name)))

(defn load-items [path]

  (with-open [rdr (io/reader (io/resource path))]

    (->> (line-seq rdr)

         (map line->item-tuple)

         (into {}))))
```

`load-items`函数返回一个项目 ID 到电影名称的映射，这样我们就可以通过电影的 ID 来查找电影的名称。

```
(defn ex-7-4 []

  (-> (load-items "u.item")

      (get 1)))

;; "Toy Story (1995)"
```

有了这些简单的功能，是时候了解不同类型的推荐系统了。



# 推荐系统的类型

对于推荐问题，通常有两种方法。两者都利用了事物之间相似性的概念，就像我们在前一章中遇到的那样。

一种方法是从一个我们知道用户喜欢的项目开始，然后推荐其他具有相似属性的项目。例如，如果用户对动作冒险电影感兴趣，我们可能会向他们展示我们可以提供的所有动作冒险电影的列表。或者，如果我们有更多的可用数据，而不仅仅是类型——也许是标签列表——那么我们可以推荐标签最多的电影。这种方法被称为基于内容的 T2 过滤，因为我们使用商品本身的属性来为相似的商品生成推荐。

另一种推荐方法是将用户偏好的某种度量作为输入。这可以是电影的数字分级的形式，或者是购买或先前观看的电影的数字分级的形式。一旦我们有了这些数据，我们就可以识别具有相似评级(或购买历史、观看习惯等)的其他用户偏好的电影，而所述用户还没有声明偏好。这种方法考虑到了其他用户的行为，所以它通常被称为**协同过滤**。协同过滤是一种强大的推荐手段，因为它利用了所谓的“群体智慧”。

在这一章中，我们将主要研究协同过滤方法。然而，通过利用相似性的概念，我们将为您提供实现基于内容的推荐所需的概念。

## 协同过滤

通过只考虑用户与项目的关系，这些技术不需要了解项目本身的属性。这使得协同过滤成为一种非常通用的技术——被讨论的项目可以是任何可以被分级的东西。我们可以将协同过滤描述为试图填充一个稀疏矩阵的行为，该矩阵包含用户的已知评级。我们希望能够用预测的评级代替未知的评级，然后推荐得分最高的预测。

![Collaborative filtering](graphics/7180OS_07_090.jpg)

请注意，每个问号都位于行和列的交叉处。这些行包含特定用户对所有他们已评级的电影的偏好。这些列包含对特定电影进行评级的所有用户对该电影的评级。仅使用矩阵中的其他数字替换矩阵中的问号是协同过滤的核心挑战。



# 基于项目和基于用户的推荐器

在协同过滤领域，我们可以有效地区分两种类型的过滤——基于项目的推荐和基于用户的推荐。使用基于项目的推荐器，我们获取一组用户已经评价很高的项目，并寻找其他类似的项目。下图显示了这一过程:

![Item-based and user-based recommenders](graphics/7180OS_07_100.jpg)

推荐者可能会基于图表中呈现的信息推荐项目 **B** ，因为它与已经被高度评价的两个项目相似。

我们可以将这种方法与下图所示的基于用户的推荐过程进行对比。基于用户的推荐旨在识别与所讨论的用户具有相似品味的用户，以推荐他们已经评级很高但用户还没有评级的项目。

![Item-based and user-based recommenders](graphics/7180OS_07_110.jpg)

基于用户的推荐者很可能推荐项目 **B** ，因为它已经被另外两个具有相似品味的用户评价很高。在本章中，我们将实现两种类型的推荐器。让我们从一个最简单的方法开始——基于项目推荐的**斜率一** 预测器。



# 斜坡一号推荐人

Slope One 推荐器是 Daniel Lemire 和 Anna Maclachlan 在 2005 年的一篇论文中介绍的算法家族的一部分。在本章中，我们将介绍加权斜率一推荐器。

### 注意

你可以在[http://lemire.me/fr/abstracts/SDM2005.html](http://lemire.me/fr/abstracts/SDM2005.html)阅读介绍斜坡一号推荐人的论文。

为了说明加权斜率一推荐如何工作，让我们考虑四个用户的简单例子，标记为 **W** 、 **X** 、 **Y** 和 **Z** ，他们对三部电影——Amadeus、Braveheart 和 Casablanca 进行了评级。每个用户提供的评分如下图所示:

![Slope One recommenders](graphics/7180OS_07_112.jpg)

与任何推荐问题一样，我们希望用一些关于用户如何评价电影的估计来取代问号:最高的预测评级可以用来向用户推荐新电影。

加权斜率一是一个分两步的算法。首先，我们必须计算每对项目的评分之间的差异。其次，我们将使用这组差异来进行预测。

## 计算项目差异

Slope One 算法的第一步是计算每对项目之间的平均差异。下面的等式可能看起来吓人，但实际上很简单:

![Calculating the item differences](graphics/7180OS_07_01.jpg)

该公式计算出![Calculating the item differences](graphics/7180OS_07_02.jpg)，这是项目 *i* 和 *j* 的评分之间的平均差值。它通过对取自*S*I， [j] *(R)* 的所有 *u* 进行求和来做到这一点，这是对两个项目都进行了评价的所有用户的集合。求和的数量是*u*[I]*-u*[j]，用户对项目 *i* 和 *j* 的评分差除以![Calculating the item differences](graphics/7180OS_07_05.jpg)，集合 *S* [i] ， [j] *(R)* 的基数，或者已评分的人数

让我们通过将该算法应用于上图中的评级来使这一点更加具体。让我们计算一下《阿玛迪斯》和《勇敢的心》的收视率差距。

有两个用户对两部电影都进行了评级，所以![Calculating the item differences](graphics/7180OS_07_05.jpg)是两个。对于这些用户中的每一个，我们取他们对两部电影的评分之差，并将它们加在一起。

![Calculating the item differences](graphics/7180OS_07_06.jpg)

结果是 *2* ，这意味着平均而言，用户投票给 Amadeus 的评分比 Braveheart 高两个等级。如你所料，如果我们计算另一个方向上的差异，在勇敢的心和阿玛迪斯之间，我们得到 *-2* :

![Calculating the item differences](graphics/7180OS_07_07.jpg)

我们可以把这个结果看作是两部电影的平均评分差，由所有给两部电影评分的人来判断。如果我们再进行几次计算，我们可能会得到下图中的矩阵，它显示了三部电影中每一部电影的评分的平均成对差异:

![Calculating the item differences](graphics/7180OS_07_114.jpg)

根据定义，主对角线上的值为零。我们可以在下面的 Clojure 代码中表达计算结果，而不是手动继续计算，这将在每个用户已评分的项目对之间建立一系列差异:

```
(defn conj-item-difference [dict [i j]]

  (let [difference (-  (:rating j) (:rating i))]

    (update-in dict [(:item i) (:item j)] conj difference)))

(defn collect-item-differences [dict items]

  (reduce conj-item-difference dict

          (for [i items

                j items

                :when (not= i j)]

            [i j])))

(defn item-differences [user-ratings]

  (reduce collect-item-differences {} user-ratings))
```

以下示例使用我们在本章开始时定义的函数将`ua.base`文件加载到评级序列中。`collect-item-differences`函数获取每个用户的评分列表，并为每对评分项目计算评分之间的差异。`item-differences`函数对所有用户进行缩减，从而为对两个项目都进行了评价的所有用户建立项目之间的成对差异序列:

```
(defn ex-7-5 []

  (->> (load-ratings "ua.base")

       (group-by :user)

       (vals)

       (item-differences)

       (first)))

;; [893 {558 (-2 4), 453 (-1), 637 (-1), 343 (-2 -2 3 2) ...]
```

我们将两个方向的列表存储为嵌套映射中包含的值，因此我们可以使用`get-in`检索任意两个项目之间的差异:

```
(defn ex-7-6 []

  (let [diffs (->> (load-ratings "ua.base")

                   (group-by :user)

                   (vals)

                   (item-differences))]

    (println "893:343" (get-in diffs [893 343]))

    (println "343:893" (get-in diffs [343 893]))))

;; 893:343 (-2 -2 3 2)

;; 343:893 (2 2 -3 -2)
```

要使用这些差异进行预测，我们需要将它们总结成平均值，并跟踪平均值所基于的评分数:

```
(defn summarize-item-differences [related-items]

  (let [f (fn [differences]

            {:mean  (s/mean differences)

             :count (count  differences)})]

    (map-vals f related-items)))

(defn slope-one-recommender [ratings]

  (->> (item-differences ratings)

       (map-vals summarize-item-differences)))

(defn ex-7-7 []

  (let [recommender (->> (load-ratings "ua.base")

                         (group-by :user)

                         (vals)

                         (slope-one-recommender))]

    (get-in recommender [893 343])))

;; {:mean 0.25, :count 4}
```

这种方法的一个实际好处是，我们只需执行一次前面的步骤。从这一点开始，我们可以通过调整平均差异来合并未来的用户评级，并仅对用户已经评级的项目进行计数。例如，如果用户已经对 10 个项目进行了评级，这些项目已经被合并到先前的数据结构中，第 11 个评级只要求我们重新计算 11 个项目的差异。没有必要从零开始执行计算上昂贵的差分过程来合并新信息。

## 提出建议

既然我们已经计算了每一对商品的平均差异，我们就有了向用户推荐新商品所需的一切。为了说明这一点，让我们回到前面的一个例子。

用户 **X** 已经为**阿玛迪斯**和**勇敢的心**提供了评分。我们想推断他们会如何评价电影《卡萨布兰卡》,这样我们就可以决定是否向他们推荐这部电影。

![Making recommendations](graphics/7180OS_07_116.jpg)

为了对用户进行预测，我们需要两样东西——我们刚才计算的差异矩阵和用户自己以前的评分。给定这两件事，我们可以使用以下公式计算给定用户 *u* 的项目 *j* 的预测评级![Making recommendations](graphics/7180OS_07_08.jpg):

![Making recommendations](graphics/7180OS_07_09.jpg)![Making recommendations](graphics/7180OS_07_10.jpg)

和以前一样，这个方程看起来比实际更复杂，所以让我们从分子开始，一步一步来。

![Making recommendations](graphics/7180OS_07_11.jpg)表达式意味着我们对用户 *u* 已经评分的所有 *i* 项目进行求和(这显然不包括 *j* ，我们试图预测评分的项目)。我们计算的总和超过了用户对 *i* 和 *j* 的评分与用户对 *i* 的评分之差。我们将这个数量乘以*C*j，[I]——对两者都进行了评价的用户数量。

![Making recommendations](graphics/7180OS_07_13.jpg)分母就是所有评价过 *j* 的用户和用户 *u* 评价过的任何一部电影的总和。向下调整分子的大小是一个常量因素，以确保输出可以被解释为评级。

让我们通过使用差异表和先前提供的评级计算用户 X 对“卡萨布兰卡”的预测评级来说明先前的公式:

![Making recommendations](graphics/7180OS_07_14.jpg)![Making recommendations](graphics/7180OS_07_15.jpg)

因此，根据之前的评分，我们可以预测用户 X 会给卡萨布兰卡**评分 3.375** 。通过对同样由对用户 X 评价的任何其他项目进行评价的人所评价的所有项目执行相同的过程，我们可以为用户 X 得到一组推荐。

Clojure 代码计算所有此类候选人的加权评分:

```
(defn candidates [recommender {:keys [rating item]}]

  (->> (get recommender item)

       (map (fn [[id {:keys [mean count]}]]

              {:item id

               :rating (+ rating mean)

               :count count}))))

(defn weighted-rating [[id candidates]]

  (let [ratings-count (reduce + (map :count candidates))

        sum-rating (map #(* (:rating %) (:count %)) candidates)

        weighted-rating (/ (reduce + sum-rating) ratings-count)]

    {:item id

     :rating weighted-rating

     :count  ratings-count}))
```

接下来，我们计算加权评分，这是每个候选人的加权平均评分。加权平均值确保大量用户产生的差异比少量用户产生的差异更重要:

```
(defn slope-one-recommend [recommender rated top-n]

  (let [already-rated  (set (map :item rated))

        already-rated? (fn [{:keys [id]}]

                         (contains? already-rated id))

        recommendations (->> (mapcat #(candidates recommender %)

                                     rated)

                             (group-by :item)

                             (map weighted-rating)

                             (remove already-rated?)

                             (sort-by :rating >))]

    (take top-n recommendations)))
```

最后，我们从候选项池中删除任何我们已经评级的项目，并通过评级降序排列剩余的项目:我们可以只取最高评级的结果，并将其作为我们的首选推荐。以下示例计算用户 ID 1 的最高评分:

```
(defn ex-7-8 []

  (let [user-ratings (->> (load-ratings "ua.base")

                          (group-by :user)

                          (vals))

        user-1       (first user-ratings)

        recommender  (->> (rest user-ratings)

                          (slope-one-recommender))

        items     (load-items "u.item")

        item-name (fn [item]

                    (get items (:item item)))]

    (->> (slope-one-recommend recommender user-1 10)

         (map item-name))))
```

前面的示例需要一段时间来构建 Slope One 推荐器并输出差异。这将需要几分钟的时间，但完成后，您应该会看到如下内容:

```
;; ("Someone Else's America (1995)" "Aiqing wansui (1994)"

;;  "Great Day in Harlem, A (1994)" "Pather Panchali (1955)"

;;  "Boys, Les (1997)" "Saint of Fort Washington, The (1993)"

;;  "Marlene Dietrich: Shadow and Light (1996) " "Anna (1996)"

;;  "Star Kid (1997)" "Santa with Muscles (1996)")
```

尝试在 REPL 中运行`slope-one-recommender`并为多个用户预测推荐。你会发现，一旦建立了差异，推荐就变得非常快。

## 用户和商品推荐者的实际考虑

正如我们在上一节中看到的，为所有项目编译成对差异是一项耗时的工作。基于项目的推荐器的优点之一是项目之间的成对差异可能随着时间的推移保持相对稳定。差异矩阵只需要定期计算。正如我们已经看到的，增量更新也是非常容易的；对于已经评价了 10 个项目的用户，如果他们评价了一个额外的项目，我们只需要调整他们现在已经评价的 11 个项目的差异。每当我们想要更新矩阵时，我们不需要从头开始计算差异。

基于项目的推荐器的运行时间与它们存储的项目数量成比例。在用户数量与项目数量相比较少的情况下，实现基于用户的推荐器可能更有效。例如，内容聚合网站的条目数量可能比用户数量多几个数量级，是基于用户推荐的良好候选。

我们在上一章中遇到的`Mahout`库包含了创建各种推荐器的工具，包括基于用户的推荐器。接下来我们来看看这些。



# 用 Mahout 构建基于用户的推荐系统

Mahout 库附带了许多内置的类，它们被设计用来协同工作以帮助构建定制的推荐引擎。Mahout 构造推荐器的功能在`org.apache.mahout.cf.taste`名称空间中。

### 注意

Mahout 的推荐引擎功能来自于它在 2008 年合并的 Taste 开源项目。

在前一章中，我们发现了如何利用 Mahout 与 Clojure 的 Java 互操作功能进行集群。在这一章中，我们将使用 Mahout 的推荐器，在`org.apache.mahout.cf.taste.impl.recommender`包中有`GenericUserBasedRecommender`。

与许多基于用户的推荐器一样，我们也需要定义一个相似性度量来量化两个用户有多相似。我们还将把用户邻域定义为每个用户的 10 个最相似用户的集合。

首先，我们必须加载数据。Mahout 包含一个实用程序类`FileDataModel`，用于将 MovieLens 数据加载到`org.apache.mahout.cf.taste.impl.model.file`包中，我们接下来将使用这个包:

```
 (defn load-model [path]

  (-> (io/resource path)

      (io/file)

      (FileDataModel.)))
```

加载完数据后，我们可以使用以下代码生成建议:

```
(defn ex-7-9 []

  (let [model        (load-model "ua.base")

        similarity   (EuclideanDistanceSimilarity. model)

        neighborhood (NearestNUserNeighborhood. 10 similarity

                                                model)

        recommender  (GenericUserBasedRecommender. model

                                                   neighborhood

                                                   similarity)

        items     (load-items "u.item")

        item-name (fn [id] (get items id))]

    (->> (.recommend recommender 1 5)

         (map #(item-name (.getItemID %))))))

;; ("Big Lebowski, The (1998)" "Peacemaker, The (1997)"

;;  "Rainmaker, The (1997)" "Game, The (1997)"

;;  "Cool Hand Luke (1967)")
```

我们在前面的例子中使用的距离度量是欧几里德距离。这将每个用户置于由他们对电影的评级所定义的高维空间中。

![Building a user-based recommender with Mahout](graphics/7180OS_07_120.jpg)

先前的图表根据三个用户对电影 **A** 和 **B** 的评级，将他们 **X** 、 **Y** 和 **Z** 放置在二维图表上。我们可以看到用户 **Y** 和 **Z** 基于这两部电影，彼此之间的相似度要高于对用户 **X** 的相似度。

如果我们试图为用户 **Y** 产生推荐，我们可能会推理出其他被用户 **X** 评价很高的项目也是不错的选择。



# k-最近邻

我们的 Mahout 基于用户的推荐器通过查看最相似用户的邻居来进行推荐。这就是俗称的***k*-最近邻**或 ***k* -NN** 。

用户邻域可能看起来很像我们在上一章中遇到的 *k* 均值聚类，但事实并非如此。这是因为每个用户都位于他们自己社区的中心。通过聚类，我们的目标是建立更少数量的分组，但是通过 *k* -NN，有多少用户就有多少邻居；每个用户都是自己的邻域质心。

### 注意

Mahout 还定义了`ThresholdUserNeighbourhood`,我们可以用它来构建一个仅包含彼此具有一定相似性的用户的邻域。

*k* -NN 算法意味着我们只根据 *k* 最相似用户的口味生成推荐。这有直观的意义；与你品味最相似的用户最有可能提供有意义的推荐。

两个问题自然而然地出现了——什么是最好的社区规模？我们应该使用哪种相似性度量？要回答这些问题，我们可以求助于 Mahout 的推荐器评估功能，看看我们的推荐器在各种不同的配置下对我们的数据表现如何。



# 使用 Mahout 进行推荐者评估

Mahout 提供了一组类来帮助评估我们的推荐者。就像我们在[第四章](ch04.html "Chapter 4. Classification")、*分类*中对`clj-ml`库进行的交叉验证一样，Mahout 的评估通过将我们的评级分成两组来进行:测试集和训练集。

通过在训练集上训练我们的推荐器，然后在测试集上评估它的性能，我们可以了解我们的算法在真实数据上的表现如何。为了处理根据 Mahout 的评估器提供的训练数据训练模型的任务，我们必须提供一个符合`RecommenderBuilder`接口的对象。该接口只定义了一个方法:`buildRecommender`。我们可以使用 reify 创建一个匿名的`RecommenderBuilder`类型:

```
(defn recommender-builder [sim n]

  (reify RecommenderBuilder

    (buildRecommender [this model]

      (let [nhood (NearestNUserNeighborhood. n sim model)]

        (GenericUserBasedRecommender. model nhood sim)))))
```

Mahout 在`org.apache.mahout.cf.taste.impl.eval`名称空间中提供了各种赋值器。在下面的代码中，我们通过传入一个推荐器构建器和我们加载的数据模型，使用`RMSRecommenderEvaluator`类构建一个均方根误差评估器:

```
(defn evaluate-rmse [builder model]

  (-> (RMSRecommenderEvaluator.)

      (.evaluate builder nil model 0.7 1.0)))
```

我们在前面的代码中传递来评估的`nil`值表示我们没有提供定制的模型构建器，这意味着`evaluate`函数将根据我们提供的模型使用默认的模型构建器。数字`0.7`和`1.0`是用于训练的数据的比例，以及用于评估的测试数据的比例。在早期的代码中，我们使用 70%的数据进行训练，并在剩下的 100%数据上评估模型。**均方根误差** ( **RMSE** )评估器将计算每个测试数据的预测等级和实际等级之间的均方根误差。

我们可以使用前面的两个函数来评估基于用户的推荐器的性能，使用欧几里德距离和 10 的邻域，如下所示:

```
(defn ex-7-10 []

  (let [model   (load-model "ua.base")

        builder (recommender-builder 10

                 (EuclideanDistanceSimilarity. model))]

    (evaluate-rmse builder model)))

;; 0.352
```

您的结果当然可能不同，因为评估是在数据的随机子集上执行的。

我们在前一章中定义欧几里德距离 *d* 为正值，其中零表示完全相似。这可以通过以下方式转换成相似性度量 *s* :

![Recommender evaluation with Mahout](graphics/7180OS_07_16.jpg)

不幸的是，先前的测量会对具有更多共同评价项目的用户产生偏差，因为每个维度都提供了进一步分离的机会。为了纠正这一点，Mahout 将欧几里得相似度计算为:

![Recommender evaluation with Mahout](graphics/7180OS_07_17.jpg)

这里， *n* 是维数。由于这个公式可能导致相似性超过 1，所以 Mahout 在 1 处截取相似性。

## 评估距离测量

在前一章中，我们遇到了各种各样的距离和相似性度量；特别是，我们利用了雅克卡距离、欧几里德距离和余弦距离。Mahout 在`org.apache.mahout.cf.taste.impl.similarity`包中包含了这些相似性度量的实现，分别为`TanimotoCoefficientSimilarity`、`EuclideanDistanceSimilarity`和`UncenteredCosineSimilarity`。

我们刚刚评估了欧几里德相似度在我们评级数据上的表现，所以让我们看看其他人表现如何。现在，让我们试试 Mahout 提供的另外两个相似性度量— `PearsonCorrelationSimilarity`和`SpearmanCorrelationSimilarity`。

### 皮尔逊相关相似度

皮尔逊相关度相似度是基于用户喜好之间的相关性的相似性度量。下图是两个用户对三部电影 **A** 、 **B** 和 **C** 的评分。

![The Pearson correlation similarity](graphics/7180OS_07_140.jpg)

欧几里德距离的一个潜在缺点是，它无法解释这样的情况，即一个用户与另一个用户在他们对电影的相对评级上完全一致，但在他们的评级上往往更加慷慨。考虑前面例子中的两个用户。他们对电影 **A** 、 **B** 和 **C** 的评分之间存在完美的相关性，但用户 **Y** 对电影的评分高于用户 **X** 。这两个用户之间的欧几里德距离可以用下面的公式来计算:

![The Pearson correlation similarity](graphics/7180OS_07_18.jpg)

然而，在某种意义上，他们是完全一致的。回到[第 3 章](ch03.html "Chapter 3. Correlation")、*相关性*我们计算两个系列之间的皮尔逊相关性为:

![The Pearson correlation similarity](graphics/7180OS_07_19.jpg)

这里，![The Pearson correlation similarity](graphics/7180OS_07_20.jpg)和![The Pearson correlation similarity](graphics/7180OS_07_21.jpg)。前面给出的例子产生的皮尔逊相关系数为 1。

让我们试着用皮尔逊相关相似度来做预测。Mahout 用`PearsonCorrelationSimilarity`类实现了 Pearson 关联:

```
(defn ex-7-11 []

  (let [model   (load-model "ua.base")

        builder (recommender-builder

                 10 (PearsonCorrelationSimilarity. model))]

    (evaluate-rmse builder model)))

;; 0.796
```

事实上，使用皮尔逊相关性，电影数据的 RMSE 已经增加。

皮尔逊相关相似性在数学上等价于已被居中的数据(平均值为零的数据)的余弦相似性。在我们之前举例说明的两个用户 *X* 和 *Y* 的例子中，均值并不相同，因此余弦相似性度量将给出与皮尔逊相关相似性不同的结果。Mahout 实现余弦相似度为`UncenteredCosineSimilarity`。

虽然皮尔逊方法有直观的意义，但它在推荐引擎的环境中有一些缺点。它没有考虑两个用户共有的评价项目的数量。如果它们仅共享一个项目，则不能计算相似性。此外，如果一个用户总是给项目相同的评级，则无法计算该用户和任何其他用户之间的相关性，即使另一个用户也是如此。也许数据中的评级种类不够多，皮尔逊相关相似性无法很好地发挥作用。

### 斯皮尔曼等级相似度

哪些用户可能相似的另一种方式是排名不是特别紧密相关，但是用户之间保留排名的排序。考虑下面的图表，该图表显示了两个用户对五部不同电影的评级:

![Spearman's rank similarity](graphics/7180OS_07_160.jpg)

我们可以看到，用户评分之间的线性相关性并不完美，因为他们的评分并不是绘制在一条直线上。这将导致中等皮尔逊相关相似性和更低的余弦相似性。然而，他们偏好之间的排序是相同的。如果我们比较用户偏好的排序列表，它们会完全一样。

Spearman 的等级相关系数使用这个度量来计算用户之间的差异。它被定义为排序项目之间的皮尔逊相关系数:

![Spearman's rank similarity](graphics/7180OS_07_22.jpg)

这里， *n* 是等级数，![Spearman's rank similarity](graphics/7180OS_07_23.jpg)是项目 *i* 的等级差。Mahout 用我们在下一段代码中使用的`SpearmanCorrelationSimilarity`类实现了 Spearman 的秩相关相似性。该算法有更多的工作要做，所以我们在一个更小的子集上进行评估，只有 10%的测试数据:

```
(defn ex-7-12 []

  (let [model   (load-model "ua.base")

        builder (recommender-builder

                 10 (SpearmanCorrelationSimilarity. model))]

    (-> (RMSRecommenderEvaluator.)

        (.evaluate builder nil model 0.9 0.1))))

;; 0.907
```

RMSE 的评估分数甚至比皮尔森相关相似性的分数还要高。似乎到目前为止，电影镜头数据的最佳相似性度量是欧几里德相似性。

## 确定最佳邻域大小

在之前的比较中，我们没有改变的一个方面是推荐所基于的用户社区的规模。让我们看看 RMSE 是如何受到邻域大小的影响的:

```
(defn ex-7-13 []

  (let [model (load-model "ua.base")

        sim   (EuclideanDistanceSimilarity. model)

        ns    (range 1 10)

        stats (for [n ns]

                (let [builder (recommender-builder n sim)]

                  (do (println n)

                      (evaluate-rmse builder model))))]

    (-> (c/scatter-plot ns stats

                        :x-label "Neighborhood size"

                        :y-label "RMSE")

        (i/view))))
```

随着邻域从 1 增加到 10，前面的代码创建了欧几里德相似性的 RMSE 散点图。

![Determining optimum neighborhood size](graphics/7180OS_07_165.jpg)

或许令人惊讶的是，随着社区规模的增长，预测评级的 RMSE 也会上升。最准确的预测评级是基于两个人的邻居。但是，也许这不应该让我们感到惊讶:对于欧几里德相似性，最相似的其他用户被定义为与用户的评级最接近的用户。邻域越大，我们对同一项目的评价范围就越多样化。

较早的 RMSE 范围在 **0.25** 和 **0.38** 之间。单以此为基础，很难知道推荐人表现好不好。 **0.38** 的错误评级在实践中有很大影响吗？例如，如果我们总是猜测一个评级恰好 **0.38** 太高(或太低)，我们将推荐一个与用户自己的相对值完全一致的值。幸运的是，Mahout 提供了另一种评估器，它从信息检索的字段返回各种统计数据。我们接下来会看这些。

## 信息检索统计

我们让更好地处理如何改进我们的建议的一个方法是使用评估者，该评估者提供关于评估者在许多不同方面表现如何的更多细节。`GenericRecommenderIRStatsEvaluator`函数包括几个信息检索统计数据来提供这一细节。

在许多情况下，没有必要去猜测用户会给一部电影分配的确切等级；呈现一个从最好到最差的有序列表就足够了。事实上，即使是确切的顺序可能也不是特别重要。

### 注意

信息检索系统是那些响应用户查询返回结果的系统。推荐系统可以被认为是信息检索系统的子集，其中查询是与用户相关联的先前评级的集合。

**信息检索统计** ( **IR stats** )评估员对待推荐评估有点像搜索引擎评估。搜索引擎应该努力返回尽可能多的用户正在寻找的结果，同时不返回大量不需要的信息。这些比例通过统计精度和召回率来量化。

### 精度

一个信息检索系统的精度是它返回的相关条目的百分比。如果正确的建议是**真阳性**，而不正确的建议是**假阳性**，那么精度可以测量为返回的**真阳性**的总数:

![Precision](graphics/7180OS_07_24.jpg)

因为我们返回一定数量的推荐，例如前 10 个，所以我们将讨论精度为 10。例如，如果模型返回 10 个推荐，其中 8 个是用户真正的前 10 名中的一部分，那么模型的精度是 80 %10。

### 回忆

召回补充了精确度，这两个指标经常被一起引用。召回衡量返回的相关建议的比例:

![Recall](graphics/7180OS_07_25.jpg)

我们可以认为这是推荐者实际做出的可能的好推荐的比例。例如，如果系统只推荐了用户最喜欢的 10 部电影中的 5 部，那么我们可以说召回率是 50%。

## 看象人的信息检索评估器

信息检索的统计可以在逐个用户的基础上将推荐问题重新定义为搜索问题。`GenericRecommenderIRStatsEvaluator`不是将数据随机分成测试集和训练集，而是为每个用户评估推荐器的性能。它通过删除一些用户评价最高的项目(比如前五名)来做到这一点。然后，评估者将看到系统实际上推荐了多少用户真正的前五名评分项目。

我们的实现如下:

```
(defn evaluate-ir [builder model]

  (-> (GenericRecommenderIRStatsEvaluator.)

      (.evaluate builder nil model nil 5

         GenericRecommenderIRStatsEvaluator/CHOOSE_THRESHOLD

         1.0)

      (bean)))

(defn ex-7-14 []

  (let [model   (load-model "ua.base")

        builder (recommender-builder

                 10 (EuclideanDistanceSimilarity. model))]

    (evaluate-ir builder model)))
```

前面代码中的“at”值是`5`，我们在`GenericRecommenderIRStatsEvaluator/CHOOSE_THRESHOLD`之前传递它，使 Mahout 计算一个可感知相关性阈值。前面的代码返回以下输出:

```
;; {:recall 0.002538071065989847, :reach 1.0,

;;  :precision 0.002538071065989847,

;;  :normalizedDiscountedCumulativeGain 0.0019637198336778725,

;;  :fallOut 0.0011874376015289575,

;;  :f1Measure 0.002538071065989847,

;;  :class org.apache.mahout.cf.taste.impl.eval.IRStatisticsImpl}
```

赋值器返回一个`org.apache.mahout.cf.taste.eval.IRStatistics`的实例，我们可以用 Clojure 的`bean`函数将其转换成地图。地图包含由评估者计算的所有信息检索统计数据。它们的含义将在下一节解释。

### F-测度与调和平均值

也称为**F1 测度** 或 **平衡 F 值**，F 测度是精度和召回的加权调和平均值:

![F-measure and the harmonic mean](graphics/7180OS_07_26.jpg)

调和平均数与更常见的算术平均数有关，事实上，它是毕达哥拉斯的三个平均数之一。它被定义为倒数的算术平均值的倒数，在涉及速率和比率的情况下特别有用。

例如，考虑一辆车以某一速度 *x* 行驶了距离 *d* ，然后再次以速度 *y* 行驶了距离 *d* 。速度以行驶距离与所用时间的比率来衡量，因此平均速度是 *x* 和 *y* 的调和平均值。如果 *x* 是 60 mph，而 *y* 是 40 mph，那么平均速度就是 48 mph，我们可以这样计算:

![F-measure and the harmonic mean](graphics/7180OS_07_27.jpg)

请注意，这低于算术平均值，即 50 英里/小时。如果相反 *d* 代表一定量的时间而不是距离，那么车辆以速度 *x* 行驶一定量的时间，然后以速度 *y* 行驶相同量的时间，那么其平均速度将是 *x* 和 *y* 的算术平均值，或者 50 mph。

F-Measure 可以推广到*F*[β]-允许与精度或召回相关联的权重被独立调整的度量:

![F-measure and the harmonic mean](graphics/7180OS_07_28.jpg)

常见的度量有 *F* [2] ，权重召回精度两倍， *F* [0.5] ，权重召回精度两倍。

### 脱落

也称为**假阳性**率，即不相关推荐在所有不相关推荐中的比例:

![Fall-out](graphics/7180OS_07_29.jpg)

不像我们目前看到的其他 IR 统计数据，落差越低，我们的推荐者做得越好。

### 归一化贴现累计收益

**贴现** **累积收益** ( **DCG** )是基于推荐实体的分级相关性的推荐系统性能的度量。它在 0 和 1 之间变化，1 代表完美的排名。

折扣累积收益的前提是，在搜索结果列表中出现在较低位置的高度相关的结果应该根据它们的相关性以及它们在结果列表中出现的程度而受到惩罚。它可以用以下公式计算:

![Normalized discounted cumulative gain](graphics/7180OS_07_30.jpg)

这里， *rel* [i] 是位置 *i* 处结果的相关性， *p* 是排名中的位置。前面介绍的版本是一个流行的公式，它强调检索相关的结果。

由于搜索结果列表的长度因查询而异，因此我们不能只使用 DCG 来一致地比较结果。相反，我们可以根据相关性对结果进行排序，并再次计算 DCG 。由于这将为结果提供最佳可能的累积贴现收益(如我们按照相关性顺序对它们进行的排序)，因此该结果被称为 **理想贴现累积收益** ( **IDCG** )。

取 DCG 和 IDCG 的比值，得出归一化贴现累积收益:

![Normalized discounted cumulative gain](graphics/7180OS_07_31.jpg)

在一个完美的排名算法中， *DCG* 将等于 *IDCG* ，导致 *nDCG* 为 1.0。由于 *nDCG* 提供 0 到 1 范围内的结果，它提供了一种比较不同查询引擎的相对性能的方法，其中每个引擎返回不同数量的结果。

### 绘制信息检索结果

我们可以用下面的代码绘制信息检索评估的结果:

```
(defn plot-ir [xs stats]

  (-> (c/xy-plot xs (map :recall stats)

                 :x-label "Neighbourhood Size"

                 :y-label "IR Statistic"

                 :series-label "Recall"

                 :legend true)

      (c/add-lines xs (map :precision stats)

                   :series-label "Precision")

      (c/add-lines xs

                   (map :normalizedDiscountedCumulativeGain stats)

                   :series-label "NDCG")

      (i/view)))

(defn ex-7-15 []

  (let [model   (load-model "ua.base")

        sim     (EuclideanDistanceSimilarity. model)

        xs      (range 1 10)

        stats   (for [n xs]

                  (let [builder (recommender-builder n sim)]

                    (do (println n)

                        (evaluate-ir builder model))))]

    (plot-ir xs stats)))
```

这将生成下图:

![Plotting the information retrieval results](graphics/7180OS_07_168.jpg)

在前面的图表中，我们可以看到最高精度对应于邻域大小为 2；咨询最相似的用户产生最少的误报。您可能已经注意到了，尽管报告的精确度和召回率都很低。随着邻域变大，推荐器将有更多的候选推荐。但是，请记住，信息检索统计是按 5 计算的，这意味着只有前五个推荐会被计算在内。

在推荐者的上下文中，关于这些度量有一个微妙的问题——精确度完全基于*我们能多好地预测用户已经评价了*的其他项目。推荐者将因为推荐用户没有评级的稀有项目而受到惩罚，即使它们是用户喜欢的项目的精彩推荐。

## 带有布尔偏好的推荐

在这一章中有一个假设，用户给一个项目的评分是一个重要的事实。到目前为止，我们看到的距离度量试图以不同的方式预测用户未来评级的数值。

另一种距离测量方法认为，用户对一个项目的评价远没有他们对它进行评价的事实重要。换句话说，所有的评级，甚至是差的，都可以被同等对待。想想看，对于每一部用户评价很差的电影，都有更多的用户甚至懒得看——更不用说评价了。在许多其他情况下，布尔偏好是作出推荐的主要基础；例如，用户在社交媒体上的喜欢或收藏。

要使用布尔相似性度量，我们首先必须将我们的模型转换为布尔偏好模型，我们可以使用以下代码来完成:

```
(defn to-boolean-preferences [model]

  (-> (GenericBooleanPrefDataModel/toDataMap model)

      (GenericBooleanPrefDataModel.)))

(defn boolean-recommender-builder [sim n]

  (reify RecommenderBuilder

    (buildRecommender [this model]

      (let [nhood (NearestNUserNeighborhood. n sim model)]

        (GenericBooleanPrefUserBasedRecommender.

         model nhood sim)))))
```

将用户的分级视为布尔值可以将用户的电影分级列表简化为集合表示，正如我们在上一章中看到的，Jaccard 索引可以用于确定集合相似性。Mahout 实现了一个与 Jaccard 索引密切相关的相似性度量，称为 **谷本系数**。

### 注意

Tanimoto 系数适用于向量，其中每个索引代表一个可以是 0 或 1 的特征，而 Jaccard 索引适用于可能包含或不包含元素的集合。使用哪种度量仅取决于您的数据表示，这两种度量是等效的。

让我们使用 Mahout 的红外统计评估器绘制几个不同面积的红外统计数据:

```
(defn ex-7-16 []

  (let [model   (to-boolean-preferences (load-model "ua.base"))

        sim     (TanimotoCoefficientSimilarity. model)

        xs      (range 1 10)

        stats   (for [n xs]

                  (let [builder

                        (boolean-recommender-builder n sim)]

                    (do (println n)

                        (evaluate-ir builder model))))]

    (plot-ir xs stats)))
```

前面的代码生成了下面的图表:

![Recommendation with Boolean preferences](graphics/7180OS_07_169.jpg)

对于布尔推荐器，较大的邻域提高了精确度分数。考虑到我们观察到的欧几里得相似性，这是一个有趣的结果。请记住，虽然布尔偏好，没有相对项目偏好的概念，他们要么评级或不评级。最相似的用户，因此形成一个邻域的组，将是那些简单地对相同项目进行评级的用户。这个组越大，我们就越有机会预测用户评价的项目。

此外，因为没有布尔偏好的相对分数，所以前面的图表中没有标准化的贴现累积收益。缺少顺序可能会使布尔偏好看起来不如其他数据理想，但是它们可能非常有用，正如我们接下来将看到的。

### 隐性与显性反馈

事实上，一种常见的技术是简单地观察用户活动，而不是试图从用户那里获得他们喜欢和不喜欢的明确评级。例如，在一个电子商务网站上，所查看的一组项目可以提供用户感兴趣的产品种类的指示。同样，用户在网站上浏览的页面列表也是他们感兴趣阅读的内容类型的有力指示器。

使用点击和页面浏览量等隐性来源可以极大地增加预测所依据的信息量。它还避免了所谓的“冷启动”问题，即用户必须提供明确的评级，然后你才能提供任何推荐；一旦用户到达你的站点，他们就会开始生成数据。

在这些情况下，每个页面视图都可以被视为代表用户偏好的一大组页面中的一个元素，并且可以使用布尔相似性度量来推荐相关内容。对于一个受欢迎的网站来说，这样的集合显然会增长得非常大非常快。不幸的是，Mahout 0.9 的推荐引擎被设计成在内存中的单个服务器上运行。所以，他们限制了我们可以处理的数据量。

在我们看一个替代的推荐器之前，我们先来看看执行降维的方法，这个推荐器被设计成在一个机器集群上运行，并且随着你拥有的数据量而扩展。我们将从概率上减少非常大的集合的大小的方法开始。



# 大型集合的概率方法

大型集合出现在数据科学的许多上下文中。正如前面提到的，我们在处理用户的隐式反馈时可能会遇到这些问题，但是接下来描述的方法可以应用于任何可以表示为集合的数据。

## 用布隆过滤器测试集合成员

Bloom filters 是一种数据结构，它提供了一种压缩集合大小的方法，同时保留了我们判断给定项目是否是集合成员的能力。这种压缩的代价是一些不确定性。Bloom filter 告诉我们一个项目何时可能在一个集合中，尽管它会告诉我们它是否在集合中。在节省磁盘空间确实值得牺牲的情况下，它们是集合压缩的一个非常流行的选择。

布隆过滤器的基本数据结构是一个位向量——可能包含 1 或 0(或真或假)的单元序列。压缩的级别(以及不确定性的相应增加)可以用两个参数来配置——T2 k 哈希函数和 T4 m 位。

![Testing set membership with Bloom filters](graphics/7180OS_07_190.jpg)

上图展示了获取一个输入项(顶部的方块)并多次散列的过程。每个哈希函数输出一个整数，该整数用作位向量的索引。匹配散列索引的元素被设置为 1。下图显示了一个不同的元素被散列到一个不同的位向量中，生成一组不同的索引，这些索引将被赋予值 1:

![Testing set membership with Bloom filters](graphics/7180OS_07_200.jpg)

我们可以使用下面的 Clojure 实现 Bloom filters。我们使用谷歌的 MurmurHash 实现，使用不同的种子来提供 *k* 不同的哈希函数:

```
(defn hash-function [m seed]

  (fn [x]

    (-> (Hashing/murmur3_32 seed)

        (.hashUnencodedChars x)

        (.asInt)

        (mod m))))

(defn hash-functions [m k]

  (map (partial hash-function m) (range k)))

(defn indices-fn [m k]

  (let [f (apply juxt (hash-functions m k))]

    (fn [x]

      (f x))))

(defn bloom-filter [m k]

  {:filter     (vec (repeat m false))

   :indices-fn (indices-fn m k)})
```

前面的代码将 Bloom filter 定义为包含一个`:filter`(位向量)和一个`:indices`函数的映射。`indices`函数处理应用 *k* 散列函数来生成 *k* 索引的任务。我们将 0 表示为`false`，将 1 表示为`true`，但是效果是一样的。在下面的示例中，我们使用代码通过`5`哈希函数创建长度为`8`的布隆过滤器:

```
(defn ex-7-17 []

  (bloom-filter 8 5))

;; {:filter [false false false false false false false false],

;;  :indices-fn #<Bloom_filter$indices_fn$fn__43538 

;;  cljds.ch7.Bloom_filter$indices_fn$fn__43538@3da200c>}
```

响应是两个键的映射——过滤器本身(一个布尔值向量，全部为假),以及一个索引函数，该函数由五个哈希函数生成。我们可以用一个简单的`Bloom-assoc`函数将早期的代码放在一起:

```
(defn set-bit [seq index]

  (assoc seq index true))

(defn set-bits [seq indices]

  (reduce set-bit seq indices))

(defn bloom-assoc [{:keys [indices-fn] :as bloom} element]

  (update-in bloom [:filter] set-bits (indices-fn element)))
```

给定一个布隆过滤器，我们简单地调用`indices-fn`函数来获取我们需要在布隆过滤器中设置的索引:

```
(defn ex-7-18 []

  (-> (bloom-filter 8 5)

      (bloom-assoc "Indiana Jones")

      (:filter)))

;; [true true false true false false false true]
```

为了确定 Bloom filter 是否包含一个项目，我们只需要查询是否所有应该为真的索引实际上都为真。如果是，我们推断该项目已被添加到过滤器中:

```
(defn bloom-contains? [{:keys [filter indices-fn]} element]

  (->> (indices-fn element)

       (map filter)

       (every? true?)))

(defn ex-7-19 []

  (-> (bloom-filter 8 5)

      (bloom-assoc "Indiana Jones")

      (bloom-contains? "Indiana Jones")))

;; true
```

我们将`"Indiana Jones"`添加到布隆过滤器中，发现它包含`"Indiana Jones"`。让我们搜索另一部哈里森·福特的电影`"The Fugitive"`:

```
(defn ex-7-20 []

  (-> (bloom-filter 8 5)

      (bloom-assoc "Indiana Jones")

      (bloom-contains? "The Fugitive")))

;; false
```

到目前为止，一切顺利。但是我们已经用一些 T2 精度换取了这种巨大的压缩。让我们搜索一部不应该出现在布鲁姆滤镜里的电影。或许，1996 年的电影`Bogus`:

```
(defn ex-7-21 []

  (-> (bloom-filter 8 5)

      (bloom-assoc "Indiana Jones")

      (bloom-contains? "Bogus (1996)")))

;; true
```

这不是我们想要的。过滤器声称包含`"Bogus (1996)"`，尽管我们还没有将它关联到过滤器中。这是布隆过滤器做出的权衡；虽然一个过滤器永远不会声称一个项目没有被添加时，它可能会错误地声称一个项目已经被添加时，它并没有。

### 注意

在本章前面我们遇到的信息检索术语中，布隆过滤器具有 100%的召回率，但是它们的精确度低于 100%。少多少是可以通过我们为 m 和 T2 选择的值来配置的。

在 MovieLens 数据集的 1682 部电影中，总共有 56 部电影在添加“印第安纳·琼斯”后被布隆过滤器错误地报告，误报率为 3.3%。假设我们只使用了五个散列函数和一个八元素过滤器，您可能期望它高得多。当然，我们的 Bloom filter 只包含一个元素，当我们添加更多元素时，获得冲突的概率将急剧上升。事实上，假阳性的概率大约是:

![Testing set membership with Bloom filters](graphics/7180OS_07_32.jpg)

这里， *k* 和 *m* 是哈希函数的个数和之前滤波器的长度， *n* 是添加到集合中的项目的个数。对于我们之前的奇异绽放，这给出了:

![Testing set membership with Bloom filters](graphics/7180OS_07_33.jpg)

所以，事实上，理论上的假阳性率比我们观察到的还要低。

Bloom filters 是一种非常通用的算法，当我们想要测试集合成员资格，并且没有资源来显式存储集合中的所有项目时，它非常有用。精度可通过选择 *m* 和 *k* 的值来配置，这意味着可以选择您愿意容忍的假阳性率。因此，它们被用于各种各样的数据密集型系统。

布隆过滤器的一个缺点是，它不可能检索您添加到过滤器的值；尽管我们可以使用过滤器来测试集合的成员资格，但是在没有彻底检查的情况下，我们无法说出集合包含了什么。对于推荐系统(实际上对于其他系统也是如此，比如聚类)，我们主要感兴趣的是两个集合之间的相似性，而不是它们的精确内容。但在这里，花开让我们失望；我们不能可靠地使用压缩过滤器来衡量两组项目之间的相似性。

接下来，我们将介绍一种算法，它将保持由 Jaccard 相似性度量的集合相似性。这样做的同时还保留了布隆过滤器提供的可配置压缩。



# 大型集合与 MinHash 的 Jaccard 相似性

布隆过滤器是确定项目是否是集合成员的概率数据结构。当比较用户或项目的相似性时，我们通常感兴趣的是集合之间的交集，而不是它们的精确内容。MinHash 是一种技术，它能够以这样一种方式压缩一个大集合，即我们仍然可以对压缩的表示执行 Jaccard 相似性。

让我们参考 MovieLens 数据集中两个最多产的评分者来看看它是如何工作的。用户 405 和 655 分别评价了 727 和 675 部电影。在下面的代码中，我们提取他们的评级，并在传递给 Incanter 的`jaccard-index`函数之前将它们转换成集合。回想一下，这会返回他们评价的电影占他们评价的所有电影的比率:

```
(defn rated-items [user-ratings id]

  (->> (get user-ratings id)

       (map :item)))

(defn ex-7-22 []

  (let [ratings      (load-ratings "ua.base")

        user-ratings (group-by :user ratings)

        user-a       (rated-items user-ratings 405)

        user-b       (rated-items user-ratings 655)]

    (println "User 405:" (count user-a))

    (println "User 655:" (count user-b))

    (s/jaccard-index (set user-a) (set user-b))))

;; User 405: 727

;; User 655: 675

;; 158/543
```

这两大组评级之间有大约 29%的相似性。让我们看看如何使用 MinHash 减少这些集合的大小，同时保持它们之间的相似性。

MinHash 算法与布隆过滤器有许多共同之处。我们的第一个任务是挑选 k 个散列函数。这些 *k* 散列函数被用来散列集合中的每个元素，而不是散列集合表示本身。对于每个 *k* 散列函数，MinHash 算法存储由任何集合元素生成的最小值。因此，输出是一组 *k* 数；每个都等于该散列函数的最小散列值。输出被称为 MinHash 签名。

下图说明了两个集合(每个集合包含三个元素)转换为具有 2 的 *k* 的 MinHash 签名的过程:

![Jaccard similarity for large sets with MinHash](graphics/7180OS_07_210.jpg)

输入集共享总共四个唯一元素中的两个元素，这相当于 Jaccard 指数为 0.5。两组的 MinHash 签名分别是`#{3, 0}`和`#{3, 55}`，这相当于 0.33 的 Jaccard 指数。因此，MinHash 减少了我们输入集的大小(在这种情况下，只减少了一个)，同时保留了它们之间的近似相似性。

与布隆过滤器一样，适当选择 *k* 允许您指定可以容忍的精度损失。我们可以使用下面的 Clojure 代码实现 MinHash 算法:

```
(defn hash-function [seed]

  (let [f (Hashing/murmur3_32 seed)]

    (fn [x]

      (-> (.hashUnencodedChars f (str x))

          (.asInt)))))

(defn hash-functions [k]

  (map hash-function (range k)))

(defn pairwise-min [a b]

  (map min a b))

(defn minhasher [k]

  (let [f (apply juxt (hash-functions k))]

    (fn [coll]

      (->> (map f coll)

           (reduce pairwise-min)))))
```

在下面的代码中，我们定义了一个 *k* 为 10 的`minhasher`函数，并使用它对用户 405 和 655 的压缩评级执行一个使用 Jaccard 索引的设置测试:

```
 (defn ex-7-23 []

  (let [ratings      (load-ratings "ua.base")

        user-ratings (group-by :user ratings)

        minhash (minhasher 10)

        user-a  (minhash (rated-items user-ratings 405))

        user-b  (minhash (rated-items user-ratings 655))]

    (println "User 405:" user-a)

    (println "User 655:" user-b)

    (s/jaccard-index (set user-a) (set user-b))))

;; User 405: #{-2147145175 -2141119028 -2143110220 -2143703868 –

;; 2144897714 -2145866799 -2139426844 -2140441272 -2146421577 –

;; 2146662900}

;; User 655: #{-2144975311 -2140926583 -2141119028 -2141275395 –

;; 2145738774 -2143703868 -2147345319 -2147134300 -2146421577 –

;; 2146662900}

;; 1/4
```

基于我们的 MinHash 签名的 Jaccard 指数非常接近原始集合的指数——25%对 29%——尽管我们将集合压缩到每个集合只有 10 个元素。

小得多的集合的好处是双重的:显然存储空间大大减少了，但是检查两个集合之间的相似性所需的计算复杂度也减少了。与包含数百个元素的集合相比，检查仅包含 10 个元素的集合的相似性要少得多。因此，在我们需要进行大量集合相似性测试的情况下，MinHash 不仅是一个节省空间的算法，还是一个节省时间的算法；例如，在推荐系统中出现的情况。

如果我们试图建立一个用户邻域来推荐商品，我们仍然需要执行大量的集合测试来确定哪些用户是最相似的。事实上，对于大量用户来说，彻底检查每一个其他用户可能是非常耗时的，即使在我们已经计算了 MinHash 签名之后。最终的概率技术将着眼于解决这个特定的问题:在寻找相似物品时，如何减少必须比较的候选物的数量。

## 使用区分位置的散列法减少对比较

在前面的章节中，我们计算了大量文档的相似度矩阵。路透社语料库中有 20，000 份文件，这已经是一个非常耗时的过程。随着数据集的大小增加一倍，检查每一对项目所需的时间会增加四倍。因此，大规模进行这种分析可能会非常耗时。

例如，假设我们有一百万个文档，我们为每个文档计算了长度为 250 的 MinHash 签名。这意味着我们使用 1000 字节来存储每个文档。由于所有签名都可以存储在一个千兆字节中，因此为了提高速度，它们都可以存储在主系统存储器中。然而，有![Reducing pair comparisons with locality-sensitive hashing](graphics/7180OS_07_34.jpg)对文档，或 499，999，500，000 个成对组合需要检查。即使比较两个签名只需要一微秒，计算所有相似性仍然需要将近 6 天的时间。

本地敏感散列法 ( **LSH** )，通过显著减少必须进行的成对比较的数量来解决这个问题。这是通过将可能具有最小相似性阈值的集合存储在一起来实现的；仅需要检查存储在一起的集合的相似性。

### 桶装签名

我们将散列到同一个桶的任何项目对视为候选对，并且仅检查候选对的相似性。目的是只有相似的项目应该成为候选项对。碰巧散列到同一个桶的不同对将会是误报，并且我们寻求最小化这些。散列到不同桶的相似对是假阴性，我们同样也寻求最小化这些。

如果我们已经计算了项目的 MinHash 签名，一种有效的方法是将签名矩阵分成 *b* 个带，每个带由 *r* 个元素组成。下图对此进行了说明:

![Bucketing signatures](graphics/7180OS_07_220.jpg)

在前面的章节中已经编写了生成 MinHash 签名的代码，在 Clojure 中执行 LSH 只是简单地将签名划分成一定数量的带，每个带的长度为 *r* 。每个带都被散列(为简单起见，我们对每个带使用相同的散列函数)到特定的桶:

```
(def lsh-hasher (hash-function 0))

(defn locality-sensitive-hash [r]

  {:r r :bands {}})

(defn buckets-for [r signature]

  (->> (partition-all r signature)

       (map lsh-hasher)

       (map-indexed vector)))

(defn lsh-assoc [{:keys [r] :as lsh} {:keys [id signature]}]

  (let [f (fn [lsh [band bucket]]

            (update-in lsh [:bands band bucket] conj id))]

    (->> (buckets-for r signature)

         (reduce f lsh))))
```

前面的示例将区分位置的散列简单地定义为一个包含空带和一些值的映射， *r* 。当我们将一个项目与`lsh-assoc`关联到 LSH 中时，我们基于 *r* 的值将签名分成多个带，并确定每个带的桶。项目的 ID 被添加到每个桶中。时段按区带 ID 进行分组，以便不同区带中共享一个时段的项目不会被一起分时段:

```
(defn ex-7-24 []

  (let [ratings (load-ratings "ua.base")

        user-ratings (group-by :user ratings)

        minhash (minhasher 27)

        user-a  (minhash (rated-items user-ratings 13))

        lsh     (locality-sensitive-hash 3)]

    (lsh-assoc lsh {:id 13 :signature user-a})))

;; {:r 3, :bands {8 {220825369 (13)}, 7 {-2054093854 (13)},

;; 6 {1177598806 (13)}, 5 {-1809511158 (13)}, 4 {-143738650 (13)},

;; 3 {-704443054 (13)}, 2 {-1217282814 (13)},

;; 1 {-100016681 (13)}, 0 {1353249231 (13)}}}
```

前面的例子显示了对具有 *k=27* 和 *r=3* 的用户 13 的签名执行 LSH 的结果。返回 9 个波段的桶。接下来，我们将更多的项添加到区分位置的散列中:

```
(defn ex-7-25 []

  (let [ratings (load-ratings "ua.base")

        user-ratings (group-by :user ratings)

        minhash (minhasher 27)

        user-a  (minhash (rated-items user-ratings 13))

        user-b  (minhash (rated-items user-ratings 655))]

    (-> (locality-sensitive-hash 3)

        (lsh-assoc {:id 13  :signature user-a})

        (lsh-assoc {:id 655 :signature user-b}))))

;; {:r 3, :bands {8 {220825369 (655 13)}, 7 {1126350710 (655),

;; -2054093854 (13)}, 6 {872296818 (655), 1177598806 (13)},

;; 5 {-1272446116 (655), -1809511158 (13)}, 4 {-154360221 (655),

;; -143738650 (13)}, 3 {123070264 (655), -704443054 (13)},

;; 2 {-1911274538 (655), -1217282814 (13)}, 1 {-115792260 (655),

;; -100016681 (13)}, 0 {-780811496 (655), 1353249231 (13)}}}
```

在前面的例子中，我们可以看到用户 id`655`和`13`被放在波段`8`的同一个桶中，尽管它们在所有其他波段的不同桶中。

对于一个特定的带，签名一致的概率是 *s* ^r ，其中 *s* 是集合的真实相似度， *r* 是每个带的长度。由此得出，签名在至少一个特定频带中不一致的概率是![Bucketing signatures](graphics/7180OS_07_35.jpg)，因此，签名在所有频带中不一致的概率是![Bucketing signatures](graphics/7180OS_07_36.jpg)。因此，我们可以说两个项目成为候选项对的概率是![Bucketing signatures](graphics/7180OS_07_37.jpg)。

不考虑 *b* 和 *r* 的具体值，这个等式描述了一个 S 曲线。阈值(成为候选的概率为 0.5 的相似度的值)是 *b* 和 *r* 的函数。在阈值附近，S 曲线急剧上升。因此，相似度高于阈值的配对很可能成为候选，而低于阈值的配对相应地不太可能成为候选。

![Bucketing signatures](graphics/7180OS_07_230.jpg)

为了搜索候选对，我们现在只需要对目标签名执行相同的过程，并且查看哪些其他项目散列到相同带中的相同桶:

```
(defn lsh-candidates [{:keys [bands r]} signature]

  (->> (buckets-for r signature)

       (mapcat (fn [[band bucket]]

                 (get-in bands [band bucket])))

       (distinct)))
```

前面的代码返回与目标签名在至少一个波段中共享至少一个桶的项目的不同列表:

```
(defn ex-7-26 []

  (let [ratings (load-ratings "ua.base")

        user-ratings (group-by :user ratings)

        minhash   (minhasher 27)

        user-b    (minhash (rated-items user-ratings 655))

        user-c    (minhash (rated-items user-ratings 405))

        user-a    (minhash (rated-items user-ratings 13))]

    (-> (locality-sensitive-hash 3)

        (lsh-assoc {:id 655 :signature user-b})

        (lsh-assoc {:id 405 :signature user-c})

        (lsh-candidates user-a))))

;; (655)
```

在前面的例子中，我们将用户`655`和`405`的签名关联到本地敏感散列中。然后我们询问用户 ID `13`的候选人。结果是包含单个 ID `655`的序列。因此，`655`和`13`是候选对，应该检查相似性。用户`405`被算法判断为不够相似，因此我们不会检查他们的相似性。

### 注意

要了解更多关于位置敏感哈希、MinHash 和其他处理海量数据的有用算法的信息，请在[http://www.mmds.org/](http://www.mmds.org/)免费参考优秀的*海量数据集挖掘*在线书籍。

局部敏感散列是一种显著减少成对比较空间的方法，我们在比较集合的相似性时需要进行成对比较。因此，通过为 *b* 和 *r* 设置适当的值，位置敏感散列允许我们预先计算用户邻域。给定一个目标用户，查找相似用户的任务就像查找在任何频段上共享同一个桶的其他用户一样简单；一种任务，其时间复杂度与频带数量有关，而与用户数量无关。



# 降维

像 MinHash 和 LSH 这样的算法旨在减少必须存储的数据量，而不损害原始数据的本质。它们是压缩的一种形式，它们定义了有用的表示，保留了我们做有用工作的能力。特别是，米纳什和 LSH 被设计用来处理可以表示为一个集合的数据。

事实上，有一整类降维算法可以处理不容易表示为集合的数据。在前一章 k-means 聚类中，我们看到了某些数据如何最有效地表示为加权向量。降低表示为向量的数据的维度的常见方法是主分量分析和奇异值分解。为了演示这些，我们将返回到 Incanter，并使用其包含的数据集之一:Iris 数据集:

```
(defn ex-7-27 []

  (i/view (d/get-dataset :iris)))
```

前面的代码应该返回下表:

![Dimensionality reduction](graphics/7180OS_07_240.jpg)

Iris 数据集的前四列包含鸢尾属植物的萼片长度、萼片宽度、花瓣长度和花瓣宽度的测量值。数据集是按植物种类排序的。第 0 至 49 行代表刚毛鸢尾，第 50 至 99 行代表绿鸢尾，高于 100 的行包含海滨鸢尾。确切的物种并不重要；我们只会对它们之间的差异感兴趣。

## 绘制虹膜数据集

让我们在散点图上可视化虹膜数据集的一些属性。我们将利用下面的辅助函数将每个物种绘制成单独的颜色:

```
(defn plot-iris-columns [a b]

  (let [data (->> (d/get-dataset :iris)

                  (i/$ [a b])

                  (i/to-matrix))]

    (-> (c/scatter-plot (i/$ (range 50) 0 data)

                        (i/$ (range 50) 1 data)

                        :x-label (name a)

                        :y-label (name b))

        (c/add-points (i/$ (range 50 100) 0 data)

                      (i/$ (range 50 100) 1 data))

        (c/add-points (i/$ [:not (range 100)] 0 data)

                      (i/$ [:not (range 100)] 1 data))

        (i/view))))
```

定义了这个函数后，让我们看看这三个物种的萼片宽度和长度是如何比较的:

```
(defn ex-7-28 []

  (plot-iris-columns :Sepal.Width

                     :Sepal.Length))
```

前面的示例应该会生成以下图表:

![Plotting the Iris dataset](graphics/7180OS_07_250.jpg)

在比较这两个属性时，我们可以看到其中一个物种与另外两个物种有很大的不同，但其中两个物种几乎无法区分:几个点的宽度和高度均匀重叠。

相反，让我们画出花瓣的宽度和高度，看看它们是如何比较的:

```
(defn ex-7-29 []

  (plot-iris-columns :Petal.Width

                     :Petal.Length))
```

这将生成以下图表:

![Plotting the Iris dataset](graphics/7180OS_07_255.jpg)

这在区分不同物种方面做得更好。这部分是因为花瓣宽度和长度的差异更大——例如，长度在 *y* 轴上延伸了整整 6 个单位。这种更大的传播带来的一个有益的副作用是，它让我们能够更清晰地区分鸢尾属的物种。

## 主成分分析

原则上,成分分析，通常缩写为 PCA，我们希望找到一种使方差最大化的数据轮换。在之前的散点图中，我们确定了一种查看数据的方式，这种方式在 *y* 轴上提供了高度的方差，但是 *x* 轴的方差没有那么大。

虹膜数据集中有四个维度，每个维度代表花瓣或萼片的长度和宽度。主成分分析允许我们确定是否有另一个基础，它是所有可用维度的线性组合，最好地重新表达我们的数据，以最大化方差。

我们可以用 Incanter.stats 的`principle-components`函数应用主成分分析。在下面的代码中，我们向它传递一个数据矩阵，并绘制前两个返回的旋转:

```
(defn ex-7-30 []

  (let [data (->> (d/get-dataset :iris)

                  (i/$ (range 4))

                  (i/to-matrix))

        components (s/principal-components data)

        pc1 (i/$ 0 (:rotation components))

        pc2 (i/$ 1 (:rotation components))

        xs (i/mmult data pc1)

        ys (i/mmult data pc2)]

    (-> (c/scatter-plot (i/$ (range 50) 0 xs)

                        (i/$ (range 50) 0 ys)

                        :x-label "Principle Component 1"

                        :y-label "Principle Component 2")

        (c/add-points (i/$ (range 50 100) 0 xs)

                      (i/$ (range 50 100) 0 ys))

        (c/add-points (i/$ [:not (range 100)] 0 xs)

                      (i/$ [:not (range 100)] 0 ys))

        (i/view))))
```

前面的示例生成了以下图表:

![Principle component analysis](graphics/7180OS_07_280.jpg)

请注意，轴不再被识别为萼片或花瓣——组件已被导出为所有维度值的线性组合，并定义了一个新的基础来查看每个组件内最大化方差的数据。事实上，`principle-component`函数为每个维度返回`:std-dev`和`:rotation`。

### 注意

演示主成分分析的互动示例，见[http://setosa.io/ev/principal-component-analysis/](http://setosa.io/ev/principal-component-analysis/)。

作为获取数据的主成分的结果，横跨 *x* 和 *y* 轴的方差甚至大于之前显示花瓣宽度和长度的散点图。因此，与不同种类的鸢尾相对应的点被尽可能广泛地展开，因此可以清楚地观察到种类的相对差异。

## 奇异值分解

与 PCA 密切相关的一个技术是**奇异值分解** ( **SVD** )。事实上，SVD 是一种比 PCA 更通用的技术，PCA 也试图改变矩阵的基础。

### 注意

在[http://arxiv.org/pdf/1404.1100.pdf](http://arxiv.org/pdf/1404.1100.pdf)有一个关于 PCA 及其与 SVD 关系的极好的数学描述。

顾名思义，SVD 将一个矩阵分解成三个相关的矩阵，通常称为 *U* 、*σ*(或 *S* )和 *V* 矩阵，这样:

![Singular value decomposition](graphics/7180OS_07_38.jpg)

如果 *X* 是 m×n 矩阵， *U* 是 m×m 矩阵，*σ*是 m×n 矩阵， *V* 是 n×n 矩阵。*σ*实际上是一个对角线矩阵，这意味着除了主对角线(左上到右下)上的单元格之外，所有单元格都为零。尽管很明显，它不必是正方形的。SVD 返回的矩阵的列按奇异值排序，最重要的维数排在最前面。因此，SVD 允许我们通过丢弃最不重要的维度来更近似地表示矩阵 *X* 。

例如，我们的 150×4 虹膜矩阵的分解将导致 150×150 的 *U* 、150×4 的*σ*和 4×4 的 *V* 。将这些矩阵相乘将产生我们的原始虹膜矩阵。

然而，我们可以选择只取前两个奇异值，并调整我们的矩阵，使得 *U* 为 150 x 2，*σ*为 2 x 2，而 *V* 为 2 x 4。让我们构造一个函数，该函数获取一个矩阵，并通过从每个 *U* 、*σ*和 *V* 矩阵中获取这个数量的列，将其投影到指定数量的维度中:

```
(defn project-into [matrix d]

  (let [svd (i/decomp-svd matrix)]

    {:U (i/$ (range d) (:U svd))

     :S (i/diag (take d (:S svd)))

     :V (i/trans

         (i/$ (range d) (:V svd)))}))
```

这里， *d* 是我们想要保留的维数。让我们用一个简单的例子来说明这一点，通过使用`s/sample-mvn`获取由 Incanter 生成的多元正态分布，并将其简化为一维:

```
(defn ex-7-31 []

  (let [matrix (s/sample-mvn 100

                             :sigma (i/matrix [[1 0.8]

                                               [0.8 1]]))]

    (println "Original" matrix)

    (project-into matrix 1)))

;; Original  A 100x2 matrix

;; :U  A 100x1 matrix

;; :S  A 1x1 matrix

;; :V  A 1x2 matrix
```

前一个例子的输出包含了数据的最重要的方面，这些方面被简化为一个维度。为了重建二维原始数据集的近似，我们可以简单地将三个矩阵相乘。在下面的代码中，我们将分布的一维近似投影回二维:

```
(defn ex-7-32 []

  (let [matrix (s/sample-mvn 100

                             :sigma (i/matrix [[1 0.8]

                                               [0.8 1]]))

        svd (project-into matrix 1)

        projection (i/mmult (:U svd)

                            (:S svd)

                            (:V svd))]

    (-> (c/scatter-plot (i/$ 0 matrix) (i/$ 1 matrix)

                        :x-label "x"

                        :y-label "y"

                        :series-label "Original"

                        :legend true)

        (c/add-points (i/$ 0 projection) (i/$ 1 projection)

                      :series-label "Projection")

        (i/view))))
```

这会产生以下图表:

![Singular value decomposition](graphics/7180OS_07_290.jpg)

请注意 SVD 如何保留了多元分布的主要特征，即强对角线，但折叠了非对角线点的方差。这样，SVD 保留了数据中最重要的结构，同时丢弃了不太重要的信息。希望前面的例子比 PCA 例子更清楚地表明，保留的特征不需要在原始数据中是显式的。在这个例子中，强对角线是数据的一个*潜在*特征。

### 注意

潜在特征是那些不能直接观察到的，但是可以从其他特征中推断出来的特征。有时，潜在特征指的是可以直接测量的方面，例如前一个例子中的相关性，或者在推荐的情况下，它们可以被认为代表潜在的偏好或态度。

观察了在早期合成数据上的奇异值分解的原理，让我们看看它在虹膜数据集上的表现:

```
(defn ex-7-33 []

  (let [svd (->> (d/get-dataset :iris)

                 (i/$ (range 4))

                 (i/to-matrix)

                 (i/decomp-svd))

        dims 2

        u (i/$     (range dims) (:U svd))

        s (i/diag  (take dims   (:S svd)))

        v (i/trans (i/$ (range dims) (:V svd)))

        projection (i/mmult u s v)]

    (-> (c/scatter-plot (i/$ (range 50) 0 projection)

                        (i/$ (range 50) 1 projection)

                        :x-label "Dimension 1"

                        :y-label "Dimension 2")

        (c/add-points (i/$ (range 50 100) 0 projection)

                      (i/$ (range 50 100) 1 projection))

        (c/add-points (i/$ [:not (range 100)] 0 projection)

                      (i/$ [:not (range 100)] 1 projection))

        (i/view))))
```

此代码生成以下图表:

![Singular value decomposition](graphics/7180OS_07_300.jpg)

在比较了 PCA 和 SVD 的 Iris 图之后，应该清楚这两种方法是密切相关的。这个散点图看起来很像我们之前看到的 PCA 图的倒置版本。

现在让我们回到电影推荐的问题上来，看看降维能有什么帮助。在下一节中，我们将利用 Apache Spark 分布式计算框架和相关的机器学习库 MLlib 来对降维数据执行电影推荐。



# 用 Apache Spark 和 MLlib 进行大规模机器学习

Spark 项目([https://spark.apache.org/](https://spark.apache.org/))是一个强调低延迟作业执行的集群计算框架。这是一个相对较新的项目，2009 年从加州大学伯克利分校的 AMP 实验室发展而来。

虽然 Spark能够与 Hadoop 共存(例如，通过连接到存储在 **Hadoop 分布式文件系统** ( **HDFS** )上的文件)，但它通过将大量计算保留在内存中来实现更快的作业执行时间。与 Hadoop 的两阶段 MapReduce 范式(在每次迭代之间将文件存储在磁盘上)相比，Spark 的内存中模型对于某些应用程序来说可以快几十倍或几百倍，特别是那些对数据执行多次迭代的应用程序。

在[第 5 章](ch05.html "Chapter 5. Big Data")、*大数据*中，我们发现了迭代算法对于在大量数据上实施优化技术的价值。这使得 Spark 成为大规模机器学习的绝佳选择。事实上， MLlib 库([https://spark.apache.org/mllib/](https://spark.apache.org/mllib/))是建立在 Spark 之上的，实现了各种开箱即用的机器学习算法。

我们不会在这里提供 Spark 的深入介绍，但会使用 Clojure 库，Sparkling(【https://github.com/gorillalabs/sparkling】)解释运行 Spark 作业所需的关键概念。Sparkling 将 Spark 的大部分功能包装在友好的 Clojure 接口之后。特别是，使用 thread-last 宏`->>`将 Spark 操作链接在一起，可以使用 Sparkling 编写的 Spark 作业看起来更像我们使用 Clojure 自己的序列抽象来处理数据的代码。

### 注意

一定要看看 Flambo，它使用线程优先的宏来链接任务:[https://github.com/yieldbot/flambo](https://github.com/yieldbot/flambo)。

我们将根据 MovieLens 的评分来制作推荐，所以第一步是用 Sparkling 加载这些数据。

## 用 Sparkling 加载数据

Spark 可以从受 Hadoop 支持的任何存储源加载数据，包括本地文件系统和 HDFS，以及其他数据源，如 Cassandra、HBase 和亚马逊 S3。让我们从基础开始，写一个简单统计收视率的工作。

MovieLens 评级存储为一个文本文件，可以使用`sparkling.core`名称空间中的`text-file`函数(在代码中称为`spark`)在 Sparkling 中加载。为了告诉 Spark 文件所在的位置，我们传递一个 URI，它可以指向一个远程源，比如`hdfs://..., s3n://....`，因为我们是在本地模式下运行 Spark，它可能只是一个本地文件路径。一旦我们有了文本文件，我们将调用`spark/count`来获得行数:

```
(defn count-ratings [sc]

  (-> (spark/text-file sc "data/ml-100k/ua.base")

      (spark/count)))

(defn ex-7-34 []

  (spark/with-context sc (-> (conf/spark-conf)

                             (conf/master "local")

                             (conf/app-name "ch7"))

    (count-ratings sc)))

;; 90570
```

如果您运行前面的示例，您可能会看到 Spark 的许多日志记录语句被打印到控制台。最后一行将是已经计算的计数。

注意，我们必须将一个 Spark 上下文作为第一个参数传递给`text-file`函数。Spark 上下文告诉 Spark 如何访问您的集群。最基本的配置指定了 Spark 主机的位置和 Spark 应该用于这项工作的应用程序名称。对于本地运行，Spark master 是`"local"`，这对基于 REPL 的交互开发很有用。

## 映射数据

Sparkling 提供了许多 Clojure 核心序列函数的类似物，比如 map、reduce 和 filter。在本章的开始，我们用`:item`、`:user`和`:rating`键将我们的评级存储为一个映射。虽然我们可以再次将数据解析到地图中，但是让我们将每个评级解析到一个`Rating`对象中。这将使我们在本章后面更容易与 MLlib 交互。

在`org.apache.spark.mllib.recommendation`包中定义了`Rating`类。构造函数接受三个数字参数:用户、项目和用户对项目的评分的表示。除了创建一个`Rating`对象，我们还计算时间对`10`取模，返回一个介于 0 和 9 之间的数字，并创建两个值的`tuple`:

```
(defn parse-rating [line]

  (let [[user item rating time] (->> (str/split line #"\t")

                                     (map parse-long))]

    (spark/tuple (mod time 10)

                 (Rating. user item rating))))

(defn parse-ratings [sc]

  (->> (spark/text-file sc "data/ml-100k/ua.base")

       (spark/map-to-pair parse-rating)))

(defn ex-7-35 []

  (spark/with-context sc (-> (conf/spark-conf)

                             (conf/master "local")

                             (conf/app-name "ch7"))

    (->> (parse-ratings sc)

         (spark/collect)

         (first))))

;; #sparkling/tuple [8 #<Rating Rating(1,1,5.0)>]
```

返回值是一个带有整数键(定义为时间模`10`)和评级值的元组。当我们将数据分成测试集和训练集时，拥有一个将数据分成十组的键将会很有用。

## 分布式数据集和元组

Spark 广泛使用元组来表示成对的键和值。在前面的例子中，键是一个整数，但这不是必需的——键和值可以是 Spark 可序列化的任何类型。

Spark 中的数据集用表示为**弹性分布式数据集** ( **RDDs** )。事实上，rdd 是 Spark 提供的核心抽象——跨集群中可以并行操作的所有节点划分的容错记录集合。rdd 有两种基本类型:表示任意对象序列的 rdd(比如由`text-file`返回的那种——行序列),以及表示键/值对序列的 rdd。

我们可以简单地在普通 rdd 和对 rdd 之间转换，这在前面的例子中用`map-to-pair`函数完成。由我们的`parse-rating`函数返回的元组指定了用于序列中每一对的键和值。与 Hadoop 一样，不要求数据集中的键是唯一的。事实上，正如我们将看到的，键通常是将相似的记录组合在一起的一种有用的方法。

## 过滤数据

现在，让我们根据键的值过滤我们的数据，并创建一个可用于训练的全部数据的子集。与同名的核心 Clojure 函数一样，Sparkling 提供了一个`filter`函数，该函数将只保留那些谓词返回逻辑 true 的行。

给定我们的评级对 RDD，我们可以只过滤那些键值小于 8 的评级。由于键大致均匀地分布在整数 0-9 之间，这将保留大约 80%的数据集:

```
(defn training-ratings [ratings]

  (->> ratings

       (spark/filter (fn [tuple]

                       (< (s-de/key tuple) 8)))

       (spark/values)))
```

我们的评级存储在一对 RDD 中，所以过滤的结果也是一对 RDD。我们对结果调用`values`,这样我们只剩下一个只包含`Rating`对象的普通 RDD。这将是我们传递给机器学习算法的 RDD。我们执行完全相同的过程，但是对于大于或等于 8 的键，要获得我们将使用的测试数据。

## 持久性和缓存

Spark 的动作很懒，在需要的时候才会计算。同样，一旦数据被计算出来，它就不会被显式缓存。有时，我们希望保留数据。特别是，如果我们正在运行一个迭代算法，我们不希望数据集在每次执行迭代时都从源重新计算。如果需要保存转换数据集的结果以供作业中的后续使用，Spark 提供了持久化 rdd 的能力。像 rdd 本身一样，持久性是容错的，这意味着如果任何分区丢失，它将使用最初创建它的转换重新计算。

我们可以使用`spark/persist`函数持久化一个 RDD，该函数期望我们通过 RDD，并且为我们的应用配置最合适的存储级别。在大多数情况下，这将是内存存储。但是，如果重新计算数据的计算成本很高，我们可以将数据溢出到磁盘，甚至跨磁盘复制缓存，以实现快速故障恢复。内存中是最常见的，所以 Sparkling 提供了`spark/cache`函数简写，它将在 RDD 上设置这个存储级别:

```
(defn ex-7-36 []

  (spark/with-context sc (-> (conf/spark-conf)

                             (conf/master "local")

                             (conf/app-name "ch7"))

    (let [ratings (spark/cache (parse-ratings sc))

          train (training-ratings ratings)

          test  (test-ratings ratings)]

      (println "Training:" (spark/count train))

      (println "Test:"     (spark/count test)))))

;; Training: 72806

;; Test: 8778
```

在前面的例子中，我们缓存了调用`parse-ratings`的结果。这意味着评级的加载和解析只执行一次，训练和测试评级功能都使用缓存的数据来过滤和执行它们的计数。对`cache`的调用优化了作业的性能，并允许 spark 避免不必要的重新计算数据。



# 使用 MLlib 在 Spark 上进行机器学习

我们已经介绍了足够多的 Spark now 基础知识，可以使用我们的 rdd 进行机器学习。虽然 Spark 处理基础设施，但执行机器学习的实际工作是由名为 MLlib 的 apache Spark 子项目处理的。

### 注意

MLlib 库的所有功能都在[https://spark.apache.org/docs/latest/mllib-guide.html](https://spark.apache.org/docs/latest/mllib-guide.html)上。

MLlib 提供了大量用于 Spark 的机器学习算法，包括本书其他地方涉及的回归、分类和聚类算法。在这一章中，我们将使用 MLlib 提供的算法来执行协同过滤:交替最小二乘法。

## 使用交替最小二乘法的电影推荐

在[第 5 章](ch05.html "Chapter 5. Big Data")、*大数据*中，我们发现了如何使用梯度下降来识别参数，以最小化大量数据的成本函数。在这一章中，我们已经看到了如何使用 SVD 通过分解来计算数据矩阵中的潜在因子。

**交替最小二乘** ( **ALS** )算法可以被认为是这两种方法的结合。它是一种迭代算法，使用最小二乘估计将用户-电影排名矩阵分解为两个潜在因素矩阵:用户因素和电影因素。

![Movie recommendations with alternating least squares](graphics/7180OS_07_310.jpg)

因此，交替最小二乘法基于这样的假设，即用户的评级是基于电影的一些潜在属性，这些潜在属性无法直接测量，但可以从评级矩阵中推断出来。前面的图表显示了如何将用户电影评级的稀疏矩阵分解为包含用户因素和电影因素的两个矩阵。该图仅将三个因素与每个用户和电影相关联，但是让我们通过仅使用两个因素来使它变得更加简单。

我们可以假设，所有的电影都存在于一个二维空间中，通过它们的动作、浪漫程度以及它们可能有多真实(或不真实)来识别。我们设想这样一个空间如下:

![Movie recommendations with alternating least squares](graphics/7180OS_07_320.jpg)

我们同样可以想象所有用户在一个等价的二维空间中代表，在那里他们的品味简单地表达为他们对**浪漫** / **动作**和**现实主义** / **逃避现实**的相对偏好。

一旦我们将所有电影和用户简化为他们的因素表示，预测问题就简化为一个简单的矩阵乘法——给定一部电影，我们对用户的预测评级就是他们的因素的乘积。ALS 面临的挑战是计算两个因子矩阵。

## ALS 带 Spark 和 MLlib

在撰写本文时，MLlib 库还没有 Clojure 包装器，所以我们将使用 Clojure 的互操作功能来直接访问。MLlib 对交替最小二乘法的实现由`org.apache.spark.mllib.recommendation`包中的 ALS 类提供。训练 ALS 几乎和用我们的 RDD 和提供的参数在类上调用`train`静态方法一样简单:

```
(defn alternating-least-squares [data {:keys [rank num-iter

                                              lambda]}]

  (ALS/train (to-mllib-rdd data) rank num-iter lambda 10))
```

稍微复杂一点的是,我们之前的 Sparkling job 返回的训练数据的 RDD 被表示为一个`JavaRDD`类型。MLlib 因为没有 Java API，所以期望接收标准的 Spark `RDD`类型。这两者之间的转换是一个足够简单的过程，尽管有些繁琐。以下函数在 RDD 类型之间来回转换；进入`RDDs`供 MLlib 消费，然后返回`JavaRDDs`用于发泡:

```
(defn to-mlib-rdd [rdd]

  (.rdd rdd))

(defn from-mlib-rdd [rdd]

  (JavaRDD/fromRDD rdd scala/OBJECT-CLASS-TAG))
```

`from-mllib-rdd`中的第二个参数是在`sparkling.scalaInterop`名称空间中定义的值。这是与 Scala 的函数定义生成的 JVM 字节码进行交互所必需的。

### 注意

关于 Clojure/Scala 互操作的更多信息，请参考 Tobias Kortkamp 在[http://t6.github.io/from-scala/](http://t6.github.io/from-scala/)的`scala`图书馆的优秀作品。

前面的样板文件已经完成，我们终于可以对培训评级执行 ALS 了。我们在下面的例子中这样做:

```
(defn ex-7-37 []

  (spark/with-context sc (-> (conf/spark-conf)

                             (conf/master "local")

                             (conf/app-name "ch7"))

    (-> (parse-ratings sc)

        (training-ratings)

        (alternating-least-squares {:rank 10

                                    :num-iter 10

                                    :lambda 1.0}))))
```

该函数接受几个参数— `rank`、`num-iter`和`lambda`，并返回一个 MLlib `MatrixFactorisationModel`函数。等级是用于因子矩阵的特征的数量。

## 用肌萎缩侧索硬化症做预测

一旦我们用计算出`MatrixFactorisationModel`，我们就可以用它来用`recommendProducts`方法进行预测。这将接收要推荐的用户的 ID 和要返回的推荐数量:

```
(defn ex-7-38 []

  (spark/with-context sc (-> (conf/spark-conf)

                             (conf/master "local")

                             (conf/app-name "ch7"))

    (let [options {:rank 10

                   :num-iter 10

                   :lambda 1.0}

          model (-> (parse-ratings sc)

                    (training-ratings )

                    (alternating-least-squares options))]

      (into [] (.recommendProducts model 1 3)))))

;; [#<Rating Rating(1,1463,3.869355232995907)>

;; #<Rating Rating(1,1536,3.7939806028920993)>

;; #<Rating Rating(1,1500,3.7130689437266646)>]
```

您可以看到，模型的输出和输入一样，都是`Rating`对象。它们包含用户 ID、商品 ID 和作为因子矩阵乘积计算的预测等级。让我们利用本章开始时定义的函数来给这些评级命名:

```
(defn ex-7-39 []

  (spark/with-context sc (-> (conf/spark-conf)

                             (conf/master "local")

                             (conf/app-name "ch7"))

    (let [items   (load-items "u.item")

          id->name (fn [id] (get items id))

          options {:rank 10

                   :num-iter 10

                   :lambda 1.0}

          model (-> (parse-ratings sc)

                    (training-ratings )

                    (alternating-least-squares options))]

      (->> (.recommendProducts model 1 3)

           (map (comp id->name #(.product %)))))))

;; ("Boys, Les (1997)" "Aiqing wansui (1994)"

;; "Santa with Muscles (1996)")
```

虽然还不太清楚这些是不是好的建议。为此，我们需要评估 ALS 模型的性能。

## 评估 ALS

与 Mahout 不同，Spark 不包含模型的内置评估器，所以我们必须自己编写。最简单的评估器之一，也是我们在本章已经使用过的评估器，是均方根误差(RMSE)评估器。

我们评估的第一步是使用模型来预测我们所有训练集的评级。Spark 的 ALS 实现包括一个我们可以使用的预测函数，它将接受一个包含所有用户 id 和商品 id 的 RDD，以返回对以下内容的预测:

```
(defn user-product [rating]

  (spark/tuple (.user rating)

               (.product rating)))

(defn user-product-rating [rating]

  (spark/tuple (user-product rating)

               (.rating rating))) 

(defn predict [model data]

  (->> (spark/map-to-pair user-product data)

       (to-mlib-rdd data)

       (.predict model)

       (from-mlib-rdd)

       (spark/map-to-pair user-product-rating)))
```

我们之前调用的`.recommendProducts`方法使用模型返回特定用户的产品推荐。相比之下，`.predict`方法将一次预测许多用户和项目的评分。

![Evaluating ALS](graphics/7180OS_07_330.jpg)

我们对`.predict`函数的调用的结果是一对 RDD，其中的关键是本身是一个用户和产品的元组。RDD 对的值就是预测的评级。

## 计算误差平方和

为了计算用户对产品的预测评分和实际评分之间的差异，我们需要根据匹配的用户/产品元组将`predictions`和`actuals`连接在一起。由于`predictions`和`actuals`rdd 中的键是相同的，我们可以简单地将它们都传递给 Sparkling 的`join`函数:

```
(defn squared-error [y-hat y]

  (Math/pow (- y-hat y) 2))

(defn sum-squared-errors [predictions actuals]

  (->> (spark/join predictions actuals)

       (spark/values)

       (spark/map (s-de/val-val-fn squared-error))

       (spark/reduce +)))
```

我们可以将整个`sum-squared-errors` 功能想象成以下流程，比较预测和实际评级:

![Calculating the sum of squared errors](graphics/7180OS_07_340.jpg)

一旦我们已经计算出`sum-squared-errors`，计算均方根就是简单地用它除以计数并求平方根:

```
(defn rmse [model data]

  (let [predictions  (spark/cache (predict model data))

        actuals (->> (spark/map-to-pair user-product-rating

                                        data)

                     (spark/cache))]

    (-> (sum-squared-errors predictions actuals)

        (/ (spark/count data))

        (Math/sqrt))))
```

`rmse`函数将采用一个模型和一些数据，并根据实际评级计算预测的 RMSE。在本章的前面，我们绘制了随着基于用户的推荐器的邻域大小的变化，RMSE 的不同值。现在让我们采用相同的技术，但是改变因子矩阵的秩:

```
(defn ex-7-40 []

  (spark/with-context sc (-> (conf/spark-conf)

                             (conf/master "local")

                             (conf/app-name "ch7"))

    (let [options {:num-iter 10 :lambda 0.1}

          training (-> (parse-ratings sc)

                       (training-ratings)

                       (spark/cache))

          ranks    (range 2 50 2)

          errors   (for [rank ranks]

                     (doto (-> (als training

                                    (assoc options :rank rank))

                               (rmse training))

                       (println "RMSE for rank" rank)))]

      (-> (c/scatter-plot ranks errors

                          :x-label "Rank"

                          :y-label "RMSE")

          (i/view)))))
```

前面的代码生成了以下图形:

![Calculating the sum of squared errors](graphics/7180OS_07_350.jpg)

观察当我们增加因素矩阵的等级时，我们的模型返回的评级如何变得越来越接近模型被训练的评级。随着因素矩阵维度的增长，它可以捕捉到更多个人用户评分的变化。

但是我们真正想做的是看看推荐器在测试集上的表现有多好——它还没有看到的数据。本章的最后一个示例`ex-7-41`，再次运行前面的分析，但是针对测试集而不是训练集测试模型的 RMSE。该示例生成以下绘图:

![Calculating the sum of squared errors](graphics/7180OS_07_355.jpg)

正如我们所希望的，随着因子矩阵的秩增加，预测的 RMSE 下降。更大的因子矩阵能够捕捉更多的隐藏在评级中的潜在特征，并且更准确地预测用户将给予电影的评级。



# 总结

在这一章中，我们已经讨论了很多内容。虽然主题主要是推荐系统，但是我们也讨论了降维并介绍了 Spark 分布式计算框架。

我们首先讨论了基于内容的方法和基于协同过滤的方法在推荐问题上的区别。在协同过滤的背景下，我们讨论了项目-项目推荐器并建立了一个 Slope One 推荐器。我们还讨论了用户-用户推荐器，并使用 Mahout 的各种相似性度量和评估器的实现来实现和测试几个基于用户的推荐器。评估的挑战提供了引入信息检索统计的机会。

在这一章中，我们花了很多时间讨论几种不同类型的降维。例如，我们了解了 Bloom filters 和 MinHash 提供的概率方法，以及主成分分析和奇异值分解提供的分析方法。虽然不是专门针对推荐系统的，但我们看到了如何使用这种技术来帮助实现更有效的相似性比较。

最后，我们介绍了分布式计算框架 Spark，并了解了交替最小二乘算法如何使用降维来发现评级矩阵中的潜在因素。我们使用 Spark、MLlib 和 Clojure 库 Sparkling 实现了 ALS 和 RMSE 评估器。

这一章我们学到的很多技术都是很一般的，下一章也不会有什么不同。我们将继续探索 Spark 和 Sparkling 库，学习网络分析:对连接和关系的研究。