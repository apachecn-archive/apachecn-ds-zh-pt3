

# 六、聚类

|   | 具有共同品质的事物总是很快寻找同类。 |   |
|   | 马库斯·奥勒留 |

在前面的章节中，我们介绍了多种学习算法:线性和逻辑回归、C4.5、朴素贝叶斯和随机森林。在每种情况下，我们都需要通过提供特征和期望的输出来训练算法。例如，在线性回归中，期望的输出是奥运会游泳运动员的体重，而对于其他算法，我们提供了一个类:乘客是幸存还是死亡。这些是 **监督学习算法**的例子:我们告诉我们的算法想要的输出，它将试图学习一个模型来再现它。

还有另一类学习算法，称为**无监督学习**。无监督算法能够在没有一组参考答案的情况下对数据进行操作。我们甚至不知道数据中的结构是什么；该算法将尝试自己确定结构。

聚类是无监督学习算法的一个例子。聚类分析的结果是在某些方面彼此更加相似的输入数据的分组。该技术是通用的:任何具有概念相似性或彼此距离的集合实体都可以被聚类。例如，我们可以根据共享关注者的相似性对社交媒体账户进行聚类，或者我们可以通过测量受访者对问卷的回答的相似性来对市场研究的结果进行聚类。

聚类的一个常见应用是识别共享相似主题的文档。这给我们提供了一个谈论文本处理的理想机会，本章将介绍各种专门处理文本的技术。

# 下载数据

这一章利用了**路透社-21578** 数据集:1987 年发表在路透社新闻专线上的一组古老的文章。它是最广泛使用的文本分类测试工具之一。Reuters-21578 集合中的文章和注释文本的版权归 Reuters Ltd .所有。Reuters Ltd .和 Carnegie Group，Inc .已同意仅出于研究目的免费分发该数据。

### 注意

你可以从 Packt Publishing 的网站或者从[https://github.com/clojuredatascience/ch6-clustering](https://github.com/clojuredatascience/ch6-clustering)下载本章的示例代码。

像往常一样，示例代码中有一个脚本，用于下载文件并将其解压缩到数据目录中。您可以使用以下命令从项目目录中运行它:

```

script/download-data.sh

```

或者，在撰写本文的时间，路透社的数据集可以从[http://KDD . ics . UCI . edu/databases/Reuters 21578/Reuters 21578 . tar . gz](http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.tar.gz)下载。本章的其余部分将假设文件已经下载并安装到项目的数据目录中。



# 提取数据

运行前面的脚本后，文章将被解压到目录`data/reuters-sgml`。摘录中的每个`.sgm`文件包含大约 1000 篇短文，这些短文使用标准通用标记语言(SGML)包装在 XML 风格的标签中。我们可以利用 Lucene 文本索引器中已经编写的解析器，而不是为格式编写自己的解析器。

```
(:import [org.apache.lucene.benchmark.utils ExtractReuters])

(defn sgml->txt [in-path out-path]

  (let [in-file  (clojure.java.io/file in-path)

        out-file (clojure.java.io/file out-path)]

    (.extract (ExtractReuters. in-file out-file))))
```

这里我们利用 Clojure 的 Java interop 简单地调用 Lucene 的`ExtractReuters`类上的 extract 方法。每篇文章都被提取为它自己的文本文件。

该代码可以通过执行以下命令来运行:

```

lein extract-reuters

```

在项目目录中的命令行上。输出将是一个新的目录，`data/reuters-text`，包含超过 20，000 个单独的文本文件。每个文件包含一篇路透社的文章。

如果你的磁盘空间不足，你现在可以删除`reuters-sgml`和`reuters21578.tar.gz`文件:在本章中我们将使用的唯一文件是`reuters-text`目录中的内容。现在来看几个。



# 检查数据

1987 年是“黑色星期一”的一年。10 月 19 日，全球股市崩盘，道琼斯工业平均指数下跌 508 点，至 1738.74 点。类似于`reut2-020.sgm-962.txt`中的文章描述了这一事件:

```
19-OCT-1987 16:14:37.57

WALL STREET SUFFERS WORST EVER SELLOFF

Wall Street tumbled to its worst point loss ever and the worst percentage decline since the First World War as a frenzy of stock selling stunned even the most bearish market participants. "Everyone is in awe and the word 'crash' is on everyone's mind," one trader said.     The Dow Jones industrial average fell 508 points to 1738, a level it has not been at since the Autumn of 1986\.     Volume soared to 603 mln shares, almost doubling the previous record of 338 mln traded just last Friday. Reuter &#3;
```

这篇文章的结构代表了语料库中的大多数文章。第一行是时间戳，表示文章发布的时间，后面是一个空行。这篇文章有一个标题，通常是大写，但不总是大写，然后是另一个空行。最后是文章正文。在处理这样的半结构化文本时，经常会出现多个空格、奇怪的字符和缩写。

其他文章只是标题，例如`reut2-020.sgm-761.txt`:

```
20-OCT-1987 17:09:34.49

REAGAN SAYS HE SEES NO RECESSION
```

这些是我们将要进行聚类分析的文件。



# 聚类文本

聚类是寻找彼此相似的对象组的过程。目标是一个集群中的对象应该比其他集群中的对象更相似。像分类一样，与其说它是一种特定的算法，不如说它是解决一般问题的一类通用算法。

尽管有各种各样的聚类算法，但所有算法都在某种程度上依赖于距离度量。对于确定两个对象属于相同还是不同聚类的算法，它必须能够确定它们之间的距离(或者，如果您喜欢，相似性)的定量度量。这需要用数字来衡量距离:距离越小，两个物体之间的相似性越大。

因为聚类是一种可以应用于不同数据类型的通用技术，所以有大量可能的距离度量。尽管如此，大多数数据都可以用一些常见的抽象概念来表示:集合、空间中的点或向量。对于其中的每一项，都有一个常用的衡量标准。

## 词集和 Jaccard 索引

如果你的数据可以被表示为一组事物，那么 Jaccard 指数，也被称为 **Jaccard 相似度**的，就可以被使用。这是概念上最简单的度量之一:它是集合交集除以集合并集，或者集合中唯一元素总数中共有元素的数量:

![Set-of-words and the Jaccard index](graphics/7180OS_06_01.jpg)

很多东西都可以用集合来表示。社交网络上的帐户可以表示为朋友或关注者的集合，客户可以表示为购买或查看的产品的集合。对于我们的文本文档，集合表示可以简单地是所使用的唯一单词的集合。

![Set-of-words and the Jaccard index](graphics/7180OS_06_100.jpg)

在 Clojure 中计算 Jaccard 指数非常简单:

```
(:require [clojure.set :as set])

(defn jaccard-similarity [a b]

  (let [a (set a)

        b (set b)]

    (/ (count (set/intersection a b))

       (count (set/union a b)))))

(defn ex-6-1 []

  (let [a [1 2 3]

        b [2 3 4]]

    (jaccard a b)))

;; => 1/2
```

它的优点是，集合不必具有相同的基数，距离度量才有意义。在前面的图中， **A** 比 **B** 要“大”,然而由并集划分的交集仍然是它们相似性的合理反映。为了将 Jaccard 索引应用于文本文档，我们需要将它们翻译成单词集。这就是**记号化**的过程。

## 标记路透社文件

标记化是一种技术的名称，这种技术将一串文本分割成更小的单元，以便进行分析。一种常见的方法是将一个文本字符串分割成单个单词。一个明显的分隔符是空格，这样`"tokens like these"`就变成了`["tokens" "like" "these"]`。

```
(defn tokenize [s]

  (str/split s #"\W+"))
```

这既方便又简单，但不幸的是，语言是微妙的，很少有简单的规则可以普遍适用。例如，我们的标记器将撇号视为空白:

```
(tokenize "doesn't handle apostrophes")

;; ["doesn" "t" "handle" "apostrophes"]
```

连字符也被视为空白:

```
(tokenize "good-looking user-generated content")

;; ["good" "looking" "user" "generated" "content"]
```

去掉它们会改变句子的意思。但是，并不是所有的连字符都应该保留:

```
(tokenize "New York-based")

;; ["New" "York" "based"]
```

术语`"New"`、`"York"`和`"based"`正确地代表了短语的主题，但是最好将`"New York"`组合成一个术语，因为它代表了一个特定的名称，并且确实应该完整地保存下来。另一方面，`York-based`本身就是一个毫无意义的象征。

简而言之，文本是杂乱的，从自由的 T2 文本中可靠地解析含义是一个极其丰富和活跃的研究领域。特别是，为了从文本中提取名称(例如`"New York"`)，我们需要考虑术语使用的上下文。根据语法功能标记一个句子中的标记的技术被称为**词性标记器**。

### 注意

有关高级标记化和词性标注的更多信息，请参见 https://github.com/dakrone/clojure-opennlp[的`clojure-opennlp`](https://github.com/dakrone/clojure-opennlp) 库。

在这一章中，我们有大量的文档，所以我们将继续使用我们简单的记号赋予器。我们会发现——尽管它有不足之处——它的表现足以从文档中提取意义。

让我们编写一个函数，从文件名中返回文档的令牌:

```
(defn tokenize-reuters [content]

  (-> (str/replace content  #"^.*\n\n" "")

      (str/lower-case)

      (tokenize)))

(defn reuters-terms [file]

  (-> (io/resource file)

      (slurp)

      (tokenize-reuters)))
```

我们从文件顶部删除了时间戳，并在标记之前将文本变成小写。在下一节中，我们将看到如何度量标记化文档之间的相似性。

### 对文档应用 Jaccard 索引

对我们的输入文档进行了标记后，我们可以简单地将得到的标记序列传递给之前定义的`jaccard-similarity`函数。让我们比较来自路透社语料库的几个文档的相似性:

```
(defn ex-6-2 []

  (let [a (set (reuters-terms "reut2-020.sgm-761.txt"))

        b (set (reuters-terms "reut2-007.sgm-750.txt"))

        s (jaccard a b)]

    (println "A:" a)

    (println "B:" b)

    (println "Similarity:" s)))

A: #{recession says reagan sees no he}

B: #{bill transit says highway reagan and will veto he}

Similarity: 1/4
```

Jaccard 索引输出一个介于 0 和 1 之间的数字，因此它根据这些文档标题中的单词判断它们有 25%的相似性。请注意，我们已经忘记了标题中单词的顺序。没有进一步的技巧，我们很快就会看到，Jaccard 索引只查看两个集合之间的共同项。我们丢失的另一个方面是一个术语在文档中出现的次数。多次重复同一个单词的文档在某种意义上可能认为这个单词更重要。例如，`reut2-020.sgm-932.txt`有这样一个标题:

```
19-OCT-1987 16:41:40.58

NYSE CHAIRMAN JOHN PHELAN SAYS NYSE WILL OPEN TOMORROW ON TIME
```

纽约证券交易所在标题中出现了两次。我们可以推断这个标题特别是关于纽约证券交易所的，也许比只提到过一次纽约证券交易所的标题更重要。

### 词袋和欧氏距离

相对于单词集方法的一个可能的改进是**单词袋方法**。这将保留文档中术语的字数。术语“计数”可以通过距离测量与结合起来，以获得潜在的更准确的结果。

最常见的距离概念之一是欧几里德距离度量。在几何学中，欧几里得度量是我们如何计算空间中两点之间的距离。在二维中，欧几里得距离由**毕达哥拉斯公式**给出:

![The bag-of-words and Euclidean distance](graphics/7180OS_06_02.jpg)

这将两点之间的差异表示为两点之间直线距离的长度。

![The bag-of-words and Euclidean distance](graphics/7180OS_06_110.jpg)

这可以扩展到三维空间:

![The bag-of-words and Euclidean distance](graphics/7180OS_06_03.jpg)

以及广义的到 *n* 维度:

![The bag-of-words and Euclidean distance](graphics/7180OS_06_04.jpg)

其中*A*T2 I 和*B*I 为 *A* 或 *B* 在尺寸 *i* 处的值。因此，距离度量是两个文档之间的总体相似性，其中考虑了每个单词出现的次数。

```
(defn euclidean-distance [a b]

  (->> (map (comp i/sq -) a b)

       (apply +)

       (i/sqrt)))
```

由于每个单词现在表示空间中的一个维度，我们需要确保当我们计算欧几里德距离度量时，我们正在比较每个文档的相同维度中的大小。否则，我们可能真的是在拿“苹果”和“橘子”做比较。

## 将文本表示为向量

与 Jaccard 索引不同，欧几里德距离依赖于单词在维度中的一致排序。字数，或者术语频率，代表了该文档在一个大的多维空间中的位置，我们需要确保当我们比较值时，我们在正确的维度上这样做。让我们用术语 **频率向量**来表示我们的文档。

想象一下，文档中出现的所有单词都被赋予了唯一的编号。例如，单词“苹果”可以被分配数字 53，单词“桔子”可以被分配数字 21，597。如果所有的数字都是唯一的，它们可以对应于一个词在词向量中出现的索引。

这些向量的维数可能非常大。可能的最大维数是向量的基数。对应于一个单词的索引处的元素的值通常是该单词在文档中出现的次数。这就是所谓的作为**项频率** ( **tf** )，权重。

为了能够比较文本向量，相同的单词总是出现在向量的相同索引处是很重要的。这意味着我们必须为我们创建的每个向量使用相同的单词/索引映射。这个单词/索引映射就是我们的字典。

## 创建字典

为了创建一个有效的字典，我们需要确保两个单词的索引不冲突。做到这一点的一种方法是让一个单调递增的计数器随着每个添加到字典中的单词而递增。添加单词时的计数成为该单词的索引。为了以线程安全的方式向字典中添加单词和增加计数器，我们可以使用 atom:

```
(def dictionary

  (atom {:count 0

         :words {}}))

(defn add-term-to-dict [dict word]

  (if (contains? (:terms dict) word)

    dict

    (-> dict

        (update-in [:terms] assoc word (get dict :count))

        (update-in [:count] inc))))

(defn add-term-to-dict! [dict term]

  (doto dict

    (swap! add-term-to-dict term)))
```

为了更新一个原子，我们必须在一个`swap!`函数中执行我们的代码。

```
(add-term-to-dict! dictionary "love")

;; #<Atom@261d1f0a: {:count 1, :terms {"love" 0}}>
```

添加另一个字将导致计数增加:

```
(add-term-to-dict! dictionary "music")

;; #<Atom@261d1f0a: {:count 2, :terms {"music" 1, "love" 0}}>
```

并且将同一个单词添加两次不会有任何效果:

```
(add-term-to-dict! dictionary "love")

;; #<Atom@261d1f0a: {:count 2, :terms {"music" 1, "love" 0}}>
```

在 atom 内部执行这个更新可以确保每个单词都有自己的索引，即使字典被多个线程同时更新。

```
(defn build-dictionary! [dict terms]

  (reduce add-term-to-dict! dict terms))
```

构建一个完整的字典非常简单，只需将我们的`add-term-to-dict!`函数简化为包含一组术语的字典原子。



# 创建词频向量

为了计算欧几里德距离，让我们首先从字典和文档中创建一个向量。这将允许我们容易地比较文档之间的术语频率，因为它们将占据向量的相同索引。

```
(defn term-id [dict term]

  (get-in @dict [:terms term]))

(defn term-frequencies [dict terms]

  (->> (map #(term-id dict %) terms)

       (remove nil?)

       (frequencies)))

(defn map->vector [dictionary id-counts]

  (let [zeros (vec (replicate (:count @dictionary) 0))]

    (-> (reduce #(apply assoc! %1 %2) (transient zeros) id-counts)

        (persistent!))))

(defn tf-vector [dict document]

  (map->vector dict (term-frequencies dict document)))
```

`term-frequencies`函数为文档中的每个术语创建一个术语 ID 到频率计数的映射。`map->vector`函数简单地获取这个映射，并在由术语 ID 给出的向量索引处关联频率计数。由于可能有很多项，而且向量可能很长，为了提高效率，我们使用 Clojure 的`transient!`和`persistent!`函数临时创建一个可变向量。

让我们为`reut2-020.sgm-742.txt`打印文档、字典和结果向量:

```
(defn ex-6-3 []

  (let [doc  (reuters-terms "reut2-020.sgm-742.txt")

        dict (build-dictionary! dictionary doc)]

    (println "Document:" doc)

    (println "Dictionary:" dict)

    (println "Vector:" (tf-vector dict doc))))
```

输出如下所示(为便于阅读，对格式进行了调整):

```
;; Document: [nyse s phelan says nyse will continue program

;;            trading curb until volume slows]

;; Dictionary: #<Atom@bb156ec: {:count 12, :terms {s 1, curb 8,

;;             phelan 2, says 3, trading 7, nyse 0, until 9,

;;             continue 5, volume 10, will 4, slows 11,

;;             program 6}}>

;; Vector: [2 1 1 1 1 1 1 1 1 1 1 1]
```

输入中有 12 个术语，字典中有 12 个术语，并且返回 12 个元素的向量。

```
(defn print-distance [doc-a doc-b measure]

  (let [a-terms (reuters-terms doc-a)

        b-terms (reuters-terms doc-b)

        dict (-> dictionary

                 (build-dictionary! a-terms)

                 (build-dictionary! b-terms))

        a (tf-vector dict a-terms)

        b (tf-vector dict b-terms)]

    (println "A:" a)

    (println "B:" b)

    (println "Distance:" (measure a b))))

(defn ex-6-4 []

  (print-distance "reut2-020.sgm-742.txt"

                  "reut2-020.sgm-932.txt"

                  euclidean-distance))

;; A: [2 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0]

;; B: [2 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1]

;; Distance: 3.7416573867739413
```

和 Jaccard 指数一样，欧氏距离不能降到零以下。不过，与 Jaccard 指数不同的是，该指数的价值可以无限增长。

## 向量空间模型和余弦距离

向量空间模型可以被认为是单词集和单词包模型的推广。像单词袋模型一样，向量空间模型将每个文档表示为一个向量，其中的每个元素表示一个术语。每个索引处的值是对单词重要性的度量，它可能是也可能不是术语频率。

如果您的数据在概念上表示一个向量(也就是说，一个特定方向上的大小)，那么余弦距离可能是最合适的选择。余弦距离度量将两个元素的相似性确定为它们的矢量表示之间的角度的余弦。

![The vector space model and cosine distance](graphics/7180OS_06_120.jpg)

如果两个向量都指向同一个方向，那么它们之间的角度将为零，并且零的余弦为 1。余弦相似度可以用以下方式定义:

![The vector space model and cosine distance](graphics/7180OS_06_05.jpg)

这是一个比我们之前讨论过的方程更复杂的方程。它依赖于计算两个向量的点积和各自的大小。

```
(defn cosine-similarity [a b]

  (let [dot-product (->> (map * a b)

                         (apply +))

        magnitude (fn [d]

                    (->> (map i/sq d)

                         (apply +)

                         (i/sqrt)))]

    (/ dot-product (* (magnitude a) (magnitude b)))))
```

余弦相似度的例子如下所示:

![The vector space model and cosine distance](graphics/7180OS_06_130.jpg)

余弦相似性通常用作高维空间中的相似性度量，其中每个向量包含许多零，因为它可以非常有效地进行评估:只需要考虑非零维度。由于大多数文本文档只使用所有单词的一小部分(因此对于大部分维度来说是零)，余弦度量通常用于文本聚类。

在向量空间模型中，我们需要一个一致的策略来衡量每一项的重要性。在单词集模型中，所有的术语都被同等地计数。这相当于将该点的向量值设置为 1。在词袋模型中，术语频率被计算在内。我们现在将继续使用术语频率，但是我们将很快看到如何使用一个更复杂的重要性度量，称为**术语频率-逆文档频率** ( **TF-IDF** )。

```
(defn ex-6-5 []

  (print-distance "reut2-020.sgm-742.txt"

                  "reut2-020.sgm-932.txt"

                  cosine-similarity))

;; A: [2 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0]

;; B: [2 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1]

;; Distance: 0.5012804118276031
```

余弦值越接近`1`，两个实体越相似。要将`cosine-similarity`转换为距离度量，我们可以简单地从`1`中减去`cosine-similarity`。

尽管前面提到的所有度量对相同的输入产生不同的度量，但是它们都满足约束条件，即 *A* 和 *B* 之间的距离应该与 *B* 和 *A* 之间的差相同。通常，相同的底层数据可以被转换来表示一个集合(Jaccard)、空间中的一个点(Euclidean)或一个向量(Cosine)。有时候，知道哪个是正确的唯一方法是尝试一下，看看结果有多好。

出现在一个文档中的唯一单词的数量与出现在正在处理的集合中的任何文档中的唯一单词的数量相比通常很少。因此，这些高维文档向量相当稀疏。

## 删除停用词

标题之间的许多相似之处是由经常出现的单词产生的，这些单词并没有给内容增加太多的意义。例子有“一”、“说”和“和”。我们应该过滤掉这些，以避免产生虚假的相似性。

考虑以下两个习惯用法:

*   `"Music is the food of love"`
*   `"War is the locomotive of history"`

我们可以使用下面的 Clojure 代码计算它们之间的余弦相似性:

```
(defn ex-6-6 []

  (let [a (tokenize "music is the food of love")

        b (tokenize "war is the locomotive of history")]

    (add-documents-to-dictionary! dictionary [a b])

    (cosine-similarity (tf-vector dictionary a)

                       (tf-vector dictionary b))))

;; 0.5
```

这两个文档显示了`0.5`的相似性，尽管事实上它们仅有的共同单词是`is`、`the`和`of`。理想情况下，我们会想要删除这些。

## 词干

现在让我们考虑另一个短语:

*   `"Music is the food of love"`
*   `"It's lovely that you're musical"`

让我们也比较一下它们的余弦相似性:

```
(defn ex-6-7 []

  (let [a (tokenize "music is the food of love")

        b (tokenize "it's lovely that you're musical")]

    (add-documents-to-dictionary! dictionary [a b])

    (cosine-similarity (tf-vector dictionary a)

                       (tf-vector dictionary b))))

;; 0.0
```

尽管这两个句子指的是音乐和积极的情感，但这两个短语的余弦相似度为零:这两个短语之间没有共同的单词。这是有意义的，但并没有表达我们通常想要的行为，即捕捉“概念”之间的相似性，而不是所使用的精确单词。

解决这个问题的一个方法是对单词进行词干处理，这样可以将它们还原到它们的根源。意思相同的单词更有可能源于同一个词根。Clojure 库词干分析器([https://github.com/mattdw/stemmers](https://github.com/mattdw/stemmers))将为我们做这件事，幸运的是，他们也将删除停用词。

```
(defn ex-6-8 []

  (let [a (stemmer/stems "music is the food of love")

        b (stemmer/stems "it's lovely that you're musical")]

    (add-documents-to-dictionary! dictionary [a b])

    (cosine-similarity (tf-vector dictionary a)

                       (tf-vector dictionary b))))

;; 0.8164965809277259
```

好多了。在去除词干和停用词之后，短语之间的相似度从 0.0 下降到 0.82。这是一个好的结果，因为尽管句子使用了不同的词，但它们表达的情感是相关的。



# 利用 k-means 和咒语进行聚类

最后，在对我们的输入文档进行了标记化、词干化和矢量化之后——并且有了可供选择的距离度量——我们就可以对数据进行聚类了。我们要看的第一个聚类算法叫做 *k 均值聚类*。

*k* -means 是一种迭代算法，过程如下:

1.  随机选取 *k 个*簇质心。
2.  将每个数据点分配给质心最近的聚类。
3.  将每个聚类质心调整为其指定数据点的平均值。
4.  重复直到收敛或达到最大迭代次数。

下图显示了 *k=3* 个集群的过程:

![Clustering with k-means and Incanter](graphics/7180OS_06_140.jpg)

在上图中，我们可以看到迭代 1 时的初始聚类质心没有很好地表示数据的结构。虽然这些点清楚地排列在三组中，但初始质心(用十字表示)都分布在图形的顶部区域周围。这些点根据它们最近的质心进行着色。随着迭代的进行，我们可以看到簇质心是如何移动到更接近它们在每组点中心的“自然”位置的。

在我们定义主 *k* -means 函数之前，先定义几个实用函数是有用的:一个函数计算一个聚类的质心，一个函数将数据分组到各自的聚类中。

```
(defn centroid [xs]

  (let [m (i/trans (i/matrix xs))]

    (if (> (i/ncol m) 1)

      (i/matrix (map s/mean m))

     m)))

(defn ex-6-9 []

  (let [m (i/matrix [[1 2 3]

                     [2 2 5]])]

    (centroid m)))

;; A 3x1 matrix

;;  -------------

;; 1.50e+00

;; 2.00e+00

;; 4.00e+00
```

`centroid`函数简单地计算输入矩阵每一列的平均值。

```
(defn clusters [cluster-ids data]

  (->> (map vector cluster-ids data)

       (conj-into {})

       (vals)

       (map i/matrix))) 

(defn ex-6-10 []

  (let [m (i/matrix [[1 2 3]

                     [4 5 6]

                     [7 8 9]])]

    (clusters [0 1 0] m)))

;; A 1x3 matrix

;; -------------

;; 4.00e+00  5.00e+00  6.00e+00 

;;  A 2x3 matrix

;; -------------

;; 7.00e+00  8.00e+00  9.00e+00 

;; 1.00e+00  2.00e+00  3.00e+00
```

`clusters`函数根据提供的集群 id 将一个较大的矩阵分割成一系列较小的矩阵。聚类 ID 作为与聚类点长度相同的元素序列提供，列出序列中该索引处的点的聚类 ID。共享一个公共簇 ID 的项目将被分组在一起。有了这两个函数，下面是完成的`k-means`函数:

```
(defn k-means [data k]

  (loop [centroids (s/sample data :size k)

         previous-cluster-ids nil]

    (let [cluster-id (fn [x]

                       (let [distance  #(s/euclidean-distance x %)

                             distances (map distance centroids)]

                         (->> (apply min distances)

                              (index-of distances))))

          cluster-ids (map cluster-id data)]

      (if (not= cluster-ids previous-cluster-ids)

        (recur (map centroid (clusters cluster-ids data))

               cluster-ids)

        clusters))))
```

我们首先通过对输入数据进行采样来选取`k`个随机簇质心。然后，我们使用 loop/recurve 继续更新簇质心，直到`previous-cluster-ids`与`cluster-ids`相同。此时，没有文档移动集群，因此集群已经收敛。

## 聚类路透社文档

现在让我们使用我们的`k-means`函数对路透社文档进行聚类。让我们从简单的算法开始，选择一个小样本的大文档。较大的文档将使算法更有可能确定它们之间有意义的相似性。让我们将最小阈值设置为 500 个字符。这意味着至少我们的输入文档会有一个标题和几个句子的正文。

```
(defn ex-6-11 []

  (let [documents (fs/glob "data/reuters-text/*.txt")

        doc-count 100

        k 5

        tokenized (->> (map slurp documents)

                       (remove too-short?)

                       (take doc-count)

                       (map stem-reuters))]

    (add-documents-to-dictionary! dictionary tokenized)

    (-> (map #(tf-vector dictionary %) tokenized)

        (k-means k))))
```

我们使用`fs`库(【https://github.com/Raynes/fs】T4)来创建一个文件列表，通过使用匹配所有文本文件的模式调用`fs/glob`来执行我们的聚类。我们删除那些太短的，标记前 100 个，并将它们添加到字典中。我们为输入创建`tf`向量，然后对它们调用`k-means`。

如果您运行前面的例子，您将收到一个聚集的文档向量列表，这不是很有用。让我们创建一个`summary`函数，它使用字典来报告每个集群中最常见的术语。

```
(defn cluster-summary [dict clusters top-term-count]

  (for [cluster clusters]

    (let [sum-terms (if (= (i/nrow cluster) 1)

                      cluster

                      (->> (i/trans cluster)

                           (map i/sum)

                           (i/trans)))

          popular-term-ids (->> (map-indexed vector sum-terms)

                                (sort-by second >)

                                (take top-term-count)

                                (map first))

          top-terms (map #(id->term dict %) popular-term-ids)]

      (println "N:" (i/nrow cluster))

      (println "Terms:" top-terms))))

(defn ex-6-12 []

  (cluster-summary dictionary (ex-6-11) 5))
```

*k* -means 本质上是一种随机算法，对质心的起始位置敏感。我得到以下输出，但您的输出几乎肯定会有所不同:

```
;; N: 2

;; Terms: (rocket launch delta satellit first off weather space)

;;  N: 4

;; Terms: (said will for system 000 bank debt from bond farm)

;; N: 12

;; Terms: (said reuter for iranian it iraq had new on major)

;; N: 62

;; Terms: (said pct dlr for year mln from reuter with will)

;; N: 20

;; Terms: (said for year it with but dlr mln bank week)
```

不幸的是，我们似乎没有得到很好的结果。第一组包含两篇关于火箭和太空的文章，第三组似乎由关于伊朗的文章组成。大多数文章中最流行的词是“说”。



# 使用 TF-IDF 实现更好的聚类

**术语** **频率-逆文档频率** ( **TF-IDF** )是一种对文档向量中的术语进行加权的通用方法，以便在整个数据集中流行的术语不会像不太常用的术语那样被高度加权。这抓住了一个直觉信念——也是我们之前观察到的——像“说”这样的词并不是构建集群的坚实基础。

## 齐夫定律

Zipf 定律指出，任何单词的频率与其在频率表中的排名成反比。因此，最频繁出现的单词的出现频率大约是第二频繁出现的单词的两倍，是第二频繁出现的单词的三倍，依此类推。让我们看看这是否适用于我们的路透社语料库:

```
(defn ex-6-13 []

  (let [documents (fs/glob "data/reuters-text/*.txt")

        doc-count 1000

        top-terms 25

        term-frequencies (->> (map slurp documents)

                              (remove too-short?)

                              (take doc-count)

                              (mapcat tokenize-reuters)

                              (frequencies)

                              (vals)

                              (sort >)

                              (take top-terms))]

    (-> (c/xy-plot (range (inc top-terms)) term-frequencies

                   :x-label "Terms"

                   :y-label "Term Frequency")

        (i/view))))
```

使用前面的代码，我们可以计算出前 1000 个路透社文档中前 25 个最常用术语的频率图。

![Zipf's law](graphics/7180OS_06_150.jpg)

在前 1000 个文档中，最流行的术语出现了近 10000 次。第 25 个^(最受欢迎的 T2 词汇总共出现了大约 1000 次。事实上，数据显示，单词在路透社语料库中出现的频率比它们在频率表中的位置更高。这很可能是由于路透社语料库的公告性质，它倾向于重复使用相同的短词。)

## 计算 TF-IDF 重量

计算 TF-IDF 只需要对我们已经创建的代码做两处修改。首先，我们必须跟踪一个给定术语出现在多少个文档中。其次，在构建文档向量时，我们必须对术语进行适当的加权。

既然我们已经创建了一个术语字典，我们也可以在那里存储每个术语的文档频率。

```
(defn inc-df! [dictionary term-id]

  (doto dictionary

    (swap! update-in [:df term-id] (fnil inc 0))))

(defn build-df-dictionary! [dictionary document]

  (let [terms    (distinct document)

        dict     (build-dictionary! dictionary document)

        term-ids (map #(term-id dictionary %) document)]

    (doseq [term-id term-ids]

      (inc-df! dictionary term-id))

    dict))
```

`build-df-dictionary`函数早先接受一个字典和一系列术语。我们从不同的术语中构建字典，并查找每个术语的`term-id`。最后，我们迭代术语 id，并为每个 id 增加`:df`。

如果一个文档有单词 *w* [1] ，…， *w* [n] ，那么单词 *w* [i] 的逆文档频率定义为:

![Calculating the TF-IDF weight](graphics/7180OS_06_06.jpg)

也就是它出现在文档中的数量的倒数。如果一个单词在一组文档中经常出现，则其 *DF* 值较大，而其 *IDF* 值较小。对于大量的文档，通常通过将 *IDF* 值乘以一个常数(通常是文档计数 *N* )来归一化该值，因此 *IDF* 等式如下所示:

![Calculating the TF-IDF weight](graphics/7180OS_06_07.jpg)

单词 *w* [i] 的 TF-IDF 权重 *W* [i] 由词频和逆文档频的乘积给出:

![Calculating the TF-IDF weight](graphics/7180OS_06_08.jpg)

然而，前面等式中的 *IDF* 值仍然不理想，因为对于大型语料库来说， *IDF* 项的范围通常比 *TF* 大得多，并且可能压倒其效果。为了减少这个问题，并且平衡 *TF* 和 *IDF* 项的权重，通常的做法是使用 *IDF* 值的对数来代替:

![Calculating the TF-IDF weight](graphics/7180OS_06_09.jpg)

这样， TF-IDF 权重*w*I 换一个字*w*I 就变成了:

![Calculating the TF-IDF weight](graphics/7180OS_06_10.jpg)

这是一个典型的 TF-IDF 加权:常用词被赋予较小的权重，不经常出现的词被赋予较大的权重。确定文档主题的重要单词通常具有高的 *TF* 和适度大的 *IDF* ，因此两者的乘积成为大值，从而在结果向量中赋予这些单词更多的重要性。

```
(defn document-frequencies [dict terms]

  (->> (map (partial term-id dict) terms)

       (select-keys (:df @dict))))

(defn tfidf-vector [dict doc-count terms]

  (let [tf (term-frequencies dict terms)

        df (document-frequencies dict (distinct terms))

        idf   (fn [df] (i/log (/ doc-count df)))

        tfidf (fn [tf df] (* tf (idf df)))]

    (map->vector dict (merge-with tfidf tf df))))
```

前面的代码根据之前定义的`term-frequencies`和从字典中提取的 `document-frequencies`计算 TF-IDF。

## 使用 TF-IDF 的 k 均值聚类

随着前面的调整到位，我们可以为路透社的文件计算 TF-IDF 向量。以下示例是使用新的`tfidf-vector`函数对`ex-6-12`的修改:

```
(defn ex-6-14 []

  (let [documents (fs/glob "data/reuters-text/*.txt")

        doc-count 100

        k 5

        tokenized (->> (map slurp documents)

                       (remove too-short?)

                       (take doc-count)

                       (map stem-reuters))]

    (reduce build-df-dictionary! dictionary tokenized)

    (-> (map #(tfidf-vector dictionary doc-count %) tokenized)

        (k-means k)

        (cluster-summary dictionary 10))))
```

前面的代码与前面的例子非常相似，但是我们替换了新的`build-df-dictionary`和`tfidf-vector`函数。如果您运行该示例，您应该会看到看起来比以前好一点的输出:

```
N: 5

Terms: (unquot unadjust year-on-year novemb grew sundai labour m-3 ahead 120)

N: 15

Terms: (rumor venezuela azpurua pai fca keat ongpin boren gdp moder)

N: 16

Terms: (regan drug lng soviet bureau deleg gao dean fdic algerian)

N: 46

Terms: (form complet huski nrc rocket north underwrit card oat circuit)

N: 18

Terms: (freez cocoa dec brown bean sept seixa telex argentin brown-forman)
```

虽然顶部的单词可能很难解释，因为它们已经被词干化，但是这些代表了每个聚类中最不寻常的常见单词。请注意,“said”不再是所有集群中评分最高的单词。

## 使用 n 元语法进行更好的聚类

从前面的单词列表中可以清楚地看到，将我们的文档简化为无序的术语序列牺牲了多少。如果没有句子的上下文，很难对每一个簇的内容有一个模糊的认识。

然而，在向量空间模型中没有任何固有的东西排除了对我们的输入记号的顺序的维护。我们可以简单地创造一个新的术语来表示单词的组合。组合词，可能依次代表几个输入单词，被称为 ***n* -gram** 。

一个 *n* 字母的例子可能是“纽约”，或者“股票市场”。事实上，因为它们包含两个术语，这些被称为**。*n*-克数可以是任意长度。一个 *n* 字母越长，它承载的上下文就越多，但也越稀有。**

***n*——克与**缩**的概念密切相关。当我们堆砌我们的*n*g 时，我们正在创造重叠的术语序列。术语“挤挤”来自术语像屋顶瓦片一样重叠的方式。**

```
(defn n-grams [n words]

  (->> (partition n 1 words)

       (map (partial str/join " ")))) 

(defn ex-6-15 []

  (let [terms (reuters-terms "reut2-020.sgm-761.txt")]

    (n-grams 2 terms)))

;; ("reagan says" "says he" "he sees" "sees no" "no recession")
```

**例如，使用 2-grams 将允许我们在数据集中区分单词“椰子”的以下用法:“椰子油”、“椰子种植者”、“椰子种植园”、“椰子农民”、“椰子协会”、“椰子管理局”、“椰子产品”、“椰子出口”、“椰子产业”，以及相当讨人喜欢的“椰子酋长”。每一对单词定义了一个不同的概念——有时略有不同——我们可以在文档中捕捉和比较这些概念。**

**通过组合多个长度的 *n* 克的结果，我们可以利用 *n* 克和 shingling 获得两个世界的最佳结果:**

```
(defn multi-grams [n words]

  (->> (range 1 (inc n))

       (mapcat #(n-grams % words))))

(defn ex-6-16 []

  (let [terms (reuters-terms "reut2-020.sgm-761.txt")]

    (multi-grams 4 terms)))

;; ("reagan" "says" "he" "sees" "no" "recession" "reagan says" 

;; "says he" "he sees" "sees no" "no recession" "reagan says he" 

;; "says he sees" "he sees no" "sees no recession" "reagan says he 

;; sees" "says he sees no" "he sees no recession")
```

**虽然词干和停用词的去除具有减小我们的字典大小的效果，并且使用 TF-IDF 具有提高文档中每个术语的权重效用的效果，但是产生 *n* -grams 具有大规模扩展我们需要容纳的术语数量的效果。**

**这种特性的爆炸将会立即淹没我们的*k*-在咒语中的意思。幸运的是，有一个名为 **Mahout** 的机器学习库，它是专门设计来运行诸如 *k* 之类的算法的——意思是对大量数据进行处理。**

**

# 用 Mahout 进行大规模聚类

mahout([http://mahout.apache.org/](http://mahout.apache.org/))是一个用于分布式计算环境的机器学习库。该库的 0.9 版本面向 Hadoop，也是我们将在这里使用的版本。

### 注意

在撰写本文时，Mahout 0.10 刚刚发布，目标也是 Spark。Spark 是另一种分布式计算框架，我们将在下一章介绍。

我们在前一章看到，Hadoop 的一个抽象是序列文件:Java 键和值的二进制表示。Mahout 的许多算法都希望对序列文件进行操作，我们需要创建一个序列文件作为 Mahout 的 *k* -means 算法的输入。Mahout 的 *k* -means 算法也希望将它的输入作为一个向量接收，由 Mahout 的向量类型之一表示。

虽然 Mahout 包含了从文本中提取向量的类和实用程序，但我们将借此机会演示如何一起使用 Parkour 和 Mahout。我们不仅可以对创建的向量进行更细粒度的控制，还可以展示更多 Parkour 指定 Hadoop 作业的功能。

## 将文本文件转换成序列文件

我们不会定义一个自定义作业来将我们的文本文档转换成序列文件表示，尽管:Mahout 已经定义了一个有用的`SequenceFilesFromDirectory`类来转换文本文件的目录。我们将使用它来创建一个表示`reuters-txt`目录全部内容的文件。

虽然序列文件在物理上可能存储在不同的块中(例如，在 HDFS 上)，但它在逻辑上是一个文件，将所有输入文档表示为键/值对。键是文件的名称，值是文件的文本内容。

![Converting text documents to a sequence file](graphics/7180OS_06_160.jpg)

以下代码将处理转换:

```
(:import [org.apache.mahout.text

           SequenceFilesFromDirectory])

(defn text->sequencefile [in-path out-path]

  (SequenceFilesFromDirectory/main

   (into-array String (vector "-i" in-path

                              "-o" out-path

                              "-xm" "sequential"

                              "-ow"))))

(defn ex-6-17 []

  (text->sequencefile "data/reuters-text"

                      "data/reuters-sequencefile"))
```

是一个 Mahout 实用程序类，是设计用于命令行调用的一套类的一部分。

### Tip

由于运行前面的示例是后续示例的先决条件，因此也可以在命令行上运行:

```

lein create-sequencefile

```

我们直接调用的`main`函数，将原本在命令行中传递的参数作为字符串数组传递。

## 使用跑酷创建驯象矢量

既然我们已经有了 Reuters 语料库的序列文件表示，我们需要将每个文档(现在表示为单个键/值对)转换成一个向量。我们在前面看到了如何使用一个被建模为 Clojure 原子的共享字典来实现这一点。atom 确保即使在多线程环境中，每个不同的术语都有自己的 ID。

我们将使用跑酷和 Hadoop 来生成我们的向量，但这提出了一个挑战。当 MapReduce 编程的本质是映射器并行操作并且不共享任何状态时，我们如何给每个单词分配唯一的 ID？Hadoop 没有提供 Clojure 原子的等效物来在集群中的节点间共享可变状态，事实上，最小化共享状态是扩展分布式应用的关键。

因此，创建一组共享的惟一 id 对我们的跑酷工作来说是一个有趣的挑战:让我们看看如何以分布式方式为我们的字典生成惟一 id。

## 创建分布式唯一 id

然而，在我们研究特定于 Hadoop 的解决方案之前，值得注意的是，创建集群范围唯一标识符的一个简单方法是创建通用唯一标识符，或 UUID。

```
(defn uuid []

  (str (java.util.UUID/randomUUID)))
```

这创建了一个长的字节串:`3a65c7db-6f41-4087-a2ec-8fe763b5f185`形式，这实际上保证了不会与世界上任何其他地方生成的任何其他 UUID 冲突。

虽然这有助于生成唯一的 id，但是可能的 id 数量非常大，Mahout 的稀疏向量表示需要用表示为整数的向量的基数来初始化。用`uuid`生成的 id 太大了。此外，它不能帮助我们协调 id 的创建:集群中的每台机器都会生成不同的 UUIDs 来表示相同的术语。

解决这个问题的一个方法是使用术语本身来生成一个唯一的 ID。如果我们使用一致的散列函数从每个输入项中创建一个整数，那么集群中的所有机器都会生成相同的 ID。因为一个好的散列函数很可能为唯一的输入项产生唯一的输出，所以这种技术很可能工作得很好。会有一些哈希冲突(两个单词哈希到同一个 ID)，但这应该占整体的一小部分。

### 注意

散列特征本身以创建唯一 ID 的方法通常被称为“散列技巧”。尽管它通常用于文本矢量化，但也可以应用于涉及大量要素的任何问题。

然而，产生在整个集群中唯一的不同 id 的挑战给我们提供了一个机会来讨论 Parkour 展示的 Hadoop 的一个有用特性:分布式缓存。

## 使用 Hadoop 分发唯一 id

让我们考虑一下，如果我们要计算唯一的、集群范围的 id，我们的跑酷映射器和缩减器可能会是什么样子。映射器很简单:我们希望计算我们遇到的每个术语的文档频率，所以下面的映射器简单地返回每个唯一术语的向量:向量的第一个元素(键)是术语本身，第二个元素(值)是`1`。

```
(defn document-count-m

  {::mr/source-as :vals}

  [documents]

  (->> documents

       (r/mapcat (comp distinct stemmer/stems))

       (r/map #(vector % 1))))
```

缩减器的工作是将这些关键字/值对记录下来，并对它们进行缩减，使得每个唯一的关键字都有一个唯一的 ID。一种简单的方法是确保集群上只有一个缩减器。因为所有的术语都将被传递给这个单一的进程，所以 reducer 可以简单地保存一个内部计数器，并为每个术语分配一个 ID，就像我们之前对 Clojure 原子所做的那样。不过，这并没有利用 Hadoop 的分布式功能。

跑酷的一个特性我们还没有介绍，那就是可以从每个映射器和缩减器中访问的运行时上下文。Parkour 将`parkour.mapreduce/*context*`动态变量绑定到任务的 Hadoop 任务上下文，我们的映射器和 reducers 在该任务中运行。任务上下文包含以下属性:

| 

财产

 | 

类型

 | 

描述

 |
| --- | --- | --- |
| `mapred.job.id` | 线 | 作业的 ID |
| `mapred.task.id` | （同 Internationalorganizations）国际组织 | 任务尝试 ID |
| `mapred.task.partition` | （同 Internationalorganizations）国际组织 | 作业中任务的 ID |

最后一个属性是`mapred.task.partition`属性，它是 Hadoop 分配的任务数，保证是一个在集群中唯一的单调递增的整数。这个数字是我们任务的全局偏移量。在每个任务中，我们还可以保存一个本地偏移量，并在每个单词被处理后输出。这两个偏移量(全局和局部)一起为整个分类中的术语提供了唯一的标识符。

下图显示了在三个独立的映射器上处理八个术语的过程:

![Distributed unique IDs with Hadoop](graphics/7180OS_06_170.jpg)

每个映射器只知道自己的分区号和该项的本地偏移量。然而，计算一个唯一的全局 ID 只需要这两个数字。前面的**计算偏移量**框决定了每个任务分区的全局偏移量。分区 **1** 的全局偏移量为 **0** 。分区 **2** 有一个 **3** 的全局偏移量，因为分区 **1** 处理了 **3** 字。分区 **3** 有一个 **5** 的偏移量，因为分区 **1** 和 **2** 之间处理了 **5** 字，以此类推。

为了让前面的方法工作，我们需要知道三件事:映射器的全局偏移量、术语的本地偏移量以及每个映射器处理的术语总数。这三个数字可以用来为每个术语定义一个惟一的、集群范围的 ID。创建这三个数字的缩减器定义如下。它引入了几个新概念，我们稍后会讨论。

```
(defn unique-index-r

  {::mr/source-as :keyvalgroups,

   ::mr/sink-as dux/named-keyvals}

  [coll]

  (let [global-offset (conf/get-long mr/*context*

                                     "mapred.task.partition" -1)]

    (tr/mapcat-state

     (fn [local-offset [word doc-counts]]

       [(inc local-offset)

        (if (identical? ::finished word)

          [[:counts [global-offset local-offset]]]

          [[:data [word [[global-offset local-offset]

                         (apply + doc-counts)]]]])])

     0 (r/mapcat identity [coll [[::finished nil]]]))))
```

缩减器执行的第一步是获取这个特定缩减器的任务分区`global-offset`。我们使用`mapcat-state`，一个在转换器库中定义的函数([https://github.com/brandonbloom/transduce](https://github.com/brandonbloom/transduce))来构建一个格式`[[:data ["apple" [1 4]] [:data ["orange" [1 5]] ...]`的元组序列，其中数字向量`[1 4]`分别代表全局和局部偏移量。最后，当我们到达这个 reduce 任务的末尾时，我们以`[:counts [1 5]]`的格式向序列添加一个元组。这表示这个特定的缩减器分区`1`的最终本地计数`5`。因此，单个 reducer 计算了我们计算所有术语 id 所需的所有三个元素。

提供给`::mr/source-as`的关键字不是我们以前遇到过的。在前一章中，我们看到了整形选项`:keyvals`、`:keys`和`:vals`如何让 Parkour 知道我们希望如何提供我们的数据，以及我们将提供的数据结构作为回报。对于减压器，跑酷描述了一组更全面的整形函数，说明了输入可以分组的事实。下图说明了可用的选项:

![Distributed unique IDs with Hadoop](graphics/7180OS_06_175.jpg)

提供给`::mr/sink-as`的选项也不是我们以前遇到过的。`parkour.io.dux`名称空间提供了解复用输出的选项。实际上，这意味着，通过作为`dux/named-keyvals`接收，单个缩减器可以写入几个不同的输出。换句话说，我们在数据管道中引入了一个分支:一些数据写入一个分支，其余的写入另一个分支。

设置了`dux/named-keyvals`的接收器规范后，我们的元组的第一个元素将被解释为要写入的目的地；元组的第二个元素将被视为要写入的键/值对。因此，我们可以将`:data`(本地和全局偏移量)写到一个目的地，将`:counts`(每个映射器处理的术语数)写到另一个目的地。

接下来将介绍使用我们定义的映射器和缩减器的作业。正如我们在前一章中指定的跑酷作业一样，我们将输入、映射、划分、减少和输出步骤链接在一起。

```
(defn df-j [dseq]

  (-> (pg/input dseq)

      (pg/map #'document-count-m)

      (pg/partition (mra/shuffle [:string :long]))

      (pg/reduce #'unique-index-r)

      (pg/output :data (mra/dsink [:string index-value])

                 :counts (mra/dsink [:long :long]))))
```

前面的代码和我们之前看到的作业规范有两个主要区别。首先，我们的输出指定了两个命名的接收器:我们的缩减器的每个输出都有一个。其次，我们使用名称空间`parkour.io.avro`作为`mra`，用`(mra/dsink [:string long-pair])`为我们的数据指定一个模式。

在前一章中，我们利用特瑟的`FressianWritable`将任意 Clojure 数据结构序列化到磁盘。这很有效，因为`FressianWritable`的内容不需要被 Hadoop 解释:值是完全不透明的。有了 Parkour，我们可以选择定义自定义的键/值对类型。由于键和值确实需要被 Hadoop 解释为单独的实体(为了读取、分区和写入序列文件)，Parkour 允许我们使用`parkour.io.avro`命名空间定义一个“元组模式”，它明确定义了键和值的类型。`long-pair`是用于在单个元组中存储局部和全局偏移量的自定义模式:

```
(def long-pair (avro/tuple-schema [:long :long]))
```

而且，由于模式是可组合的，我们可以在定义输出模式`(mra/dsink [:string long-pair])`时引用`long-pair`模式。

### 注意

Parkour 使用库`Acbracad`来序列化使用 Avro 的 Clojure 数据结构。有关序列化选项的更多信息，请查阅位于 https://github.com/damballa/abracadT2 的 Abracad 文档。

让我们来看看 Parkour 展示的 Hadoop 的另一个特性，它使我们的 term ID 作业比其他方式更加高效:分布式缓存。

## 与分布式缓存共享数据

正如我们在上一节中讨论的，如果我们知道特定映射器的每个单词的本地偏移量，并且我们知道每个映射器总共处理了多少条记录，那么我们就可以为每个单词计算一个唯一的、连续的 ID。

几页前,展示的流程图包含两个中央方框，分别标有**计算偏移量**和**全局 ID** 。这些方框直接映射到我们接下来要介绍的功能:

```
(defn global-id [offsets [global-offset local-offset]]

  (+ local-offset (get offsets global-offset)))

(defn calculate-offsets [dseq]

  (->> (into [] dseq)

       (sort-by first)

       (reductions (fn [[_ t] [i n]]

                     [(inc i) (+ t n)])

                   [0 0])

       (into {})))
```

一旦我们计算出用于生成唯一 id 的偏移地图，我们真的希望它们可以用于我们的所有地图，并减少作为共享资源的任务。已经以分布式方式生成了补偿，我们也希望以分布式方式消费它。

分布式缓存是 Hadoop 允许任务访问公共数据的方式。这是一种共享少量数据(小到足以驻留在内存中的数据)的更有效的方式，而不是通过潜在的高成本数据连接。

![Sharing data with the distributed cache](graphics/7180OS_06_180.jpg)

在从分布式缓存中读取之前，我们必须向其中写入一些内容。这可以通过跑酷的`parkour.io.dval`命名空间来实现:

```
(defn unique-word-ids [conf df-data df-counts]

  (let [offsets-dval (-> (calculate-offsets df-counts)

                         (dval/edn-dval))]

    (-> (pg/input df-data)

        (pg/map #'word-id-m offsets-dval)

        (pg/output (mra/dsink [word-id]))

        (pg/fexecute conf `word-id)

        (->> (r/map parse-idf)

             (into {}))

        (dval/edn-dval))))
```

这里，我们用`dval/edn-dval`函数将两组数据写入分布式缓存。第一个是刚刚定义的`calculate-offsets`函数的结果，它被传递给`word-id-m`映射器供其使用。写入分布式缓存的第二组数据是它们的输出。我们将看到这是如何在`word-id-m`函数中生成的，如下所示:

```
(defn word-id-m

  {::mr/sink-as :keys}

  [offsets-dval coll]

  (let [offsets @offsets-dval]

    (r/map

     (fn [[word word-offset]]

       [word (global-id offsets word-offset)])

     coll)))
```

`dval/edn-dval`返回的值实现了`IDRef`接口。这意味着我们可以使用 Clojure 的`deref`函数(或 deref 宏字符`@`)来检索它包装的值，就像我们对 Clojure 的 atoms 所做的一样。第一次取消对分布式值的引用会导致数据从分布式缓存下载到本地映射器缓存。一旦数据在本地可用，Parkour 就会负责重建我们以 EDN 格式写入的 Clojure 数据结构(偏移图)。

## 从输入文档中构建 Mahout 向量

在前面的章节中，我们绕道介绍了几个新的跑酷和 Hadoop 概念，但我们最终能够为 Mahout 构建文本向量，为每个术语使用唯一的 id。为了简洁起见，省略了一些进一步的代码，但是整个工作对于在`cljds.ch6.vectorizer`示例代码名称空间中的视图是可用的。

如前所述，Mahout 的 *k* -means 实现希望我们使用它的一个 vector 类来提供输入的向量表示。因为我们的字典很大，而且大多数文档很少使用这些术语，所以我们将使用稀疏向量表示。下面的代码利用一个`dictionary`分布值为每个输入文档创建一个`org.apache.mahout.math.RandomAccessSparseVector`:

```
(defn create-sparse-tfidf-vector [dictionary [id doc]]

  (let [vector (RandomAccessSparseVector. (count dictionary))]

    (doseq [[term tf] (-> doc stemmer/stems frequencies)]

      (let [term-info (get dictionary term)

            id  (:id term-info)

            idf (:idf term-info)]

        (.setQuick vector id (* tf idf))))

    [id vector]))

(defn create-tfidf-vectors-m [dictionary coll]

  (let [dictionary @dictionary]

    (r/map #(create-sparse-tfidf-vector dictionary %) coll)))
```

最后，我们使用了`create-tfidf-vectors-m`函数的,它将我们讨论过的所有内容整合到一个 Hadoop 任务中:

```
(defn tfidf [conf dseq dictionary-path vector-path]

  (let [doc-count (->> dseq (into []) count)

        [df-data df-counts] (pg/execute (df-j dseq) conf df)

        dictionary-dval (make-dictionary conf df-data

                                         df-counts doc-count)]

    (write-dictionary dictionary-path dictionary-dval)

    (-> (pg/input dseq)

        (pg/map #'create-tfidf-vectors-m dictionary-dval)

        (pg/output (seqf/dsink [Text VectorWritable] vector-path))

        (pg/fexecute conf `vectorize))))
```

这个任务处理字典的创建，将字典写入分布式缓存，然后使用字典——使用我们刚刚定义的映射器——将每个输入文档转换为 Mahout 向量。为了确保序列文件与 Mahout 兼容，我们将最终输出的键/值类设置为`Text`和`VectorWritable`，其中键是文档的原始文件名，值是内容的 Mahout 向量表示。

我们可以通过运行以下命令来调用此作业:

```
(defn ex-6-18 []

  (let [input-path  "data/reuters-sequencefile" 

        output-path "data/reuters-vectors"]

    (vectorizer/tfidf-job (conf/ig) input-path output-path)))
```

作业将把字典写出到`dictionary-path`(我们将再次需要它)，把向量写出到`vector-path`。

### Tip

由于运行前面的示例是后续示例的先决条件，因此也可以在命令行上运行:

```

lein create-vectors

```

接下来，我们将发现如何使用这些向量通过 Mahout 实际执行聚类。



# 用 Mahout 运行 k-means 聚类

现在我们有了一个适合 Mahout 使用的向量序列文件，是时候实际运行 *k* 了——这意味着对整个数据集进行聚类。与我们当地的咒语版本不同，Mahout 在处理完整的路透社语料库时不会有任何问题。

和`SequenceFilesFromDirectory`类一样，我们也为 Mahout 的另一个命令行程序`KMeansDriver`创建了一个包装器。Clojure 变量名更容易看出每个命令行参数的用途。

```
(defn run-kmeans [in-path clusters-path out-path k]

  (let [distance-measure  "org.apache.mahout.common.distance.CosineDistanceMeasure"

        max-iterations    100

        convergence-delta 0.001]

    (KMeansDriver/main

     (->> (vector "-i"  in-path

                  "-c"  clusters-path

                  "-o"  out-path

                  "-dm" distance-measure

                  "-x"  max-iterations

                  "-k"  k

                  "-cd" convergence-delta

                  "-ow"

                  "-cl")

          (map str)

          (into-array String)))))
```

我们提供字符串`org.apache.mahout.common.distance.CosineDistanceMeasure`向驱动程序表明我们想要使用 Mahout 的余弦距离度量实现。Mahout 还包括一个`EuclideanDistanceMeasure`和一个`TanimotoDistanceMeasure`(类似于 Jaccard 距离，Jaccard 索引的补充，但是它将对向量而不是集合进行操作)。还定义了其他几种距离度量；查阅 Mahout 文档以获得所有可用的选项。

有了前面的`run-kmeans`函数，我们只需要让 Mahout 知道在哪里访问我们的文件。和上一章一样，我们假设 Hadoop 在本地模式下运行，所有文件路径都是相对于项目根目录的:

```
(defn ex-6-19 []

  (run-kmeans "data/reuters-vectors/vectors"

              "data/kmeans-clusters/clusters"

              "data/kmeans-clusters"

              10))
```

这个例子可能会运行一段时间，因为 Mahout 会遍历我们的大型数据集。

## 查看 k 均值聚类结果

一旦完成，我们将希望看到每个集群的集群摘要，就像我们对 Incanter 实现所做的那样。幸运的是，Mahout 定义了一个`ClusterDumper`类来为我们做这件事。当然，我们需要提供我们的集群的位置，但是我们也将提供我们的字典的位置。提供字典意味着输出可以返回每个分类的前几个术语。

```
(defn run-cluster-dump [in-path dict-path points-dir out-path]

  (let [distance-measure

        "org.apache.mahout.common.distance.CosineDistanceMeasure"]

    (ClusterDumper/main

     (->> (vector "-i" in-path

                  "-o" out-path

                  "-d" dict-path

                  "--pointsDir" points-dir

                  "-dm" distance-measure

                  "-dt" "sequencefile"

                  "-b" "100"

                  "-n" "20"

                  "-sp" "0"

                  "--evaluate")

          (map str)

          (into-array String)))))
```

接下来，我们定义实际调用`run-cluster-dump`函数的代码:

```
(defn path-for [path]

  (-> (fs/glob path)

      (first)

      (.getAbsolutePath)))

(defn ex-6-20 []

  (run-cluster-dump

   (path-for "data/kmeans-clusters/clusters-*-final")

   "data/reuters-vectors/dictionary/part-r-00000"

   "data/kmeans-clusters/clusteredPoints"

   "data/kmeans-clusterdump"))
```

我们再次利用`me.raynes.fs`库来确定最终的集群包含在哪个目录中。Mahout 将把`-final`添加到包含最终集群的目录中，但是我们事先不知道这将是哪个目录。`fs/glob`函数将找到一个与模式`clusters-*-final`匹配的目录，并用真实目录名包含的迭代号替换`*` 。

## 解释聚类输出

如果您在任何文本编辑器中打开由前面的示例`data/kmeans-clusterdump`创建的文件，您将看到表示 Mahout 集群的顶部术语的输出。该文件会很大，但接下来会提供一段摘录:

```
:VL-11417{n=312 c=[0.01:0.039, 0.02:0.030, 0.07:0.047, 0.1:0.037, 0.10:0.078, 0.11:0.152, 0.12:0.069,

  Top Terms:

    tonnes              =>   2.357810452962533

    department          =>   1.873890568048526

    wheat               =>  1.7797807546762319

    87                  =>  1.6685682321206117

    u.s                 =>   1.634764205186795

    mln                 =>  1.5050923755535712

    agriculture         =>  1.4595903158187866

    ccc                 =>  1.4314624499051998

    usda                =>  1.4069041441648433

    dlrs                =>  1.2770121846443567
```

第一行包含关于集群的信息:ID(在本例中是`VL-11417`)后面是花括号，包含集群的大小和集群质心的位置。由于文本已经转换为重量和数字 id，质心本身是不可能解释的。但是，质心描述下面的顶部术语暗示了群集的内容；它们是集群结合的术语。

```
VL-12535{n=514 c=[0:0.292, 0.25:0.015, 0.5:0.012, 00:0.137, 00.46:0.018, 00.50:0.036, 00.91:0.018, 0

  Top Terms:

    president           =>   3.330068911559851

    reagan              =>   2.485271333256584

    chief               =>  2.1148699971952327

    senate              =>   1.876725117983985

    officer             =>  1.8531712558019022

    executive           =>  1.7373591731030653

    bill                =>  1.6326750159727461

    chairman            =>  1.6280977206471365

    said                =>  1.6279512813119108

    house               =>  1.5771017798189988
```

之前的两个聚类暗示了数据集中存在的两个明确的主题，尽管由于 *k* -means 算法的随机性质，您的聚类可能会有所不同。

根据您的初始质心和您让算法运行的迭代次数，您可能会看到在某些方面看起来“更好”或“更差”的簇。这将基于对聚类术语如何搭配的本能反应。但是，仅仅通过查看最常用的术语，通常并不清楚集群的表现如何。在任何情况下，直觉都不是判断无监督学习算法质量的一种非常可靠的方式。理想情况下，我们想要的是一些量化的方法来衡量集群的表现。



# 聚类评估措施

在我们在上一节中看到的文件的底部，您将看到一些统计数据，这些统计数据表明了数据的聚集程度:

```
Inter-Cluster Density: 0.6135607681542804

Intra-Cluster Density: 0.6957348405534836
```

这两个数字可以认为相当于我们在[第二章](ch02.html "Chapter 2. Inference")、*推论*和[第三章](ch03.html "Chapter 3. Correlation")、*相关性*中看到的测度内方差和测度间方差。理想情况下，我们寻求的是与集群之间的密度相比，集群内的方差更低(或密度更高)。

## 簇间密度

簇间密度是簇质心之间的平均距离。好的集群可能没有太靠近彼此的中心。如果是的话，这将表明集群正在创建具有相似特征的组，并且可能在集群成员之间画出难以支持的区别。

![Inter-cluster density](graphics/7180OS_06_190.jpg)

因此，理想情况下，我们的聚类将产生具有**大的聚类间距离**的聚类。

## 集群内密度

相比之下，簇内密度是对簇紧密程度的度量。理想情况下，聚类将识别彼此相似的项目组。紧凑聚类表示一个聚类中的所有项目都非常相似。

![Intra-cluster density](graphics/7180OS_06_200.jpg)

因此，最佳的聚类结果产生紧凑、不同的聚类，具有**高的聚类内密度**和**低的聚类间密度**。

然而，并不总是清楚有多少聚类被数据证明是合理的。考虑以下显示分组到不同数量的聚类中的相同数据集。很难有把握地说出理想的集群数量是多少。

![Intra-cluster density](graphics/7180OS_06_210.jpg)

尽管前面的图示是人为的，但它说明了聚类数据的一个普遍问题。通常没有一个明确的“最佳”聚类数。最有效的聚类在很大程度上取决于数据的最终用途。

然而，我们可以通过确定一些质量分数的值如何随着聚类的数量而变化来推断哪些可能是更好的 k 值。质量分数可以是统计量，例如簇间或簇内密度。随着集群的数量接近其理想值，我们期望这个质量分数的值会提高。相反，当集群的数量偏离其理想值时，我们预计质量会下降。因此，为了合理地了解数据集中有多少聚类是合理的，我们应该针对不同的 *k* 值多次运行该算法。

## 用跑酷计算均方根误差

最常见的群集质量度量之一是误差平方和 T2。对于每个点，误差是到最近的群集质心的测量距离。因此，总聚类 SSE 是一个聚类点到其对应质心的所有聚类的总和:

![Calculating the root mean square error with Parkour](graphics/7180OS_06_11.jpg)

其中[i] 是簇 *S* [i] ， *k* 是簇的总数， *n* 是点的总数。

因此，为了计算 Clojure 中的 *RMSE* ，我们需要能够将集群中的每个点与其对应的集群质心相关联。Mahout 将聚类中心和聚类点保存在两个独立的文件中，所以在下一节中，我们将把它们合并起来。

### 加载聚类点和质心

给定一个父目录(例如`data/reuters-kmeans/kmeans-10`)，下面的函数将使用 Parkour 的`seqf/dseq`函数从序列文件加载键/值对，将点加载到存储在由集群 ID 索引的地图中的向量中。在这种情况下，键是集群 ID(整数)，值是 TF-IDF 向量。

```
(defn load-cluster-points [dir]

  (->> (points-path dir)

       (seqf/dseq)

       (r/reduce

        (fn [accum [k v]]

          (update-in accum [k] conj v)) {})))
```

上述函数的输出是一个以聚类 ID 为关键字的映射，其值是聚类点的序列。同样，下面的函数将把每个集群转换成一个 map，由集群 ID 作为键，其值是包含键`:id`和`:centroid`的 map。

```
(defn load-cluster-centroids [dir]

  (let [to-tuple (fn [^Cluster kluster]

                   (let [id (.getId kluster)]

                     [id  {:id id

                           :centroid (.getCenter kluster)}]))]

    (->> (centroids-path dir)

         (seqf/dseq)

         (r/map (comp to-tuple last))

         (into {}))))
```

有两个由聚类 ID 设置关键点的地图意味着组合聚类点和聚类质心是一件简单的事情，在地图上调用`merge-with`提供一个定制的合并函数。在下面的代码中，我们将聚集的点合并到包含集群`:id`和`:centroid`的地图中。

```
(defn assoc-points [cluster points]

  (assoc cluster :points points))

(defn load-clusters [dir]

  (->> (load-cluster-points dir)

       (merge-with assoc-points

                   (load-cluster-centroids dir))

       (vals)))
```

最终的输出是一个单一的映射，以集群 ID 为键，每个值是一个`:id`、`:centroid`和`:points`的映射。我们将在下一节中使用此地图来计算聚类 RMSE。

## 计算集群 RMSE

为了计算 RMSE，我们需要能够建立每个点和它相关的群集质心之间的距离。因为我们使用 Mahout 的`CosineDistanceMeasure`来执行初始聚类，所以我们也应该使用余弦距离来评估聚类。事实上，我们可以简单地利用 Mahout 的实现。

```
(def measure

  (CosineDistanceMeasure.))

(defn distance [^DistanceMeasure measure a b]

  (.distance measure a b))

(defn centroid-distances [cluster]

  (let [centroid (:centroid cluster)]

    (->> (:points cluster)

         (map #(distance measure centroid %)))))

(defn squared-errors [cluster]

  (->> (centroid-distances cluster)

       (map i/sq)))

(defn root-mean-square-error [clusters]

  (->> (mapcat squared-errors clusters)

       (s/mean)

       (i/sqrt)))
```

如果 RMSE 相对于聚类数作图，你会发现它随着聚类数的增加而下降。单个聚类将具有最高的 RMSE 误差(原始数据集与平均值的方差)，而当每个点都在自己的聚类中时，最低的 RMSE 将是退化情况(RMSE 为零)。显然，这两个极端中的任何一个都不能很好地解释数据的结构。然而，RMSE 并不是直线下降的。当集群的数量从 1 开始增加时，它急剧下降，但是一旦超过集群的“自然”数量，它将下降得更慢。

因此，判断理想聚类数的一种方法是绘制 RMSE 随聚类数的变化曲线。这叫做**肘法**。

## 用肘法确定最佳 k

为了让使用肘方法确定 *k* 的值，我们将不得不重新运行*k*-意味着多次。下面的代码为`2`和`21`之间的所有 *k* 实现了这一点。

```
(defn ex-6-21 []

  (doseq [k (range 2 21)

          :let [dir (str "data/kmeans-clusters-" k)]]

    (println dir)

    (run-kmeans "data/reuters-vectors/vectors"

                (str dir "/clusters")

                dir k)))
```

这将需要一点时间来运行，所以可能是时候去做一杯热饮了:`println`语句将记录每一次集群运行，让您知道已经取得了多少进展。在我的笔记本电脑上，整个过程大约需要 15 分钟。

完成后，您应该能够运行该示例来为每个聚类值生成 RMSE 散点图:

```
(defn ex-6-22 []

  (let [ks (range 2 21)

        ys (for [k ks

                 :let [dir (str "data/kmeans-clusters-" k)

                       clusters (load-clusters dir)]]

             (root-mean-square-error clusters))]

    (-> (c/scatter-plot ks ys

                        :x-label "k"

                        :y-label "RMSE")

        (i/view))))
```

这应该返回一个类似如下的图:

![Determining optimal k with the elbow method](graphics/7180OS_06_220.jpg)

前面的散点图显示了 RMSE 与聚类数的关系。应该很清楚当 *k* 超过大约 13 个集群时，RMSE 变化的速率是如何减慢的，并且增加集群的数量进一步产生收益递减。因此，前面的图表表明，对于我们的路透社数据，大约 13 个集群是一个很好的选择。

肘方法提供了一种直观的手段来确定理想的集群数量，但有时很难在实践中应用。这是因为我们必须解释由每个 *k* 的 RMSE 定义的曲线形状。如果 *k* 很小，或者 RMSE 包含很多噪音，那么可能不明显肘部在哪里，或者根本就没有肘部。

### 注意

由于聚类是一种无监督的学习算法，我们在这里假设聚类的内部结构是验证聚类质量的唯一方法。如果真正的聚类标签是已知的，那么就有可能使用我们在[第 4 章](ch04.html "Chapter 4. Classification")、*分类*中遇到的那种外部验证措施(如熵)来验证模型的成功。

其他聚类评估方案旨在提供一种更清晰的方法来确定精确的聚类数。我们将讨论的两个指数是邓恩指数和戴维斯-波尔丁指数。两者都是内部评估方案，这意味着它们只查看聚类数据的结构。每种方法都旨在以不同的方式识别出产生了最紧密的、分离良好的聚类的聚类。

## 用邓恩指数确定最佳 k

邓恩指数提供了另一种选择最佳 T2 数的方法。邓恩指数没有考虑集群数据中剩余的平均误差，而是考虑了两种“最坏情况”的比率:两个集群质心之间的最小距离除以最大集群直径。因此，较高的指数表示较好的聚类，因为通常我们喜欢较大的聚类间距离和较小的聚类内距离。

对于 *k* 簇，我们可以用以下方式表示邓恩指数:

![Determining optimal k with the Dunn index](graphics/7180OS_06_12.jpg)

其中 *δ(C [i] ，C [j] )* 两个簇之间的距离 *C* [i] 和 *C* [j] 和![Determining optimal k with the Dunn index](graphics/7180OS_06_13.jpg)代表最大簇的大小(或散布)。

有几种可能的方法来计算一个集群的分散。我们可以取一个聚类内最远的两个点之间的距离，或者该聚类内数据点之间的所有成对距离的平均值，或者来自聚类质心本身的每个数据点的平均值。在下面的代码中，我们通过取距离聚类质心的中值距离来计算大小。

```
(defn cluster-size [cluster]

  (-> cluster

      centroid-distances

      s/median))

(defn dunn-index [clusters]

  (let [min-separation (->> (combinations clusters 2)

                            (map #(apply separation %))

                            (apply min))

        max-cluster-size (->> (map cluster-size clusters)

                              (apply max))]

    (/ min-separation max-cluster-size)))
```

前面的代码利用`clojure.math.combinatorics`([https://github.com/clojure/math.combinatorics/](https://github.com/clojure/math.combinatorics/))中的`combinations`函数来产生所有两两组合的集群的惰性序列。

```
(defn ex-6-23 []

  (let [ks (range 2 21)

        ys (for [k ks

                 :let [dir (str "data/kmeans-clusters-" k)

                       clusters (load-clusters dir)]]

             (dunn-index clusters))]

    (-> (c/scatter-plot ks ys

                        :x-label "k"

                        :y-label "Dunn Index")

        (i/view))))
```

我们使用前面代码中的`dunn-index`函数为从 *k=2* 到 *k=20* 的集群生成散点图:

![Determining optimal k with the Dunn index](graphics/7180OS_06_230.jpg)

更高的邓恩指数表示更好的聚类。因此，看起来最佳的聚类是对于 *k=2* ，其次是 *k=6* ，紧随其后的是 *k=12* 和 *k=13* 。让我们尝试另一种集群评估方案，并比较结果。

## 用戴维斯-波尔丁指数确定最佳 k 值

Davies-Bouldin 指数是一种替代评估方案，用于测量聚类中所有值的平均大小比率和分离度。对于每个聚类，找到一个备选聚类,该备选聚类最大化聚类大小之和除以聚类间距离的比率。戴维斯-波尔丁指数定义为数据中所有聚类的平均值:

![Determining optimal k with the Davies-Bouldin index](graphics/7180OS_06_14.jpg)![Determining optimal k with the Davies-Bouldin index](graphics/7180OS_06_15.jpg)

其中 *δ(C [i] ，C [j] )* 为两个星团质心 *C* [i] 和 *C* [j] ， *S* [i] 和 *S* [j] 为散点。我们可以使用以下代码计算戴维斯-波尔丁指数:

```
(defn scatter [cluster]

  (-> (centroid-distances cluster)

      (s/mean)))

(defn assoc-scatter [cluster]

  (assoc cluster :scatter (scatter cluster)))

(defn separation [a b]

  (distance measure (:centroid a) (:centroid b)))

(defn davies-bouldin-ratio [a b]

  (/ (+ (:scatter a)

        (:scatter b))

     (separation a b)))

(defn max-davies-bouldin-ratio [[cluster & clusters]]

  (->> (map #(davies-bouldin-ratio cluster %) clusters)

       (apply max)))

(defn rotations [xs]

  (take (count xs)

        (partition (count xs) 1 (cycle xs))))

(defn davies-bouldin-index [clusters]

  (let [ds (->> (map assoc-scatter clusters)

                (rotations)

                (map max-davies-bouldin-ratio))]

    (s/mean ds)))
```

现在让我们在散点图上为集群 *k=2* 到 *k=20* 绘制戴维斯-波尔丁曲线:

```
(defn ex-6-24 []

  (let [ks (range 2 21)

        ys (for [k ks

                 :let [dir (str "data/kmeans-clusters-" k)

                       clusters (load-clusters dir)]]

             (davies-bouldin-index clusters))]

    (-> (c/scatter-plot ks ys

                        :x-label "k"

                        :y-label "Davies-Bouldin Index")

        (i/view))))
```

这将生成以下图:

![Determining optimal k with the Davies-Bouldin index](graphics/7180OS_06_240.jpg)

与 Dunn 指数不同，对于好的聚类方案，Davies-Bouldin 指数被最小化，因为通常我们寻找大小紧凑并且具有高的簇间距离的簇。在图表之前的表明 *k=2* 是理想的集群大小，接着是 *k=13* 。



# k-means 的缺点

*k* -means 是最流行的聚类算法之一，因为它相对容易实现，并且可以很好地扩展到非常大的数据集。尽管它很受欢迎，但也有几个缺点。

*k* -means 是随机的，不保证找到聚类的全局最优解。事实上，该算法可能对异常值和噪声数据非常敏感:最终聚类的质量可能高度依赖于初始聚类质心的位置。换句话说，*k*-意味着将有规律地发现一个局部的而不是全局的最小值。

![The drawbacks of k-means](graphics/7180OS_06_250.jpg)

上图说明了 *k* -means 如何基于较差的初始聚类质心收敛到局部最小值。如果初始聚类质心位置合适，甚至可能出现非最优聚类，因为 *k* -means 更喜欢具有相似大小和密度的聚类。当聚类的大小和密度不近似相等时， *k* -means 可能无法收敛到最自然的聚类:

![The drawbacks of k-means](graphics/7180OS_06_260.jpg)

另外，*k*-表示强烈偏好球状星团。具有更复杂形状的簇不能被 *k* 均值算法很好地识别。

在下一章，我们将看到各种各样的降维技术如何帮助解决这些问题。但是在我们到达那里之前，让我们发展一种定义距离的另一种方式的直觉:作为一个元素离一组事物有多远的度量。

## 马氏距离度量

在本章的开头，我们通过展示 Jaccard、Euclidean 和 cosine 度量与数据表示的关系，看到了在给定数据的情况下，一些距离度量可能比其他度量更合适。选择距离度量和聚类算法时要考虑的另一个因素是数据的内部结构。考虑以下散点图:

![The Mahalanobis distance measure](graphics/7180OS_06_270.jpg)

很明显，箭头所指的点不同于其他点。我们可以清楚地看到它远离其他的分布，因此代表一种异常。然而，如果我们从平均值(“质心”)开始计算所有点的欧几里德距离，那么该点将会消失在其他同样远甚至更远的点中:

```
(defn ex-6-25 []

  (let [data (dataset-with-outlier)

        centroid  (i/matrix [[0 0]])

        distances (map #(s/euclidean-distance centroid %) data)]

    (-> (c/bar-chart (range 202) distances

                     :x-label "Points"

                     :y-label "Euclidean Distance") 

        (i/view))))
```

前面的代码生成了下面的图表:

![The Mahalanobis distance measure](graphics/7180OS_06_280.jpg)

Mahalanobis 距离在计算距离时考虑了变量之间的协方差。在二维空间中，我们可以把欧几里得距离想象成一个从形心向外生长的圆:圆边缘上的所有点都与形心等距。 Mahalanobis 距离拉伸和倾斜该圆，以校正不同变量的各自标度，并说明它们之间的相关性。我们可以在下面的例子中看到效果:

```
(defn ex-6-26 []

  (let [data (dataset-with-outlier)

        distances    (map first (s/mahalanobis-distance data))]

    (-> (c/bar-chart (range 202) distances

                     :x-label "Points"

                     :y-label "Mahalanobis Distance")

        (i/view))))
```

前面的代码使用`incanter.stats`提供的函数来绘制同一组点之间的 Mahalanobis 距离。结果如下图所示:

![The Mahalanobis distance measure](graphics/7180OS_06_290.jpg)

这张图表清楚地指出某一点比其他点要远得多。这符合我们的看法，即这个点尤其应被视为离其他点更远。



# 维度的诅咒

然而，有一个马哈拉诺比斯距离度量无法克服的事实，这就是众所周知的维数灾难。随着数据集中维度数量的增加，每个点与其他点之间的距离趋于相等。我们可以用下面的代码非常简单地演示这一点:

```
(defn ex-6-27 []

  (let [distances (for [d (range 2 100)

                        :let [data (->> (dataset-of-dimension d)

                                        (s/mahalanobis-distance)

                                        (map first))]]

                    [(apply min data) (apply max data)])]

    (-> (c/xy-plot (range 2 101) (map first distances)

                   :x-label "Number of Dimensions"

                   :y-label "Distance Between Points"

                   :series-label "Minimum Distance"

                   :legend true)

        (c/add-lines (range 2 101) (map second distances)

                     :series-label "Maximum Distance")

        (i/view))))
```

前面的代码在由 100 个点组成的合成数据集中找到任意两对点之间的最小和最大距离。随着维数接近集合中元素的数目，我们可以看到每对元素之间的最小和最大距离是如何相互接近的:

![The curse of dimensionality](graphics/7180OS_06_300.jpg)

效果是惊人的:随着维度数量的增加，最近的两点之间的距离也会增加。最远两点之间的距离也会增加，但速度较慢。最后，在 100 个维度和 100 个数据点的情况下，每个点看起来都彼此距离相等。

当然，这是合成的，随机生成的数据。如果我们试图聚集我们的数据，我们隐含地希望它有一个可识别的内部结构，我们可以梳理出来。尽管如此，随着维度数量的增加，这种结构将变得越来越难以识别。



# 总结

在这一章中，我们已经学习了聚类的过程，并且讨论了流行的 *k* -means 聚类算法来聚类大量的文本文档。

这为应对文本处理带来的特殊挑战提供了机会，在文本处理中，数据通常是杂乱的、模糊的和高维的。我们看到了停用词和词干如何帮助减少维数，以及 TF-IDF 如何帮助识别最重要的维数。我们也看到了如何以大量术语的扩散为代价，用字母组合来帮助梳理出每个单词的上下文。

我们已经更详细地探索了跑酷，并了解了如何使用它来编写复杂的、可伸缩的 Hadoop 作业。特别是，我们已经看到了如何利用分布式缓存和自定义元组模式来编写表示为 Clojure 数据结构的 Hadoop 作业流程数据。我们使用这两者来实现一种生成惟一的、集群范围的术语 id 的方法。

最后，我们见证了高维空间带来的挑战:所谓的“维数灾难”。在下一章，我们将更详细地讨论这个话题，并描述各种技术来对付它。当我们考虑推荐问题时，我们将继续探索“相似性”和“差异”的概念:我们如何将用户和项目匹配在一起。**