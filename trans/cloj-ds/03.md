<title>Chapter 3. Correlation</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 第三章。相互关系

|   | “我对人了解得越多，我就越喜欢我的狗。” |   |
|   | ——*马克·吐温* |

在前几章中，我们已经考虑了如何根据汇总统计数据描述样本，以及如何从中推断总体参数。这种分析能告诉我们总体上和具体样本的一些情况，但它不允许我们对单个元素做出非常精确的陈述。这是因为将数据简化为两个统计量(均值和标准差)会丢失太多信息。

我们常常想更进一步，在两个或多个变量之间建立关系，或者在给定一个变量的情况下预测另一个变量。这就把我们带入了相关性和回归性的研究。相关性涉及两个或多个变量之间关系的强度和方向。回归决定了这种关系的性质，并使我们能够从中做出预测。

线性回归是我们第一个机器学习算法。给定一个数据样本，我们的模型将学习一个线性方程，允许它对新的、看不见的数据进行预测。为了做到这一点，我们将返回到 Incanter，研究奥运会运动员身高和体重之间的关系。我们将介绍矩阵的概念，并展示如何使用咒语来操纵它们。

# 关于数据

本章将利用 2012 年伦敦奥运会运动员的数据，由卫报新闻和媒体有限公司提供。数据最初来源于卫报在 http://www.theguardian.com/data[的优秀数据博客](http://www.theguardian.com/data)。

### 注意

从出版商的网站或者从[https://github.com/clojuredatascience/ch3-correlation](https://github.com/clojuredatascience/ch3-correlation)下载本章的示例代码。

查阅本章示例代码中的`Readme` 文件，或者在 http://wiki.clojuredatascience.com查阅本书的 wiki 以获得更多关于数据的信息。

<title>Inspecting the data</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 检查数据

当面对一个新的数据集时，首要任务是研究它，以确保我们理解它包含的内容。

`all-london-2012-athletes.xlsx`文件足够小，本章的示例代码已经提供给它了。我们可以用咒语检查数据，正如我们在[第一章](ch01.html "Chapter 1. Statistics")、*统计*中使用`incanter.excel/read-xls`和`incanter.core/view`函数所做的那样:

```
(ns cljds.ch3.examples

  (:require [incanter.charts :as c]

            [incanter.core :as i]

            [incanter.excel :as xls]

            [incanter.stats :as s]))

(defn athlete-data []

  (-> (io/resource "all-london-2012-athletes.xlsx")

      (str)

      (xls/read-xls)))

(defn ex-3-1 []

  (i/view (athlete-data)))
```

如果您运行这段代码(无论是在 REPL 中还是在带有`lein run –e 3.1`的命令行上)，您应该会看到以下输出:

![Inspecting the data](graphics/7180OS_03_100.jpg)

幸运的是，数据在列中被清楚地标注出来，并包含以下信息:

*   运动员的名字
*   他们竞争的国家
*   年龄(年)
*   以厘米为单位的高度
*   重量(千克)
*   性别为字符串“M”或“F”
*   字符串形式的出生日期
*   字符串形式的出生地(带国家)
*   赢得的金牌
*   获得银牌
*   获得铜牌
*   获得的金牌、银牌和铜牌总数
*   他们参加的运动
*   事件作为逗号分隔的列表

尽管这些数据被清楚地标注出来，但是在身高、体重和出生地方面的数据差距还是很明显的。我们必须小心，确保这些不会绊倒我们。

<title>Visualizing the data</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 可视化数据

首先，我们将考虑 2012 年伦敦奥运会运动员的身高分布。让我们将高度值绘制成直方图，看看数据是如何分布的，记住首先要过滤零值:

```
(defn ex-3-2 []

  (-> (remove nil? (i/$ "Height, cm" (athlete-data)))

      (c/histogram :nbins 20

                   :x-label "Height, cm"

                   :y-label "Frequency")

      (i/view)))
```

此代码生成以下直方图:

![Visualizing the data](graphics/7180OS_03_110.jpg)

正如我们所料，数据大致呈正态分布。我国运动员的平均身高约为 177 厘米。让我们来看看 2012 年奥运会游泳运动员的体重分布:

```
(defn ex-3-3 []

  (-> (remove nil? (i/$ "Weight" (athlete-data)))

      (c/histogram :nbins 20

                   :x-label "Weight"

                   :y-label "Frequency")

      (i/view)))
```

此代码生成以下直方图:

![Visualizing the data](graphics/7180OS_03_120.jpg)

该数据显示出明显的偏差。峰右边的尾部比左边的长得多，所以我们说偏斜是正的。我们可以用 Incanter 的`incanter.stats/skewness`函数来量化数据的偏斜度:

```
(defn ex-3-4 []

  (->> (swimmer-data)

       (i/$ "Weight")

       (remove nil?) 

       (s/skewness)))

;; 0.238
```

幸运的是，通过使用 Incanter 的`incanter.core/log`函数对重量取对数，可以有效地缓解这种偏斜:

```
(defn ex-3-5 []

  (-> (remove nil? (i/$ "Weight" (athlete-data)))

      (i/log)

      (c/histogram :nbins 20

                   :x-label "log(Weight)"

                   :y-label "Frequency")

      (i/view)))
```

此代码会产生以下直方图:

![Visualizing the data](graphics/7180OS_03_130.jpg)

这更接近于正态分布。这表明体重是按照一种**对数正态分布**分布的。

<title>The log-normal distribution</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 对数正态分布

对数正态分布就是对数呈正态分布的一组值的分布。对数的底数可以是除 1 以外的任何正数。像正态分布一样，对数正态分布在描述许多自然发生的现象时也很重要。

对数表示一个固定数(底数)的幂，它必须被提升以产生一个给定的数。通过将对数绘制成直方图，我们已经表明这些幂近似正态分布。对数通常以 10 为底或以 *e* 为底:约等于 2.718 的超越数。Incanter 的`log`函数和它的逆函数`exp`都使用 base *e* 。 *log [e]* 又被称为**自然对数**或 *ln* ，因为它的性质使其特别适用于微积分。

对数正态分布往往出现在生长率与大小无关的生长过程中。这就是众所周知的*吉布拉特定律*，由罗伯特·吉布拉特于 1931 年正式定义，他注意到它适用于公司的成长。由于增长率是规模的一部分，大公司往往比小公司增长得更快。

### 注意

正态分布出现在许多小的变化具有加法效应的情况下，而对数正态分布出现在许多小的变化具有乘法效应的情况下。

自那以后，人们发现吉布拉特定律适用于许多情况，包括城市的大小，以及根据 Wolfram MathWorld 的说法，乔治·萧伯纳句子中的字数。

在本章的其余部分，我们将使用体重数据的自然对数，以便我们的数据近似呈正态分布。奥林匹克游泳运动员说，我们将选择一群体型大致相似的运动员。

## 可视化关联

确定两个变量是否相关的最快最简单的方法之一是在散点图上查看它们。我们将过滤数据，只选择游泳者，然后根据体重绘制身高图:

```
(defn swimmer-data []

  (->> (athlete-data)

       (i/$where {"Height, cm" {:$ne nil} "Weight" {:$ne nil}

                  "Sport" {:$eq "Swimming"}})))

(defn ex-3-6 []

  (let [data (swimmer-data)

        heights (i/$ "Height, cm" data)

        weights (i/log (i/$ "Weight" data))]

    (-> (c/scatter-plot heights weights

                        :x-label "Height, cm"

                        :y-label "Weight")

        (i/view))))
```

这段代码产生了下面的图:

![Visualizing correlation](graphics/7180OS_03_140.jpg)

输出清楚地显示了两个变量之间的关系。该图表具有两个以平均值为中心的相关正态分布变量的典型偏斜椭圆形状。下图比较了散点图与高度和原木重量的概率分布:

![Visualizing correlation](graphics/7180OS_03_150.jpg)

接近一个分布尾部的点也倾向于接近另一个分布的相同尾部，反之亦然。因此，这两种分布之间存在一种关系，我们将在接下来的几节中展示如何量化这种关系。如果我们仔细观察前面的散点图，我们会看到由于测量结果被四舍五入(身高和体重分别被四舍五入为厘米和千克)，这些点被分成列和行。在这种情况下，有时最好是*抖动*数据，以使关系的强度更清晰。没有抖动，看似一个点实际上是许多点共享完全相同的一对值。引入一些随机噪声降低了这种可能性。

## 战战兢兢

由于每个值都被四舍五入到最接近的厘米，因此被捕获为 180 的值实际上可能在 179.5 厘米和 180.5 厘米之间。为了消除这种影响，我们可以向每个高度数据点添加-0.5 到 0.5 范围内的随机噪声。

体重数据点被捕捉到最接近的千克，因此值 80 实际上可能是 79.5 千克和 80.5 千克之间的任何值。我们可以在相同的范围内添加随机噪声来消除这种影响(尽管很明显，这必须在我们取对数之前完成):

```
(defn jitter [limit]

  (fn [x]

    (let [amount (- (rand (* 2 limit)) limit)]

      (+ x amount))))

(defn ex-3-7 []

  (let [data (swimmer-data)

        heights (->> (i/$ "Height, cm" data)

                     (map (jitter 0.5)))

        weights (->> (i/$ "Weight" data)

                     (map (jitter 0.5))

                     (i/log))]

    (-> (c/scatter-plot heights weights

                        :x-label "Height, cm"

                        :y-label "Weight")

        (i/view))))
```

抖动的图形如下所示:

![Jittering](graphics/7180OS_03_160.jpg)

正如在第一章、*统计*中向散点图引入透明度一样，抖动是一种确保我们不会让偶然因素——如数据量或舍入伪像——模糊我们查看数据模式的能力的机制。

<title>Covariance</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 协方差

量化两个变量之间关系强度的一种方法是它们的协方差。这衡量两个变量一起变化的趋势。

如果我们有两个数列， *X* 和 *Y* ，它们与平均值的偏差为:

![Covariance](graphics/7180OS_03_01.jpg)![Covariance](graphics/7180OS_03_02.jpg)

其中*X[I]为索引 *i* 处 *X* 的值，*Y[I]为索引 *i* 处 *Y* 的值，![Covariance](graphics/7180OS_03_03.jpg)为 *X* 的平均值，![Covariance](graphics/7180OS_03_04.jpg)为 *Y* 的平均值。如果 *X* 和 *Y* 趋向于一起变化，它们与平均值的偏差趋向于具有相同的符号:如果它们小于平均值，则为负；如果它们大于平均值，则为正。如果我们将它们相乘，当它们符号相同时，乘积为正，当它们符号不同时，乘积为负。对于每个给定的样本，将乘积相加给出了两个变量在相同方向上偏离平均值的趋势的度量。**

协方差定义为这些乘积的平均值:

![Covariance](graphics/7180OS_03_05.jpg)

可以使用以下代码在 Clojure 中计算协方差:

```
(defn covariance [xs ys]

  (let [x-bar (s/mean xs)

        y-bar (s/mean xs)

        dx (map (fn [x] (- x x-bar)) xs)

        dy (map (fn [y] (- y y-bar)) ys)]

    (s/mean (map * dx dy))))
```

或者，我们可以使用`incanter.stats/covariance`函数。我们奥运会游泳运动员的身高和体重的协方差是`1.354`，但这是一个很难解释的数字。单位是投入单位的乘积。

因此，协方差很少单独作为汇总统计量报告。使数字更容易理解的一个解决方案是将偏差除以标准偏差的乘积。这将把单位转换为标准分数，并将输出限制为介于`-1`和`+1`之间的数。这个结果被称为**皮尔逊相关**。

<title>Pearson's correlation</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 皮尔逊相关

皮尔逊的相关性通常被赋予变量名 *r* ，并以如下方式计算，其中*dx[I]和*dy[I]如前计算:**

![Pearson's correlation](graphics/7180OS_03_06.jpg)

由于标准偏差是变量 *X* 和 *Y* 的常数值，等式可以简化如下，其中*σ[X]和*σ[Y]分别是 *X* 和 *Y* 的标准偏差:**

![Pearson's correlation](graphics/7180OS_03_07.jpg)

这有时被称为皮尔逊积差相关系数，或简称为*相关系数*，通常用字母 *r* 表示。

我们以前写过计算标准差的函数。结合我们的函数来计算协方差，得出以下皮尔逊相关的实现:

```
(defn correlation [x y]

  (/ (covariance x y)

     (* (standard-deviation x)

        (standard-deviation y))))
```

或者，我们可以使用`incanter.stats/correlation`函数。

因为标准分数是无量纲的，所以 *r* 也是无量纲的。如果 *r* 为-1.0 或 1.0，则变量完全负相关或完全正相关。

如果 *r* 为零，这并不一定意味着变量不相关。皮尔逊相关性只衡量线性关系。变量之间仍可能存在某种非线性关系，这种关系没有被 *r* 捕捉到，如下图所示:

![Pearson's correlation](graphics/7180OS_03_170.jpg)

注意，中心示例的相关性未定义，因为 *y* 的标准偏差为零。因为我们的 T2 r T3 方程会涉及到将协方差除以零，所以结果是没有意义的。在这种情况下，变量之间不可能有任何关联； *y* 的值总是平均值。对标准偏差的简单检查就可以证实这一点。

可以计算我们的游泳运动员的身高和对数体重数据的相关系数:

```
(defn ex-3-8 []

  (let [data (swimmer-data)

        heights (i/$ "Height, cm" data)

        weights (i/log (i/$ "Weight" data))]

    (correlation heights weights)))
```

这产生了答案`0.867`，它量化了我们已经在散点图上观察到的强正相关。

## 样本 r 和总体ρ

像均值或标准差一样，相关系数也是一个统计量。它描述了一个样本；在这种情况下，成对值的样本:身高和体重。我们已知的样本相关系数用字母 *r* 表示，未知的总体相关系数用希腊字母 rho: ![Sample r and population rho](graphics/7180OS_03_08.jpg)表示。

正如我们在上一章中发现的，我们不应该假设我们在样本中测量的结果适用于总体。在这种情况下，我们的人口可能是所有最近奥运会的游泳运动员。举个例子，将举重等其他奥运项目或无竞争力的游泳运动员一概而论是不合适的。

即使在适当的人群中——比如最近奥运会的游泳运动员——我们的样本也只是许多不同相关系数的潜在样本之一。作为对![Sample r and population rho](graphics/7180OS_03_08.jpg)的估计，我们能在多大程度上相信我们的*r*将取决于两个因素:

*   样本的大小
*   *r 的大小*

显然，对于一个公平的样本来说，样本越大，我们就越能相信它是总体人口的代表。对你来说，也许直觉上并不明显的是， *r* 的大小也会影响我们对它代表![Sample r and population rho](graphics/7180OS_03_08.jpg)的信心。原因是大的系数不太可能是偶然或由随机抽样误差引起的。

<title>Hypothesis testing</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 假设检验

在前一章中，我们介绍了假设检验作为一种量化给定假设(比如两个样本来自同一个总体)为真的概率的方法。我们将使用相同的过程来量化基于我们的样本在更广泛的人群中存在相关性的概率。

首先，我们必须制定两个假设，一个无效假设和一个替代假设:

![Hypothesis testing](graphics/7180OS_03_09.jpg)![Hypothesis testing](graphics/7180OS_03_10.jpg)

*H[0]是假设人口相关性为零。换句话说，我们保守的观点是，测量的相关性纯粹是由于随机抽样误差。*

*H[1]是人口相关性不为零的替代可能性。请注意，我们没有指定相关的方向，只指定了一个方向。这意味着我们正在执行一个双尾测试。*

样本 *r* 的标准误差由下式给出:

![Hypothesis testing](graphics/7180OS_03_11.jpg)

这个公式只有在![Hypothesis testing](graphics/7180OS_03_08.jpg)接近零时才是准确的(回想一下 *r* 的大小会影响我们的信心)，但幸运的是，这正是我们在零假设下所假设的。

同样，我们可以利用*t*-分布来计算我们的*t*-统计量:

![Hypothesis testing](graphics/7180OS_03_12.jpg)

df 项是我们数据的自由度。对于相关性测试，自由度是 *n - 2* ，其中 *n* 是样本的大小。将该值代入公式，我们得到:

![Hypothesis testing](graphics/7180OS_03_13.jpg)

这给了我们一个*t*-`102.21`的值。要将其转换成一个 *p* 值，我们需要参考*t*-分布。Incanter 为具有`incanter.stats/cdf-t`功能的 *t* 分配提供了**累积分配功能** ( **CDF** )。CDF 的值对应于单尾测试的 *p* 值。我们将该值乘以 2，因为我们正在执行双尾测试:

```
(defn t-statistic [x y]

  (let [r (correlation x y)

        r-square (* r r)

        df (- (count x) 2)]

    (/ (* r df)

       (i/sqrt (- 1 r-square)))))

(defn ex-3-9 []

  (let [data (swimmer-data)

        heights (i/$ "Height, cm" data)

        weights (i/log (i/$ "Weight" data))

        t-value (t-statistic heights weights)

        df (- (count heights) 2)

        p  (* 2 (s/cdf-t t-value :df df :lower-tail? false))]

    (println "t-value" t-value)

    (println "p value " p)))
```

p 的值小到基本为零，这意味着零假设为真的可能性基本不存在。我们被迫接受另一种假设。

<title>Confidence intervals</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 置信区间

已经确定在更广泛的人群中确实存在相关性，我们可能想要通过计算置信区间来量化我们期望![Confidence intervals](graphics/7180OS_03_08.jpg)位于的值的范围。与上一章中的均值一样， *r* 的置信区间表示总体参数![Confidence intervals](graphics/7180OS_03_08.jpg)位于两个特定值之间的概率(以百分比表示)。

然而，当试图计算平均值不存在的相关系数的标准误差时，出现了复杂情况。因为 *r* 的绝对值不能超过 **1** ，当 *r* 接近其范围的极限时， *r* 的可能样本的分布是偏斜的。

![Confidence intervals](graphics/7180OS_03_180.jpg)

上图显示了![Confidence intervals](graphics/7180OS_03_08.jpg)为 0.6 时 *r* 样本的负偏态分布。

幸运的是，一种叫做**费希尔 z 变换**的变换将稳定 *r* 在其整个范围内的方差。这类似于当我们取对数时，我们的重量数据如何变成正态分布。

*z*-变换的方程式为:

![Confidence intervals](graphics/7180OS_03_14.jpg)

*z* 的标准误差为:

![Confidence intervals](graphics/7180OS_03_15.jpg)

因此，计算置信区间的过程是使用*z*-变换将 *r* 转换为 *z* ，根据*SE[z]计算置信区间，然后将置信区间转换回 *r* 。*

为了根据*SE[z]计算置信区间，我们可以从平均值中减去标准偏差的数量，从而得到我们想要的置信度。1.96 是一个常用的数字，因为它是包含 95%面积的平均值的标准偏差数。换句话说，样本平均值的 1.96 个标准误差 *r* 包含 95%确定性的真实总体相关性 *ρ* 。*

![Confidence intervals](graphics/7180OS_03_190.jpg)

我们可以使用 Incanter 的`incanter.stats/quantile-normal`函数来验证这一点。这将返回与给定累积概率相关的标准分数，假设是单尾测试。

然而，如上图所示，我们想从每个尾部减去相同的量-2.5%，这样 95%的置信区间就以零为中心。一个简单的转换是在执行双尾测试时将差异减半至 100%。因此，95%的期望置信度意味着我们查找 97.5%的临界值:

```
(defn critical-value [confidence ntails]

  (let [lookup (- 1 (/ (- 1 confidence) ntails))]

    (s/quantile-normal lookup)))

(critical-value 0.95 2)

=> 1.96
```

因此，![Confidence intervals](graphics/7180OS_03_08.jpg)在 *z* 空间的 95%置信区间由下式给出:

![Confidence intervals](graphics/7180OS_03_16.jpg)

将我们的公式代入*z[r]和*SE[z]得到:**

![Confidence intervals](graphics/7180OS_03_17.jpg)

对于`r = 0.867`和`n = 859`，这分别给出了`1.137`和`1.722`的下限和上限。为了将这些从*z*-分数转换回*r*-值，我们使用以下等式，即*z*-变换的逆变换:

![Confidence intervals](graphics/7180OS_03_18.jpg)

可以使用以下代码计算转换和置信区间:

```
(defn z->r [z]

  (/ (- (i/exp (* 2 z)) 1)

     (+ (i/exp (* 2 z)) 1)))

(defn r-confidence-interval [crit x y]

  (let [r   (correlation x y)

        n   (count x)

        zr  (* 0.5 (i/log (/ (+ 1 r)

                             (- 1 r))))

        sez (/ 1 (i/sqrt (- n 3)))]

    [(z->r (- zr (* crit sez)))

     (z->r (+ zr (* crit sez)))]))

(defn ex-3-10 []

  (let [data (swimmer-data)

        heights  (i/$ "Height, cm" data)

        weights  (i/log (i/$ "Weight" data))

        interval (r-confidence-interval 1.96 heights weights)]

    (println "Confidence Interval (95%): " interval)))
```

这使得![Confidence intervals](graphics/7180OS_03_08.jpg)在`0.850`和`0.883`之间有 95%的置信区间。我们可以非常自信地说在更广泛的奥林匹克级游泳运动员人群中，身高和体重之间存在着很强的正相关关系。

<title>Regression</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 回归

虽然知道两个变量是相关的可能是有用的，但我们不能单独使用这一信息来预测给定身高的奥运游泳运动员的体重，反之亦然。在建立相关性时，我们测量了关系的强度和符号，但没有测量斜率。为了进行预测，需要知道一个变量的预期变化率(给定另一个变量的单位变化)。

我们想要确定的是一个方程，它将一个变量的特定值，称为独立变量**，与另一个变量**的期望值，称为非独立变量****。例如，如果我们的线性方程预测给定身高的体重，那么身高是我们的自变量，体重是我们的因变量。****

### ****注意****

****这些方程描述的线被称为**回归线**。这个术语是由 19 世纪英国学者弗朗西斯·高尔顿爵士引入的。他和他的学生卡尔·皮尔逊(他定义了相关系数)在 19 世纪发展了多种研究线性关系的方法，这些方法统称为回归技术。****

****请记住，相关性并不意味着因果关系，相关和独立这两个术语也不意味着因果关系——它们只是数学输入和输出的名称。一个经典的例子是派往火灾的消防车数量和火灾造成的损失之间的高度正相关。显然，派消防车去救火本身不会造成损害。没有人会建议减少投入火灾的消防车数量来减少损失。在这种情况下，我们应该寻找一个额外的变量，它与其他变量有因果关系，并解释它们之间的相关性。在前面的例子中，这可能是火的*大小。这种隐藏的原因被称为**混淆** **变量**，因为它们混淆了我们确定其因变量之间关系的能力。*****

## ****线性方程****

****两个变量我们可以用 *x* 和 *y* 来表示，这两个变量可能精确地相关，也可能不精确地相关。标有 *x* 的自变量和标有 *y* 的因变量之间最简单的关系是一条直线，用公式表示:****

****![Linear equations](graphics/7180OS_03_19.jpg)****

****这里，参数 *a* 和 *b* 的值分别确定线的精确高度和陡度。参数 *a* 被称为截距或常数，参数 *b* 被称为梯度或斜率。例如，在摄氏和华氏温标之间的映射中， *a = 32* 和 *b = 1.8* 。将 *a* 和 *b* 的这些值代入我们的等式，得出:****

****![Linear equations](graphics/7180OS_03_20.jpg)****

****为了计算华氏 10 度，我们用 10 代替 *x* :****

****![Linear equations](graphics/7180OS_03_21.jpg)****

****因此，我们的等式告诉我们，10 摄氏度是 50 华氏度，事实确实如此。使用 Incanter，我们可以很容易地编写一个将摄氏温度映射到华氏温度的函数，并使用`incanter.charts/function-plot`将其绘制成图表:****

```
**(defn celsius->fahrenheit [x]

  (+ 32 (* 1.8 x)))

(defn ex-3-11 []

  (-> (c/function-plot celsius->fahrenheit -10 40

                       :x-label "Celsius"

                       :y-label "Fahrenheit")

      (i/view)))**
```

****这段代码产生了下面的线图:****

****![Linear equations](graphics/7180OS_03_200.jpg)****

****注意红线是如何在华氏 32 度时穿过摄氏零度的。截距 *a* 是 *y* 的值，其中 *x* 为零。****

****直线的斜率由 *b* 决定；这个方程接近于 2。看看华氏温标的量程是摄氏温标量程的两倍。换句话说，直线垂直扫描的速度几乎是水平扫描的两倍。****

## ****残差****

****不幸的是，我们将要研究的关系很少像摄氏和华氏之间的映射那样清晰。直线方程很少允许我们用 *x* 精确地指定 *y* 。通常会有误差，因此:****

****![Residuals](graphics/7180OS_03_22.jpg)****

****这里， *ε* 是一个误差项，代表参数 *a* 和 *b* 对于给定值 *x* 的计算值与 *y* 的实际值之差。如果我们对 *y* 的预测值是![Residuals](graphics/7180OS_03_23.jpg)(读作“y-hat”)，那么误差就是两者之差:****

****![Residuals](graphics/7180OS_03_24.jpg)****

****这个误差被称为残差。残差可能是由于像测量误差这样的随机因素或者未知的非随机因素造成的。例如，如果我们试图预测作为身高函数的体重，未知因素可能包括饮食、健康水平和体型(或者简单地四舍五入到最接近的千克)。****

****如果我们为 *a* 和 *b* 选择不理想的参数，那么每个 *x* 的残差将比它需要的要大。因此，我们想要找到的参数是那些最小化所有值 *x* 和 *y* 的残差的参数。****

****<title>Ordinary least squares</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 普通最小二乘法

为了优化我们的线性模型的参数，我们想要设计一个成本函数，也称为损失函数 T2，它量化了我们的预测与数据的吻合程度。我们不能简单地将正负残差相加，因为如果符号相反，即使很大的残差也会相互抵消。

我们可以在计算总和之前对这些值求平方，这样正负残差都计入成本。这也具有对大误差的惩罚比对小误差的惩罚更多的效果，但不会大到最大残差总是占优势的程度。

表达为优化问题，我们寻求识别最小化残差平方和的系数。这叫**普通最小二乘法** ( **OLS** )，用 OLS 计算回归线斜率的公式是:

![Ordinary least squares](graphics/7180OS_03_25.jpg)

虽然这看起来比前面的方程更复杂，但它实际上只是残差平方和除以均值的平方差和。这与我们已经讨论过的等式中的许多项相同，可以简化为:

![Ordinary least squares](graphics/7180OS_03_26.jpg)

截距是允许该斜率的直线通过 *X* 和 *Y* 的平均值的术语:

![Ordinary least squares](graphics/7180OS_03_27.jpg)

*a* 和 *b* 的这些值是我们最小二乘估计的系数。

## 斜率和截距

我们已经编写了`covariance`、`variance`和`mean`、函数，我们需要这些函数来计算游泳高度和体重数据的斜率和截距。因此，斜率和截距的计算很简单:

```
(defn slope [x y]

  (/ (covariance x y)

     (variance x)))

(defn intercept [x y]

  (- (s/mean y)

     (* (s/mean x)

        (slope x y))))

(defn ex-3-12 []

  (let [data (swimmer-data)

        heights (i/$ "Height, cm" data)

        weights (i/log (i/$ "Weight" data))

        a (intercept heights weights)

        b (slope heights weights)]

    (println "Intercept: " a)

    (println "Slope: " b)))
```

输出给一个大约为`0.0143`的斜率和一个大约为`1.6910`的截距。

## 释义

**截距值**是自变量(`height`)为零时因变量(对数权重)的值。为了找出这个值等于多少千克，我们可以使用`incanter.core/exp`函数，它执行`incanter.core/log`函数的逆函数。我们的模型似乎表明，零身高的奥运会游泳运动员的最佳体重猜测是 5.42 公斤。这是没有意义的，超出你的训练数据范围进行推断是不明智的。

斜率值显示了 *x* 中每单位变化 *y* 的变化量。我们的模型表明，身高每增加一厘米，我们的奥运游泳运动员的体重平均增加 1.014 公斤。由于我们的模型是基于所有奥运会游泳运动员的，这是在不考虑任何其他因素，如年龄、性别或体型的情况下，单位身高增加的平均效果。

## 可视化

我们可以用`incanter.charts/function-plot`和一个基于系数 *a* 和 *b* 计算![Visualization](graphics/7180OS_03_23.jpg)的简单函数 *x* 来形象化我们的线性方程的输出。

```
(defn regression-line [a b]

  (fn [x] 

    (+ a (* b x))))

(defn ex-3-13 []

  (let [data (swimmer-data)

        heights (->> (i/$ "Height, cm" data)

                     (map (jitter 0.5)))

        weights (i/log (i/$ "Weight" data))

        a (intercept heights weights)

        b (slope heights weights)]

    (-> (c/scatter-plot heights weights

                        :x-label "Height, cm"

                        :y-label "log(Weight)")

        (c/add-function (regression-line a b) 150 210)

        (i/view))))
```

`regression-line` 函数返回一个计算![Visualization](graphics/7180OS_03_28.jpg)的函数 *x* 。

![Visualization](graphics/7180OS_03_210.jpg)

我们还可以使用`regression-line`函数来计算每个残差，显示我们的估计值![Visualization](graphics/7180OS_03_23.jpg)偏离每个测量值 *y* 的程度。

```
(defn residuals [a b x y]

  (let [estimate (regression-line a b)

        residual (fn [x y]

                   (- y (estimate x)))]

    (map residual x y)))

(defn ex-3-14 []

  (let [data (swimmer-data)

        heights (->> (i/$ "Height, cm" data)

                     (map (jitter 0.5)))

        weights (i/log (i/$ "Weight" data))

        a (intercept heights weights)

        b (slope heights weights)]

    (-> (c/scatter-plot heights (residuals a b heights weights)

                        :x-label "Height, cm"

                        :y-label "Residuals")

        (c/add-function (constantly 0) 150 210)

        (i/view))))
```

**残差图**是一个图，它在 *y* 轴上显示残差，在 *x* 轴上显示独立变量。如果残差图中的点围绕水平轴随机分布，则线性模型非常适合数据:

![Visualization](graphics/7180OS_03_220.jpg)

除了图表左侧的一些异常值，残差图似乎表明线性模型非常适合数据。绘制残差图对于验证线性模型是否合适非常重要。线性模型对您的数据做出了某些假设，如果违反这些假设，您构建的模型将会失效。

## 假设

显然，线性回归的主要假设是因变量和自变量之间存在线性关系。此外，残差不得相互关联，也不得与自变量相关。换句话说，我们期望误差相对于因变量和自变量具有零均值和恒定方差。残差图允许我们快速确定是否是这种情况。

残差图的左侧比右侧有更大的残差。这与较矮运动员的体重差异较大相对应。当一个变量的方差相对于另一个变量发生变化时，称这些变量为**异方差**。这是回归分析中的一个问题，因为它否定了建模误差不相关且呈正态分布的假设，以及它们的方差不随被建模的效应而变化的假设。

我们残差的异方差相当小，应该不会对我们模型的质量产生太大影响。如果图表左侧的方差更明显，将导致方差的最小二乘估计不正确，进而影响我们基于标准误差做出的推断。

<title>Goodness-of-fit and R-square</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 拟合优度和 R 平方

尽管我们可以从残差图中看出，线性模型非常适合我们的数据，但量化它有多好将是可取的。也称为**决定系数**， *R ²* 在零和一之间变化，表示线性回归模型的解释力。它计算由自变量解释或说明的因变量的变化比例。

一般来说，*R²越接近 1，回归线对点的拟合越好，用 *X* 来解释 *Y* 的变化就越多。*R²可以用以下公式计算:**

![Goodness-of-fit and R-square](graphics/7180OS_03_29.jpg)

这里， *var(ε)* 是残差的方差， *var(Y)* 是 *Y* 的方差。为了理解这意味着什么，让我们假设你正试图猜测某人的体重。如果你对它们一无所知，你最好的策略是猜测总体权重的平均值。这样，你的猜测与其真实体重相比的均方误差将是 *var(Y)* 或者总体中体重的方差。

但是如果我告诉你他们的身高，你会根据回归模型猜测。在这种情况下，您的均方误差将是 *var(ε)* 或模型残差的方差。

术语 *var(ε)/ var(Y)* 是有和没有解释变量的均方误差之比，是模型未解释的可变性的分数。补码*R²是模型解释的可变性的分数。*

### 注意

与 *r* 一样，低*R²并不意味着这两个变量不相关。可能只是因为它们的关系不是线性的。*

*R²值描述了直线与数据的拟合程度。*最佳拟合*的直线是使*R²的值最小的直线。随着系数远离其最佳值增加或减少，*R²将总是增加。***

![Goodness-of-fit and R-square](graphics/7180OS_03_240.jpg)

左图显示了一个模型的方差，该模型总是猜测 *y* 的平均值，右图显示了与模型 *f* 未解释的残差相关的较小的正方形。从纯粹的几何角度来看，你可以看到这个模型如何解释了 *y* 的大部分差异。以下代码通过将残差的方差除以 *y* 值的方差来计算*R²:*

```
(defn r-squared [a b x y]

  (let [r-var (variance (residuals a b x y))

        y-var (variance y)]

    (- 1 (/ r-var y-var))))

(defn ex-3-15 []

  (let [data (swimmer-data)

        heights (i/$ "Height, cm" data)

        weights (i/log (i/$ "Weight" data))

        a (intercept heights weights)

        b (slope heights weights)]

    (r-squared a b heights weights)))
```

这给出了一个值`0.753`。换句话说，2012 年奥运会游泳运动员体重差异的 75%以上可以用身高来解释。

在简单回归模型(具有单个独立变量)的情况下，决定系数*R²和相关系数 *r* 之间的关系是直接的:*

![Goodness-of-fit and R-square](graphics/7180OS_03_30.jpg)

0.5 的相关系数可能意味着 *Y* 的一半可变性由 *X* 来解释，但实际上， *R ²* 将是 0.5 ² 或 0.25。

<title>Multiple linear regression</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 多元线性回归

到目前为止，我们已经了解了如何用一个独立变量建立回归线。然而，通常需要建立一个包含几个独立变量的模型。这就是所谓的**多元线性回归**。

每个独立变量都需要自己的系数。让我们指定一个新的变量 *β* ，读作“beta ”,来保存我们所有的系数，而不是通过字母表来表示每一个:

![Multiple linear regression](graphics/7180OS_03_31.jpg)

这个模型相当于我们的二元线性回归模型，只要我们确保 T5 和 T7 总是等于 1，那么这个模型就相当于。这确保了 *β [1]* 始终是代表我们截距的常数因子。*x[1]被称为**偏差** **术语**。*

根据β推广线性方程后，很容易扩展到我们想要的任意多个系数:

![Multiple linear regression](graphics/7180OS_03_34.jpg)

从*x[1]到 *x [n]* 的每一个值都对应于一个独立变量，这可能有助于解释 *y* 的值。从*β[1]到*β[n]的每一个值都对应一个系数，这个系数决定了这个自变量的相对贡献。***

我们简单的线性回归旨在仅用身高来解释体重，但许多其他因素也有助于解释某人的体重:他们的年龄、性别、饮食和体型。我们知道我们的奥运游泳运动员的年龄，所以我们也可以建立一个包含这些额外数据的模型。

我们已经提供了作为单个值序列的独立变量，但是对于多个参数，我们需要为每个 *x* 提供几个值。我们可以使用 Incanter 的`i/$`函数来选择多个列，并将每个 *x* 作为 Clojure 向量来操作，但是有一个更好的方法:矩阵。

<title>Matrices</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 矩阵

矩阵是一个由数字组成的二维网格。维度表示为矩阵中的行数和列数。

例如， *A* 是一个四行两列的矩阵:

![Matrices](graphics/7180OS_03_35.jpg)

在数学符号中，矩阵通常会用大写字母分配给变量，以区别于方程中的其他变量。

我们可以使用 Incanter 的`incanter.core/to-matrix`函数从数据集构建一个矩阵:

```
(defn ex-3-16 []

  (->> (swimmer-data)

       (i/$ ["Height, cm" "Weight"])

       (i/to-matrix)))
```

Incanter 还定义了`incanter.core/matrix`函数，该函数将接受一系列标量值或一系列序列，并在可能的情况下将它们转换为矩阵:

```
(defn ex-3-17 []

  (->> (swimmer-data)

       (i/$ "Height, cm")

       (i/matrix)))
```

如果在 REPL 中运行，输出将是矩阵内容的摘要:

```
  A 859x1 matrix

 ---------------

 1.66e+02

 1.92e+02

 1.73e+02

 ...

 1.88e+02

 1.87e+02

 1.83e+02
```

Incanter 返回一个与前面的示例完全相同的表示，只显示矩阵的顶部和底部三行。矩阵经常会变得非常大，而 Incanter 注意不要让信息淹没 REPL。

## 尺寸

第 *i ^第行 *j ^第列中的元素被称为*A[ij]。因此，在我们前面的例子中:***

![Dimensions](graphics/7180OS_03_36.jpg)

矩阵最基本的属性之一是它的大小。Incanter 提供了`incanter.core/dim`、`ncol`和`nrow`函数来查询矩阵维度。

## 矢量

向量是只有一列的矩阵的特殊情况。向量中的行数称为其维数:

![Vectors](graphics/7180OS_03_37.jpg)

这里， *y* 是一个四维向量。第 *i ^个T5 个元素简称为*y[I]。**

除非另有说明，否则数学文献中的向量是单索引的。所以，*y[1]指的是第一个元素，而不是第二个。向量一般赋给等式中的小写变量。Incanter 的 API 不区分向量和单列矩阵，我们可以通过向`incanter.core/matrix`函数传递单个序列来创建向量。*

## 建筑

正如我们所见，从 Clojure 序列和咒语数据集构建矩阵是可能的。如果维度兼容，也可以用更小的构建块来构建矩阵。Incanter 提供了`incanter.core/bind-columns`和`incanter.core/bind-rows`函数来堆叠矩阵。

例如，我们可以通过以下方式将一列 1 添加到另一个矩阵的前面:

```
(defn add-bias [x]

  (i/bind-columns (repeat (i/nrow x) 1) x))
```

事实上，我们想为我们的偏项做这个。回想一下*β[1]将代表一个常量值，那么我们必须保证我们对应的 *x [1]* 也是常量。如果没有偏置项，当 *x* 的值为零时， *y* 必须为零。*

## 加法和标量乘法

标量是一个简单数字的名称。当我们给一个矩阵添加一个标量时，就好像我们给矩阵的每个元素分别添加了一个数字。Incanter 提供了`incanter.core/plus`函数来将标量和矩阵相加。

Matrix-matrix 加法的工作原理是将每个对应位置的元素相加。只有相同维数的矩阵才能相加。如果矩阵的维数相同，就说它们是相容的。

![Addition and scalar multiplication](graphics/7180OS_03_38.jpg)

`plus`功能也将添加兼容的矩阵。函数将减去标量或与 T2 兼容的矩阵。将矩阵乘以标量导致矩阵中的每个元素都乘以标量。

![Addition and scalar multiplication](graphics/7180OS_03_39.jpg)

`incanter.core/mult`执行矩阵-标量乘法，而`incanter.core/div` 执行相反的操作。

我们也可以在兼容的矩阵上使用`mult`和`div`，但是当我们谈到矩阵乘法时，这种乘除的基于元素的方法不是我们通常想要做的。

## 矩阵-向量乘法

矩阵相乘的标准方式由`incanter.core/mmult`函数处理，该函数应用复杂的矩阵乘法算法。例如，3×2 矩阵乘以 2×1 矩阵的结果是 3×1 矩阵。乘法左边的列数必须与右边的行数相匹配:

![Matrix-vector multiplication](graphics/7180OS_03_40.jpg)![Matrix-vector multiplication](graphics/7180OS_03_41.jpg)![Matrix-vector multiplication](graphics/7180OS_03_42.jpg)

为了得到 *Ax* ，将 *A* 的每一行与 *x* 的相应元素逐个相乘，并将结果相加。例如，矩阵 *A* 的第一行包含元素 *1* 和 *3* 。这些成对地乘以向量 *x* 中的元素 *1* 和 *5* 。然后，将这些乘积相加得到 *16* 。这被称为**点积**和是矩阵乘法通常想要的。

## 矩阵-矩阵乘法

矩阵-矩阵乘法的过程与矩阵-向量乘法非常相似。从矩阵 *A* 和 *B* 的相应元素中逐行逐列地成对获取乘积的和。

![Matrix-matrix multiplication](graphics/7180OS_03_40.jpg)![Matrix-matrix multiplication](graphics/7180OS_03_43.jpg)![Matrix-matrix multiplication](graphics/7180OS_03_44.jpg)

和以前一样，只有当第一个矩阵的列数等于第二个矩阵的行数时，我们才能将矩阵相乘。如果第一矩阵 *A* 的维数为![Matrix-matrix multiplication](graphics/7180OS_03_45.jpg)并且第二矩阵 *B* 的维数为![Matrix-matrix multiplication](graphics/7180OS_03_46.jpg)，那么如果矩阵要相乘，则*n[A]和*m[B]必须相等。**

![Matrix-matrix multiplication](graphics/7180OS_03_250.jpg)

在前面的视觉示例中:

![Matrix-matrix multiplication](graphics/7180OS_03_47.jpg)![Matrix-matrix multiplication](graphics/7180OS_03_48.jpg)

幸运的是，我们不需要自己记住这个过程。Incanter 使用非常高效的算法为我们执行矩阵代数。

## 换位

转置一个矩阵意味着在从左上角到右下角的主对角线上翻转矩阵。矩阵 *A* 的转置表示为 *A ^T* :

![Transposition](graphics/7180OS_03_49.jpg)

列和行已被更改，因此:

![Transposition](graphics/7180OS_03_50.jpg)

因此，如果:

![Transposition](graphics/7180OS_03_36.jpg)

然后:

![Transposition](graphics/7180OS_03_51.jpg)

Incanter 提供了`incanter.core/trans`函数来转置一个矩阵。

## 单位矩阵

某些矩阵具有特殊性质，在矩阵代数中有规律地使用。其中最重要的是单位矩阵。这是一个正方形矩阵，主对角线上是 1，其他地方都是 0:

![The identity matrix](graphics/7180OS_03_52.jpg)

单位矩阵是矩阵乘法的单位矩阵。与标量乘以数字 1 一样，矩阵乘以单位矩阵没有效果。

Incanter 提供了`incanter.core/identity-matrix`函数来构造单位矩阵。因为它们总是正方形，我们只提供一个参数来对应宽度和高度。

## 倒置

如果我们有一个正方形矩阵 *A* ，则 *A* 的逆矩阵表示为 *A ^(-1)* ，它将具有以下性质，其中 *I* 是单位矩阵:

![Inversion](graphics/7180OS_03_53.jpg)

恒等式矩阵是它自己的逆矩阵。并非所有矩阵都是可逆的，不可逆矩阵也被称为**奇异**或**退化**矩阵。我们可以用`incanter.core/solve` 函数计算一个矩阵的逆。`solve`如果传递了一个奇异矩阵会引发一个异常。

<title>The normal equation</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 法线方程

既然我们已经涵盖了矩阵和向量操作的基础知识，我们就可以研究**正规方程**了。这是一个使用矩阵代数来计算 OLS 线性回归模型系数的方程:

![The normal equation](graphics/7180OS_03_54.jpg)

我们读到“求 *β* ，乘以 *X* 转置 *X* 的倒数，乘以 *X* 转置 *y* ”，其中 *X* 是我们样本的独立变量(包括截距项)的矩阵， *y* 是包含我们样本的因变量的向量。结果 *β* 包含计算的系数。这个正规方程相对容易从多元回归方程中推导出来，应用矩阵乘法的规则，但是数学不在本书的讨论范围之内。

我们可以使用刚刚遇到的函数用 Incanter 实现法线方程:

```
(defn normal-equation [x y]

  (let [xtx  (i/mmult (i/trans x) x)

        xtxi (i/solve xtx)

        xty  (i/mmult (i/trans x) y)]

    (i/mmult xtxi xty)))
```

这个正规方程以非常简洁的方式表达了最小二乘线性回归的数学。我们可以如下使用它(记得加上偏置项):

```
(defn ex-3-18 []

  (let [data (swimmer-data)

        x (i/matrix (i/$ "Height, cm" data))

        y (i/matrix (i/log (i/$ "Weight" data)))]

    (normal-equation (add-bias x) y)))
```

这产生了以下矩阵:

```
 A 2x1 matrix

 -------------

 1.69e+00

 1.43e-02
```

这些是截距和斜率参数对应的*β[1]T3 和 *β [2]* 的值。令人高兴的是，它们与我们之前计算的值一致。*

## 更多功能

法线方程的部分优势在于，我们现在已经实现了支持多元线性回归所需的一切。让我们编写一个函数来将感兴趣的特征转换为矩阵:

```
(defn feature-matrix [col-names dataset]

  (-> (i/$ col-names dataset)

      (i/to-matrix)))
```

该功能将允许我们一步选择特定的列作为矩阵。

### 注意

特征是独立变量的同义词，广泛用于机器学习。其他同义词有预测变量、回归变量、解释变量或简单的输入变量。

首先，让我们选择身高和年龄作为我们的两个特征:

```
(defn ex-3-19 []

  (feature-matrix ["Height, cm" "Age"] (swimmer-data)))
```

这将返回以下两列矩阵:

```
A 859x2 matrix

 ---------------

 1.66e+02  2.30e+01

 1.92e+02  2.20e+01

 1.73e+02  2.00e+01

 ...

 1.88e+02  2.40e+01

 1.87e+02  1.90e+01

 1.83e+02  2.20e+01
```

我们的正规方程函数将接受这个新矩阵，而不做任何进一步的改变:

```
(defn ex-3-20 []

  (let [data (swimmer-data)

        x (->> data

                (feature-matrix ["Height, cm" "Age"])

                (add-bias))

        y (->> (i/$ "Weight" data)

                (i/log)

                (i/matrix))]

    (normal-equation x y)))
```

它将返回以下系数:

```
 A 3x1 matrix

 -------------

 1.69e+00

 1.40e-02

 2.80e-03
```

这三个数字分别对应于截距、高度斜率和年龄斜率。为了确定我们的模型是否因这些新数据而得到显著改进，我们可以计算新模型的*R²值，并将其与早期模型进行比较。*

<title>Multiple R-squared</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 多重 R 平方

在之前计算 *R ²* 时，我们看到了模型是如何解释方差的:

![Multiple R-squared](graphics/7180OS_03_55.jpg)

由于方差是均方误差，我们可以将 *var(ε)* 和 *var(y)* 项乘以样本大小，并得出 R ² 的以下替代等式:

![Multiple R-squared](graphics/7180OS_03_56.jpg)

这是简单的残差平方和除以均值的差平方和。Incanter 包含和`incanter.core/sum-of-squares`功能，这使得表达非常简单:

```
(defn r-squared [coefs x y]

   (let [fitted      (i/mmult x coefs)

         residuals   (i/minus y fitted)

         differences (i/minus y (s/mean y))

         rss         (i/sum-of-squares residuals)

         ess         (i/sum-of-squares differences)]

     (- 1 (/ rss ess))))
```

我们用变量名`rss`表示**残差平方和**，用`ess`表示**解释平方和**。我们可以为新模型计算矩阵*R²T9 】,如下所示:*

```
(defn ex-3-21 []

  (let [data (swimmer-data)

        x (->> (feature-matrix ["Height, cm" "Age"] data)

               (add-bias))

        y (->> (i/$ "Weight" data)

               (i/log)

               (i/matrix))

        beta (normal-equation x y)]

    (r-squared beta x y)))
```

这产生了值`0.757`。我们的 *R ²* 值因为包含了年龄值而有了少量的增加。因为我们使用了多个自变量，*R²现在称为 **多重决定**的**系数。***

<title>Adjusted R-squared</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 调整后的 R 平方

随着我们向回归中添加更多的独立变量，我们可能会因为我们的 *R ²* 值总是增加而受到鼓舞。添加一个新的自变量不会增加预测因变量的难度——如果新变量没有解释力，那么它的系数将简单地为零，R ² 将保持与没有自变量时相同。

然而，这并没有告诉我们一个模型是否因为增加了一个新的变量而得到了改进。如果我们想知道我们的新变量是否真的有助于它产生更好的拟合，我们可以使用调整后的*R²，通常写成![Adjusted R-squared](graphics/7180OS_03_57.jpg)，读作“R-bar 平方”与 *R ^( 2 )* 不同，![Adjusted R-squared](graphics/7180OS_03_57.jpg)只会在新的独立变量增加 *R ^( 2 )* 超过预期时才会增加:*

```
(defn matrix-adj-r-squared [coefs x y]

  (let [r-squared (matrix-r-squared coefs x y)

        n (count y)

        p (count coefs)]

    (- 1

       (* (- 1 r-squared)

          (/ (dec n)

             (dec (- n p)))))))
```

调整后的*R²取决于两个附加参数 *n* 和 *p* ，分别对应于样本大小和模型参数的个数:*

```
(defn ex-3-22 []

  (let [data (swimmer-data)

        x (->> (feature-matrix ["Height, cm" "Age"] data)

               (add-bias))

        y (->> (i/$ "Weight" data)

               (i/log)

               (i/matrix))

        beta (normal-equation x y)]

    (adj-r-squared beta x y)))
```

这个例子返回一个值`0.756`。这仍然大于原始模型，因此年龄当然具有一定的解释力。

## 咒语的线性模型

虽然实现了我们自己版本的正规方程和*R²提供了一个引入矩阵代数的宝贵机会，但重要的是要注意，Incanter 提供了`incanter.stats/linear-model`函数，它可以做我们已经介绍过的所有事情，甚至更多。*

该函数期望使用 *y* 和 *x* 进行调用(作为序列，或者在多重回归的情况下，作为矩阵)。我们还可以传入一个可选的关键字参数— `intercept`和一个布尔值—表明我们是否希望 Incanter 为我们添加截取项。该函数将返回包含线性模型系数— `:coefs`和拟合数据— `:fitted`，以及`:residuals`、`:r-square`和`:adj-r-square`等的图。

它还将返回系数的显著性测试和 95%的置信区间，分别作为`:t-probs`和`:coefs-ci`键，以及`:f-prob`键，对应于回归模型作为一个整体的显著性测试。

### 模型显著性的 f 检验

由`linear-model`返回的`:f-prob`键是使用 *F* 测试的整个模型的显著性测试。正如我们在前一章中发现的，当同时执行多个显著性测试时，一个 *F* 测试是合适的。在多元线性回归的情况下，我们正在测试模型的任何系数，除了截距项，是否在统计上与零没有区别。

因此，我们的无效假设和替代假设是:

![The F-test of model significance](graphics/7180OS_03_58.jpg)![The F-test of model significance](graphics/7180OS_03_59.jpg)

这里， *j* 是参数向量中不包括截距的某个索引。我们计算的*F*-统计量是解释的方差与未解释的(剩余)方差之比。这可以用表示为**均方模型** ( **MSM** )除以**均方误差****(**MSE**):**

**![The F-test of model significance](graphics/7180OS_03_60.jpg)**

**MSM 等于除以模型自由度的**解释的平方和** ( **ESS** ，其中模型自由度是模型中参数的数量，不包括截距项。MSE 等于 **残差平方和** ( **RSS** )除以残差自由度，其中残差自由度为样本大小减去模型参数数量。**

**一旦我们计算了*F*-统计量，我们就在由相同的两个自由度参数化的 *F* 分布中查找它:**

```
(defn f-test [y x]

  (let [coefs       (normal-equation x y)

        fitted      (i/mmult x coefs)

        difference  (i/minus fitted (s/mean y))

        residuals   (i/minus y fitted)

        ess         (i/sum-of-squares difference)

        rss         (i/sum-of-squares residuals)

        p           (i/ncol x)

        n           (i/nrow y)

        df1         (- p 1)

        df2         (- n p)

        msm         (/ ess df1)

        mse         (/ rss df2)

        f-stat      (/ msm mse)]

    (s/cdf-f f-stat :df1 df1 :df2 df2 :lower-tail? false)))

(defn ex-3-23 []

  (let [data (swimmer-data)

        x (->> (feature-matrix ["Height, cm" "Age"] data)

               (add-bias))

        y (->> (i/$ "Weight" data)

               (i/log))

        beta (:coefs (s/linear-model y x :intercept false))]

    (f-test beta x y)))
```

**测试返回结果`1.11x10e-16`。这是一个很小的数字；因此，我们可以肯定这个模型是有意义的。**

**请注意，对于较小的数据样本，*F*-测试量化了线性模型是否合适的不确定性。例如，对于五个随机样本，数据有时显示几乎没有任何线性关系，并且 *F 检验甚至在 50%的置信区间判断数据无关紧要。***

## **分类变量和虚拟变量**

**在这一点上，我们可能会尝试将`"Sex"`作为一个特性包含在我们的回归分析中，但是我们会遇到一个问题。输入表示为`"M"`或`"F"`，而不是一个数字。这是一个分类变量的例子:一个可以取一组有限值的变量，这些值是无序的并且(通常)不是数字。分类变量的其他例子是运动员参加的运动或他们最擅长的特定项目。**

**普通最小二乘法依赖于剩余距离的数值来最小化。游泳和田径之间的距离是多少？这可能意味着在我们的回归方程中不可能包含分类变量。**

### **注意**

**分类变量或名义变量不同于连续变量，因为它们不在数轴上。有时类别由数字表示，比如邮政编码，但是我们不应该假设数字类别一定是有序的，或者类别之间的间隔是相等的。**

**幸运的是，许多分类变量可以被认为是二分法，事实上，我们的样本数据包含两类`sex`。如果我们将它们转换成两个数字，例如 0 和 1，那么它们可以包含在我们的回归模型中。**

**当一个类别(如体育)有两个以上的值时，我们可以为每种类型的体育包含一个独立变量。我们会为游泳创建一个变量，为举重创建另一个变量，等等。游泳的价值对游泳者来说是 1，对其他人来说是 0。**

**由于`sex`可能是我们回归模型的一个有用的解释变量，让我们将女性转换为`0`，男性转换为`1`。我们可以使用 Incanter 的`incanter.core/add-derived-column`函数添加一个包含虚拟变量的派生列。**

**让我们计算一下我们的![Categorical and dummy variables](graphics/7180OS_03_57.jpg)值，看看它是否有所提高:**

```
(defn dummy-mf [sex]

  (if (= sex "F")

    0.0 1.0))

(defn ex-3-25 []

  (let [data (->> (swimmer-data)

                  (i/add-derived-column "Dummy MF"

                                        ["Sex"]

                                        dummy-mf))

        x (->> data

               (feature-matrix ["Height, cm"

                                "Age"

                                "Dummy MF"])

               (add-bias))

        y (->> (i/$ "Weight" data)

               (i/log)

               (i/matrix))

        beta (normal-equation x y)]

    (adj-r-squared beta x y)))
```

**代码产生值`0.809`。利用身高、年龄和性别特征，我们已经成功解释了 80%以上的奥运游泳运动员体重差异。**

## **相对力量**

**在这一点上，问一下解释观察到的体重最重要的特征是什么可能是有用的:是年龄、性别还是身高？我们可以利用调整后的 *R ²* 来看看值变化了多少，但是这需要我们为每个想要测试的变量重新运行回归。**

**我们不能看系数的大小，因为它们适用的数据范围有很大不同:身高以厘米为单位，年龄以岁为单位，性别作为虚拟变量在 0 到 1 的范围内测量。**

**为了比较系数的相对贡献，我们可以计算标准化回归系数，或β权重。**

**![Relative power](graphics/7180OS_03_62.jpg)**

**为了计算β权重，我们将每个系数乘以相关自变量和模型因变量的标准偏差之比。这可以通过下面的 Clojure 代码来实现:**

```
(defn beta-weight [coefs x y]

  (let [sdx (map s/sd (i/trans x))

        sdy (s/sd y)]

    (map #(/ (* %1 %2) sdy) sdx coefs)))

(defn ex-3-26 []

  (let [data (->> (swimmer-data)

                  (i/add-derived-column "Dummy MF"

                                        ["Sex"]

                                        dummy-mf))

        x (->> data

               (feature-matrix ["Height, cm"

                                "Age"

                                "Dummy MF"])

               (add-bias))

        y (->> (i/$ "Weight" data)

               (i/log)

               (i/matrix))

        beta (normal-equation x y)]

    (beta-weight beta x y)))
```

**此输出(四舍五入到三位小数):**

```
(0.0 0.650 0.058 0.304)
```

**这表明身高是最重要的解释变量，其次是性别，然后是年龄。将转换成标准化系数告诉我们，身高增加一个标准差，平均体重增加`0.65`个标准差。**

**<title>Collinearity</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 共线性

在这一点上，我们可能会尝试继续向我们的模型添加特性，以增加它的解释力。

例如，我们也有一个`"Date of birth"`列，我们可能会尝试将它也包含在内。这是一个日期，但是我们可以很容易地将它转换成一个适用于回归的数字。我们可以通过使用`clj-time`库从他们的出生日期中提取年份来实现:

```
(defn to-year [str]

  (-> (coerce/from-date str)

      (time/year)))

(defn ex-3-27 []

  (let [data (->> (swimmer-data)

                  (i/add-derived-column "Dummy MF"

                                        ["Sex"]

                                        dummy-mf)

                  (i/add-derived-column "Year of birth"

                                        ["Date of birth"]

                                        to-year))

        x (->> data

               (feature-matrix ["Height, cm"

                                "Age"

                                "Dummy MF"

                                "Year of birth"])

               (add-bias))

        y (->> (i/$ "Weight" data)

               (i/log)

               (i/matrix))

        beta (normal-equation x y)]

    (beta-weight beta x y)))

;; (-0.0 0.650 0.096 0.304 0.038)
```

新的“出生年份”特性的 beta 权重只有`0.038`，小于我们之前计算的年龄特性的权重。然而，年龄特征的年龄权重现在显示为值`0.096`。自从我们增加了`"Year of birth"`这个特性后，它的相对重要性增加了 65%。添加新功能改变了现有功能的重要性，这一事实表明我们有问题。

通过包含额外的`"Year of birth"`参数，我们无意中破坏了回归估计器的规则。让我们看看为什么:

```
(defn ex-3-28 []

  (let [data (->> (swimmer-data)

                  (i/add-derived-column "Year of birth"

                                        ["Date of birth"]

                                        to-year))

        x (->> (i/$ "Age" data)

               (map (jitter 0.5)))

        y (i/$ "Year of birth" data)]

    (-> (c/scatter-plot x y

                        :x-label "Age"

                        :y-label "Year of birth")

        (i/view))))
```

下面的散点图显示了游泳者的年龄(抖动)与出生年份的关系。正如您所料，这两个变量密切相关:

![Collinearity](graphics/7180OS_03_260.jpg)

这两个特征如此高度相关，以至于算法无法确定它们中的哪一个最好地解释了在 *y* 中观察到的变化。当我们处理被称为**共线性**的多元线性回归时，这是一个不理想的问题。

## 多重共线性

对于产生最佳系数估计值的多元回归，基础数据必须符合与简单回归相同的假设加上一个额外的假设——不存在完美的**多重共线性**。这意味着独立变量不应该彼此精确地线性相关。

### 注意

在实践中，自变量往往以某种方式共线。例如，考虑年龄和身高或性别和身高本身相互关联。只有当这种情况变得极端时，才会出现严重的系数误差。

如果自变量实际上不是独立的，那么线性回归就不能确定每个自变量的相对贡献。如果两个特征如此强烈地相关，以至于它们总是一起变化，那么算法如何区分它们的相对重要性？因此，系数估计中可能存在高方差和高标准误差。

我们已经看到了高度多重共线性的一个症状:当在方程中添加或删除独立变量时，回归系数会发生显著变化。另一个症状是当多元回归中某个特定自变量的系数不显著，但使用相同自变量的简单回归模型的系数显著时。

虽然这些提供了多重共线性的线索，为了证实，我们必须直接看独立变量的相互关系。确定相关性的一种方法是检查每个独立变量之间的相关性，寻找 0.8 或更大的系数。虽然这种简单的方法经常奏效，但它可能无法考虑独立变量与其他变量一起具有线性关系的情况。

评估多重共线性最可靠的方法是对所有其他独立变量回归每个独立变量。当这些方程中的任何一个*R²接近 1.0 时，存在高度多重共线性。事实上，这些 *R ²* 中最大的一个作为存在多重共线性程度的指示器。*

一旦确定，有几种方法可以解决多重共线性:

*   增加样本量。更多的数据可以产生更精确的参数估计，标准误差更小。
*   将功能合二为一。如果您有几个测量本质上相同属性的要素，请想办法将它们统一成一个要素。
*   丢弃有问题的变量。
*   限制预测的等式。共线性会影响模型的系数，但结果可能仍然很适合数据。

由于年龄和出生年份携带着基本相同的信息，我们也可以丢弃一个。通过计算每个特征和因变量的二元回归，我们可以很容易地看出两者中哪一个包含更多的解释力。

“年龄” *R ²* = 0.1049，而“出生年份” *R ²* = 0.1050。

正如所料，这两个特征实际上没有什么不同，都可以解释大约 10%的体重差异。由于出生年份略微解释了更多的差异，我们将保留它并放弃年龄特征。

<title>Prediction</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 预测

最后，我们到达线性回归最重要的用途之一:预测。我们已经训练了一个模型，能够根据身高、性别和出生年份的数据预测奥运会游泳运动员的体重。

马克·施皮茨是 9 次奥运会游泳冠军，他在 1972 年奥运会上赢得了 7 枚金牌。他出生于 1950 年，根据他的维基百科页面，身高 183 厘米，体重 73 公斤。让我们看看我们的模型预测他的体重。

我们的多元回归模型要求这些值以矩阵形式呈现。需要按照模型学习特征的顺序提供每个参数，以便应用正确的系数。在偏差项之后，我们的特征向量需要包含身高、性别和出生年份，其单位与我们的模型被训练的单位相同:

![Prediction](graphics/7180OS_03_63.jpg)

我们的 *β* 矩阵包含每个特征的系数:

![Prediction](graphics/7180OS_03_64.jpg)

我们的模型的预测将是每行的 *β* 系数和特征 *x* 的乘积之和:

![Prediction](graphics/7180OS_03_65.jpg)

由于矩阵乘法通过分别将每个矩阵的行和列的乘积相加来产生每个元素，所以产生我们的结果就像将 *β* 的转置与 *x [spitz]* 向量相乘一样简单。

回想一下，结果矩阵的维数将是第一个矩阵的行数和第二个矩阵的列数:

![Prediction](graphics/7180OS_03_66.jpg)

![Prediction](graphics/7180OS_03_67.jpg)是一个![Prediction](graphics/7180OS_03_68.jpg)矩阵和一个![Prediction](graphics/7180OS_03_69.jpg)矩阵的乘积。结果是一个![Prediction](graphics/7180OS_03_70.jpg)矩阵:

![Prediction](graphics/7180OS_03_270.jpg)

在代码中计算这个非常简单:

```
(defn predict [coefs x]

  (-> (i/trans coefs)

      (i/mmult x)

      (first)))
```

我们调用`first`从矩阵中返回第一个(也是唯一的)元素，而不是矩阵本身:

```
(defn ex-3-29 []

  (let [data (->> (swimmer-data)

                  (i/add-derived-column "Dummy MF"

                                        ["Sex"]

                                        dummy-mf)

                  (i/add-derived-column "Year of birth"

                                        ["Date of birth"]

                                        to-year))

        x (->> data

               (feature-matrix ["Height, cm"

                                "Dummy MF"

                                "Year of birth"])

               (add-bias))

        y (->> (i/$ "Weight" data)

               (i/log)

               (i/matrix))

        beta (normal-equation x y)

        xspitz (i/matrix [1.0 183 1 1950])]

    (i/exp (predict beta xspitz))))
```

这返回`84.21`，对应于 84.21 kg 的预期重量。这比马克·施皮茨报道的 73 公斤重得多。我们的模型似乎表现得不太好。

## 预测的置信区间

我们之前计算了人口参数的置信区间。也可以为特定的预测构建置信区间，称为 **预测区间**。预测区间通过提供一个最小值和一个最大值来量化预测中的不确定性，在这两个值之间，真实值有一定的概率。

![The confidence interval of a prediction](graphics/7180OS_03_23.jpg)的预测区间比总体参数的置信区间更宽，例如平均值。这是因为置信区间只需要考虑我们在估计均值时的不确定性，而预测区间也必须考虑 *y* 与均值的方差。

![The confidence interval of a prediction](graphics/7180OS_03_280.jpg)

前一幅图像显示了外部预测区间和内部置信区间之间的关系。我们可以使用以下公式计算预测间隔:

![The confidence interval of a prediction](graphics/7180OS_03_71.jpg)

这里，![The confidence interval of a prediction](graphics/7180OS_03_72.jpg)是预测，加减区间。我们正在利用*t*-分布，其中自由度是![The confidence interval of a prediction](graphics/7180OS_03_73.jpg)，样本大小减去参数数量。这和我们之前计算的 *F* 测试是一样的。虽然该公式看起来有些吓人，但转换成如下示例所示的代码相对简单，该代码计算 95%的预测区间:

```
 (defn prediction-interval [x y a]

  (let [xtx    (i/mmult (i/trans x) x)

        xtxi   (i/solve xtx)

        xty    (i/mmult (i/trans x) y)

        coefs  (i/mmult xtxi xty)

        fitted (i/mmult x coefs)

        resid  (i/minus y fitted)

        rss    (i/sum-of-squares resid)

        n      (i/nrow y)

        p      (i/ncol x)

        dfe    (- n p)

        mse    (/ ssr dfe)

        se-y   (first (i/mmult (i/trans a) xtxi a))

        t-stat (i/sqrt (* mse (+ 1 se-y)))]

    (* (s/quantile-t 0.975 :df dfe) t-stat)))
```

由于*t*-统计量是由误差的自由度参数化的，它考虑了模型中存在的不确定性。

如果我们想要计算平均值的置信区间而不是预测区间，我们可以在计算`t-stat`时简单地省略对`se-y`加 1。

上述代码可用于生成以下图表，显示预测间隔如何随自变量的值而变化:

![The confidence interval of a prediction](graphics/7180OS_03_290.jpg)

在上图中，一个样本大小为 5 的模型显示了 95%的预测区间是如何随着我们远离平均高度而增加的。将上述公式应用于马克·施皮茨得出以下结果:

```
(defn ex-3-30 []

  (let [data (->> (swimmer-data)

                  (i/add-derived-column "Dummy MF"

                                        ["Sex"]

                                        dummy-mf)

                  (i/add-derived-column "Year of birth"

                                        ["Date of birth"]

                                        to-year))

        x (->> data

               (feature-matrix ["Height, cm"

                                "Dummy MF"

                                "Year of birth"])

               (add-bias))

        y (->> (i/$ "Weight" data)

               (i/log)

               (i/matrix))

        xspitz (i/matrix [1.0 183 1 1950])]

    (i/exp (prediction-interval x y xspitz))))
```

这返回了从 72.7 千克到 97.4 千克的范围。这个范围刚好包括马克 73 公斤的体重，所以我们的预测在 95%的预测区间内。不过，令人不安的是，它离边界太近了。

## 模型范围

马克·施皮茨出生于 1950 年，比 2012 年奥运会最年长的游泳运动员还要早几十年。通过使用马克的出生年份来预测他的体重，我们犯了试图超出我们的训练数据进行推断的错误。我们已经超出了我们模型的范围。

这还有第二个问题。我们的数据完全基于目前参加国际标准比赛的游泳运动员，而马克已经很多年没有参加比赛了。换句话说，马克现在不是我们用来训练模型的人群的一部分。为了解决这两个问题，我们需要查阅马克 1979 年的详细资料，当时他是一名游泳运动员。

据[http://www . topendsports . com/athletes/swimming/Spitz-mark . htm](http://www.topendsports.com/athletes/swimming/spitz-mark.htm)报道，1972 年，22 岁的马克·施皮茨身高 185 cm，体重 79 kg。

### 注意

选择正确的特征是从任何预测算法获得良好结果的最重要的先决条件之一。

您应该努力选择特征，不仅要基于它们的预测能力，还要基于它们与建模领域的相关性。

## 最终模型

尽管它的略低*R²，让我们用年龄代替出生年份作为特征来重新训练我们的模型。这将使我们能够轻松预测过去和未来未知数据的权重，因为它更接近地模拟了我们怀疑与权重有因果关系的变量。*

这产生了大约为:

![The final model](graphics/7180OS_03_74.jpg)

我们在 1972 年奥运会上的标志是:

![The final model](graphics/7180OS_03_75.jpg)

我们可以用下面的代码来预测他的竞争体重:

```
(defn ex-3-32 []

  (let [data (->> (swimmer-data)

                  (i/add-derived-column "Dummy MF"

                                        ["Sex"]

                                        dummy-mf))

        x (->> data

               (feature-matrix ["Height, cm"

                                "Dummy MF"

                                "Age"])

               (add-bias))

        y (->> (i/$ "Weight" data)

               (i/log)

               (i/matrix))

        beta (normal-equation x y)

        xspitz (i/matrix [1.0 185 1 22])]

    (i/exp (predict beta xspitz))))
```

这返回`78.47`，对应于 78.47 kg 的预测。这已经非常接近马克 79 公斤的真实比赛体重了。

<title>Summary</title><link href="epub.css" rel="stylesheet" type="text/css"> 

# 总结

在这一章中，我们学习了如何确定两个或多个变量是否有线性关系。我们已经看到了如何表达它们与 *r* 的相关性强度，以及线性模型如何很好地解释与*R²和![Summary](graphics/7180OS_03_57.jpg)的方差。我们还执行了假设检验并计算了置信区间，以推断相关性的真实总体参数的范围。*

建立了变量之间的相关性后，我们能够使用普通的最小二乘回归和简单的 Clojure 函数建立预测模型。然后，我们使用 Incanter 的矩阵功能和法方程来概括我们的方法。这个简单的模型通过确定模型参数 *β* 展示了机器学习的原理，这些参数是从我们的样本数据中推断出来的，可以用来进行预测。我们的模型能够预测新运动员的预期体重，该体重落在真实值的预测区间内。

在下一章中，我们将看到如何使用类似的技术将数据分类到不同的类中。我们将演示各种不同的分类方法，并介绍一种非常通用的参数优化技术，该技术适用于各种机器学习模型，包括线性回归。******