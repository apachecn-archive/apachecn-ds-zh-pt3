

# 第四章。分类

|   | 一条举世公认的真理是，一个富有的单身男人一定想要一个妻子 |   |
|   | ——简·奥斯汀，《傲慢与偏差》 |

在上一章中，我们学习了如何使用线性回归进行数值预测。我们建立的模型能够了解奥运会游泳运动员的特征与其体重之间的关系，并且我们能够使用该模型来预测新游泳运动员的体重。与所有回归技术一样，我们的输出是一个数字。

然而，并不是所有的预测都需要一个数值解——有时我们希望我们的预测是项目。例如，我们可能想预测选民在选举中会支持哪个候选人。或者我们可能想知道客户可能会购买几种产品中的哪一种。在这些情况下，结果是从许多可能的离散选项中选择一个。我们称这些选项为类，我们将在本章建立的模型是分类器。

我们将了解几种不同类型的分类器，并在一个样本数据集上比较它们的性能——泰坦尼克号上的乘客名单。预测和分类与概率论和信息论密切相关，所以我们也会更详细地讨论这些。我们将从测量组间相对概率的方法开始这一章，然后继续将统计显著性测试应用于组本身。

# 关于数据

本章将利用泰坦尼克号上乘客的数据，该船在 1912 年的处女航中撞上冰山后沉没。乘客的存活率受到多种因素的强烈影响，包括阶级和性别。

该数据集来自迈克尔·芬德利精心编制的数据集。更多关于数据来源的信息，包括原始来源的链接，请查阅位于[http://wiki.clojuredatascience.com](http://wiki.clojuredatascience.com)的书的维基。

### 注意

本章的示例代码可从 Packt Publishing 的网站或从[https://github.com/clojuredatascience/ch4-classification](https://github.com/clojuredatascience/ch4-classification)获得。

数据足够小，可以与源代码一起包含在数据目录中。



# 检查数据

在前一章中，我们遇到了分类变量，即运动员数据集中的二分变量“性别”。该数据集还包含许多其他分类变量，包括“运动”、“事件”和“国家”。

让我们来看看 Titanic 数据集(使用`clojure.java.io`库访问文件资源，使用`incanter.io`库读取文件资源):

```
(defn load-data [file]

  (-> (io/resource file)

      (str)

      (iio/read-dataset :delim \tab :header true)))

(defn ex-4-1 []

  (i/view (load-data :titanic)))
```

上述代码生成了下表:

![Inspecting the data](graphics/7180OS_04_100.jpg)

泰坦尼克号数据集也包括分类变量。比如— **:性别**、 **:pclass** (客运级)、**:上船**(表示上船港口的字母)。这些都是字符串值，以**女**、**首**、 **C** 等类别为例，但类不一定都是字符串值。像 **:ticket** 、 **:boat** 和 **:body** 这样的列也可以被认为包含分类变量。尽管有数值，但它们只是应用于事物的标签。

### 注意

分类变量是一种只能取离散数量的值的变量。这与连续变量相反，连续变量可以在其范围内取任何值。

其他代表计数的数字就不那么容易定义了。字段 **:sibsp** 报告有多少同伴(配偶或兄弟姐妹)与一名乘客一起旅行。这些是计数，他们的单位是人。但是它们也可以很容易地代表标签，用 **0** 代表“没有同伴的乘客”，用 **1** 代表“有一个同伴的乘客”，等等。只有一小组标签，所以用数字表示字段很方便。换句话说，我们可以选择将 **:sibsp** (和**:parch**——相关父母和子女的计数)表示为分类或数字特征。

由于分类变量在数字线上没有意义，我们无法绘制图表来显示这些数字之间的关系。不过，我们可以构建一个频率表，显示每个组中的乘客数量是如何分布的。因为有两组两个变量，所以总共有四组。

可以使用 Incanter core 的`$rollup`功能对数据进行汇总:

```
(defn frequency-table [sum-column group-columns dataset]

  (->> (i/$ group-columns dataset)

       (i/add-column sum-column (repeat 1))

       (i/$rollup :sum sum-column group-columns)))

(defn ex-4-2 []

  (->> (load-data "titanic.tsv")

       (frequency-table :count [:sex :survived])))
```

Incanter 的`$rollup`要求我们提供三个参数——一个用来“卷起”一组行的函数，一个要卷起的列，以及其唯一值定义感兴趣的组的列。任何将序列简化为单个值的函数都可以用作 rollup 函数，但是有些函数太常见了，我们可以提供关键字`:min`、`:max`、`:sum`、`:count`和`:mean`来代替。

该示例生成下表:

```
| :survived |   :sex | :count |

|-----------+--------+--------|

|         n |   male |    682 |

|         n | female |    127 |

|         y |   male |    161 |

|         y | female |    339 |
```

这张图表显示了乘客分成不同组的频率,“死亡的男性”,“幸存的女性”等等。有几种方法可以理解这样的频率计数；先说最常见的。



# 与相对风险和几率的比较

前面的 Incanter 数据集是我们数据的一个容易理解的表示，但是为了单独提取每个组的数字，我们希望将数据存储在一个更容易访问的数据结构中。让我们编写一个函数，将数据集转换为一系列嵌套地图:

```
(defn frequency-map [sum-column group-cols dataset]

  (let [f (fn [freq-map row]

            (let [groups (map row group-cols)]

              (->> (get row sum-column)

                   (assoc-in freq-map groups))))]

    (->> (frequency-table sum-column group-cols dataset)

         (:rows)

         (reduce f {}))))
```

例如，我们可以使用下面的`frequency-map`函数来计算`:sex`和`:survived`的嵌套映射:

```
(defn ex-4-3 []

  (->> (load-data "titanic.tsv")

       (frequency-map :count [:sex :survived])))

;; => {"female" {"y" 339, "n" 127}, "male" {"y" 161, "n" 682}}
```

更一般地说，给定任何数据集和列序列，这将更容易提取我们感兴趣的计数。我们将比较男性和女性的存活率，因此让我们使用 Clojure 的`get-in`函数来提取男性和女性的死亡人数以及男性和女性的总人数:

```
(defn fatalities-by-sex [dataset]

  (let [totals (frequency-map :count [:sex] dataset)

        groups (frequency-map :count [:sex :survived] dataset)]

    {:male (/ (get-in groups ["male" "n"])

              (get totals "male"))

     :female (/ (get-in groups ["female" "n"])

                (get totals "female"))}))

(defn ex-4-4 []

  (-> (load-data "titanic.tsv")

      (fatalities-by-sex)))

;; {:male 682/843, :female 127/466}
```

从这些数字中，我们可以计算出简单的比率。相对风险是一个事件发生在两个独立群体中的概率比率:

![Comparisons with relative risk and odds](graphics/7180OS_04_01.jpg)![Comparisons with relative risk and odds](graphics/7180OS_04_02.jpg)

其中 *P(事件)*为事件发生的概率。男性在泰坦尼克号上死亡的风险是 *682* 除以*843*；作为女性在泰坦尼克号上死亡的风险是 127 除以 466:

```
(defn relative-risk [p1 p2]

  (float (/ p1 p2)))

(defn ex-4-5 []

  (let [proportions (-> (load-data "titanic.tsv")

                        (fatalities-by-sex))]

    (relative-risk (get proportions :male)

                   (get proportions :female))))

;; 2.9685
```

换句话说，如果你是男人，在泰坦尼克号上死亡的风险几乎是男人的三倍。相对风险通常用于医疗保健，以显示一个人患病的机会如何受到其他因素的影响。相对风险为 1 意味着两组之间的风险没有差异。

相比之下，比值比可以是正的，也可以是负的，它衡量的是在一个群体中你在其他属性上的优势。正如任何相关，没有因果关系是隐含的。这两种属性当然可以通过第三种属性联系起来——它们的共同原因:

![Comparisons with relative risk and odds](graphics/7180OS_04_03.jpg)![Comparisons with relative risk and odds](graphics/7180OS_04_04.jpg)

男性死亡的几率是 *682* : *161* ，女性死亡的几率是 *127* : *339* 。几率比就是两者之比:

```
(defn odds-ratio [p1 p2]

  (float

   (/ (* p1 (- 1 p2))

      (* p2 (- 1 p1)))))

(defn ex-4-6 []

  (let [proportions (-> (load-data "titanic.tsv")

                        (fatalities-by-sex))]

    (odds-ratio (get proportions :male)

                (get proportions :female))))

;; 11.3072
```

这个例子显示了比值比对陈述相对位置是如何敏感的，并且可以产生大得多的数字。

### Tip

当呈现比率时，确保你知道它们是相对风险比率还是优势比率。虽然这两种方法看起来相似，但它们输出的结果范围却非常不同。

比较相对风险和优势比的两个等式。每种情况下的分子都是一样的，但是对于风险来说，分母都是女性，而对于优势比来说，存活下来的是女性。



# 比例的标准误差

很明显，泰坦尼克号上幸存的女性比例远大于男性。但是，就像我们在第 2 章、*推论*中遇到的停留时间差异一样，我们应该问自己这些差异是否仅仅是由于偶然而发生的。

我们已经在前面的章节中看到了如何基于样本的标准误差构建统计数据的置信区间。标准误差是基于样本的方差，但是一个比例的方差是什么呢？无论我们取多少样本，都只会产生一个比例——在整个样本中的比例。

很明显，这个比例仍然会受到某种差异的影响。当我们掷一枚硬币 10 次时，我们预计会得到大约 5 个正面，但也不是不可能连续得到 10 个正面。

## 使用自举进行估计

在[第 2 章](ch02.html "Chapter 2. Inference")、*推断*中，我们学习了均值等自举统计，并了解了自举是如何通过模拟来有效估计参数的。让我们用 bootstrapping 来估计泰坦尼克号幸存女性乘客比例的标准差。

我们可以用 0 和 1 的序列来表示这 466 名女性乘客。零代表死亡的乘客，一代表幸存的乘客。这是一种方便的表示，因为它意味着整个序列的总和等于幸存乘客的总数。通过从这 466 个 0 和 1 的序列中抽取 466 个元素的重复随机样本，并每次取和，我们可以得到比例方差的估计值:

```
(defn ex-4-7 []

  (let [passengers (concat (repeat 127 0)

                           (repeat 339 1))

        bootstrap (s/bootstrap passengers i/sum :size 10000)]

    (-> (c/histogram bootstrap

                     :x-label "Female Survivors"

                     :nbins 20)

        (i/view))))
```

前面的代码生成如下直方图:

![Estimation using bootstrapping](graphics/7180OS_04_110.jpg)

直方图似乎显示了一个平均值为 339 的正态分布——测量的女性幸存者人数。该分布的标准偏差是抽样幸存者的标准误差，我们可以简单地从自举样本中计算出来，如下所示:

```
(defn ex-4-8 []

  (-> (concat (repeat 127 0)

              (repeat 339 1))

      (s/bootstrap i/sum :size 10000)

      (s/sd)))

;; 9.57
```

您的标准差可能略有不同，这取决于自举样本中的随机变化。不过，应该很接近了。

标准偏差的单位是人——女性乘客——所以为了计算出比例的标准误差，我们必须将其除以样本中的乘客总数，466。这产生了比例为 0.021 的标准误差。



# 二项分布

前面的直方图看起来很像正态分布，但实际上它是二项式分布。这两种分布非常相似，但是二项式分布用于建模我们想要确定二元事件预期发生多少次的情况。

让我们在直方图上绘制二项式分布和正态分布，看看它们是如何比较的:

```
(defn ex-4-9 []

  (let [passengers (concat (repeat 127 0)

                           (repeat 339 1))

        bootstrap (s/bootstrap passengers i/sum :size 10000)

        binomial (fn [x]

                   (s/pdf-binomial x :size 466 :prob (/ 339 466)))

        normal (fn [x]

                 (s/pdf-normal x :mean 339 :sd 9.57))]

    (-> (c/histogram bootstrap

                     :x-label "Female Survivors"

                     :series-label "Bootstrap"

                     :nbins 20

                     :density true

                     :legend true)

        (c/add-function binomial 300 380

                        :series-label "Biomial")

        (c/add-function normal 300 380

                        :series-label "Normal")

        (i/view))))
```

前面的代码生成了下面的图表:

![The binomial distribution](graphics/7180OS_04_120.jpg)

注意在前面的图表中，对应于二项式分布的线是锯齿状的——它代表事物的离散计数，而不是像正态分布这样的连续值。

## 比例公式的标准误差

我们已经根据经验计算了标准误差，发现它等于 0.021，只使用了女性幸存者的比例和女性乘客的总数。尽管了解比例的标准误差实际测量的是什么很有启发性，但有一个公式可以让我们一步到位:

![The standard error of a proportion formula](graphics/7180OS_04_05.jpg)

代入女性幸存者的数量，我们得出以下结果:

![The standard error of a proportion formula](graphics/7180OS_04_06.jpg)

幸运的是，这个数字非常接近我们通过自举计算的标准误差。当然，这并不准确，因为我们的自举计算有自己的采样误差。

```
(defn standard-error-proportion [p n]

  (-> (- 1 p)

      (* p)

      (/ n)

      (i/sqrt)))

(defn ex-4-10 []

  (let [survived (->> (load-data "titanic.tsv")

                      (frequency-map :count [:sex :survived]))

        n (reduce + (vals (get survived "female")))

        p (/ (get-in survived ["female" "y"]) n)]

    (se-proportion p n)))

;; 0.0206
```

比例的标准误差方程给了我们一个重要的启示——当 *p* 接近 0.5 时， *p(1 - p)* 的值最大。这意味着比例中最大的标准误差是当比例接近一半时。

如果这让你感到惊讶，考虑一下这个——当比例为 50%时，样本中的变化最大。就像掷硬币一样，我们无法预测下一个值会是多少。随着样本中的比例增加(或减少)，数据变得越来越同质。结果，变化减小，因此标准误差也相应减小。



# 显著性检验比例

让我们回到男性或女性死亡率的测量差异是否仅仅是由于偶然的问题上来。就像第二章、*推论*中的[一样，我们的 *z* 测试就是用比例差除以合并的标准误差:](ch02.html "Chapter 2. Inference")

![Significance testing proportions](graphics/7180OS_04_07.jpg)

上式中， *p* [1] 表示幸存女性比例，即 *339/466 = 0.73* 。而*p*2 表示幸存的男性比例，即 *161/843 = 0.19* 。

为了计算*z*-统计量，我们需要将两个比例的标准误差相加。我们的比例分别测量男性和女性的存活率，因此合并的标准误差就是男性和女性总和的标准误差，或总存活率，如下所示:

![Significance testing proportions](graphics/7180OS_04_08.jpg)

将这些值代入公式，得到*z*-统计值:

![Significance testing proportions](graphics/7180OS_04_09.jpg)

使用一个*z*-分数意味着我们将使用正态分布来查找*p*-值:

```
(defn ex-4-11 []

  (let [dataset     (load-data "titanic.tsv")

        proportions (fatalities-by-sex dataset)

        survived    (frequency-map :count [:survived] dataset)

        total  (reduce + (vals survived))

        pooled (/ (get survived "n") total)

        p-diff (- (get proportions :male)

                  (get proportions :female))

        z-stat (/ p-diff (se-proportion pooled total))]

    (- 1 (s/cdf-normal (i/abs z-stat)))))

;; 0.0
```

因为我们有一个单尾检验，所以*p*-值是*z*-得分小于 39.95 的概率。响应为零，对应于非常非常显著的结果。这让我们可以拒绝零假设，并得出结论，男女存活率之间的差异肯定不仅仅是偶然的。

## 调整大样本的标准误差

你可能想知道我们为什么要讨论标准误差。我们掌握的泰坦尼克号乘客的数据并不是更广泛人群的样本。是人口。只有一艘泰坦尼克号，只有一次命运的旅程。

虽然从某种意义上说这是真的，但泰坦尼克号的灾难可能以多种方式发生。如果“妇女和儿童优先”的指示没有得到遵守或得到更普遍的遵守，就会得到一套不同的结果。如果有足够的救生艇供每个人使用，或者疏散过程进行得更顺利，那么这也会反映在结果中。

标准误差和显著性测试允许我们将灾难视为无数潜在的类似灾难之一，并确定观察到的差异可能是系统性的还是纯粹的巧合。

也就是说，有时我们更感兴趣的是，我们对样本代表一个有限的、量化的总体有多大的信心。当样本开始测量超过约 10%的人口时，我们可以向下调整标准误差，以考虑减少的不确定性:

![Adjusting standard errors for large samples](graphics/7180OS_04_10.jpg)

这可以用 Clojure 写成:

```
(defn se-large-proportion [p n N]

  (* (se-proportion p n)

     (i/sqrt (/ (- N n)

                (- n 1)))))
```

其中 *N* 是总人口的规模。随着样本数量相对于人口数量的增加， *(N - n)* 趋向于零。如果你对整个人口进行抽样，那么任何比例上的差异——无论多小——都将被判定为显著。



# 卡方多重显著性检验

并不是所有的类别都是二分法的(比如男性和女性，存活和死亡)。虽然我们期望分类变量有有限的类别数，但是对于一个特定属性可以拥有的类别数没有硬性的上限。

我们可以使用其他分类变量来区分泰坦尼克号上的乘客，例如他们乘坐的舱位。泰坦尼克号上有三个类级别，我们在本章开始时构造的`frequency-table`函数已经能够处理多个类。

```
(defn ex-4-12 []

  (->> (load-data "titanic.tsv")

       (frequency-table :count [:survived :pclass])))
```

该代码生成以下频率表:

```
| :pclass | :survived | :count |

|---------+-----------+--------|

|   third |         y |    181 |

|   third |         n |    528 |

|  second |         y |    119 |

|  second |         n |    158 |

|   first |         n |    123 |

|   first |         y |    200 |
```

这三个类给了我们一个额外的方法来减少存活率的数据。随着类别数量的增加，读取频率表中的模式变得越来越困难，所以让我们将其可视化。

## 可视化类别

尽管饼状图最初被设计用来表示比例，但通常不是表示整体的一部分的好方法。人们很难在视觉上比较一个圆的切片面积。线性表示数量，如堆积条形图，几乎总是更好的方法。这些区域不仅更容易解释，而且更容易并排比较。

我们可以将数据可视化为堆叠条形图:

```
(defn ex-4-13 []

  (let [data (->> (load-data "titanic.tsv")

                  (frequency-table :count [:survived :pclass]))]

    (-> (c/stacked-bar-chart :pclass :count

                             :group-by :survived

                             :legend true

                             :x-label "Class"

                             :y-label "Passengers"

                             :data data)

        (i/view))))
```

前面的代码生成了下面的图表:

![Visualizing the categories](graphics/7180OS_04_130.jpg)

数据清楚地显示了死亡乘客数量和死亡乘客比例的差异，这在头等舱和三等舱之间最为明显。我们想确定这种差异是否显著。

我们可以在每一对比例之间执行*z*-测试，但是，正如我们在[第 2 章](ch02.html "Chapter 2. Inference")、*推论*中所了解的，这更有可能导致第一类错误，并导致我们发现一个实际上并不存在的重要结果。

多类别显著性检验的问题似乎需要进行 *F* 检验，但是 *F* 检验是基于组内和组间一些连续变量的方差比。因此，我们想要的是一个类似的测试，只关心组间的相对比例。这是*X*2 测试所基于的前提。

## 卡方检验

发音为*kai square*,*X*、 ² 检验是一种应用于分类数据集合的统计检验，用于评估集合中这些类别的比例之间的任何观察到的差异偶然出现的可能性有多大。

因此，当执行a*X*2 测试时，我们的无效假设是，观察到的各组之间的比例差异只是随机变化的结果。我们可以认为这是两个分类变量之间的独立性测试。如果类别 *A* 是乘客类别，类别 *B* 是他们是否幸存，则零假设是乘客类别和存活率相互独立。另一个假设是，类别不是独立的——乘客类别和生存在某种程度上相互关联。

*X* ² 统计量通过将从样本中观察到的频率计数与在独立性假设下计算的频率表进行比较来计算。这个频率表是对类别独立时数据的估计。假设独立性，我们可以使用行、列和总计，按照以下方式计算频率表:

|   | 

幸存

 | 

死亡

 | 

总数

 |
| --- | --- | --- | --- |
| 头等舱 | *323*500/1309 = 123.4* | *323*809/1309 = 199.6* | *323* |
| 二等 | *277*500/1309 = 105.8* | *277*809/1309 = 171.2* | *277* |
| 三级 | *709*500/1309 = 270.8* | *709*809/1309 = 438.2* | *709* |
| 总数 | *500* | *809* | *1309* |

一个简单的公式仅使用每行和列的总计来计算每个单元格的值，并假设在单元格中均匀分布。这是我们的预期频率表。

```
(defn expected-frequencies [data]

  (let [as (vals (frequency-map :count [:survived] data))

        bs (vals (frequency-map :count [:pclass] data))

        total (-> data :rows count)]

    (for [a as

          b bs]

      (* a (/ b total)))))

(defn ex-4-14 []

  (-> (load-data "titanic.tsv")

      (expected-frequencies)))

;; => (354500/1309 138500/1309 9500/77 573581/1309 224093/1309 15371/77)
```

为了证明不同类别存活率之间的统计学显著差异，我们需要证明假设独立的频率和观察到的频率之间的差异不可能仅仅是偶然出现的。

## 卡方统计量

*X*

![The chi-squared statistic](graphics/7180OS_04_11.jpg)

*F* [ij] 是假设类别 *i* 和 *j* 独立的期望频率，而 *f* [ij] 是类别 *i* 和 *j* 的观测频率。因此，我们需要为我们的数据提取观察到的频率。我们可以在 Clojure 中计算如下:

```
(defn observed-frequencies [data]

  (let [as (->> (i/$rollup :sum :count :survived data)

                (summary :count [:survived]))

        bs (->> (i/$rollup :sum :count :pclass data)

                (summary :count [:pclass]))

        actual (summary :count [:survived :pclass] data)]

    (for [a (keys as)

          b (keys bs)]

      (get-in actual [a b]))))
```

与前面的`expected-frequencies`函数一样，`observed-frequencies`函数返回每个类别组合的频率计数序列。

```
(defn ex-4-15 []

  (-> (load-data "titanic.tsv")

      (observed-frequencies)))

;; (200 119 181 123 158 528)
```

这个序列——以及上一个示例中的期望值序列—为我们提供了计算*X*2 统计数据所需的全部信息:

```
(defn chisq-stat [observed expected]

  (let [f (fn [observed expected]

            (/ (i/sq (- observed expected)) expected))]

    (reduce + (map f observed expected))))

(defn ex-4-16 []

  (let [data (load-data "titanic.tsv")

        observed (observed-frequencies data)

        expected (expected-frequencies data)]

    (float (chisq-stat observed expected))))

;; 127.86
```

现在我们有了测试统计数据，我们需要在相关分布中查找它，以确定结果是否显著。不出所料，我们所指的分布是*X*2 分布。

## 卡方检验

*X*

![The chi-squared test](graphics/7180OS_04_12.jpg)

这里， *a* 是属性 *A* 的类别数， *b* 是属性 *B* 的类别数。对于我们的泰坦尼克号数据来说， *a* 是 *3* 而 *b* 是 *2* ，那么我们的自由度参数就是 *2* 。

我们的 *X* ² 测试只需要对照 *X* ² 累积分布函数(CDF)查看我们的 *X* ² 统计量。让我们现在就开始吧:

```
(defn ex-4-17 []

  (let [data (load-data "titanic.tsv")

        observed (observed-frequencies data)

        expected (expected-frequencies data)

        x2-stat  (chisq-stat observed expected)]

    (s/cdf-chisq x2-stat :df 2 :lower-tail? false)))

;; 1.721E-28
```

这是一个非常小的数字，接近于零，所以我们可以在任何显著性水平上轻松地拒绝零假设。换句话说，我们可以绝对肯定观察到的差异不是随机抽样误差的结果。

尽管看到手工执行的 *X* ² 很有用，但是 Incanter stats 名称空间有一个函数`chisq-test`，用于在一个步骤中执行 *X* ² 测试。要使用它，我们只需以矩阵形式提供我们的原始观察表:

```
(defn ex-4-18 []

  (let [table  (->> (load-data "titanic.tsv")

                    (frequency-table :count [:pclass :survived])

                    (i/$order [:survived :pclass] :asc))

        frequencies (i/$ :count table)

        matrix      (i/matrix frequencies 3)]

    (println "Observed:"     table)

    (println "Frequencies:"  frequencies)

    (println "Observations:" matrix)

    (println "Chi-Squared test:")

    (-> (s/chisq-test :table matrix)

        (clojure.pprint/pprint))))
```

在代码之前，我们从 Titanic 数据中计算了一个频率表，然后使用`i/$order`对内容进行排序，因此我们得到了这样一个表:

```
| :survived | :pclass | :count |

|-----------+---------+--------|

|         n |   first |    123 |

|         n |  second |    158 |

|         n |   third |    528 |

|         y |   first |    200 |

|         y |  second |    119 |

|         y |   third |    181 |
```

我们采用 count 列，并使用`(i/matrix frequencies 3)`将其转换成一个包含三列的矩阵:

```
A 2x3 matrix

 -------------

 1.23e+02  1.58e+02  5.28e+02

 2.00e+02  1.19e+02  1.81e+02
```

该矩阵是咒语`s/chisq-test`功能所需的唯一输入。运行该示例，您将看到响应是一个包含键`:X-sq`、 *X* ² 统计数据和`:p-value`(测试结果)等的映射。

我们已经确立，阶级和幸存的范畴，以及性别和幸存的范畴当然不是独立的。这类似于前一章中发现的变量——身高、性别和体重——之间的相关性。

现在，和那时一样，下一步是利用变量之间的相关性进行预测。在前一章中，我们的输出是一个预测数字——重量——在这一章中，我们的输出将是一个类——一个关于乘客是否幸存的预测。根据其他属性将项目分配到其预期类别是分类的过程。



# 用逻辑回归分类

在前一章中，我们看到了线性回归如何从输入向量 *x* 和系数向量 *β* 中产生预测值 *ŷ* :

![Classification with logistic regression](graphics/7180OS_04_13.jpg)

这里， *ŷ* 可以是任意实数。逻辑回归以非常相似的方式进行，但是调整预测以保证答案仅在零和一之间:

![Classification with logistic regression](graphics/7180OS_04_14.jpg)

零和一代表两个不同的类。变化是简单的；我们简单地将预测封装在一个函数 *g* 中，该函数将输出限制在 0 和 1 之间:

![Classification with logistic regression](graphics/7180OS_04_15.jpg)

其中 *g* 称为**s 形**函数。这个看似微小的变化足以将线性回归转化为逻辑回归，并将实值预测转化为类。

## 乙状结肠功能

sigmoid 函数也被称为*逻辑函数*，如下所示:

![The sigmoid function](graphics/7180OS_04_16.jpg)

对于正输入，逻辑函数迅速上升到 1，而对于负输入，逻辑函数迅速下降到 0。这些输出对应于预测的类别。对于接近于零的值，逻辑函数返回接近于 **0.5** 的值。这对应于正确输出类的不确定性增加。

![The sigmoid function](graphics/7180OS_04_140.jpg)

结合我们已经看到的公式，已经产生了以下对逻辑假设的完整定义:

![The sigmoid function](graphics/7180OS_04_17.jpg)

与线性回归一样，参数向量 *β* 包含我们想要学习的系数，而 *x* 是我们的输入特征向量。我们可以用下面的高阶函数在 Clojure 中表达这一点。给定一个系数向量，该函数返回一个函数，该函数将为给定的 *x* 计算 *ŷ* :

```
(defn sigmoid-function [coefs]

  (let [bt (i/trans coefs)

        z  (fn [x] (- (first (i/mmult bt x))))]

    (fn [x]

      (/ 1

         (+ 1

            (i/exp (z x)))))))
```

如果逻辑函数的 *β* 为`[0]`，则该特征不具有任何预测能力。对于任何输入 *x* ，该函数将输出`0.5`，对应于完全不确定性:

```
(let [f (sigmoid-function [0])]

  (f [1])

  ;; => 0.5

  (f [-1])

  ;; => 0.5

  (f [42])

  ;; => 0.5

  )
```

但是，如果非零值作为系数提供，sigmoid 函数可以返回除`0.5`以外的值。给定正的 *x* ，正的 *β* 将导致更大概率的正类，而负的 *β* 将对应于给定正的x 的更大概率的负类。

```
(let [f (sigmoid-function [0.2])

      g (sigmoid-function [-0.2])]

  (f [5])

  ;; => 0.73

  (g [5])

  ;; => 0.27

  )
```

由于大于`0.5`的值对应于正类，小于`0.5`的值对应于负类，因此 sigmoid 函数输出可以简单地四舍五入为最接近的整数，以获得输出类。这将导致恰好`0.5`的值被分类为正类。

既然我们有了可以返回类预测的`sigmoid-function`，我们需要学习产生最佳预测的参数*β**ŷ*。在前一章中，我们看到了计算线性模型系数的两种方法——使用协方差计算斜率和截距，以及使用矩阵计算法线方程。在这两种情况下，方程能够找到一个线性解决方案，使我们的模型的最小二乘估计最小化。

对于我们的线性模型来说，平方误差是一个合适的函数，但是它不能很好地转化为分类，因为分类只在 0 和 1 之间测量。我们需要另一种方法来确定我们的预测有多不正确。

## 逻辑回归成本函数

与线性回归一样，逻辑回归算法必须从数据中学习。`cost`函数是一种让算法知道它做得好或差的方式。

以下是用于逻辑回归的`cost`函数，该函数根据输出类应该是零还是一来施加不同的成本。单个培训示例的成本是这样计算的:

![The logistic regression cost function](graphics/7180OS_04_18.jpg)

这一对函数抓住了直觉，即如果 *ŷ* = 0 但是 *y* = 1，那么该模型应该被罚以非常大的代价。对称来说，如果 *ŷ* = 1 而 *y* = 0，那么模型也应该受到重罚。当模型与数据非常吻合时，成本急剧下降到零。

这是单个训练点的成本。要合并单个成本并计算给定系数向量和一组训练数据的总成本，我们可以简单地取所有训练示例的平均值:

![The logistic regression cost function](graphics/7180OS_04_19.jpg)

这可以用 Clojure 中的表示如下:

```
(defn logistic-cost [ys y-hats]

  (let [cost (fn [y y-hat]

               (if (zero? y)

                 (- (i/log (- 1 y-hat)))

                 (- (i/log y-hat))))]

    (s/mean (map cost ys y-hats))))
```

现在我们有了一个`cost`函数，它可以量化我们的预测有多不正确，下一步就是利用这个信息做出更好的预测。最好的分类器将是具有最低总成本的分类器，因为根据定义，它的预测类将最接近真实类。我们可以逐步提高成本的方法叫做 **梯度下降**。

## 梯度下降参数优化

成本函数，也称为 **损失函数**，是基于我们的系数计算模型误差的函数。不同的参数将为相同的数据集生成不同的成本，我们可以在图表上直观地看到成本函数相对于参数的变化。

![Parameter optimization with gradient descent](graphics/7180OS_04_150.jpg)

上图显示了双参数模型的成本函数。成本绘制在 *y* 轴上(较高的值对应于较高的成本)，两个参数分别绘制在 *x* 和 *z* 轴上。

最佳参数是最小化成本函数的参数，对应于被识别为“全局最小值”的点处的参数。我们事先不知道这些参数会是什么，但我们可以做一个初步的、任意的猜测。这些参数由点“P”标识。

梯度下降是一种算法，通过沿着梯度向最小值下降来迭代地改善初始条件。当算法不能再下降任何一个时，就找到了最小成本。此时的参数对应于我们对最小化成本函数的参数的最佳估计。

## 用咒语渐变下降

Incanter 提供了在`incanter.optimize`名称空间中使用函数`minimize`运行渐变下降的能力。数学优化是一系列技术的通称，这些技术旨在为一些约束条件找到最佳可用的解决方案。`incanter.optimize`名称空间包含用于计算参数的函数，这些参数将最小化或最大化任意函数的值。

例如，下面的代码在给定起始位置`10`的情况下找到了`f`的最小值。由于`f`是*x*2，将产生最小值的输入是`0`:

```
(defn ex-4-19 []

  (let [f (fn [[x]]

            (i/sq x))

        init [10]]

    (o/minimize f init)))
```

事实上，如果你运行这个例子，你会得到一个非常接近于零的答案。虽然你不太可能得到精确的零，因为梯度下降往往只提供近似的答案——Incanter 的`minimize`函数接受一个默认为 0.00001 的公差参数`:tol`。如果两次迭代之间的结果差异小于这个量，则称该方程已经收敛。该函数还接受一个`:max-iter`参数，即在返回一个答案之前要采取的最大步骤数，而不考虑收敛性。

## 凸面

梯度下降并不总是保证找到所有方程的最低可能成本。例如，答案可能会找到所谓的“局部最小值”，它代表初始猜测附近的最低成本，但不代表问题的最佳整体解决方案。下图对此进行了说明:

![Convexity](graphics/7180OS_04_160.jpg)

如果初始位置对应于图上标记为 **C** 的任一点，那么算法将收敛到局部最小值。梯度下降将找到一个最小值，但它不是最佳的整体解决方案。只有在范围 **A** 到 **B** 内的初始猜测会收敛到全局最小值。

因此根据其初始化，梯度下降可能会收敛到不同的答案。对于保证最优解的梯度下降，要优化的方程需要是凸方程。这意味着只有一个全局最小值，没有局部最小值。

例如，`sin`函数没有全局最小值。我们计算最小值的结果将强烈依赖于我们的起始条件:

```
(defn ex-4-20 []

  (let [f (fn [[x]]

            (i/sin x))]

    (println (:value (o/minimize f [1])))

    (println (:value (o/minimize f [10])))

    (println (:value (o/minimize f [100])))))

A 1x1 matrix

 -------------

-2.14e+05

 A 1x1 matrix

 -------------

 1.10e+01

 A 1x1 matrix

 -------------

 9.90e+01
```

幸运的是，logistic 回归是凸函数。这意味着梯度下降将能够确定对应于全局最小值的系数的值，而不管我们的起始位置。



# 用咒语实现逻辑回归

我们可以用 Incanter 的`minimize`函数定义一个逻辑回归函数如下:

```
(defn logistic-regression [ys xs]

  (let [cost-fn (fn [coefs]

                  (let [classify (sigmoid-function coefs)

                        y-hats   (map (comp classify i/trans) xs)]

                    (logistic-cost ys y-hats)))

        init-coefs (repeat (i/ncol xs) 0.0)]

    (o/minimize cost-fn init-coefs)))
```

`cost-fn`接受系数矩阵。我们使用先前定义的`sigmoid-function`和基于输入数据的一系列预测`y-hats`从系数中创建一个分类器。最后，我们可以根据提供的系数计算并返回`logistic-cost`值。

为了执行逻辑回归，我们通过选择`sigmoid-function`的最佳参数来最小化逻辑`cost-fn`。因为我们必须从某处开始，我们的初始系数只是每个参数的`0.0`。

`minimize`函数期望接收数字形式的输入。像前一章中的运动员数据一样，我们必须将我们的巨大数据转换成一个特征矩阵，并为我们的分类数据创建虚拟变量。

## 创建特征矩阵

让我们定义一个函数`add-dummy`，它将为给定的列创建一个虚拟变量。当输入列中的值等于特定值时，虚拟列将包含一个`1`。如果输入列中的值不包含该值，虚拟列将是`0`。

```
(defn add-dummy [column-name from-column value dataset]

  (i/add-derived-column column-name

                        [from-column]

                        #(if (= % value) 1 0)

                        dataset))
```

这个简单的函数使得将我们的海量数据转换成特征矩阵变得非常简单:

```
(defn matrix-dataset []

  (->> (load-data "titanic.tsv")

       (add-dummy :dummy-survived :survived "y")

       (i/add-column :bias (repeat 1.0))

       (add-dummy :dummy-mf :sex "male")

       (add-dummy :dummy-1 :pclass "first")

       (add-dummy :dummy-2 :pclass "second")

       (add-dummy :dummy-3 :pclass "third")

       (i/$ [:dummy-survived :bias :dummy-mf

             :dummy-1 :dummy-2 :dummy-3])

       (i/to-matrix)))
```

我们的输出矩阵将完全由 0 和 1 组成。特征矩阵中的第一个元素是决定存活率的虚拟变量。这是我们的班级标签。`0`对应灭亡，`1`对应生存。第二个是一个`bias`术语，它总是包含值`1.0`。

定义了我们的`matrix-dataset`和`logistic-regression`函数后，运行逻辑回归就像这样简单:

```
(defn ex-4-21 []

  (let [data (matrix-dataset)

        ys (i/$ 0 data)

        xs (i/$ [:not 0] data)]

    (logistic-regression ys xs)))
```

我们向 Incanter 的`i/$`函数提供`0`来选择矩阵的第一列(类)，向[ `:not 0`提供`0`来选择其他的一切(特性):

```
;; [0.9308681940090573 -2.5150078795265753 1.1782368822555778

;;  0.29749924127081434 -0.5448679293359383]
```

如果你运行这个例子，你会发现它返回一个数字向量。这个向量对应于逻辑模型系数的最佳估计。

## 评估逻辑回归分类器

上一节中计算的向量包含我们的逻辑模型的系数。我们可以通过将它们传递给我们的`sigmoid-function`来进行预测，就像这样:

```
(defn ex-4-22 []

  (let [data (matrix-dataset)

        ys (i/$ 0 data)

        xs (i/$ [:not 0] data)

        coefs (logistic-regression ys xs)

        classifier (comp logistic-class

                      (sigmoid-function coefs)

                      i/trans)]

    (println "Observed: " (map int (take 10 ys)))

    (println "Predicted:" (map classifier (take 10 xs)))))

;; Observed:  (1 1 0 0 0 1 1 0 1 0)

;; Predicted: (1 0 1 0 1 0 1 0 1 0)
```

您可以看到分类器并没有做得很好——它被一些类弄糊涂了。在前十个结果中，有四个类是不正确的，这仅仅比 chance 好一点。让我们看看在整个数据集中正确识别的类的比例:

```
(defn ex-4-23 []

  (let [data (matrix-dataset)

        ys (i/$ 0 data)

        xs (i/$ [:not 0] data)

        coefs (logistic-regression ys xs)

        classifier (comp logistic-class

                      (sigmoid-function coefs)

                      i/trans)

        y-hats (map classifier xs)]

    (frequencies (map = y-hats (map int ys)))))

;; {true 1021, false 288}
```

在前面的代码中，我们像以前一样训练一个分类器，并简单地映射整个数据集，寻找与观察到的类相等的预测。我们使用 Clojure core 的`frequencies`函数来提供类相等的次数的简单计数。

1309 次预测中有 1021 次是正确的，相当于 78%的正确率。我们的分类器绝对比机会表现得更好。

## 混乱矩阵

虽然正确率是一个计算和理解的简单方法，但是它容易受到分类器系统地低估或高估一个给定类的情况的影响。作为一个极端的例子，考虑一个总是将乘客分类为已经死亡的分类器。在我们的泰坦尼克号数据集上，这样的分类器看起来有 68%的正确率，但在大多数乘客幸存的另一个数据集上，它的表现会非常糟糕。

一个`confusion-matrix`函数显示训练集中有多少错误分类的项目，分为真阳性、真阴性、假阳性和假阴性。混淆矩阵中每个输入类别都有一行，每个模型类别都有一列。我们可以在 Clojure 中创建一个这样的:

```
(defn confusion-matrix [ys y-hats]

  (let [classes   (into #{} (concat ys y-hats))

        confusion (frequencies (map vector ys y-hats))]

    (i/dataset (cons nil classes)

               (for [x classes]

                 (cons x

                       (for [y classes]

                         (get confusion [x y])))))))
```

然后，我们可以对逻辑回归的结果运行混淆矩阵，如下所示:

```
(defn ex-4-24 []

  (let [data (matrix-dataset)

        ys (i/$ 0 data)

        xs (i/$ [:not 0] data)

        coefs (logistic-regression ys xs)

        classifier (comp logistic-class

                      (sigmoid-function coefs)

                      i/trans)

        y-hats (map classifier xs)]

    (confusion-matrix (map int ys) y-hats)))
```

它返回以下矩阵:

```
|   |   0 |   1 |

|---+-----+-----|

| 0 | 682 | 127 |

| 1 | 161 | 339 |
```

我们可以看到模型如何返回`682`真阴性和`339`真阳性，总计 1021 个正确预测的结果。一个好模型的混淆矩阵将由沿对角线的计数占主导地位，在非对角线位置有小得多的数。一个完美分类器在所有非对角线单元中为零。

## 卡帕统计

kappa 统计可用于比较两对类别，以查看类别的一致程度。简单地看百分比一致更可靠，因为这个等式旨在说明一些一致仅仅是由于偶然发生的可能性。

kappa 统计数据模拟了每个类别在每个序列中出现的频率，并将其作为计算因素。例如，如果我在 50%的时间里猜对了掷硬币的结果，但我总是猜正面，kappa 统计值将为零。这是因为这个协议不仅仅是偶然的。

为了计算 kappa 统计量，我们需要知道两件事:

*   *p(a)* :这是实际观察到的一致的概率
*   p(e) :这是预期一致的概率

*p(a)* 的值是我们之前计算的百分之 78 的一致性。它是真阳性和真阴性的总和除以样本的大小。

为了计算 *p(e)* 的值，我们需要知道数据中存在的负类的比例，以及我们的模型预测的负类的比例。在我们的数据中，消极阶层的比例是![The kappa statistic](graphics/7180OS_04_20.jpg)，即 62%。这是在泰坦尼克号灾难中死亡的概率。在我们的模型中，负类的比例可以从混淆矩阵中计算为![The kappa statistic](graphics/7180OS_04_21.jpg)，或者 64%。

数据和模型可能偶然一致的概率， *p(e)* ，是模型和数据都具有负类![The kappa statistic](graphics/7180OS_04_22.jpg)的概率加上数据和模型都具有正类![The kappa statistic](graphics/7180OS_04_23.jpg)的概率。因此随机一致的概率 *p(e)* 大约是 53%。

前面的信息是我们计算 kappa 统计所需的全部信息:

![The kappa statistic](graphics/7180OS_04_24.jpg)

在中代入我们刚刚计算的值得到:

![The kappa statistic](graphics/7180OS_04_25.jpg)

我们可以在 Clojure 中计算如下:

```
(defn kappa-statistic [ys y-hats]

  (let [n (count ys)

        pa (/ (count (filter true? (map = ys y-hats))) n)

        ey (/ (count (filter zero? ys)) n)

        eyh (/ (count (filter zero? y-hats)) n)

        pe (+ (* ey eyh)

              (* (- 1 ey)

                 (- 1 eyh)))]

    (/ (- pa pe)

       (- 1 pe))))

(defn ex-4-25 []

   (let [data (matrix-dataset)

         ys (i/$ 0 data)

         xs (i/$ [:not 0] data)

         coefs (logistic-regression ys xs)

         classifier (comp logistic-class

                       (sigmoid-function coefs)

                       i/trans)

         y-hats (map classifier xs)]

     (float (kappa-statistic (map int ys) y-hats))))

;; 0.527
```

kappa 值介于 0 和 1 之间，1 表示两个输出类别完全一致。只有一个输出类别的完全一致在 kappa 中没有定义——如果我在 100%的时间里猜对了一个硬币投掷的结果，但是硬币总是正面朝上，就没有办法知道这个硬币是一个公平的硬币。



# 概率

到目前为止，我们已经在本书中遇到了多种形式的概率:作为*p*-值、置信区间，以及最近作为逻辑回归的输出，其中结果可以被认为是输出类为正的概率。我们为 kappa 统计量计算的概率是计数相加并除以总数的结果。例如，符合的概率计算为模型和数据符合的次数除以样本数。这种计算概率的方法被称为**频率主义者**，因为它关注的是事情发生的频率。

逻辑回归(前舍入)的输出`1.0`对应于输入在正类中的确定性；`0.0`的输出对应于输入不在正类中的确定性。`0.5`的输出对应于输出类的完全不确定性。例如，如果 *ŷ = 0.7* ，那么 *y = 1* 的概率是 70%。我们可以这样写:

![Probability](graphics/7180OS_04_26.jpg)

我们说 *y-hat 等于 y 等于一个给定 x 的概率，由β*参数化。这个新的符号表达了这样一个事实，即我们的预测 *ŷ* ，是由包括 *x* 和 *β* 在内的输入来通知的。这些向量中包含的值影响我们对输出概率的计算，并相应地影响我们对 *y* 的预测。

频率主义者的概率观点的另一种选择是贝叶斯观点 T2。概率的贝叶斯概念将先验信念结合到概率计算中。为了说明区别，让我们再看一次抛硬币的例子。

让我们想象一枚硬币连续抛 14 次，10 次都是正面朝上。你被要求打赌在接下来的两次投掷中它是否会正面着地。你愿意打赌吗？

对于一个常客来说，硬币连续两次正面朝上的概率是![Probability](graphics/7180OS_04_27.jpg)。这比 50%略高，所以打赌是有意义的。

贝叶斯理论会以不同的方式构建这个问题。如果事先认为硬币是公平的，那么数据有多符合这种想法呢？超过 14 次投掷的比例的标准误差为 0.12。![Probability](graphics/7180OS_04_28.jpg)和![Probability](graphics/7180OS_04_29.jpg)之差除以标准误差约为 1.77，对应于约为 0.08 的 *p* 值的。根本没有足够的证据来否定硬币是公平的这一理论。如果硬币是公平的，那么连续两次正面朝上的概率是![Probability](graphics/7180OS_04_30.jpg)，我们可能会输掉这场赌局。

### 注意

在 18 世纪，皮埃尔·西蒙·拉普拉斯提出“明天太阳升起的概率是多少？”为了说明用概率论评估陈述的似然性的困难。

概率的贝叶斯观点产生了一个非常有用的定理，叫做**贝叶斯定理**。

## 贝叶斯定理

我们在上一节中提出的逻辑回归方程是条件概率的一个例子:

![Bayes theorem](graphics/7180OS_04_26.jpg)

我们预测 *ŷ* 的概率由值 *x* 和 *β* 决定。条件概率是已知一件事的可能性。例如，我们已经考虑了诸如“假设乘客是女性，生还的可能性”这样的问题。

假设我们对 *x* 、 *y* 和 *z* 感兴趣，概率的基本符号如下:

*   ![Bayes theorem](graphics/7180OS_04_31.jpg):这是 *A* 发生的概率
*   ![Bayes theorem](graphics/7180OS_04_32.jpg):这是 *A* 和 *B* 同时发生的联合概率
*   ![Bayes theorem](graphics/7180OS_04_33.jpg):这是 *A* 或 *B* 发生的概率
*   ![Bayes theorem](graphics/7180OS_04_80.jpg):这是给定 *B* 已经发生的情况下 *A* 发生的概率
*   ![Bayes theorem](graphics/7180OS_04_81.jpg):这个是假设 *C* 已经发生，那么 *A* 和 *B* 都发生的概率

前述变量之间的关系用以下公式表示:

![Bayes theorem](graphics/7180OS_04_34.jpg)

利用这一点，我们可以求解![Bayes theorem](graphics/7180OS_04_80.jpg)假设![Bayes theorem](graphics/7180OS_04_35.jpg)得到所谓的贝叶斯定理:

![Bayes theorem](graphics/7180OS_04_36.jpg)

我们将此解读为“给定 *B* 的 *A* 的概率等于 *B* 的概率，给定 *A* ，乘以 *A* 的概率全部超过 *B* 的概率”。

![Bayes theorem](graphics/7180OS_04_31.jpg)是先验概率:对 *A* 的初始信任度。

![Bayes theorem](graphics/7180OS_04_80.jpg)是条件概率——考虑到 *B* 后，对 *A* 的信任程度。

商![Bayes theorem](graphics/7180OS_04_37.jpg)代表 *B* 为 *A* 提供的支持。

贝叶斯定理可能显得吓人和抽象，所以让我们看一个例子来说明它为什么有用。假设我们正在检测已经感染了 1%人口的疾病。我们有一个高度敏感和特异的测试，但并不完美:

*   99%的病人测试呈阳性
*   99%的健康患者测试结果呈阴性

假设患者检测呈阳性，那么患者实际患病的概率是多少？

前面的要点似乎暗示阳性检测意味着 99%的患病几率，但这没有考虑到这种疾病在人群中的罕见程度。由于被感染的概率(先验)很小，这极大地降低了你实际患病的机会，即使你测试呈阳性。

让我们用 10，000 个有代表性的人来研究这些数字。这意味着 100 人患病，但 9900 人健康。如果我们将测试应用于所有 10，000 人，我们会发现 99 名患者测试为患病(真阳性)，但 99 名健康人也测试为患病(假阳性)。如果你检测呈阳性，实际患病的几率是 50 %:

![Bayes theorem](graphics/7180OS_04_170.jpg)

我们可以用贝叶斯法则计算同样的例子。让 *y* 表示“生病”,让 *x* 表示事件“+”表示阳性结果:

![Bayes theorem](graphics/7180OS_04_40.jpg)

换句话说，尽管阳性测试大大增加了你患病的几率(从人口中的 1%上升)，但你实际患病的几率仍然只有一半——远没有测试准确性所暗示的 99%。

前面的例子为我们提供了简洁的数据，现在让我们在泰坦尼克号的数据上运行这个例子。

假设你是女性，幸存的概率等于你是女性的概率乘以幸存的概率除以泰坦尼克号上女性的概率:

![Bayes theorem](graphics/7180OS_04_41.jpg)

让我们提醒自己前面的列联表:

```
| :survived |   :sex | :count |

|-----------+--------+--------|

|         n |   male |    682 |

|         n | female |    127 |

|         y |   male |    161 |

|         y | female |    339 |
```

*P(survival|female)* 是的后验概率，给出了证据的生存信念程度。这是我们试图计算的值。

*P(女性|存活率)*是女性的条件概率，给定存活率:

![Bayes theorem](graphics/7180OS_04_42.jpg)

*P(生存)*是先验的，生存信念的初始程度:

![Bayes theorem](graphics/7180OS_04_43.jpg)

*P(女)*就是证据:

![Bayes theorem](graphics/7180OS_04_44.jpg)

将这些比例代入贝叶斯法则:

![Bayes theorem](graphics/7180OS_04_45.jpg)

利用贝叶斯法则，我们计算出，如果是女性，存活的概率是 76%。

请注意，我们也可以从列联表中计算出这个值，方法是查找幸存者占全部女性的比例:![Bayes theorem](graphics/7180OS_04_47.jpg)。贝叶斯规则流行的原因是它给了我们一种在没有列联表的地方计算这个概率的方法。

## 具有多个预测器的贝叶斯定理

作为一个在没有完整列联表的情况下如何使用贝叶斯规则的例子，让我们用一个三等男性的例子。三等男乘客生还概率有多大？

让我们写出这个新问题的贝叶斯规则:

![Bayes theorem with multiple predictors](graphics/7180OS_04_48.jpg)

接下来，我们有两个列联表:

```
| :survived | :pclass | :count |

|-----------+---------+--------|

|         n |   first |    123 |

|         n |  second |    158 |

|         n |   third |    528 |

|         y |   first |    200 |

|         y |  second |    119 |

|         y |   third |    181 |

| :survived |   :sex | :count |

|-----------+--------+--------|

|         n | female |    127 |

|         n |   male |    682 |

|         y | female |    339 |

|         y |   male |    161 |
```

“三等男性”在我们的任何列联表中都不是一个可以简单查找的类别。然而，通过使用贝叶斯定理我们可以这样计算它:

我们寻求的后验概率是 *P(存活|男性，第三)*。

先验生存概率和之前一样:![Bayes theorem with multiple predictors](graphics/7180OS_04_49.jpg)或者 0.38 左右。

条件概率为![Bayes theorem with multiple predictors](graphics/7180OS_04_50.jpg)。这和![Bayes theorem with multiple predictors](graphics/7180OS_04_51.jpg)一样。换句话说，我们可以将两个概率相乘:

![Bayes theorem with multiple predictors](graphics/7180OS_04_52.jpg)![Bayes theorem with multiple predictors](graphics/7180OS_04_53.jpg)

证据是既是男性又是三等公民的概率![Bayes theorem with multiple predictors](graphics/7180OS_04_54.jpg):

![Bayes theorem with multiple predictors](graphics/7180OS_04_55.jpg)![Bayes theorem with multiple predictors](graphics/7180OS_04_56.jpg)

将所有这些放在一起:

![Bayes theorem with multiple predictors](graphics/7180OS_04_57.jpg)

事实上，总共 493 名三等男性中有 75 名幸存，真实存活率为 15%。贝叶斯定理让我们可以非常精确地计算出真实答案，而无需使用完整的列联表。



# 朴素贝叶斯分类

我们使用贝叶斯定理得出的答案与实际结果略有不同的原因是，通过使用贝叶斯规则，我们在计算![Naive Bayes classification](graphics/7180OS_04_54.jpg)时做出了一个假设，即成为男性的概率和成为三等公民的概率是独立的。在下一节中，我们将使用贝叶斯定理来生成一个朴素贝叶斯分类器。

### 注意

这个算法被称为 naive 的原因是因为它假设所有变量都是独立的。我们知道通常不是这样，变量之间存在交互作用。例如，我们可能知道参数的组合使得某个阶层更有可能——例如，既是男性又是三等。

让我们看看如何使用贝叶斯规则作为分类器。生存和灭亡这两个可能类别的贝叶斯定理对于第三类中的男性显示如下:

![Naive Bayes classification](graphics/7180OS_04_58.jpg)![Naive Bayes classification](graphics/7180OS_04_59.jpg)

最可能的类别将是具有最大后验概率的类别。

![Naive Bayes classification](graphics/7180OS_04_54.jpg)似乎是两个阶层的共同因素。如果我们稍微放松一下贝叶斯定理的要求，使它不必返回概率，我们就可以去掉公因子，得到下面的结果:

![Naive Bayes classification](graphics/7180OS_04_61.jpg)![Naive Bayes classification](graphics/7180OS_04_62.jpg)

我们只是从两个方程的右边去掉了分母。由于我们不再计算概率，等号变成了![Naive Bayes classification](graphics/7180OS_04_63.jpg)，意思是“成比例”。

将我们之前数据表中的值代入等式，得出:

![Naive Bayes classification](graphics/7180OS_04_64.jpg)![Naive Bayes classification](graphics/7180OS_04_65.jpg)

我们可以立即看到，我们不是在计算概率，因为这两个类加起来不是一个。这对于我们的分类器来说无关紧要，因为无论如何我们只选择与最高值相关的类。不幸的是，我们的三等男性，我们的朴素贝叶斯模型预测他将灭亡。

让我们对一个头等舱的女性做同样的计算:

![Naive Bayes classification](graphics/7180OS_04_66.jpg)![Naive Bayes classification](graphics/7180OS_04_67.jpg)

幸运的是我们的第一位女性，模型预测她会活下来。

贝叶斯分类器是贝叶斯概率模型与决策规则(选择哪个类)的组合。前面描述的决策规则是最大后验概率规则，或 MAP 规则。

## 实现朴素贝叶斯分类器

幸运的是，用代码实现朴素贝叶斯模型比理解数学要容易得多。第一步是简单地计算对应于每个类的每个特征的每个值的例子的数量。以下代码记录每个类标签中每个参数出现的次数:

```
(defn inc-class-total [model class]

  (update-in model [class :total] (fnil inc 0)))

(defn inc-predictors-count-fn [row class]

  (fn [model attr]

    (let [val (get row attr)]

      (update-in model [class attr val] (fnil inc 0)))))

(defn assoc-row-fn [class-attr predictors]

  (fn [model row]

    (let [class (get row class-attr)]

      (reduce (inc-predictors-count-fn row class)

              (inc-class-total model class)

              predictors))))

(defn bayes-classifier [data class-attr predictors]

  (reduce (assoc-row-fn class-attr predictors) {} data))
```

标签是对应类的属性(比如我们的泰坦尼克号数据中“幸存”是对应我们类 true 和 false 的标签)，参数是对应特征(性别和类)的属性序列。

它可以这样使用:

```
(defn ex-4-26 []

  (->> (load-data "titanic.tsv")

       (:rows)

       (bayes-classifier :survived [:sex :pclass])

       (clojure.pprint/pprint)))
```

此示例生成以下贝叶斯模型:

```
{:classes

 {"n"

  {:predictors

   {:pclass {"third" 528, "second" 158, "first" 123},

    :sex {"male" 682, "female" 127}},

   :n 809},

  "y"

  {:predictors

   {:pclass {"third" 181, "second" 119, "first" 200},

    :sex {"male" 161, "female" 339}},

   :n 500}},

 :n 1309}
```

该模型只是一个作为嵌套映射实现的两级层次结构。顶层是我们的两个类——`"n"`和`"y"`，分别对应“灭亡”和“幸存”。对于每个类别，我们都有一个预测图— `:pclass`和`:sex`。每个键对应一个可能值和计数的图。除了预测图，每个类都有一个计数`:n`。

现在，我们已经计算了贝叶斯模型，我们可以实现我们的地图决策规则。下面是一个计算给定类的条件概率的函数。例如，![Implementing a naive Bayes classifier](graphics/7180OS_04_68.jpg):

```
(defn posterior-probability [model test class-attr]

  (let [observed (get-in model [:classes class-attr])

        prior (/ (:n observed)

                 (:n model))]

    (apply * prior

           (for [[predictor value] test]

             (/ (get-in observed [:predictors predictor value])

                (:n observed))))))
```

给定一个特定的`class-attr`，前面的代码将计算给定观察值的类的后验概率。实现了前面的代码后，分类器只需返回对应于最大后验概率的类:

```
(defn bayes-classify [model test]

  (let [probability (partial posterior-probability model test)

        classes     (keys (:classes model))]

    (apply max-key probability classes)))
```

前面的代码计算了每个模型类的测试输入的概率。返回的类就是具有最高后验概率的类。

## 评估朴素贝叶斯分类器

现在我们已经写了两个互补的函数，`bayes-classifier`和`bayes-classify`，我们可以使用我们的模型来进行预测。让我们在泰坦尼克号数据集上训练我们的模型，并检查它对我们已经计算过的三等男性和一等女性的预测:

```
(defn ex-4-27 []

  (let [model (->> (load-data "titanic.tsv")

                   (:rows)

                   (naive-bayes :survived [:sex :pclass]))]

    (println "Third class male:"

             (bayes-classify model {:sex "male" :pclass "third"}))

    (println "First class female:"

             (bayes-classify model {:sex "female" :pclass "first"}))))

;; Third class male: n

;; First class female: y
```

这是一个好的开始——我们的分类器与我们手工计算的结果一致。让我们来看看朴素贝叶斯分类器的正确率:

```
(defn ex-4-28 []

   (let [data (:rows (load-data "titanic.tsv"))

         model (bayes-classifier :survived [:sex :pclass] data)

         test (fn [test]

                (= (:survived test)

                   (bayes-classify model

                            (select-keys test [:sex :class]))))

         results (frequencies (map test data))]

     (/ (get results true)

        (apply + (vals results)))))

;; 1021/1309
```

通过在整个数据集上重复我们的测试并比较输出，我们可以看到我们的分类器得到正确答案的频率。78%与我们使用逻辑回归分类器得到的正确率相同。对于这样一个简单的模型，朴素贝叶斯表现得非常好。

我们可以计算一个混淆矩阵:

```
(defn ex-4-195 []

    (let [data (:rows (load-data "titanic.tsv"))

          model (bayes-classifier :survived [:sex :pclass] data)

          classify (fn [test]

                     (->> (select-keys test [:sex :pclass])

                          (bayes-classify model)))

          ys      (map :survived data)

          y-hats (map classify data)]

      (confusion-matrix ys y-hats)))
```

上述代码生成以下矩阵:

```
|   |   n |   y |

|---+-----+-----|

| n | 682 | 127 |

| y | 161 | 339 |
```

这个混淆矩阵与我们之前从逻辑回归中得到的相同。尽管采取了非常不同的方法，他们都能够以相同的准确度对数据集进行分类。

### 比较逻辑回归和朴素贝叶斯方法

虽然它们在我们的小型 Titanic 数据集上表现得一样好，但这两种分类方法通常适用于不同的任务。

尽管与逻辑回归相比，朴素贝叶斯在概念上是一个更简单的分类器，但在数据稀缺或参数数量非常大的情况下，朴素贝叶斯往往能胜过它。由于朴素贝叶斯处理大量特征的能力，它经常被用于解决诸如自动医疗诊断或垃圾邮件分类等问题。在垃圾邮件分类中，特征可以达到数万或数十万，每个单词代表一个特征，可以帮助识别邮件是否是垃圾邮件。

然而，朴素贝叶斯的一个缺点是它的独立性假设——在这个假设不成立的问题域中，其他分类器可以胜过朴素贝叶斯。在拥有大量数据的情况下，逻辑回归能够学习更复杂的模型，并且分类可能比朴素贝叶斯更准确。

还有另一种方法——虽然建模简单且相对直接——能够了解参数之间更复杂的关系。这个方法就是决策树。



# 决策树

本章我们要看的第三种分类方法是决策树。决策树将分类过程建模为一系列测试，这些测试检查要分类的项目的特定属性的值。可以认为它类似于流程图，每个测试都是流程中的一个分支。该过程继续，测试和分支，直到到达叶节点。叶节点将表示该项目的最可能的类。

决策树与逻辑回归和朴素贝叶斯有一些相似之处。虽然分类器可以支持没有哑编码的分类变量，但它也能够通过重复分支来模拟变量之间的复杂依赖关系。

在老式的室内游戏*二十个问题*中，一个人，即“回答者”，选择一个对象，但不向其他人透露他们的选择。其他所有玩家都是“提问者”，轮流提问，旨在猜测回答者想到的对象。每个问题只能用简单的“是”或“不是”来回答。提问者面临的挑战是在仅有的 20 个问题中猜出回答者正在思考的对象，并选出最能揭示回答者正在思考的对象的信息的问题。这不是一件容易的事情——问太宽泛的问题，你不会从答案中获得太多信息。问一些太具体的问题，你不会在 20 步内找到答案。

不出所料，这些担忧也出现在决策树分类中。信息是可以量化的，决策树旨在提出可能产生最大信息增益的问题。

## 信息

想象一下，我从一副普通的 52 张扑克牌中随机挑选一张。你的挑战是猜猜我选了哪张牌。但首先，我提议用“是”或“不是”来回答一个问题。你更想问哪个问题？

*   是红色的吗？(心形或菱形)
*   是图片卡吗？(杰克、皇后或国王)

我们将在接下来的几页中详细探讨这一挑战。花点时间考虑一下你的问题。

一副牌中有 26 张红牌，因此随机选择一张红牌的概率为![Information](graphics/7180OS_04_29.jpg)。一副牌中有 12 张图片牌，因此随机选择一张图片牌的概率是![Information](graphics/7180OS_04_69.jpg)。

与单个事件相关的信息 *I* 是:

![Information](graphics/7180OS_04_70.jpg)

Incanter 有一个`log2`功能，使我们能够计算这样的信息:

```
(defn information [p]

  (- (i/log2 p)))
```

这里，`log2`是以 2 为底的对数。因此:

![Information](graphics/7180OS_04_71.jpg)![Information](graphics/7180OS_04_72.jpg)

由于图片卡具有较低的概率，因此它也具有最高的信息价值。如果我们知道这张牌是一张图片牌，那么它可能只有 12 张。如果我们知道卡片是红色的，那么还有 26 种可能性。

信息通常用比特来衡量。知道卡是红色的信息内容只携带一位信息。一个计算机位只能代表 0 或 1。一位足以包含简单的 50/50 分割。知道这张卡是一张图片卡提供了两位信息。这似乎表明，最好的问题是“这是一张图片卡吗？”。一个肯定的回答会包含很多信息。

但是看看如果我们发现这个问题的答案是“不”会发生什么。发现我选的卡不是图片卡的信息内容是什么？

![Information](graphics/7180OS_04_73.jpg)![Information](graphics/7180OS_04_74.jpg)

似乎现在我们可以更好地询问卡片是否是红色的，因为信息含量更大。发现我们的卡片不是一张图片卡片仍然剩下 36 种可能性。我们显然事先不知道答案是“是”还是“不是”，那么我们如何着手选择最佳问题呢？

## 熵

熵是对不确定性的一种度量。通过计算熵，我们可以在所有可能反应的信息内容之间取得平衡。

### 注意

熵的概念是由鲁道夫·克劳修斯在十九世纪中叶作为新兴的热力学科学的一部分引入的，以帮助解释内燃机的部分功能能量是如何由于散热而损失的。在这一章我们讨论香农熵，它来自于二十世纪中期克劳德·香农关于信息论的工作。这两个概念密切相关，尽管它们来自科学的不同角落，处于非常不同的背景下。

熵， *H* ，可以用以下方式计算:

![Entropy](graphics/7180OS_04_75.jpg)

这里， *P(x)* 是 *x* 发生的概率， *I(P(x))* 是 *x* 的信息量。

例如，让我们比较一副牌的熵，其中每一类都是简单的“红色”和“非红色”。我们知道“红色”的信息量为 1，概率为![Entropy](graphics/7180OS_04_29.jpg)。“不红”也是如此，所以熵是以下总和:

![Entropy](graphics/7180OS_04_76.jpg)

以这种方式分割包产生的熵为 1。把包分成“图片”和“非图片”卡片怎么样？“图片”的信息量为 2.12，概率为![Entropy](graphics/7180OS_04_69.jpg)。“非图片”的信息量为 0.38，概率为![Entropy](graphics/7180OS_04_77.jpg):

![Entropy](graphics/7180OS_04_78.jpg)

如果我们将一副牌想象成一系列类别，正的和负的，我们可以使用 Clojure 计算这两副牌的熵:

```
(defn entropy [xs]

  (let [n (count xs)

        f (fn [x]

            (let [p (/ x n)]

              (* p (information p))))]

    (->> (frequencies xs)

         (vals)

         (map f)

         (reduce +))))

(defn ex-4-30 []

  (let [red-black (concat (repeat 26 1)

                          (repeat 26 0))]

    (entropy red-black)))

;; 1.0

(defn ex-4-202 []

  (let [picture-not-picture (concat (repeat 12 1)

                                    (repeat 40 0))]

    (entropy picture-not-picture)))

;; 0.779
```

熵是不确定性的度量。将卡片分为“图片”和“非图片”组的较低熵向我们表明，询问卡片是否是图片是最好的问题。如果我们发现我的牌不是图片牌，这仍然是我问过的最好的问题，因为这副牌中剩余的不确定性较低。熵不仅适用于数字序列，也适用于任何序列。

```
(entropy "mississippi")

;; 1.82
```

低于

```
(entropy "yellowstone")

;; 2.91
```

这尽管他们的长度相等，因为有更多的字母之间的一致性。

## 信息增益

熵向我们表明，最好问的问题——最能降低我们这副牌的熵的问题——是这张牌是否是一张图片牌。

一般来说，我们可以用熵来告诉我们一个分组是不是好分组，用的是信息增益的理论。为了说明这一点，让我们回到我们的泰坦尼克号幸存者。假设我随机挑选了一名乘客，你必须猜测他们是否幸存。这一次，在你回答之前，我想告诉你两件事之一:

*   他们的性别(男性或女性)
*   他们旅行的舱位(头等舱、二等舱还是三等舱)

你想知道哪个？

乍一看，似乎最好的问题是他们乘坐的是哪一个级别。这将把乘客分成三组，正如我们在扑克牌中看到的，小组越小越好。但是，不要忘记，我们的目标是猜测乘客的存活率。为了确定最佳提问问题，我们需要知道哪个问题能给我们带来最多的信息。

信息增益是用我们学习新信息之前和之后的熵之差来衡量的。我们来计算一下得知乘客是男性时的信息增益。首先，让我们计算所有乘客存活率的基线熵。

我们可以使用现有的熵计算，并将生存类的序列传递给它:

```
(defn ex-4-32 []

  (->> (load-data "titanic.tsv")

       (:rows)

       (map :survived)

       (entropy)))

;; 0.959
```

这是一个高熵。我们已经知道熵值为 1.0 表示 50/50 的分裂，但我们也知道泰坦尼克号上的存活率约为 38%。出现这种明显差异的原因是，熵并不是线性变化的，而是朝着 1 快速上升，如下图所示:

![Information gain](graphics/7180OS_04_180.jpg)

接下来，让我们考虑按性别拆分时的生存熵。现在我们有两组来计算熵:男性和女性。组合熵是两个组的加权平均值。我们可以使用下面的函数计算 Clojure 中任意个组的加权平均值:

```
(defn weighted-entropy [groups]

  (let [n (count (apply concat groups))

        e (fn [group]

            (* (entropy group)

               (/ (count group) n)))]

    (->> (map e groups)

         (reduce +))))

(defn ex-4-33 []

  (->> (load-data "titanic.tsv")

       (:rows)

       (group-by :sex)

       (vals)

       (map (partial map :survived))

       (weighted-entropy)))

;; 0.754
```

我们可以看到，已经按性别分组的生存类别的加权熵低于我们从乘客整体获得的 0.96。因此我们的信息增益是 *0.96 - 0.75 = 0.21* 比特。

基于我们刚刚定义的`entropy`和`weighted-entropy`函数，我们可以很容易地将增益表示为 Clojure 函数:

```
(defn information-gain [groups]

  (- (entropy (apply concat groups))

     (weighted-entropy groups)))
```

相反，如果我们将名乘客按等级分组，让我们用这个来计算收益:

```
(defn ex-4-205 []

  (->> (load-data "titanic.tsv")

       (:rows)

       (group-by :pclass)

       (vals)

       (map (partial map :survived))

       (information-gain)))

;; 0.07
```

乘客级别的信息增益为 0.07，性别的信息增益为 0.21。因此，在对存活率进行分类时，知道乘客的性别比他们乘坐的舱位更有用。

## 利用信息增益识别最佳预测器

使用我们刚刚定义的函数，我们可以构建一个有效的树分类器。在给定输出类的情况下，我们需要一种通用方法来计算特定预测器属性的信息增益。在前面的例子中，预测值是`:pclass`，类属性是`:survived`，但是我们可以创建一个通用函数，将这些关键字作为参数`class-attr`和`predictor`:

```
(defn gain-for-predictor [class-attr xs predictor]

  (let [grouped-classes (->> (group-by predictor xs)

                             (vals)

                             (map (partial map class-attr)))]

    (information-gain grouped-classes)))
```

接下来，我们需要一种方法来计算一组给定行的最佳预测值。我们可以简单地将前面的函数映射到所有期望的预测值上，并返回对应于最高增益的预测值:

```
(defn best-predictor [class-attr xs predictors]

  (let [gain (partial gain-for-predictor class-attr xs)]

    (when (seq predictors)

      (apply max-key gain predictors))))
```

让我们通过询问预测值`:sex`和`:pclass`中哪一个是最佳预测值来测试这个函数:

```
(defn ex-4-35 []

  (->> (load-data "titanic.tsv")

       (:rows)

       (best-predictor :survived [:sex :pclass])))

;; :sex
```

令人欣慰的是，我们得到了和以前一样的答案。决策树允许我们递归地应用这种逻辑来构建一个树形结构，该结构仅基于每个分支中的数据，在该分支中选择最佳问题进行提问。

## 递归构建决策树

通过对数据递归应用我们编写的函数，我们可以建立一个数据结构，它代表了树的每一层的最佳分类。首先，让我们定义一个函数，给定一个数据序列，该函数将返回**模态**(最常见的)类。当我们的决策树到达不能再分割数据的点时(或者因为熵为零，或者因为没有剩余的预测器可以分割)，我们将返回模态类。

```
(defn modal-class [classes]

  (->> (frequencies classes)

       (apply max-key val)

       (key)))
```

有了这个简单的函数，我们就可以构建决策树了。这是作为递归函数实现的。给定一个类属性、一系列预测值和一系列值，我们通过将`class-attr`映射到`xs`来构建一系列可用的类。如果熵为零，那么所有的类都是一样的，所以我们只返回第一个。

如果我们组中的类不相同，那么我们需要选择一个预测器进行分支。我们使用我们的`best-predictor`函数来选择与最高信息增益相关的预测器。我们将它从预测器列表中移除(尝试使用同一个预测器两次是没有意义的)，并构建一个`tree-branch`函数。这是对剩余预测器的`decision-tree`的部分递归调用。

最后，我们在`best-predictor`上将数据分组，并在每个组上调用我们部分应用的`tree-branch`函数。这导致整个过程再次重复，但这次只针对由`group-by`定义的数据子集。返回值与预测值一起封装在一个向量中:

```
(defn decision-tree [class-attr predictors xs]

  (let [classes (map class-attr xs)]

    (if (zero? (entropy classes))

      (first classes)

      (if-let [predictor (best-predictor class-attr

                                         predictors xs)]

        (let [predictors  (remove #{predictor} predictors)

              tree-branch (partial decision-tree

                                   class-attr predictors)]

          (->> (group-by predictor xs)

               (map-vals tree-branch)

               (vector predictor)))

        (modal-class classes)))))
```

让我们想象一下预测器`:sex`和`:pclass`的函数输出。

```
(defn ex-4-36 []

  (->> (load-data "titanic.tsv")

       (:rows)

       (decision-tree :survived [:pclass :sex])

       (clojure.pprint/pprint)))

;; [:sex

;;  {"female" [:pclass {"first" "y", "second" "y", "third" "n"}],

;;   "male" [:pclass {"first" "n", "second" "n", "third" "n"}]}]
```

我们可以看到决策树是如何用向量来表示的。向量的第一个元素是用于树分支的预测值。第二个元素是一个映射，包含这个预测器的属性，如关键字`"male"`和`"female"`，其值对应于`:pclass`上的另一个分支。

为了了解如何使用这个函数构建任意深度的树，让我们添加一个预测器`:age`。不幸的是，我们构建的树分类器只能处理分类数据，所以让我们将年龄连续变量分成三个简单的类别:`unknown`、`child`和`adult`。

```
(defn age-categories [age]

  (cond

   (nil? age) "unknown"

   (< age 13) "child"

   :default   "adult"))

(defn ex-4-37 []

  (let [data (load-data "titanic.tsv")]

    (->> (i/transform-col data :age age-categories)

         (:rows)

         (decision-tree :survived [:pclass :sex :age])

         (clojure.pprint/pprint))))
```

这个代码产生了下面的树:

```
[:sex

 {"female"

  [:pclass

   {"first" [:age {"adult" "y", "child" "n", "unknown" "y"}],

    "second" [:age {"adult" "y", "child" "y", "unknown" "y"}],

    "third" [:age {"adult" "n", "child" "n", "unknown" "y"}]}],

  "male"

  [:age

   {"unknown" [:pclass {"first" "n", "second" "n", "third" "n"}],

    "adult" [:pclass {"first" "n", "second" "n", "third" "n"}],

    "child" [:pclass {"first" "y", "second" "y", "third" "n"}]}]}]
```

注意，和以前一样，最好的总体预测仍然是乘客的性别。然而，如果性别是男性，年龄是下一个最有用的预测因素。另一方面，如果性别是女性，乘客级别是最有用的预测因素。由于树的递归性质，每个分支能够只为树的特定分支中的数据确定最佳预测器。

## 利用决策树进行分类

有了从决策树函数返回的数据结构，我们就有了将乘客分类到最可能的类别所需的所有信息。我们的分类器也将递归实现。如果一个向量作为模型传入，我们知道它将包含两个元素——预测器和分支。我们从模型中析构预测器和分支，然后确定测试所在的分支。要做到这一点，我们只需用`(get test predictor)`从测试中获得预测值。我们想要的分支将是对应于这个值的分支。

一旦我们有了分支，我们需要在分支上再次调用`tree-classify`。因为我们在尾部位置(在`if`之后没有进一步的逻辑应用)，我们可以调用`recur`，允许 Clojure 编译器优化我们的递归函数调用:

```
(defn tree-classify [model test]

  (if (vector? model)

    (let [[predictor branches] model

          branch (get branches (get test predictor))]

      (recur branch test))

    model))
```

我们继续递归调用树分类，直到`(vector? model)`返回 false。此时，我们将遍历决策树的全部深度，并到达一个叶节点。此时,`model`参数包含预测的类，所以我们简单地返回它。

```
(defn ex-4-38 []

  (let [data (load-data "titanic.tsv")

        tree (->> (i/transform-col data :age age-categories)

                  (:rows)

                  (decision-tree :survived [:pclass :sex :age]))

        test {:sex "male" :pclass "second" :age "child"}]

    (tree-classify tree test)))

;; "y"
```

决策树预测来自第二类的年轻男性将存活。

## 评估决策树分类器

如前所述，我们可以计算我们的混淆矩阵和 kappa 统计量:

```
(defn ex-4-39 []

  (let [data (-> (load-data "titanic.tsv")

                 (i/transform-col :age age-categories)

                 (:rows))

        tree (decision-tree :survived [:pclass :sex :age] data)]

    (confusion-matrix (map :survived data)

                      (map (partial tree-classify tree) data))))
```

混淆矩阵看起来像这样:

```
|   |   n |   y |

|---+-----+-----|

| n | 763 |  46 |

| y | 219 | 281 |
```

我们可以立即看到分类器正在生成大量的假阴性:`219`。让我们来计算一下 kappa 的统计数据:

```
(defn ex-4-40 []

   (let [data (-> (load-data "titanic.tsv")

                  (i/transform-col :age age-categories)

                  (:rows))

         tree (decision-tree :survived [:pclass :sex :age] data)

         ys     (map :survived data)

         y-hats (map (partial tree-classify tree) data)]

     (float (kappa-statistic ys y-hats))))

;; 0.541
```

我们的树分类器的性能远不如我们尝试过的其他分类器。我们可以尝试提高准确性的一个方法是增加我们使用的预测器的数量。与其使用粗略的年龄分类，不如使用实际的年龄数据作为特征。这将允许我们的分类器更好地区分我们的乘客。说到这里，让我们把车费也算进去:

```
(defn ex-4-41 []

   (let [data (-> (load-data "titanic.tsv")

                  (:rows))

         tree (decision-tree :survived

                             [:pclass :sex :age :fare] data)

         ys     (map :survived data)

         y-hats (map (partial tree-classify tree) data)]

     (float (kappa-statistic ys y-hats))))

;; 0.925
```

太好了！我们取得了惊人的进步；我们的新型号是最好的。通过添加更精细的预测器，我们建立了一个能够以非常高的准确度进行预测的模型。

然而，在我们过度庆祝之前，我们应该仔细想想我们的模型有多普遍。构建分类器的目的通常是对新数据进行预测。这意味着它应该在以前从未见过的数据上表现良好。我们刚刚建立的模型有一个严重的问题。为了理解它是什么，我们将求助于库 clj-ml，它包含各种用于训练和测试分类器的函数。



# 用 clj-ml 分类

虽然构建我们自己版本的逻辑回归、朴素贝叶斯和决策树提供了一个宝贵的机会来讨论它们背后的理论，但 Clojure 为我们提供了几个用于构建分类器的库。clj-ml 库是支持得比较好的库之一。

clj-ml 库目前由 Josua Eckroth 维护，并在他位于 https://github.com/joshuaeckroth/clj-ml 的 GitHub 页面上有文档记录。该库为运行前一章描述的线性回归的提供了 Clojure 接口，以及使用逻辑回归、朴素贝叶斯、决策树和其他算法的分类。

### 注意

clj-ml 中大多数机器学习功能的底层实现由 Java 机器学习库`Weka`提供。**怀卡托知识分析环境** ( **Weka** )，一个开源的机器学习项目，主要由怀卡托大学的机器学习小组发布和维护([http://www.cs.waikato.ac.nz/ml/](http://www.cs.waikato.ac.nz/ml/))。

## 用 clj-ml 加载数据

因为它对机器学习算法的专门支持，clj-ml 提供了创建数据集的函数，这些函数识别数据集的类和属性。函数`clj-ml.data/make-dataset`允许我们创建一个可以传递给 Weka 分类器的数据集。在下面的代码中，我们将`clj-ml.data`包含为`mld`:

```
(defn to-weka [dataset]

  (let [attributes [{:survived ["y" "n"]}

                    {:pclass ["first" "second" "third"]}

                    {:sex ["male" "female"]}

                    :age

                    :fare]

        vectors (->> dataset

                     (i/$ [:survived :pclass :sex :age :fare])

                     (i/to-vect))]

    (mld/make-dataset :titanic-weka attributes vectors

                      {:class :survived})))
```

`mld/make-dataset`期望接收数据集的名称、属性向量、作为行向量序列的数据集以及进一步设置的可选映射。属性标识列名，对于分类变量，还枚举所有可能的类别。分类变量，例如`:survived`，作为映射`{:survived ["y" "n"]}`传递，而连续变量，例如`:age`和`:fare`作为简单的关键字传递。数据集必须以行向量序列的形式提供。为了构建这个，我们简单地使用 Incanter 的`i/$`函数并对结果调用`i/to-vect`。

### 注意

虽然`make-dataset`是一种从任意数据源创建数据集的灵活方式，但是`clj-ml.io`提供了一种`load-instances`功能，可以从各种来源加载数据，例如 CSV 或属性关系文件格式(ARFF)文件和 MongoDB 数据库。

有了 clj-ml 理解的格式的数据集，就该训练分类器了。

## 在 clj-ml 中构建决策树

Clj-ml 实现了大量的分类器，所有的分类器都可以通过`cl/make-classifier`函数访问。我们向构造函数传递两个关键字参数:分类器类型和要使用的算法。例如，让我们看看`:decision-tree`、`:c45`算法。 **C4.5 算法** 是由 Ross Quinlan 设计的，它基于信息熵构建了一个树分类器，其构建方式与本章前面我们自己的`tree-classifier`函数相同。C4.5 以几种方式扩展了我们构建的分类器:

*   在没有一个预测器提供任何信息增益的情况下，C4.5 使用类的期望值在树的更高层创建一个决策节点
*   如果遇到了一个以前没有见过的类，C4.5 将在树的更高层创建一个决策节点，其中包含该类的期望值

我们可以用以下代码在 clj-ml 中创建一个决策树:

```
(defn ex-4-42 []

   (let [dataset (to-weka (load-data "titanic.tsv"))

         classifier (-> (cl/make-classifier :decision-tree :c45)

                        (cl/classifier-train dataset))

         classify (partial cl/classifier-classify classifier)

         ys     (map str  (mld/dataset-class-values dataset))

         y-hats (map name (map classify dataset))]

     (println "Confusion:" (confusion-matrix ys y-hats))

     (println "Kappa:" (kappa-statistic ys y-hats))))
```

上述代码返回以下信息:

```
;; Confusion:

;; |   |   n |   y |

;; |---+-----+-----|

;; | n | 712 |  97 |

;; | y | 153 | 347 |

;;

;; Kappa: 0.587
```

请注意，在训练我们的分类器或使用它进行预测时，我们不需要显式地提供 class 和 predictor 属性。Weka 数据集已经包含了关于每个实例的类属性的信息，分类器将使用它能使用的所有属性来进行预测。尽管如此，结果仍然不如我们以前得到的好。原因是 Weka 的决策树实现拒绝过拟合数据。



# 偏差和方差

过度拟合是一个问题，它发生在机器学习算法上，这些算法能够在训练数据集上生成非常准确的结果，但无法从它们所学习的内容中很好地概括出来。我们说过度拟合数据的模型具有很高的方差。当我们根据包括乘客数字年龄的数据训练决策树时，我们过度拟合了数据。

相反，某些模型可能具有非常高的偏差。这是一种模型强烈倾向于某个结果而不考虑相反的训练示例的情况。回想一下我们的分类器的例子，它总是预测幸存者将会死亡。该分类器将在具有低幸存率数据集上表现良好，但是在其他情况下表现很差。

在高偏差的情况下，模型不太可能在训练阶段对不同的输入表现良好。在方差较大的情况下，模型不太可能在与其训练数据不同的数据上表现良好。

### 注意

就像在假设检验中在第一类和第二类错误之间达成平衡一样，平衡偏差和方差对于从机器学习中产生良好的结果至关重要。

如果我们有太多的特征，学习到的假设可能非常适合训练集，但不能很好地推广到新的例子。

## 过度拟合

识别过度拟合的秘密是在没有被训练过的例子上测试分类器。如果分类器在这些例子上表现不佳，那么模型有可能过度拟合。

通常的方法是将数据集分成两组:训练集和测试集。训练集用于训练分类器，测试集用于确定分类器是否能够很好地从它所学习的内容中进行归纳。

测试集应该足够大，使其成为数据集中的代表性样本，但仍应保留大部分记录用于训练。测试集通常占整个数据集的 10-30%左右。让我们用`clj-ml.data/do-split-dataset`返回两组实例。测试集越小，训练集越大:

```
(defn ex-4-43 []

  (let [[test-set train-set] (-> (load-data "titanic.tsv")

                                 (to-weka)

                                 (mld/do-split-dataset :percentage

                                                       30))

        classifier (-> (cl/make-classifier :decision-tree :c45)

                       (cl/classifier-train train-set))

        classify (partial cl/classifier-classify classifier)

        ys     (map str  (mld/dataset-class-values test-set))

        y-hats (map name (map classify test-set))]

    (println "Confusion:" (confusion-matrix ys y-hats))

    (println "Kappa:" (kappa-statistic ys y-hats))))

;; Confusion:

;; |   |   n |   y |

;; |---+-----+-----|

;; | n | 152 |   9 |

;; | y |  65 | 167 |

;;

;; Kappa: 0.630
```

如果你将这个 kappa 统计数据与之前的进行比较，你会发现实际上我们的准确性在看不见的数据上有所提高。虽然这似乎表明我们的分类器没有过度拟合我们的训练集，但我们的分类器应该能够对新数据做出比我们实际告诉它的数据更好的预测似乎不太现实。

这表明我们在测试集中返回的值可能是幸运的。也许这只是碰巧包含了一些与训练集相比更容易分类的乘客。让我们看看如果我们从最后的 30%中取出测试集会发生什么:

```
(defn ex-4-44 []

  (let [[train-set test-set] (-> (load-data "titanic.tsv")

                                 (to-weka)

                                 (mld/do-split-dataset :percentage

                                                       70))

        classifier (-> (cl/make-classifier :decision-tree :c45)

                       (cl/classifier-train train-set))

        classify (partial cl/classifier-classify classifier)

        ys     (map str  (mld/dataset-class-values test-set))

        y-hats (map name (map classify test-set))]

    (println "Kappa:" (kappa-statistic ys y-hats))))

;; Kappa: 0.092
```

分类器正在数据集最后 30%的测试数据中挣扎。因此，为了公平地反映分类器的整体实际性能，我们需要确保在数据的几个随机子集上测试分类器的性能。

## 交叉验证

将数据集分割成训练和测试数据的互补子集的过程称为交叉验证。为了减少我们刚刚看到的输出中的可变性，与训练集相比，测试集的错误率更低，通常对数据的不同分区运行多轮交叉验证。通过平均所有运行的结果，我们可以更准确地了解模型的真实准确性。这是一种常见的做法，因此 clj-ml 包含了一个专门用于此目的的函数:

```
(defn ex-4-45 []

  (let [dataset (-> (load-data "titanic.tsv")

                    (to-weka))

         classifier (-> (cl/make-classifier :decision-tree :c45)

                        (cl/classifier-train dataset))

         evaluation (cl/classifier-evaluate classifier

                                            :cross-validation

                                            dataset 10)]

     (println (:confusion-matrix evaluation))

     (println (:summary evaluation))))
```

在前面的代码中，我们利用`cl/classifier-evaluate`在数据集上运行 10 次交叉验证。结果以地图的形式返回，其中包含关于模型性能的有用信息，例如，混淆矩阵和汇总统计信息列表，包括我们到目前为止一直在跟踪的 kappa 统计信息。我们打印出混淆矩阵和 clj-ml 提供的摘要字符串，如下所示:

```
;; === Confusion Matrix ===

;;

;;    a   b   <-- classified as

;;  338 162 |   a = y

;;   99 710 |   b = n

;;

;;

;; Correctly Classified Instances        1048            80.0611 %

;; Incorrectly Classified Instances       261            19.9389 %

;; Kappa statistic                          0.5673

;; Mean absolute error                      0.284

;; Root mean squared error                  0.3798

;; Relative absolute error                 60.1444 %

;; Root relative squared error             78.171  %

;; Coverage of cases (0.95 level)          99.3888 %

;; Mean rel. region size (0.95 level)      94.2704 %

;; Total Number of Instances             1309    
```

10 次交叉验证后的 kappa 值为 0.56，仅略低于根据训练数据验证的模型。这似乎是我们能得到的最高价格了。

## 解决高偏差

虽然过度拟合可能是由于在我们的模型中包含了太多的特征而导致的，例如当我们将年龄作为分类变量包含在决策树中时，但是高偏差可能是由其他因素导致的，包括没有足够的数据。

提高模型准确性的一个简单方法是确保训练集中没有缺失值。缺少的值必然会被模型丢弃，从而限制了模型可以学习的训练示例的数量。对于这样一个相对较小的数据集，每个示例都会对结果产生重大影响，并且数据集中会缺少许多年龄值和一个票价值。

我们可以简单地用平均值代替数字列中的缺失值。这是一个合理的默认值，也是一个公平的权衡——作为略微降低字段方差的回报，我们可能会获得更多的训练示例。

Clj-ml 在`clj-ml.filters`名称空间中包含许多过滤器，这些过滤器能够以某种方式改变数据集。一个有用的过滤器是`:replace-missing-values`，它将用数据集中的平均值替换任何缺失的数值。对于分类数据，模态分类被替换。

```
(defn ex-4-46 []

  (let [dataset (->> (load-data "titanic.tsv")

                     (to-weka)

                     (mlf/make-apply-filter

                      :replace-missing-values {}))

        classifier (-> (cl/make-classifier :decision-tree :c45)

                       (cl/classifier-train dataset))

        evaluation (cl/classifier-evaluate classifier

                                           :cross-validation

                                           dataset 10)]

    (println (:kappa evaluation))))

;; 0.576
```

简单地在年龄一栏中填入缺失值，就可以将我们的 kappa 统计数据向上推。我们的模型目前正在努力区分具有不同生存结果的乘客，更多信息可能有助于算法确定正确的类别。虽然我们可以返回数据并提取所有剩余字段，但也可以从现有要素中构建新要素。

### 注意

对于数值，增加参数数量的另一种方法是将数值的多项式版本作为特征包含进来。例如，我们可以简单地通过对现有的年龄值求平方或立方来为年龄 ² 和年龄 ³ 创建特征。虽然这些可能看起来没有给模型添加新的信息，但多项式的比例不同，并为模型提供了可供学习的替代特征。

我们要考虑的平衡偏差和方差的最后一种方法是合并多个模型的输出。



# 集成学习和随机森林

集成学习结合多个模型的输出，以获得比单独使用任何一个模型都更好的预测。原则是，许多弱学习者的综合准确度大于任何一个弱学习者单独获得的准确度。

随机森林是 Leo Breiman 和 Adele Cutler 设计并注册的一种集成学习算法。它将多个决策树组合成一个大的森林学习器。每棵树都使用可用特征的子集对数据进行训练，这意味着每棵树的数据视图都略有不同，并且能够生成与其对等树不同的预测。

在 clj-ml 中创建一个随机森林只需要将参数从`cl/make-classifier`改为`:decision-tree`、`:random-forest`。

## 装袋和增压

Bagging 和 boosting 是创建集合模型的两种相反的技术。Boosting 是一种通用技术的名称，通过训练每个新的模型来构建集成，以强调对以前的模型无法正确分类的训练示例进行正确分类。它是一个 **元算法**。

### 注意

最流行的增强算法之一是“自适应增强”的组合 **AdaBoost** 。只要每个模型的表现比随机猜测稍好，组合的输出可以显示为收敛到一个强学习者。

Bagging 是“bootstrap aggregating”的组合，是另一种元算法的名称，通常用于决策树学习器，但也可以用于其他学习器。在单棵树可能过度拟合训练数据的情况下，bagging 有助于减少组合模型的方差。它通过替换采样训练数据来做到这一点，就像本章开始时我们的自举标准误差一样。结果，集合中的每个模型具有不同的不完整的世界观，使得组合的模型不太可能学习关于训练数据的过于具体的假设。随机森林是打包算法的一个例子。

```
(defn ex-4-47 []

  (let [dataset (->> (load-data "titanic.tsv")

                     (to-weka)

                     (mlf/make-apply-filter

                      :replace-missing-values {}))

        classifier (cl/make-classifier :decision-tree

                                       :random-forest)

        evaluation (cl/classifier-evaluate classifier

                                           :cross-validation

                                           dataset 10)]

    (println (:confusion-matrix evaluation))

    (println (:summary evaluation))))
```

使用随机森林分类器，您应该观察到 kappa 值约为 0.55，略低于我们一直在优化的决策树。随机森林实现牺牲了模型的一些变化。

虽然这看起来令人失望，但实际上这也是兰登森林吸引人的部分原因。它们在偏差和方差之间取得平衡的能力使它们成为灵活的通用分类器，适用于各种各样的问题。



# 将分类器保存到文件中

最后，我们可以使用`clj-ml.utils/serialize-to-file`将分类器中的写到一个文件中:

```
(defn ex-4-48 []

  (let [dataset (->> (load-data "titanic.tsv")

                     (to-weka)

                     (mlf/make-apply-filter

                      :replace-missing-values {}))

        classifier (cl/make-classifier :decision-tree

                                       :random-forest)

        file (io/file (io/resource "classifier.bin"))]

    (clu/serialize-to-file classifier file)))
```

稍后，我们可以使用`clj-ml.utils/deserialize-from-file`加载我们训练好的分类器，并立即开始对新数据进行分类。



# 总结

在这一章中，我们学习了如何利用分类变量将数据分组。

我们已经看到了如何使用优势比和相对风险来量化组之间的差异，以及如何使用 *X* ² 测试来对组执行统计显著性测试。我们已经了解了如何使用各种技术构建适合分类任务的机器学习模型:逻辑回归、朴素贝叶斯、决策树和随机森林，以及几种评估它们的方法；混淆矩阵和 kappa 统计量。我们还了解了机器学习中高偏差和过度拟合的对立危险，以及如何通过利用交叉验证来确保您的模型不会过度拟合。最后，我们看到了 clj-ml 库如何帮助准备数据，构建许多不同类型的分类器，并保存它们以备将来使用。

在下一章中，我们将了解如何将我们迄今为止学到的一些技术应用于处理超大型数据集的任务，这些数据集超过了任何单台计算机的存储和处理能力，即所谓的**大数据**。我们将看到我们在本章中遇到的技术之一，梯度下降，是如何特别适合于大规模的参数优化的。