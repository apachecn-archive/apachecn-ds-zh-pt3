© Joshua Cook 2017 Joshua CookDocker for Data Science[https://doi.org/10.1007/978-1-4842-3012-1_1](01.html)

# 1.介绍

Joshua Cook<sup class="calibre5">1 </sup> (1)Santa Monica, California, USA   The typical data scientist consistently has a series of extremely complicated problems on their mind beyond considerations stemming from their system infrastructure. Still, it is inevitable that infrastructure issues will present themselves. To oversimplify, we might draw a distinction between the “modeling problem” and the “engineering problem.” The data scientist is uniquely qualified to solve the former, but can often come up short in solving the latter. Docker has been widely adopted by the system administrator and DevOps community as a modern solution to the challenges presented in high availability and high performance computing.[<sup class="calibre6">1</sup>](#Fn1) Docker is being used for the following: transitioning legacy applications to a more modern “microservice”-based approach, facilitating continuous integration and continuous deployment for the integration of new code, and optimizing infrastructure usage. In this book, I discuss Docker as a tool for the data scientist, in particular in conjunction with the popular interactive programming platform Jupyter. Using Docker and Jupyter, the data scientist can easily take ownership of their system configuration and maintenance, prototype easily deployable and scalable data solutions, and trivially clone entire systems with an eye toward replicability and communication. In short, I propose that skill with Docker is just enough knowledge of systems operations to make the data scientist dangerous. Having done this, I propose that Docker can add high performance and availability tools to the data scientist’s toolbelt and fundamentally change the way that models are developed, prototyped, and scaled.

## “大数据”

A precise definition of “big data ” will elude even the most seasoned data wizard. I favor the idea that big data is the exact scope of data that is no longer manageable without explicit consideration to its scope. This will no doubt vary from individual to individual and from development team to development team. I believe that mastering the concepts and techniques associated with Docker presented herein will drastically increase the size and scope of what exactly big data is for any reader.

## 学习的推荐做法

In this first chapter, you jump will headlong into using Docker and Jupyter on a cloud system. I hope that readers have a solid grasp of the Python numerical computing stack, although I believe that nearly anyone should be able to work their way through this book with enough curiosity and liberal Googling. For the purposes of working through this book, I recommend using a sandbox system. If you are able to install Docker in an isolated, non-mission critical setting, you can work through this text without fear of “breaking things.” For this purpose, I here describe the process of setting up a minimal cloud-based system for running Docker using Amazon Web Services (AWS) . As of the writing of this book, AWS is the dominant cloud-based service provider. I don’t endorse the idea that its dominance is a reason a priori to use its services. Rather, I present an AWS solution here as one that will be the easiest to adopt by the largest group of people. Furthermore, I believe that this method will generalize to other cloud-based offerings such as DigitalOcean[<sup class="calibre6">2</sup>](#Fn2) or Google Cloud Platform,[<sup class="calibre6">3</sup>](#Fn3) provided that the reader has secure shell (ssh) access to these systems and that they are running a Linux variant. I present instructions for configuring a system using Elastic Compute Cloud (EC2) . New users receive 750 hours of free usage on their T2.micro platform and I believe that this should be more than enough for the typical reader’s journey through this text. Over the next few pages, I outline the process of configuring an AWS EC2 system for the purposes of working through this text. This process consists of

1.  1.配置密钥对
2.  2.创建新的安全组
3.  3.创建新的 EC2 实例
4.  4.配置新实例以使用 Docker

### 设置新的 AWS 帐户

To begin, set up an AWS account if you do not already have one.[<sup class="calibre6">4</sup>](#Fn4) Note This work can be done in any region, although it is recommended that readers take note of which region they have selected for work (Figure [1-1](#Fig1)). For reasons I have long forgotten, I choose to work in us-west-2.![A439726_1_En_1_Fig1_HTML.jpg](Images/A439726_1_En_1_Fig1_HTML.jpg) Figure 1-1.Readers should take note of the region in which they are working

### 配置密钥对

In order to interface with your sandbox system running on AWS EC2, you will need an ssh key pair. Amazon EC2 uses public-key cryptography to facilitate all connections to running EC2 instances.[<sup class="calibre6">5</sup>](#Fn5) In your case, this means the creation of a secure connection between your local system and a sandbox system you will configure on an EC2 instance. To do this, you will create an ssh key pair locally and import the public component of the key pair into AWS. When you create a new instance, you have AWS provision the new instance with the public key, so that you can use your local private key to connect to the instance. Note Windows users are encouraged to make use of the excellent Git BASH tool available as part of the Git for Windows package here: [https://git-for-windows.github.io](https://git-for-windows.github.io) . Git BASH will include all of the necessary command line tools that you will be using, including ssh-keygen and ssh. In Listing [1-1](#Par25), you use the ssh-keygen tool to create a key pair on your local system. For these purposes (that is, a disposable sandbox AWS system), you can leave all fields blank, including a passphrase to use the ssh key. The location in which you save the key will vary from system to system. The default location on my system is ∼/.ssh/id_rsa where ∼ signifies the current user’s home directory.[<sup class="calibre6">6</sup>](#Fn6) This process will create id_rsa and id_rsa.pub, a key pair. You will place the id_rsa.pub key into systems you wish to access and thus be able to ssh into these systems using the id_rsa file. $ ssh-keygen -t rsa Generating public/private rsa key pair. Enter file in which to save the key (/home/ubuntu/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/ubuntu/.ssh/id_rsa. Your public key has been saved in /home/ubuntu/.ssh/id_rsa.pub. The key fingerprint is: SHA256:g5IYNQMf1n1jW5p36Y9I/qSPxnckhT665KtiB06xu2U ubuntu@ip-172-31-43-19 The key's randomart image is: +---[RSA 2048]----+ |  ..*. .         | |   + +. . + .    | |  . .    o *   o | |   o . .. + . + .| |  . o . So . + . | |     .  +.  . = .| |       o oE+.o.* | |        =o.o*+o o| |       ..+.o**o. | +----[SHA256]-----+ Listing 1-1.Create a New Key Pair In Listing [1-2](#Par27), you verify that the contents of the key using the cat tool. You display a public key that was created on a remote Ubuntu system, as can be seen at the end of the key (ubuntu@ip-172-31-43-19). This should appear similar on any system. $ cat ∼/.ssh/id_rsa.pub ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDdnHPEiq1a4OsDDY+g9luWQS8pCjBmR64MmsrQ9MaIaE5shIcFB1Kg3pGwJpypiZjoSh9pS55S9LckNsBfn8Ff42ALLjR8y+WlJKVk/0DvDXgGVcCc0t/uTvxVx0bRruYxLW167J89UnxnJuRZDLeY9fDOfIzSR5eglhCWVqiOzB+OsLqR1W04Xz1oStID78UiY5msW+EFg25Hg1wepYMCJG/Zr43ByOYPGseUrbCqFBS1KlQnzfWRfEKHZbtEe6HbWwz1UDL2NrdFXxZAIXYYoCVtl4WXd/WjDwSjbMmtf3BqenVKZcP2DQ9/W+geIGGjvOTfUdsCHennYIEUfEEP ubuntu@ip-172-31-43-19 Listing 1-2.Verify Newly Created ssh-key

#### 在 AWS 上创建新的密钥对

Log in to your AWS control panel and navigate to the EC2 Dashboard, as shown in Figure [1-2](#Fig2). First, access “Services” (Figure [1-2](#Fig2), #1) then access “EC2” (Figure [1-2](#Fig2), #2). The Services link can be accessed from any page in the AWS website.![A439726_1_En_1_Fig2_HTML.jpg](Images/A439726_1_En_1_Fig2_HTML.jpg) Figure 1-2.Access the EC2 control panel Once at the EC2 control panel, access the Key Pairs pane using either link (Figure [1-3](#Fig3)).![A439726_1_En_1_Fig3_HTML.jpg](Images/A439726_1_En_1_Fig3_HTML.jpg) Figure 1-3.Access key pairs in the EC2 Ddashboard From the Key Pairs pane, choose “Import Key Pair.” This will activate a modal that you can use to create a new key pair associated with a region on your AWS account. Make sure to give the key pair a computer-friendly name, like from-MacBook-2017\. Paste the contents of your public key (id_rsa.pub) into the public key contents. Prior to clicking Import, your key should appear as in Figure [1-4](#Fig4). Click Import to create the new key.![A439726_1_En_1_Fig4_HTML.jpg](Images/A439726_1_En_1_Fig4_HTML.jpg) Figure 1-4. Import a new key pair Note Many AWS assets are created uniquely by region. Key pairs created in one region will not be available in another. You have created a key pair between AWS and your local system. When you create a new instance, you will instruct AWS to provision the instance with this key pair and thus you will be able to access the cloud-based system from your local system.![A439726_1_En_1_Fig5_HTML.jpg](Images/A439726_1_En_1_Fig5_HTML.jpg) Figure 1-5.Connect to AWS from your local machine using an SSH key Note The terminology can be a bit confusing. AWS refers to an uploaded public key as a “key pair.” To be clear, you are uploading the public component of a key pair you have created on your system (e.g. id_rsa.pub). The private key will remain on your system (e.g. id_rsa).

#### 创建新的安全组

From the EC2 Dashboard, access the Security Group pane using either link (Figure [1-6](#Fig6)).![A439726_1_En_1_Fig6_HTML.jpg](Images/A439726_1_En_1_Fig6_HTML.jpg) Figure 1-6.Access security groups in the EC2 Dashboard From the Security Group pane, click “Create Security Group.” Give the security group a computer friendly group name like jupyter_docker. Give the security group a description like “Open access to Jupyter and Docker default ports.” Use the default VPC. Access the Inbound tab and configure the following security rules:

*   SSH:端口范围:22，源:任何地方
*   HTTP:端口范围:80，来源:任何地方
*   HTTPS:港口范围:443，来源:任何地方
*   自定义 TCP 规则:端口范围:2376，来源:任何地方
*   自定义 TCP 规则:端口范围:8888，来源:任何地方

When you have added all of these rules, it should appear as in Figure [1-7](#Fig7). Table [1-1](#Tab1) shows a list of ports and the services that will be accessible over these ports.![A439726_1_En_1_Fig7_HTML.jpg](Images/A439726_1_En_1_Fig7_HTML.jpg) Figure 1-7. Inbound rules for new security group Table 1-1.Ports and Usages

<colgroup class="calibre18"><col class="calibre19"> <col class="calibre19"></colgroup> 
| 港口 | 可用服务 |
| --- | --- |
| Twenty-two | 嘘 |
| Eighty | 超文本传送协议 |
| Four hundred and forty-three | 安全超文本传输协议 |
| Two thousand three hundred and seventy-six | 坞站集线器 |
| Eight thousand eight hundred and eighty-eight | 朱皮特 |

#### 创建新的 EC2 实例

To create a new instance, start from the EC2 Dashboard and click the Launch Instance button (Figure [1-8](#Fig8)).![A439726_1_En_1_Fig8_HTML.jpg](Images/A439726_1_En_1_Fig8_HTML.jpg) Figure 1-8.Launch a new instance The launching of a new instance is a multi-step process that walks the user through all configurations necessary. The first tab is “Choose AMI.” An AMI is an Amazon Machine Image[<sup class="calibre6">7</sup>](#Fn7) and contains the software you will need to run your sandbox machine. I recommend choosing the latest stable Ubuntu Server release that is free-tier eligible. At the time of writing, this was ami-efd0428f, Ubuntu Server 16.04 LTS (HVM), SSD Volume Type (Figure [1-9](#Fig9)).![A439726_1_En_1_Fig9_HTML.jpg](Images/A439726_1_En_1_Fig9_HTML.jpg) Figure 1-9.Choose the latest stable Ubuntu Server release as AMI The second tab is “Choose Instance Type.” In usage, I have found that the free tier, t2.micro (Figure [1-10](#Fig10)), is sufficient for many applications, especially the sort of sandbox-type work that might be done in working through this text. This is to say that while working through the text, you may not be doing extended work on datasets, but rather learning about how to configure different systems. As such, your memory needs may be diminished. The ultimate goal is for the reader to be able to create and destroy machines at will. At this level of mastery, the reader can choose the minimum requirements for any application.![A439726_1_En_1_Fig10_HTML.jpg](Images/A439726_1_En_1_Fig10_HTML.jpg) Figure 1-10.Use the t2.micro type The third tab, “Configure Instance,” can be safely ignored. The fourth tab is “Add Storage.” This option is also specific to intended usage. It should be noted that Jupyter Docker images can take up more than 5GB of disk space in the local image cache. For this reason, it is recommended to raise the value from the default 8GB to somewhere in the neighborhood of 20GB. The fifth tab, “Add Tags,” can be safely ignored. The sixth tab, “Configure Security Group,” is critical for the proper functioning of your systems. Previously, you configured a new security group to be used by your system. You will need to assign the security group that you just created, jupyter_docker, to the instance you are configuring. Choose “Select an existing security group,” and then select the security group you just created. Verify that ports 22, 80, 443, 2376, and 8888 are available in the Inbound Rules at the bottom of the tab (Figure [1-11](#Fig11)).![A439726_1_En_1_Fig11_HTML.jpg](Images/A439726_1_En_1_Fig11_HTML.jpg) Figure 1-11.Configure the security group for all traffic Note Most readers will receive a warning from AWS at this final phase that says something to the effect of “Improve your instances' security. Your security group, jupyter_docker, is open to the world.” It is the opinion of this author that this warning can be safely ignored. The warning is letting us know that the instance we are preparing to launch can be accessed on the open web. This is intentional and by design. In this first and last conversation about system security, we will wave our hands at the concern and quickly move to the business of developing short-lifespan scalable systems. Finally, click “Review and Launch.” Here, you see the specific configuration of the EC2 instance you will be creating. Verify that you are creating a t2.micro running the latest free tier-eligible version of Ubuntu Server and that it is available to all traffic, and then click the Launch button (Figure [1-12](#Fig12)).![A439726_1_En_1_Fig12_HTML.jpg](Images/A439726_1_En_1_Fig12_HTML.jpg) Figure 1-12.Launch the new EC2 instance In a final confirmation step, you will see a modal titled “Select an existing key pair or create a new key pair.” Select the key pair you previously created. Check the box acknowledging access to that key pair and launch the instance (Figure [1-13](#Fig13)).![A439726_1_En_1_Fig13_HTML.jpg](Images/A439726_1_En_1_Fig13_HTML.jpg) Figure 1-13.Select the key pair previously imported You should see a notification that the instance is now running. Click the View Instances tab in the lower right corner to be taken to the EC2 Dashboard Instances pane, where you should see your new instance running. Make note of the IP address of the new instance (Figure [1-14](#Fig14)).![A439726_1_En_1_Fig14_HTML.jpg](Images/A439726_1_En_1_Fig14_HTML.jpg) Figure 1-14.Note the IP address of a running instance

#### 为使用 Docker 配置新的 EC2 实例

Having set up an EC2 instance, you ssh into the instance using the IP you just obtained in order to provision the new instance with Docker (Listing [1-3](#Par56)). $ ssh ubuntu@54.244.109.176 Welcome to Ubuntu 16.04.2 LTS (GNU/Linux 4.4.0-64-generic x86_64) ... Listing 1-3.SSH into EC2 Instance Note The first time you access your EC2 instance, you should see the following message: The authenticity of host '54.244.109.176 (54.244.109.176)' can't be established ... Are you sure you want to continue connecting (yes/no)? This is expected. You should hit <ENTER> to accept or type yes and hit <ENTER>. Next (Listing [1-4](#Par59)), you install and configure Docker using a convenient install script provided by the Docker team. The script is obtained from get.docker.com and passed via pipe (|) to a shell (sh). $ curl -sSL https://get.docker.com/ | sh apparmor is enabled in the kernel and apparmor utils were already installed + sudo -E sh -c sleep 3; apt-get update ... + sudo -E sh -c docker version Client:  Version:      17.04.0-ce  API version:  1.28  Go version:   go1.7.5  Git commit:   4845c56  Built:        Mon Apr  3 18:07:42 2017  OS/Arch:      linux/amd64 Server:  Version:      17.04.0-ce  API version:  1.28 (minimum version 1.12)  Go version:   go1.7.5  Git commit:   4845c56  Built:        Mon Apr  3 18:07:42 2017  OS/Arch:      linux/amd64  Experimental: false ... Listing 1-4.Install Docker Via a Shell Script In Listing [1-5](#Par61), you add the ubuntu user to the docker group. By default, the command line docker client will require sudo access in order to issue commands to the docker daemon. You can add the ubuntu user to the docker group in order to allow the ubuntu user to issue commands to docker without sudo. $ sudo usermod -aG docker ubuntu Listing 1-5.Add the Ubuntu User to the Docker Group Finally, in order to force the changes to take effect, you reboot the system (Listing [1-6](#Par63)). As an alternative to rebooting the system, users can simply disconnect and reconnect to their remote system. $ sudo reboot Listing 1-6.Restart the Docker Daemon The reboot will have the effect of closing the secure shell to your EC2 instance. After waiting a few moments, reconnect to the system. At this point, your system will be ready for use. sudo should no longer be required to issue commands to the docker client. You can verify this by connecting to your remote system and checking the dock version (Listing [1-7](#Par65)). $ ssh ubuntu@54.244.109.176 $ docker -v Docker version 17.04.0-ce, build 4845c56 Listing 1-7.Log into the Remote System and Check the Docker Version

## 基础设施对数据的限制

Before commencing with the nuts and bolts of using Docker and Jupyter to build scalable systems for computational programming, let’s conduct a simple series of experiments with this new AWS instance. You’ll begin with a series of simple questions:

> 对于 t2.micro 来说，什么大小的数据集太大而无法加载到内存中？在 t2.micro 上，什么规模的数据集如此之大，以至于它会阻止 Jupyter 拟合不同类型的简单机器学习分类模型 [<sup class="calibre6">8</sup>](#Fn8) (例如，K 最近邻模型)？决策树模型？逻辑回归吗？支持向量分类器？

To answer these questions, you will proceed in the following fashion:

1.  1.在 AWS 实例上使用 Docker 运行 jupyter/scipy-notebook 映像。
2.  2.使用 docker stats 在运行时和加载每个数据集时监控内存使用情况。
3.  3.使用 sklearn . datasets . make _ classification 函数，使用 Jupyter 笔记本创建任意大小的数据集，并执行拟合。
4.  4.在每个模型都适合之后，重启 Python 内核。
5.  5.注意产生内存异常的数据集大小。

### 调出笔记本图片

Since you are working on a freshly provisioned AWS instance, you must begin by pulling the Docker image with which you wish to work, the jupyter/scipy-notebook. This can be done using the docker pull command, as shown in Listing [1-8](#Par77). The image is pulled from Project Jupyter’s public Docker Hub account.[<sup class="calibre6">9</sup>](#Fn9) ubuntu@ip-172-31-6-246:∼$ docker pull jupyter/scipy-notebook Using default tag: latest latest: Pulling from jupyter/scipy-notebook 693502eb7dfb: Pull complete a3782c2efb41: Pull complete 9cb32b776a40: Pull complete e539f5722cd5: Pull complete b4690d4047c6: Pull complete 121dc465f5c6: Pull complete c352772bbcfd: Pull complete eeda14d1c421: Pull complete 0057b9e76c8a: Pull complete e63bd87d75dd: Pull complete 055904fbc069: Pull complete d336770b8a83: Pull complete d61dbef85c7d: Pull complete c1559927bbf2: Pull complete ee5b638d15a3: Pull complete dc937a931aca: Pull complete 4327c0faf37c: Pull complete b37332c24e8c: Pull complete b230bdb41817: Pull complete 765fecb84d9c: Pull complete 97efa424ddfa: Pull complete ccfb7ed42913: Pull complete 2fb2abb673ce: Pull complete Digest: sha256:04ad7bdf5b9b7fe88c3d0f71b91fd5f71fb45277ff7729dbe7ae20160c7a56df Status: Downloaded newer image for jupyter/scipy-notebook:latest Listing 1-8.Pull the jupyter/scipy-notebook image. Once you have pulled the image, it is now present in your docker images cache. Anytime you wish to run a new Jupyter container, Docker will load the container from the image in your cache.

### 运行 jupyter/scipy 笔记本映像

In Listing [1-9](#Par81), you run a Jupyter Notebook server using the minimum viable docker run command. Here, the -p flag serves to link port 8888 on the host machine, your EC2 instance, to the port 8888 on which the Jupyter Notebook server is running in the Docker container. $ docker run -p 8888:8888 jupyter/scipy-notebook [I 22:10:01.236 NotebookApp] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret [W 22:10:01.326 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended. [I 22:10:01.351 NotebookApp] JupyterLab alpha preview extension loaded from /opt/conda/lib/python3.5/site-packages/jupyterlab [I 22:10:01.358 NotebookApp] Serving notebooks from local directory: /home/jovyan/work [I 22:10:01.358 NotebookApp] 0 active kernels [I 22:10:01.358 NotebookApp] The Jupyter Notebook is running at: http://[all ip addresses on your system]:8888/?token=7b02e3aadb29c42ff066a7290d81dd48e44ce62bd7f2bd0a [I 22:10:01.359 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 22:10:01.359 NotebookApp]     Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://localhost:8888/?token=7b02e3aadb29c42ff066a7290d81dd48e44ce62bd7f2bd0a. Listing 1-9.Run Jupyter Notebook Server The output from the running Jupyter Notebook server provides you with an authentication token (token=7b02e3aadb29c42ff066a7290d81dd48e44ce62bd7f2bd0a) you can use to access the Notebook server through a browser. You can do this using the URL provided with the exception that you will need to replace localhost with the IP address of your EC2 instance (Listing [1-10](#Par83)). http://54.244.109.176:8888/?token=1c32913725d84a76e7b3f04c45b91e17b77f3c3574779101. Listing 1-10.The URL of a Jupyter Instance Running on AWS with an Access Token Passed as a Query Parameter

### 监控内存使用情况

In Listing [1-11](#Par85), you have a look at your running container using the docker ps command. You will see a single container running with the jupyter/scipy-notebook image. $ docker ps CNID  IMAGE      COMMAND    CREATED     STATUS     PORTS           NAMES cfef  jupyter/      scipy...   "tini..."  10 min ago  Up 10 min  0.0.0.0:8888->  friendly_                                                   8888/tcp        curie Listing 1-11.Monitor Running Docker Containers Next, you use docker stats to monitor the active memory usage of your running containers (Listing [1-12](#Par87)). docker stats is an active process you will use to watch memory usage throughout. $ docker stats CONTAINER     CPU %  MEM USAGE / LIMIT    MEM %  NET I/O    BLOCK I/O   PIDS cfef9714b1c5  0.00%  49.73MiB / 990.7MiB  5.02%  60.3kB /   10.4MB / 0B 2                                                 1.36MB Listing 1-12.Monitor Docker Memory Usage. You can see several things here germane to the questions above. The server is currently using none of the allotted CPU.[<sup class="calibre6">10</sup>](#Fn10) You can see that the Docker container has nearly 1GB of memory available to it, and of this, it is using 5%, or about 50MB. The 1GB matches your expectation of the amount of memory available to a t2.micro.

### 多大的数据集会导致内存异常？

You are going to be using Jupyter Notebook to run the tests. First, you will create a new notebook using the Python 3 kernel (Figure [1-15](#Fig15)).![A439726_1_En_1_Fig15_HTML.jpg](Images/A439726_1_En_1_Fig15_HTML.jpg) Figure 1-15.Create a new notebook In Listing [1-13](#Par92), you examine your memory usage once more. (After launching a new notebook, the current memory usage increases to about 9% of the 1GB.) $ docker stats CONTAINER     CPU %  MEM USAGE / LIMIT    MEM %  NET I/O   BLOCK I/O    PIDS cfef9714b1c5  0.01%  87.18MiB / 990.7MiB  8.80%  64.2kB /  12.6MB /     13                                                 1.48MB    217kB Listing 1-13.Monitor Docker Memory Usage If you close and halt (Figure [1-16](#Fig16)) your running notebook , you can see memory usage return to the baseline of about 5% of the 1GB (Listing [1-14](#Par94)).![A439726_1_En_1_Fig16_HTML.jpg](Images/A439726_1_En_1_Fig16_HTML.jpg) Figure 1-16.Close and halt a running notebook $ docker stats CONTAINER     CPU %  MEM USAGE / LIMIT    MEM %  NET I/O   BLOCK I/O    PIDS cfef9714b1c5  0.00%  55.31MiB / 990.7MiB  5.58%  109kB /   12.4MB /     4                                                 1.56MB    397kB Listing 1-14.Monitor Docker Memory Usage The Python machine learning library scikit-learn [<sup class="calibre6">11</sup>](#Fn11) has a module dedicated to loading canonical datasets and generating synthetic datasets: sklearn.datasets. Relaunch your notebook and load the make_classification function from this module (Listing [1-15](#Par97), Figure [1-17](#Fig17)), using the standard Python syntax for importing a function from a module. Examine memory usage once more (Listing [1-16](#Par98)).![A439726_1_En_1_Fig17_HTML.jpg](Images/A439726_1_En_1_Fig17_HTML.jpg) Figure 1-17.Import make_classification In [1]: from sklearn.datasets import make_classification Listing 1-15.Import make_classification $ docker stats CONTAINER     CPU %  MEM USAGE / LIMIT    MEM %   NET I/O  BLOCK I/O    PIDS cfef9714b1c5  0.04%  148.3MiB / 990.7MiB  14.97%  242kB /  49.3MB /     13                                                  4.03MB   340kB Listing 1-16.Monitor Docker Memory Usage Next (Listing [1-17](#Par101), Figure [1-18](#Fig18)), you create a new classification dataset using the default values . You then use the %whos IPython magic command[<sup class="calibre6">12</sup>](#Fn12) to display the size of the dataset in memory. After this, you examine memory usage (Listing [1-18](#Par102)).![A439726_1_En_1_Fig18_HTML.jpg](Images/A439726_1_En_1_Fig18_HTML.jpg) Figure 1-18.Import make_classification In [2]: X, y = make_classification() In [3]: %whos Variable              Type        Data/Info ------------------------------------------- X                     ndarray     100x20: 2000 elems, type 'float64', 16000 bytes make_classification   function    <function make_classification at 0x7feb192669d8> y                     ndarray     100: 100 elems, type 'int64', 800 bytes Listing 1-17.Create a New Classification Dataset Using Default Values $ docker stats CONTAINER     CPU %  MEM USAGE / LIMIT    MEM %   NET I/O  BLOCK I/O  PIDS cfef9714b1c5  0.01%  152MiB / 990.7MiB    15.35%  268kB /  54.1MB /   13                                                  4.1MB    926kB Listing 1-18.Monitor Docker Memory Usage So far you are minimally taxing your system. Take note of the size of the dataset, size in Python memory, and Docker system usage associated with this default classification dataset and then restart the Python kernel (Figure [1-19](#Fig19)).![A439726_1_En_1_Fig19_HTML.jpg](Images/A439726_1_En_1_Fig19_HTML.jpg) Figure 1-19.Restart the Python kernel Next, you rerun the same experiment, increasing the size of your feature set by a factor of 10 (Listing [1-19](#Par105), Figure [1-20](#Fig20)). In Listing [1-20](#Par106), you examine Docker system usage.![A439726_1_En_1_Fig20_HTML.jpg](Images/A439726_1_En_1_Fig20_HTML.jpg) Figure 1-20.Import make_classification In [2]: X, y = make_classification(n_samples=1000, n_features=20) In [3]: %whos Variable              Type        Data/Info ------------------------------------------- X                     ndarray     100x20: 2000 elems, type `float64`, 160000 bytes make_classification   function    <function make_classification at 0x7feb192669d8> y                     ndarray     100: 100 elems, type `int64`, 8000 bytes Listing 1-19. Create a New Classification Dataset $ docker stats CONTAINER     CPU %  MEM USAGE / LIMIT    MEM %   NET I/O   BLOCK I/O   PIDS cfef9714b1c5  0.01%  149.7MiB / 990.7MiB  15.11%  286kB /   54.5MB /    13                                                  4.13MB    1.13MB Listing 1-20.Monitor Docker Memory Usage Repeat the experiment several more times, capturing the results in Table [1-2](#Tab2). Each time, restart the kernel, create a new dataset that is 10 times larger than the previous, and then examine the result in terms of memory usage using the IPython magic command %whos and the docker stats tool.Table 1-2.Classification Dataset Memory Footprint on t2.micro

<colgroup class="calibre18"><col class="calibre19"> <col class="calibre19"> <col class="calibre19"></colgroup> 
| 特征集的形状 | Python 内存中的大小 | 坞站系统使用 |
| --- | --- | --- |
| 100 × 20 | . 016MB | 152MB |
| 1000 × 20 | . 16MB | 149.7 兆字节 |
| 1000 × 200 | 1.525 兆字节 | 152.8 兆字节 |
| 10000 × 200 | 15.25 兆字节 | 162.6 兆字节 |
| 10000 × 2000 | 152.6 兆字节 | 279.7 兆字节 |
| 100000 × 2000 | 内存异常 | 不适用的 |

Restart the Python kernel after each dataset is created. Take note of the dataset size that causes a memory exception. When you attempt to create a classification dataset of size 100000 by 2000, you will hit a MemoryError, as seen in Listing [1-21](#Par110) and Figure [1-21](#Fig21).![A439726_1_En_1_Fig21_HTML.jpg](Images/A439726_1_En_1_Fig21_HTML.jpg) Figure 1-21. MemoryError when attempting to create a classification dataset In [2]: X, y = make_classification(n_samples=100000, n_features=2000) --------------------------------------------------------------------------- MemoryError                               Traceback (most recent call last) <ipython-input-2-df42c0ced9d5> in <module>() ----> 1 X, y = make_classification(n_samples=100000, n_features=2000) /opt/conda/lib/python3.5/site-packages/sklearn/datasets/samples_generator.py in make_classification(n_samples, n_features, n_informative, n_redundant, n_repeated, n_classes, n_clusters_per_class, weights, flip_y, class_sep, hypercube, shift, scale, shuffle, random_state)     179     180     # Initialize X and y --> 181     X = np.zeros((n_samples, n_features))     182     y = np.zeros(n_samples, dtype=np.int)     183 MemoryError: Listing 1-21. MemoryError When Attempting to Create a Classification Dataset And with that, you have hit the memory ceiling for your current system. It is not a particularly large dataset: 100,000 rows and 2000 columns. But then again, you are not working with a particularly large system either: a single CPU and 1GB of RAM. Certainly, you can imagine situations in which you will want to work with larger datasets on larger systems.

### 什么样的数据集太大而无法适应不同类型的简单模型？

Next, let’s answer the second question. Let’s do this by starting with a fresh Docker container. First, in Listing [1-22](#Par113), you again use docker ps to display running containers. $ docker ps CNID IMAGE     COMMAND   CREATED      STATUS      PORTS            NAMES cfef jupyter/  "tini..." 53 min ago   Up 53 min   0.0.0.0:8888->   friendly_     scipy...                                     8888/tcp         curie Listing 1-22.Monitor Running Docker Containers In Listing [1-23](#Par115), you stop and then remove this container. $ docker stop friendly_curie friendly_curie ubuntu@ip-172-31-1-64:∼$ docker rm friendly_curie friendly_curie Listing 1-23.Stop and Remove a Running Container Next, in Listing [1-24](#Par117), you launch a brand new jupyter/scipy-notebook container. $ docker run -p 8888:8888 jupyter/scipy-notebook [I 20:05:42.246 NotebookApp] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret ...     Copy/paste this URL into your browser when you connect for the first time,     to login with a token:         http://localhost:8888/?token=7a65c3c7dc6ea294a38397a48cc1ffe110ea138aef6d42c4 Listing 1-24.Run Jupyter Notebook Server Make sure to take note of the new security token (7a65c3c7dc6ea294a38397a48cc1ffe110ea138aef6d42c4) and again use the AWS instance’s IP address in lieu of localhost (Listing [1-25](#Par119)). http://54.244.109.176:8888/?token=7a65c3c7dc6ea294a38397a48cc1ffe110ea138aef6d42c4 Listing 1-25.The URL of the New Jupyter Instance Running on AWS with an Access Token Passed as a Query Parameter Before you start, measure the baseline usage for this current container via docker stats (Listing [1-26](#Par121)). $ docker stats CONTAINER     CPU %  MEM USAGE / LIMIT    MEM %   NET I/O  BLOCK I/O    PIDS 22efba43b763  0.00%  43.29MiB / 990.7MiB  4.37%   768B /   0B / 0B      2                                                  486B   Listing 1-26.Monitor Docker Memory Usage You again create a new Python 3 Notebook and set out to answer this second question. In Listing [1-27](#Par123), you examine the memory usage of your Docker machine with a brand new notebook running. $ docker stats CONTAINER     CPU %  MEM USAGE / LIMIT    MEM %   NET I/O   BLOCK I/O   PIDS 22efba43b763  0.04%  90.69MiB / 990.7MiB  9.15%   58.8kB /  0B / 217kB  13                                                  1.3MB Listing 1-27.Monitor Docker Memory Usage The approach to solving this problem will be slightly different and will make heavier use of docker stats. The %whos IPython magic command cannot be used to display memory usage of a fit model and, in fact, a trivial method for measuring memory usage does not exist.[<sup class="calibre6">13</sup>](#Fn13) You will take advantage of your knowledge of the space in memory occupied by the data created by make_classification and this baseline performance you just measured. You will use the code pattern in Listing [1-28](#Par127) to perform this analysis. from sklearn.datasets import make_classification from sklearn.<model_module> import <model> X, y = make_classification(<shape>) model = <model>() model.fit(X, y) model.score(X, y) Listing 1-28.Create a New Classification Dataset and Perform Naïve Model Fit For example, use the following code to fit the smallest KNeighborsClassifier (Listing [1-29](#Par129)), DecisionTreeClassifier (Listing [1-30](#Par130)), LogisticRegression (1-31), and SVC (Listing [1-32](#Par131)). You will then modify the scope of the data for each subsequent test. from sklearn.datasets import make_classification from sklearn.neighbors import KNeighborsClassifier X, y = make_classification(1000, 20) model = KNeighborsClassifier() model.fit(X, y) model.score(X, y) Listing 1-29.Fit the smallest KNeighborsClassifier from sklearn.datasets import make_classification from sklearn.tree import DecisionTreeClassifier X, y = make_classification(1000, 20) model = DecisionTreeClassifier() model.fit(X, y) model.score(X, y) Listing 1-30.Fit the smallest DecisionTreeClassifier from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression X, y = make_classification(1000, 20) model = LogisticRegression() model.fit(X, y) model.score(X, y) Listing 1-31.Fit the smallest LogisticRegression from sklearn.datasets import make_classification from sklearn.neighbors import SVC X, y = make_classification(1000, 20) model = SVC() model.fit(X, y) model.score(X, y) Listing 1-32.Fit the smallest SVC You then use docker stats to examine the Docker system usage. In between each test, you use docker restart (Listing [1-33](#Par134)) followed by the container id 22efba43b763 to reset the memory usage on the container. After restart the container, you will typically have to confirm restarting the Jupyter kernel as well (Figure [1-22](#Fig22)).![A439726_1_En_1_Fig22_HTML.jpg](Images/A439726_1_En_1_Fig22_HTML.jpg) Figure 1-22.Confirm a restart of the Jupyter kernel $ docker restart 22efba43b763 Listing 1-33.Restart Your Docker Container The results of this experiment are captured in Table [1-3](#Tab3) and Figure [1-23](#Fig23).Table 1-3.Classification of Dataset and Model Memory Footprint on t2.micro

<colgroup class="calibre18"><col class="calibre19"> <col class="calibre19"> <col class="calibre19"> <col class="calibre19"> <col class="calibre19"></colgroup> 
| 特征集的形状 | 模型类型 | 数据集系统使用量(MB) | 数据集和 Fit 峰值系统使用率(MB) | 差异(MB) |
| --- | --- | --- | --- | --- |
| 基线(没有运行笔记本电脑) | 不适用的 | 不适用的 | 不适用的 | Forty point nine eight |
| 基线(笔记本电脑运行) | 不适用的 | 不适用的 | 不适用的 | Seventy-six point one four |
| 100 × 20 | 近邻分类器 | Ninety-nine point nine | One hundred | Zero point one |
| 100 × 20 | 决策树分类器 | One hundred and three point two | One hundred and three point three | Zero point one |
| 100 × 20 | 逻辑回归 | One hundred and two point five | One hundred and two point six | Zero point one |
| 100 × 20 | 交换虚拟电路 | One hundred and one point two | One hundred and one point four | Zero point two |
| 1000 × 20 | 近邻分类器 | One hundred point three | One hundred point four | Zero point one |
| 1000 × 20 | 决策树分类器 | One hundred and three point seven | One hundred and three point eight | Zero point one |
| 1000 × 20 | 逻辑回归 | One hundred and four point nine | One hundred and five point one | Zero point two |
| 1000 × 20 | 交换虚拟电路 | One hundred and four point nine | One hundred and five point five | Zero point six |
| 1000 × 200 | 近邻分类器 | One hundred and six point three | One hundred and six point nine | Zero point six |
| 1000 × 200 | 决策树分类器 | One hundred and four point eight | One hundred and five point seven | Zero point nine |
| 1000 × 200 | 逻辑回归 | One hundred and two | One hundred and two point three | Zero point three |
| 1000 × 200 | 交换虚拟电路 | One hundred and four point six | One hundred and six | One point four |
| 10000 × 200 | 近邻分类器 | One hundred and fifteen point five | One hundred and seventeen point eight | Two point three |
| 10000 × 200 | 决策树分类器 | One hundred and nineteen point eight | One hundred and twenty-seven point eight | Eight |
| 10000 × 200 | 逻辑回归 | One hundred and twenty-one point three | One hundred and twenty-two point six | One point three |
| 10000 × 200 | 交换虚拟电路 | One hundred and twenty-one point one | Two hundred and eighty-six point seven | One hundred and sixty-five point six |
| 10000 × 2000 | 近邻分类器 | Two hundred and fifty-six point four | Two hundred and seventy-five point one | Eighteen point seven |
| 10000 × 2000 | 决策树分类器 | Two hundred and fifty-seven point one | Three hundred and thirty-three point six | Seventy-six point five |
| 10000 × 2000 | 逻辑回归 | Two hundred and fifty-eight point six | Five hundred and sixty-four point nine | Three hundred and six point three |
| 10000 × 2000 | 交换虚拟电路 | Two hundred and fifty-six point three | Four hundred and ninety-one point nine | Two hundred and thirty-five point six |

![A439726_1_En_1_Fig23_HTML.gif](Images/A439726_1_En_1_Fig23_HTML.gif) Figure 1-23.Dataset vs. Peak Usage by model

#### 适用于 T2 的数据测量范围。微处理器

In the previous test, a 10000 row x 2000 column dataset was the largest that you were able to successfully load into memory. In this test, you were able to successfully fit a naïve implementation of four different machine learning models against each of the datasets that you were able to load into memory. That said, you can see that neither the LogisticRegression nor the SVC (Support Vector Classifier) are capable of handling much more.

## 摘要

In this chapter, I introduced the core subjects of this text, Docker and Jupyter, and discussed a recommended practice for working through this text, which is using a disposable Linux instance on Amazon Web Services. I provided detailed instructions for configuring, launching, and provisioning such an instance. Having launched an instance, you used Docker and Jupyter to explore a toy big data example, diagnosing memory performance as you loaded and fit models using a synthetic classification dataset generated by scikit-learn. I did not intended for this chapter to have been the moment when you thoroughly grasped using Docker and Jupyter to build systems for performing data science. Rather, I hope that it has served as a substantive introduction to the topic. Rather than simply stating what Docker and Jupyter are, I wanted you to see what these two technologies are by using them. In the chapters ahead, you will explore many aspects of the Docker and Jupyter ecosystems. Later, you will learn about the open source data stores Redis, MongoDB, and PostgreSQL, and how to integrate them into your Docker-based applications. Finally, you will learn about the Docker Compose tool and how to tie all of these pieces together in a single docker-compose.yml file. Footnotes [1](#Fn1_source) [www.docker.com/use-cases](http://www.docker.com/use-cases)   [2](#Fn2_source) [www.digitalocean.com](http://www.digitalocean.com)   [3](#Fn3_source) [https://cloud.google.com](https://cloud.google.com)   [4](#Fn4_source)Instructions for creating a new AWS account can be found at [https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/](https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/) .   [5](#Fn5_source) [http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html)   [6](#Fn6_source) [www.gnu.org/software/bash/manual/html_node/Tilde-Expansion.html](http://www.gnu.org/software/bash/manual/html_node/Tilde-Expansion.html)   [7](#Fn7_source) [http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html)   [8](#Fn8_source) [http://scikit-learn.org/stable/tutorial/machine_learning_map/](http://scikit-learn.org/stable/tutorial/machine_learning_map/)   [9](#Fn9_source) [http://hub.docker.com/u/jupyter/](http://hub.docker.com/u/jupyter/)   [10](#Fn10_source)My t2.micro has but a single CPU.   [11](#Fn11_source) [http://scikit-learn.org/](http://scikit-learn.org/)   [12](#Fn12_source) [https://ipython.org/ipython-doc/3/interactive/magics.html#magic-whos](https://ipython.org/ipython-doc/3/interactive/magics.html#magic-whos)   [13](#Fn13_source) [https://stackoverflow.com/questions/449560/how-do-i-determine-the-size-of-an-object-in-python](https://stackoverflow.com/questions/449560/how-do-i-determine-the-size-of-an-object-in-python)