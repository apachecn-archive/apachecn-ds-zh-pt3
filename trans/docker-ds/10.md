© Joshua Cook 2017 Joshua CookDocker for Data Science[https://doi.org/10.1007/978-1-4842-3012-1_10](10.html)

# 10.交互式软件开发

Joshua Cook<sup class="calibre5">1 </sup> (1)Santa Monica, California, USA   Developing software as a data scientist is different from traditional software engineering and far less understood. For the traditional software developer, For any language, framworks built around reuse, extensibility, and stability exist. The most famous of these might be the Rails framework for the Ruby language. Rails is written from the ground up around its adopted paradigm, the Model-View-Controller design pattern, a pattern heavily favored in the implementation of user-facing software. Listing [10-1](#Par2) shows the creation of and the default file structure for a new Rails application. Note that the new application has clear directories created for it based upon the usage pattern. $ rails new myapp       create       ... $ tree -L 1 myapp/app myapp/app/ ├── assets ├── channels ├── controllers ├── helpers ├── jobs ├── mailers ├── models └── views Listing 10-1.A Default Rails Application Data science-specific software development has no such design pattern around which a similar framework might be built. In Chapter [3](03.html), I introduced the idea of interactive computing as an alternative to conventional programming. In this chapter, I propose that the idea of interactive computing itself be adopted as the cornerstone idea for a potential framework. You’ll develop a project framework with infrastructure defined by a docker-compose.yml, built around Jupyter as your interactive computing driver. The goals of this framework are aligned with those of an interactive computing project. This framework should facilitate ease in

*   循环
*   硬件的扩展和分布
*   共享和记录工作

## 组织计算生物学项目的快速指南

For inspiration for this framework , let’s look at the work of William Noble of the University of Washington.[<sup class="calibre6">1</sup>](#Fn1) Noble’s work describes “one good strategy for carrying out computational experiments,” focusing on “relatively mundane issues such as organizing files directories and documenting progress.” Noble focuses on a few key principles to structuring a project:

*   文件和目录组织
*   记录工作
*   执行工作
*   版本控制

Figure [10-1](#Fig1) shows Noble’s diagram for file and directory organization for a sample project called msms.![A439726_1_En_10_Fig1_HTML.jpg](img/A439726_1_En_10_Fig1_HTML.jpg) Figure 10-1.Noble’s sample project, msms

## 交互式开发的项目框架

You’ll draw directly upon this work to develop your framework. You’ll use Jupyter Notebooks, numbered in sequence, as a method for both documenting and executing your work. These notebooks become a detailed record of activity as well as the means by which you drive this activity. Furthermore, you’ll present a directory hierarchy designed around the use of the Jupyter Notebook as the driver of your work. Figure [10-2](#Fig2) shows a directory hierarchy built for interactive development.![A439726_1_En_10_Fig2_HTML.jpg](img/A439726_1_En_10_Fig2_HTML.jpg) Figure 10-2.Directory hierachy built for interactive development You’ll build the directory hierarchy of your project using the following directories:

*   数据
    *   包含原始数据文件
*   码头工人
    *   包含使用构建定义的每个图像的子目录
    *   每个子目录都将成为各自映像的构建上下文
*   ipynb
    *   包含所有 Jupyter 笔记本文件
    *   替换 bin、doc 和结果目录
    *   笔记本是驱动程序、脚本、文档和演示文稿
    *   笔记本以日期和活动命名，以便分类
*   解放运动
    *   包含在项目开发过程中定义的特定于项目的代码模块

## 项目根设计模式

In Chapter [3](03.html), I proposed that

> Jupyter 不能取代 vim，Sublime Text 或 PyCharm。Jupyter 替换 if __name__ == "__main__ ":。

The if __name__ == "__main__": design pattern provides a launch hook for running a Python program . The project framework I propose here is not built around running code in such a way, and as such does not require such a launch hook. Rather, you are building this framework around the Jupyter Notebook as a driver. What you require is a pattern for importing modules into your notebooks. Maintaining a clean project directory structure requires you to keep your notebooks and your Python modules in separate directories. Furthermore, I hold that it is less aesthetic to nest one inside of the other. This causes a problem at import time. Given a directory structure as shown in Listing [10-2](#Par33), you can’t import a module (e.g. some_module.py shown in Listing [10-3](#Par34)) from lib/ directly into a Jupyter Notebook module in ipynb/. $ tree . ├── ipynb  │   └── some_notebook.ipynb └── lib     ├── __init__.py     ├── some_module.py Listing 10-2.Sample Project Structure #!/bin/python def say_hello ():     print("Hello!") Listing 10-3.A Demo Python Module, some_module.py Let’s solve this problem by using what I will refer to as the project root design pattern (Listing [10-4](#Par36)). In [1]: from os import chdir         chdir('/home/jovyan') Listing 10-4.The Project Root Design Pattern The project root design pattern changes the current working directory of the Python kernel to be the root of the project. This is guaranteed by the configuration of the mounted volume in your docker-compose.yml file (Listing [10-8](#Par45)), where you mount the current directory (.) to the working directory of the jupyter image, home/jovyan. Thus, in running chdir('/home/jovyan') in a Jupyter Notebook (running on a jupyter image), you can guarantee that you will be at the project root. Furthermore, since you are using an absolute path in your chdir statement, you can run this command idempotently. Running this as the first command in any Jupyter Notebook means that you can import from your lib directory at will (Listing [10-5](#Par38)). In [2]: from lib.some_module import say_hello         say_hello()         Hello! Listing 10-5.Import from lib.some_module.

## 初始化项目

In Chapter [9](09.html), you used a docker-compose.yml file to design an application consisting of a Jupyter Notebook Server and a PostgreSQL database . You used the docker-compose build tool and the design of the postgres image to gather your data and seed your database. Here, you do the same again, collecting your data from the UCI Machine Learning Repository. In this chapter, however, you formalize the process of gathering the data, documenting the process using a Jupyter Notebook. In Listing [10-6](#Par42), you initialize the project. You create a global directory for your project, ch10_adult. You create three subdirectories within this project, docker/, ipynb/, and lib/. You create a new __init__.py [<sup class="calibre6">2</sup>](#Fn2) file using the touch command. This has the effect of making the lib/ directory into a Python module. Finally, you initialize the project repository as a git repository using git init. $ mkdir ch10_adult $ cd ch10_adult/ $ mkdir docker ipynb lib $ touch lib/__init__.py $ git init Initialized empty Git repository in /home/ubuntu/ch10_adult/.git/ Listing 10-6.Initialize the ch10_adult Project In Listing [10-7](#Par44), you create the docker-compose.yml file (Listing [10-8](#Par45)), which will define the infrastructure of your project. Note that you start simple . At this phase, you only have a single service, a Jupyter Notebook Server . $ vi docker-compose.yml Listing 10-7.Create the docker-compose.yml File version: '3' services:   this_jupyter:     image: jupyter/scipy-notebook     ports:       - "8888:8888"     volumes:       - .:/home/jovyan Listing 10-8.docker-compose.yml Having defined your infrastructure , you bring the application online (Listing [10-9](#Par47)). Since you have used the image: keyword rather than the build: keyword to define the image used for your service, it is not necessary to perform a build prior to the launching of your application. After launch, you use the docker-compose ps tool to examine the running containers associated with your application (Listing [10-10](#Par48)). In order to access Jupyter through your browser, you will need to obtain a current authentication token (Listing [10-11](#Par49)). $ docker-compose up -d Starting ch10adult_this_jupyter_1 Listing 10-9.Launch Initial System $ docker-compose ps Name                   Command                    State                Ports ---------------------------------------------------------------------------- ch10adult_this_jupyter_1  tini -- start-notebook.sh  Up   0.0.0.0:8888->8888/tcp Listing 10-10.Examine Running Containers $ docker exec ch10adult_this_jupyter_1 jupyter notebook list Currently running servers: http://localhost:8888/?token=d6dc404c5e3c25ffd993579aeb06eeb2a801c4cbc75f727e :: /home/jovyan Listing 10-11.Obtain Authentication Token

## 检查数据库要求

At this point, your application consists of a single service, a Jupyter Notebook server . The next phase in project development is to bring a database online. Here, you launch a notebook and use pandas [<sup class="calibre6">3</sup>](#Fn3) to examine a sample of the data in order to develop a schema to handle your data. From there you will prepare your postgres build context in order to seed your database. You begin by navigating to the Jupyter Notebook server in your browser. Note that the home directory of the Notebook server is comprised of the files of the root directory of your project. In Figure [10-3](#Fig3), you navigate to the ipynb directory where you will create a new Python 3 Notebook (Figure [10-4](#Fig4)).![A439726_1_En_10_Fig3_HTML.jpg](img/A439726_1_En_10_Fig3_HTML.jpg) Figure 10-3.Navigate to the ipynb directory ![A439726_1_En_10_Fig4_HTML.jpg](img/A439726_1_En_10_Fig4_HTML.jpg) Figure 10-4.Create a new Python 3 notebook Noble cites the need for “a chronologically organized lab notebook.” I propose that in this project framework this need is met by the Jupyter Notebook. To chronologically organize your work, you simply name your notebook files with a year-month-date format followed by a high-level description of the task to be performed. So name this first notebook 20170611-Examine_Database_Requirements.ipynb. In Listing [10-12](img/#Par55), you begin the notebook with the project root design pattern, after which you import pandas (Listing [10-13](#Par56)). In [1]: from os import chdir         chdir('/home/jovyan') Listing 10-12.The Project Root Design Pattern In [2]: import random         import pandas as pd Listing 10-13.Import Necessary Libraries In Figure [10-5](#Fig5), you use Jupyter’s capacity for annotation via markdown to include information on the dataset. The dataset is the adult dataset obtained from the UCI Machine Learning Repository.[<sup class="calibre6">4</sup>](#Fn4) You obtain the markdown you include from the dataset description included with the dataset.[<sup class="calibre6">5</sup>](#Fn5) ![A439726_1_En_10_Fig5_HTML.jpg](img/A439726_1_En_10_Fig5_HTML.jpg) Figure 10-5.Use markdown to include information on the dataset from UCI’s Machine Learning Repository Listing [10-14](img/#Par61) demonstrates the effect of the project root design pattern using a Jupyter shell call to ls. In Listing [10-15](#Par62), you make another Jupyter shell call to mkdir to create a top-level directory data, after which you make a third Jupyter shell call to use the wget tool to obtain the dataset (Listing [10-16](#Par63)). In [3]: ls -l         total 16         drwxrwxr-x 2 jovyan 1000 4096 Jun 11 23:53 docker         -rw-rw-r-- 1 jovyan 1000  145 Jun 11 23:55 docker-compose.yml         drwxrwxr-x 3 jovyan 1000 4096 Jun 12 03:12 ipynb         drwxrwxr-x 2 jovyan 1000 4096 Jun 11 23:53 lib Listing 10-14.List Current Directory In [4]: !mkdir data Listing 10-15.Create Data Directory In [5]: !wget -P data/ \         http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data         --2017-06-12 03:28:31--  http://archive.ics.uci.edu/ml/machine-learning-         databases/adult/adult.data         Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.249         Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.249|:80... connected.         HTTP request sent, awaiting response... 200 OK         Length: 3974305 (3.8M) [text/plain]         Saving to: 'data/adult.data'         data/adult.data    100%[===============>]  3.79M  7.93MB/s  in 0.5s         2017-06-12 03:28:31 (7.93 MB/s) - 'data/adult.data' saved [3974305/3974305] Listing 10-16.Get the Dataset In Listing [10-17](#Par65), you use the wc tool to count the number of lines in the file. In Listing [10-18](#Par66), you use the head tool to see if the file has a header row. Note that the two rows shown are both data rows and therefore the file does not contain a header row. In [6]: !wc -l data/adult.data         32562 data/adult.data Listing 10-17.Count the Number of Lines in the File In [7]: !head -n 2 data/adult.data         39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White,         Male, 2174, 0, 40, United-States, <=50K         50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial,         Husband, White, Male, 0, 0, 13, United-States, <=50K Listing 10-18.Check to See If the File Has a Header In Listing [10-19](#Par69), you load a 10% sample of the dataset using the pandas.read_csv function. To do this, you first define two variables, number_of_rows and sample_size. The first is the number of rows present in your dataset, the second the number of rows you would like to include in your sample. You next create a list, rows_to_skip, by using the random.sample function. random.sample takes two arguments: a list from which you wish to sample and a sample size. Here, you create a range of number from 0 to number_of_rows and request a sample set of size number_of_rows - sample_size. The returned sample list is sorted and becomes your rows to skip. This list is passed to pandas.read_csv at runtime. The effect is that pandas.read_csv will load a set of size sample_ size . The output of pandas.read_csv is a pandas.DataFrame object. You save this pandas.DataFrame as adult_df. You assign a list of column names (obtained from the dataset description at the UCI Machine Learning Repository) to the object attribute adult_df.columns. In [8]: number_of_rows = 32562         sample_size = 3300         rows_to_skip = random.sample(range(number_of_rows), number_of_rows - sample_size)         rows_to_skip.sort()         adult_df = pd.read_csv('data/adult.data', header=None, skiprows=rows_to_skip)         adult_df.columns = [             'age',             'workclass',             'fnlwgt',             'education',             'education_num',             'marital_status',             'occupation',             'relationship',             'race',             'gender',             'capital_gain',             'capital_loss',             'hours_per_week',             'native_country',             'income_label'         ] Listing 10-19.Load the File Using pandas In Figure [10-6](#Fig6), you display a sample of the loaded DataFrame using adult_df.sample(3).![A439726_1_En_10_Fig6_HTML.jpg](img/A439726_1_En_10_Fig6_HTML.jpg) Figure 10-6.A sample of the loaded DataFrame At this point, you begin to construct a schema for loading your dataset into a PostgreSQL database. In Figure [10-7](img/#Fig7), you use Jupyter markdown to annotate the datatypes of the different feature columns as providing by UCI, and then examine the adult_df.dtypes to display the data types of each column in the pandas.DataFrame. If the data is properly formatted in the CSV file, then pandas will assign the proper data types to each column. If the pandas.DataFrame data types match the data types in the meta-information providing by UCI, then you should be able to count on the integrity of the data at load time, and be able to use the suggested data types to define your schema. Note that the pandas.DataFrame data types do match the data types in the meta-information and therefore you can use the meta-information as the basis for your schema without the need for special data handling.![A439726_1_En_10_Fig7_HTML.jpg](img/A439726_1_En_10_Fig7_HTML.jpg) Figure 10-7.Comparing meta-information from UCI’s Machine Learning Repository to the DataFrame data types Having completed your preliminary work, you save the file and stop the notebook by selecting “Close and Halt” from the File Menu (Figure [10-8](img/#Fig8)). You will use what you discovered here to build your database.![A439726_1_En_10_Fig8_HTML.jpg](img/A439726_1_En_10_Fig8_HTML.jpg) Figure 10-8.Close and halt the notebook

### 通过 Git 管理项目

Before moving on to the next phase of the project, you commit all of your recent changes using git. In Listing [10-20](#Par74), you check the status of your project (that is, what changes are present in your code) using the git status tool. Because you have not made any commits since you initialized the project, there will be changes associated with your initial docker-compose.yml file definition, in addition to the work you just did preparing to write your schema. $ git status On branch master Initial commit Untracked files:   (use "git add <file>..." to include in what will be committed)         data/         docker-compose.yml         ipynb/ Listing 10-20.Check Project Status You will track each segment of work that you have done separately. First, in Listing [10-21](#Par76), you add and commit the creation of your initial docker-compose.yml file. Then, in Listing [10-22](#Par77), you do the same for your schema preparation work. Because this work is everything left unstaged you can use the -A flag to signify that you wish to add “all”. $ git add docker-compose.yml $ git commit -m 'initial docker-compose.yml file' [master (root-commit) 3029c78] initial docker-compose.yml file  1 file changed, 8 insertions(+)  create mode 100644 docker-compose.yml Listing 10-21.Add and Commit Initial docker-compose.yml File $ git add -A $ git commit -m 'schema preparation work' [master 13de8e1] schema preparation work  3 files changed, 33336 insertions(+)  create mode 100644 data/adult.data  create mode 100644 ipynb/.ipynb_checkpoints/20170611-Examine_Database_Requirements-checkpoint.ipynb  create mode 100644 ipynb/20170611-Examine_Database_Requirements.ipynb create mode 100644 docker-compose.yml Listing 10-22.Add and Commit Schema Preparation Work Note that this most recent commit also added an .ipynb_checkpoints directory. This is undesirable. In Listing [10-23](#Par79), you create a .gitignore (Listing [10-24](#Par80)) file and add a few files you do not wish to track via git. In Listing [10-25](#Par81), you reset the git HEAD to remove the most recent commit. Finally, in Listing [10-26](#Par82), you perform the whole process once more. $ vi .gitignore Listing 10-23.Create the .gitignore file. **/.ipynb_checkpoints **/*.pyc Listing 10-24. .gitignore $ git reset HEAD∼1 $ git status On branch master Untracked files:   (use "git add <file>..." to include in what will be committed)         .gitignore         data/         ipynb/ nothing added to commit but untracked files present (use "git add" to track) Listing 10-25.Reset git HEAD to the Penultimate Commit and Display Status $ git add -A $ git commit -m 'schema preparation work' [master ad896e2] schema preparation work  3 files changed, 32951 insertions(+)  create mode 100644 .gitignore  create mode 100644 data/adult.data  create mode 100644 ipynb/20170611-Examine_Database_Requirements.ipynb Listing 10-26.Add and Commit Schema Preparation Work Warning Be cautious when adding data files to a git commit. If syncing a local git repository with a cloud-based version-control system such as GitHub, the files must be less than 100MB to be uploaded to GitHub. Removing a file from a git commit, particularly when it was not the most recent commit, can be challenging and its presence will cause the sync to fail even if it has been removed from the most recent commit. The bash tool split [<sup class="calibre6">6</sup>](#Fn6) is recommended for breaking CSV files into smaller files that are less than 100MB if a cloud backup is desired.

## 向应用程序添加数据库

You initially launched your application with a basic Jupyter service for some preliminary analysis of your dataset. Having done this, let’s use what you learned to seed a PostgreSQL database with this dataset using an appropriate schema. Figure [10-9](#Fig9) shows a diagram of the next iteration of your system.![A439726_1_En_10_Fig9_HTML.jpg](img/A439726_1_En_10_Fig9_HTML.jpg) Figure 10-9.Second iteration of your application To do this, you will add a few things to your docker-compose.yml file (Listing [10-27](img/#Par90)):

*   使用构建的映像而不是现有的映像来定义 Jupyter 笔记本服务，以便包含您的数据库接口库 psycopg2。
*   利用你在第 [9](09.html) 章看到的构建钩子定义一个 PostgreSQL 服务。
*   创建供 PostgreSQL 服务使用的数据卷。

version: '3' services:   this_jupyter:     build: docker/jupyter     ports:       - "8888:8888"     volumes:       - .:/home/jovyan   this_postgres:     build: docker/postgres     volumes:       - postgres_data:/var/lib/postgresql/data volumes:   postgres_data: Listing 10-27.Next Version of Your docker-compose.yml Here, you make use of several patterns you have seen before, but in particular you must take care to mount the new data volume to the correct location within the this_postgres container. Next, you create directories for your two new build contexts (Listing [10-28](#Par93)) and create (Listing [10-29](#Par93)) a Dockerfile (Listing [10-30](#Par94)) to define your this_jupyter service. At this time, you make only a single change: adding psycopg2 to your Python 3 environment. $ mkdir docker/jupyter docker/postgres Listing 10-28.Create New Build Contexts $ vim docker/jupyter/Dockerfile Listing 10-29.Create docker/jupyter/Dockerfile FROM jupyter/scipy-notebook USER root RUN conda install --yes --name root psycopg2 USER jovyan Listing 10-30. docker/jupyter/Dockerfile Then, you define the build context for your this_postgres service. As in Chapter [9](09.html), the build context for your PostgreSQL is much more involved and includes several files: a Dockerfile (Listing [10-31](#Par98) and [32](#Par99)); a bash script, get_data.sh (Listing [10-34](#Par101)), to obtain and clean your data; and a sql file, initdb.sql (Listing [10-36](#Par103)), to initialize your database. Note The CSV file for your data obtained from the UCI Machine Learning Repository includes a blank line at the end of the file. PostgreSQL is intolerant of any aberrant behavior and rejects the data copy in your docker/postgres/initdb.sql file (Listing [10-35](#Par102)) without special handling of this blank line. As in Chapter [9](09.html), I used the sed tool to handle this issue in the get_data.sh file (Listing [10-33](#Par100)). Here, I use the pattern /^\s*$/d. This has the effect of matching any lines comprised only of white space, including blank lines (^\s*$), and deleting them using the d command. $ vim docker/postgres/Dockerfile Listing 10-31.Create docker/postgres/Dockerfile FROM postgres:alpine COPY get_data.sh /docker-entrypoint-initdb.d/get_data.sh COPY initdb.sql /docker-entrypoint-initdb.d/initdb.sql Listing 10-32. docker/postgres/Dockerfile $ vim docker/postgres/get_data.sh Listing 10-33.Create docker/postgres/get_data.sh #!/bin/bash wget -P /tmp/ http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data sed '/^\s*$/d' /tmp/adult.data > /tmp/adult-clean.csv Listing 10-34. docker/postgres/get_data.sh $ vim docker/postgres/initdb.sql Listing 10-35.Create docker/postgres/Dockerfile CREATE TABLE adult (     age INTEGER,     workclass TEXT,     fnlwgt INTEGER,     education TEXT,     education_num INTEGER,     marital_status TEXT,     occupation TEXT,     relationship TEXT,     race TEXT,     gender TEXT,     capital_gain INTEGER,     capital_loss INTEGER,     hours_per_week INTEGER,     native_country TEXT,     income_label TEXT ); COPY adult FROM '/tmp/adult.data' DELIMITER ',' CSV; Listing 10-36. docker/postgres/initdb.sql In Listing [10-37](#Par105), you display your project using tree. $ tree . ├── data  │   └── adult.data ├── docker  │   ├── jupyter  │   │   └── Dockerfile  │   └── postgres  │       ├── Dockerfile  │       ├── get_data.sh  │       └── initdb.sql ├── docker-compose.yml ├── ipynb  │   └── 20170611-Examine_Database_Requirements.ipynb └── lib Listing 10-37.Display project In Listing [10-38](#Par107), you launch your updated application. Note that you have added the --build flag to your docker-compose up command to ensure that the new build contexts are built before use. You then examine your running containers using docker-compose ps (Listing [10-39](#Par112)). $ docker-compose up -d --build Creating network "ch10adult_default" with the default driver Creating volume "ch10adult_postgres_data" with default driver Building this_jupyter ... Building this_postgres ... Successfully built de96ba591ad9 Successfully tagged ch10adult_this_postgres:latest Creating ch10adult_this_jupyter_1 Creating ch10adult_this_postgres_1 Listing 10-38.Launch the Application Configuring the seeding of a PostgreSQL database can be finicky. The difficulty of this task can be compounded when it is being done through a layer of abstraction, as you are doing here. Your initdb.sql file is being executed by the this_postgres container at runtime. This can make diagnosis and troubleshooting of any issues a challenege. I offer the following methods for verifying the success of database initialization:

*   通过 docker-compose ps 确认数据库正在运行。initdb.sql 文件中的语法错误将导致数据库在运行时失败并退出。如果发生这种情况，docker-compose ps 将显示 ch10adult_this_postgres_1 容器的退出状态(清单 [10-39](#Par112) )。
*   检查 this_postgres 服务的日志(清单 [10-40](#Par113) )。如果初始化成功，日志应该包含初始化脚本和 sql 文件的执行记录。注意，在清单 [10-40](#Par113) 中，您可以看到 32561 条记录已经被成功复制到数据库中。
*   通过对 psql 工具的 docker exec 调用连接到正在运行的容器(清单 [10-41](#Par114) )。

$ docker-compose ps           Name          Command                    State              Ports ---------------------------------------------------------------------------- ch10adult_this_jupyter_1  tini--start-notebook.sh      Up    0.0.0.0:8888->8888/tcp ch10adult_this_postgres_1  docker-entrypoint.sh postgres  Up                5432/tcp Listing 10-39.Examine Running Containers ... this_postgres_1  | /usr/local/bin/docker-entrypoint.sh: running /docker-entrypoint-initdb.d/get_data.sh this_postgres_1  | Connecting to archive.ics.uci.edu (128.195.10.249:80) this_postgres_1  | adult.data             9% |**                             |   363k  0:00:09 ETA this_postgres_1  | adult.data           100% |*******************************|  3881k  0:00:00 ETA this_postgres_1  | this_postgres_1  | this_postgres_1  | /usr/local/bin/docker-entrypoint.sh: running /docker-entrypoint-initdb.d/initdb.sql this_postgres_1  | CREATE TABLE this_postgres_1  | COPY 32561 ... Listing 10-40.Examine this_postgres Logs $ docker exec -it ch10adult_this_postgres_1 psql postgres postgres psql (9.6.3) Type "help" for help. postgres=# SELECT COUNT(*) FROM adult;  count -------  32561 (1 row) Listing 10-41.Connect to this_postgres via psql After verifying that the this_postgres service is properly configured, you commit these infrastructure changes to your git log (Listing [10-42](#Par116)). In Listing [10-43](#Par117), you add all files and commit the changes. $ git status On branch master Changes not staged for commit:   (use "git add <file>..." to update what will be committed)   (use "git checkout -- <file>..." to discard changes in working directory)         modified:   docker-compose.yml Untracked files:   (use "git add <file>..." to include in what will be committed)         docker/ Listing 10-42.Check Project Status $ git add -A $ git commit -m 'add postgres service with database seed' [master 5a35f01] add postgres service with database seed  5 files changed, 37 insertions(+), 1 deletion(-)  create mode 100644 docker/jupyter/Dockerfile  create mode 100644 docker/postgres/Dockerfile  create mode 100644 docker/postgres/get_data.sh  create mode 100644 docker/postgres/initdb.sql Listing 10-43.Add and Commit Changes Since you have stopped and relaunched your Jupyter Notebook server, you will need to obtain a new authentication token in order to access the server in the browser once more (Listing [10-44](#Par119)). $ docker exec ch10adult_this_jupyter_1 jupyter notebook list Currently running servers: http://localhost:8888/?token=6ab886ef19e02fe8ac351d0c28d03a50ab13be69a69b46d7 :: /home/jovyan Listing 10-44.Obtain the Authentication Token

## 互动发展

A major goal of this project to framework is to facilitate a new style of software development called interactive development. The interactive development of modules is as follows.

1.  1.使用 Jupyter 在笔记本中交互式编写代码。
2.  2.当一段代码变得太大或者需要重复时，在 Jupyter 中将这段代码抽象成一个函数。
3.  3.在 Jupyter 中测试这个新函数的性能。
4.  4.将此函数移到代码库中的一个模块中。
5.  5.根据需要导入代码以供使用。

Let’s demonstrate the process here with a simple method you will use often, a basic connection from Jupyter to your database. In this case, you will abstract the function into your library of code to adhere to the best practice of not repeating code. You begin by navigating to ipynb/ and creating a new file. You rename the file with today’s date and what you will be doing (e.g. 20170613-Initial_Database_Connection.ipynb). In Listing [10-45](#Par127), you begin the notebook with the project root design pattern, after which you import psycopg2(Listing [10-46](#Par128)). In [1]: from os import chdir         chdir('/home/jovyan') Listing 10-45.The Project Root Design Pattern In [2]: import psycopg2 as pg2 Listing 10-46.Import Necessary Libraries In Listing [10-47](#Par130), you connect to your database as you have done previously, instantiating a connection and a cursor from that connection. You make use of the network created for you by Docker Compose and refer to the PostgreSQL by its name on the network, this_postges (that is, the same name you have given to the PostgreSQL service). In Listing [10-48](#Par131), you use the cursor to execute a query to the database, print the results of the query, and then close the connection. In [3]: con = pg2.connect(host='this_postgres', user='postgres', database='postgres')         cur = con.cursor() Listing 10-47.Connect to postgres and Create a Cursor In [4]: cur.execute("SELECT COUNT(*) FROM adult;")         print(cur.fetchall())         con.close()         [(32561,)] Listing 10-48.Query the Database and Close the Connection The code in Listing [10-47](#Par130) is code that you will be using often. Although it is just two lines of code, it is worth abstracting into a function because you will be using it with frequency. In Figure [10-10](#Fig10), you write this function in a Jupyter cell, and then use a tab completion to display the function’s docstring .![A439726_1_En_10_Fig10_HTML.jpg](img/A439726_1_En_10_Fig10_HTML.jpg) Figure 10-10.Define a function and display its docstring You next verify that your connect_to_postgres function works as you expect in Listing [10-49](img/#Par134). In [6]: con, cur = connect_to_postgres()         cur.execute("SELECT COUNT(*) FROM adult;")         print(cur.fetchall())         con.close()         [(32561,)] Listing 10-49.Test connect_to_postgres Having verified that your function for accessing this _postgres works , you can add the function to an external Python module for import. Since you are done with this notebook, you should save and then close and halt the notebook.

### 使用 jupiter 创建一个 python 模块

You will use the Jupyter Server’s capacity for creating and editing text files to build a lib.postgres module. In Figure [10-11](#Fig11), you navigate to lib/ using the Notebook server, and then within lib/ you create a new text file. In Figure [10-12](#Fig12), you rename this file postgres.py. Next, you populate the new text file with the code in Listing [10-50](#Par137).![A439726_1_En_10_Fig11_HTML.jpg](img/A439726_1_En_10_Fig11_HTML.jpg) Figure 10-11.Create a new text file ![A439726_1_En_10_Fig12_HTML.jpg](img/A439726_1_En_10_Fig12_HTML.jpg) Figure 10-12.Rename the next file to postgres.py """Helper module for interfacing with PostgreSQL.""" import psycopg2 as pg2 def connect_to_postgres():     """Preconfigured to connect to PostgreSQL. Returns connection and cursor.     con, cur = connect_to_postgres()     """     con = pg2.connect(host='this_postgres', user='postgres', database='postgres')     return con, con.cursor() Listing 10-50. lib.postgres Module Next, you create a new notebook to test the function you have written. It may be a bit pedantic to create a new notebook for each task. You do so here to highlight the desired workflow of the interactive development method . You create a new notebook titled 20170613-Verify_Database_Connection.ipynb. In Listing [10-51](img/#Par139), you begin the notebook with the project root design pattern. In Listing [10-52](#Par140), you import lib.postgres and verify that connect_to_postgres functions as you expect. In [1]: from os import chdir         chdir('/home/jovyan') Listing 10-51.The Project Root Design Pattern In [2]: import lib.postgres as psql         con, cur = psql.connect_to_postgres()         cur.execute("SELECT COUNT(*) FROM bc_data;")         print(cur.fetchall())         con.close()         [(32561,)] Listing 10-52.Test psql.connect_to_postgres Finally, you track your work using git . In Listing [10-53](#Par142), you check the status of your project . In Listing [10-54](#Par143), you add and commit all of your recent work. $ git status On branch master Untracked files:   (use "git add <file>..." to include in what will be committed)         ipynb/20170613-Initial_Database_Connection.ipynb         ipynb/20170613-Verify_Database_Connection.ipynb         lib/ Listing 10-53.Check Status of Project $ git add -A ubuntu@LOCAL:∼/ch10_adult (master) $ git commit -m 'initial database connection' [master d2461f6] initial database connection  4 files changed, 185 insertions(+)  create mode 100644 ipynb/20170613-Initial_Database_Connection.ipynb  create mode 100644 ipynb/20170613-Verify_Database_Connection.ipynb  create mode 100644 lib/__init__.py  create mode 100644 lib/postgres.py Listing 10-54.Add All Files and Commit In Listing [10-55](#Par145), you display the current state of your project. $ tree . ├── data  │   └── adult.data ├── docker  │   ├── jupyter  │   │   └── Dockerfile  │   └── postgres  │       ├── Dockerfile  │       ├── get_data.sh  │       └── initdb.sql ├── docker-compose.yml ├── ipynb  │   ├── 20170611-Examine_Database_Requirements.ipynb  │   ├── 20170613-Initial_Database_Connection.ipynb  │   └── 20170613-Verify_Database_Connection.ipynb └── lib     ├── __init__.py     ├── postgres.py     └── __pycache__         ├── __init__.cpython-35.pyc         └── postgres.cpython-35.pyc Listing 10-55.Current Project Status

## 向应用程序添加延迟处理

You will now iterate on your application to its final state. You will add to your existing application a Redis service as well as two additional services defined as variations on the Jupyter image. In Chapter [9](09.html), I discussed how you might use Redis for caching intermediate results. Here you will explore another use for Redis as the backbone of a delayed job processing system. In addition to Redis, the delayed job processing system will use a Worker service, used for executing delayed jobs, and a Monitor service for monitoring the status of delayed jobs through a web browser. Figure [10-13](#Fig13) shows a diagram of your final application.![A439726_1_En_10_Fig13_HTML.jpg](img/A439726_1_En_10_Fig13_HTML.jpg) Figure 10-13.Final application diagram To do this, you will add a few things to your docker-compose.yml file (Listing [10-56](img/#Par180)):

*   使用 image:关键字定义 Redis 服务。
*   创建 Redis 服务要使用的数据卷。
*   使用与 Jupyter 服务相同的构建上下文来定义工作服务。
*   使用与 Jupyter 服务相同的构建上下文来定义 Monitor 服务。

While both the Worker service and the Monitor service will be defined using the docker/jupyter build context, you will extend these images at runtime using the entrypoint: keyword. This keyword specifies the command with which the image should launch (in other words, the core process that will define the behavior of the container). The Worker service will use the rqworker tool in order to interface with Redis to obtain and then execute queued jobs. You use the exec form of the entrypoint: keyword and take advantage of yaml lists to specify the instantiating process. The entrypoint consists of

*   记忆合金
    *   第 [5](05.html) 章提到的 PID 1 工具，Jupyter 用它来实例化所有容器
*   -
    *   用 tini [<sup class="calibre6">7</sup>](#Fn7) 实例化容器的最佳实践
*   rqworker
    *   您将用于运行工作服务的进程
*   -你
    *   URL 标志
*   -=伊甸园美剧 http://sfile . ydy . com =-荣誉出品本字幕仅供学习交流，严禁用于商业途径
    *   Redis 可用的 URL
    *   在 Docker Compose 创建的网络上使用 Redis 服务的名称

The Monitor service will use the rq-dashboard tool in order to provide a web-based dashboard for monitoring the status of queued jobs. You use the exec form of the entrypoint: keyword and take advantage of yaml lists to specify the instantiating process. The entrypoint consists of

*   记忆合金
*   -
*   rq-仪表板
    *   您将用于运行监控服务的进程
*   -H
    *   主机标志
*   此 _redis
    *   Docker Compose 在网络上创建的 Redis 服务的名称
*   -p
    *   港口旗帜
*   Five thousand
    *   您的监控服务可用的端口

As before, you do not explicitly specify links between containers, letting Docker Compose establish the links for you. You make sure to connect the redis_data volume to the correct location within the this_redis container. version: '3' services:   this_jupyter:     build: docker/jupyter     ports:       - "8888:8888"     volumes:       - .:/home/jovyan   this_postgres:     build: docker/postgres     volumes:       - postgres_data:/var/lib/postgresql/data   this_redis:     image: redis     volumes:       - redis_data:/data   this_worker:     build: docker/jupyter     volumes:       - .:/home/jovyan     entrypoint:       - "tini"       - "--"       - "rqworker"       - "-u"       - "redis://this_redis:6379"   this_monitor:     build: docker/jupyter     volumes:       - .:/home/jovyan     ports:       - "5000:5000"     entrypoint:       - "tini"       - "--"       - "rq-dashboard"       - "-H"       - "this_redis"       - "-p"       - "5000" volumes:   postgres_data:   redis_data: Listing 10-56.Next Version of Your docker-compose.yml Next (Listing [10-57](#Par182)), you update the Jupyter Dockerfile (Listing [10-58](#Par183)) to include the necessary libraries to drive the Worker and Monitor services . $ vim docker/jupyter/Dockerfile Listing 10-57.Update the docker/jupyter/Dockerfile FROM jupyter/scipy-notebook USER root RUN conda install --yes --name root psycopg2 RUN conda install --yes --name root redis rq RUN ["bash", "-c", "source activate root && pip install rq-dashboard"] USER jovyan Listing 10-58. docker/jupyter/Dockerfile In Listing [10-59](#Par185), you display your project using tree. $ tree . ├── data  │   └── adult.data ├── docker │   ├── jupyter  │   │   └── Dockerfile  │   └── postgres  │       ├── Dockerfile  │       ├── get_data.sh  │       └── initdb.sql ├── docker-compose.yml ├── ipynb  │   ├── 20170611-Examine_Database_Requirements.ipynb  │   ├── 20170613-Initial_Database_Connection.ipynb  │   └── 20170613-Verify_Database_Connection.ipynb └── lib     ├── __init__.py     ├── postgres.py     └── __pycache__         ├── __init__.cpython-35.pyc         └── postgres.cpython-35.pyc Listing 10-59.Display Project In Listing [10-60](#Par187), you launch your updated application. Note that you have continued to use the --build flag with your docker-compose up command to ensure that the new build contexts are built before use. You then examine your running containers using docker-compose ps (Listing [10-61](#Par188)). $ docker-compose up -d --build Creating network "ch10adult_default" with the default driver Creating volume "ch10adult_postgres_data" with default driver Creating volume "ch10adult_redis_data" with default driver ... Creating ch10adult_this_worker_1 Creating ch10adult_this_redis_1 Creating ch10adult_this_postgres_1 Creating ch10adult_this_jupyter_1 Creating ch10adult_this_monitor_1 Listing 10-60.Launch Application $ docker-compose ps           Name          Command                     State              Ports ---------------------------------------------------------------------------- ch10adult_this_jupyter_1  tini--start-notebook.sh       Up  0.0.0.0:8888->8888/tcp ch10adult_this_monitor_1  tini--rq-dashboard -H th ...   Up 0.0.0.0:5000->5000/tcp... ch10adult_this_postgres_1  docker-entrypoint.sh postgres  Up                5432/tcp ch10adult_this_redis_1  docker-entrypoint.sh redis ... Up                6379/tcp ch10adult_this_worker_1    tini--rqworker -u redis: ...  Up        8888/tcp Creating Listing 10-61.Examine Running Containers Configuring the delayed job system can also be a challenge, just as in configuring the PostgreSQL database. Troubleshooting can be done using very similar methods.

*   通过 docker-compose ps 确认服务正在运行。
*   检查服务的日志(清单 [10-62](#Par193) 和 [10-63](#Par194) )。
*   通过对 bash 工具的 docker exec 调用连接到正在运行的容器。

$ docker-compose logs this_monitor Attaching to ch10adult_this_monitor_1 this_monitor_1   | RQ Dashboard version 0.3.8 this_monitor_1   |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) Listing 10-62.Examine this_monitor Logs $ docker-compose logs this_worker Attaching to ch10adult_this_worker_1 this_worker_1    | 20:28:59 RQ worker 'rq:worker:6a695d66b402.5' started, version 0.6.0 this_worker_1    | 20:28:59 Cleaning registries for queue: default this_worker_1    | 20:28:59 this_worker_1    | 20:28:59 *** Listening on default...... Listing 10-63.Examine this_worker Logs After verifying that the new services are properly configured, you commit these infrastructure changes to your git log (Listing [10-64](#Par196)). In Listing [10-65](#Par197), you add all files and commit the changes. $ git status On branch master Changes not staged for commit:   (use "git add <file>..." to update what will be committed)   (use "git checkout -- <file>..." to discard changes in working directory)         modified:   docker-compose.yml         modified:   docker/jupyter/Dockerfile no changes added to commit (use "git add" and/or "git commit -a") Listing 10-64.Check Project Status $ git add -A ubuntu@LOCAL:∼/ch10_adult (master) $ git commit -m 'add delayed job system' [master 9564efd] add delayed job system  2 files changed, 31 insertions(+), 1 deletion(-) Listing 10-65.Add and Commit Changes Since you have stopped and relaunched your Jupyter Notebook server , you will need to obtain a new authentication token in order to access the server in the browser once more (Listing [10-66](#Par199)). $ docker exec ch10adult_this_jupyter_1 jupyter notebook list Currently running servers: http://localhost:8888/?token=46e478574fe9cf238c6e2e6bc9b9daccb7efa7154dfd9d08 :: /home/jovyan Listing 10-66.Obtain Authentication Token

## 扩展 Postgres 模块

Let’s once more demonstrate the interactive development paradigm described in this chapter as you extend the postgres module you previously created. This time you will first develop a function for encoding your target. This function will be executed row by row by one or more workers. You will

*   使用 Jupyter 在笔记本中交互式编写代码。
*   将这段代码抽象成 Jupyter 中的一个函数。
*   在 Jupyter 中测试这个新函数的性能。
*   将此函数移到代码库中的一个模块中。
*   通过作业队列将功能传递给工人。

You begin by navigating to ipynb/ and creating a new file. You rename the file with today’s date and what you will be doing (e.g. 20170619-Develop_encoding_target_function.ipynb). In Listing [10-67](#Par207), you begin the notebook with the project root design pattern, after which you import connect_to_(Listing [10-68](#Par208)). In [1]: from os import chdir         chdir('/home/jovyan') Listing 10-67.The Project Root Design Pattern In [2]: from lib.postgres import connect_to_postgres Listing 10-68.Import Database Connection In Figure [10-14](#Fig14), you use a markdown cell to include the attribute type meta-information for your dataset, as you did in 20170611-Examine_Database_Requirements.ipynb.![A439726_1_En_10_Fig14_HTML.jpg](img/A439726_1_En_10_Fig14_HTML.jpg) Figure 10-14.Display the attribute type meta-information In Listing [10-69](img/#Par211), you create new columns in your database. As in Chapter [8](08.html), you manage your transactions manually via BEGIN and COMMIT statements. Note that you close the connection after each transaction . In [3]: con, cur = connect_to_postgres()         cur.execute("""         BEGIN;         ALTER TABLE adult ADD COLUMN _id SERIAL PRIMARY KEY;         ALTER TABLE adult ADD COLUMN target BOOLEAN;         COMMIT;         """)         con.close() Listing 10-69.Create New Columns It is a best practice to monitor changes to the database. In Listing [10-70](#Par213), you use a psql via docker exec to examine the adult table. $ docker exec -it ch10adult_this_postgres_1 psql postgres postgres psql (9.6.3) Type "help" for help. postgres=# \d adult                               Table "public.adult"      Column     |  Type   |                      Modifiers ----------------+---------+-----------------------------------------------------  age            | integer |  workclass      | text    |  fnlwgt         | integer |  education      | text    |  education_num  | integer |  marital_status | text    |  occupation     | text    |  relationship   | text    |  race           | text    |  gender         | text    |  capital_gain   | integer |  capital_loss   | integer |  hours_per_week | integer |  native_country | text    |  income_label   | text    |  _id            | integer | not null default nextval('adult__id_seq'::regclass)  target         | boolean | Indexes:     "adult_pkey" PRIMARY KEY, btree (_id) Listing 10-70.Examine the adult Table via a docker exec psql Call In Listing [10-71](#Par215), you retrieve unique values for your target column, income_label. In [4]: con, cur = connect_to_postgres()         cur.execute("""SELECT DISTINCT(income_label) FROM adult;""")         print(cur.fetchall())         con.close()         [(' >50K',), (' <=50K',)] Listing 10-71.Retrieve Unique Values for Target Column named income_label While income_label is categorical in nature, it only has two values and can thus be encoded as a Boolean value. In Listing [10-72](#Par217), you write a short Jupyter script to do just this. You first query the database to retrieve the _id and income_label for a single row where the target column is NULL. You create a Boolean-valued variable greater_than_50k. Finally, you update the table for the given _id and close the connection to the database . In [5]: con, cur = connect_to_postgres()         cur.execute("""SELECT _id, income_label FROM adult WHERE target IS NULL;""")         this_id, income_label = cur.fetchone()         greater_than_50k = (income_label == ' >50K')         cur.execute("""         BEGIN;         UPDATE adult         SET target = {}         WHERE _id = {};         COMMIT;         """.format(greater_than_50k, this_id))         con.close() Listing 10-72.Encode a Single Instance’s Target as a Boolean In Listing [10-73](#Par219), you verify that the update was successful. In [6]: con, cur = connect_to_postgres()         cur.execute("""         SELECT _id, income_label, target         FROM adult WHERE _id = {};         """.format(this_id))         print(this_id, cur.fetchone())         con.close()         10 (10, ' >50K', True) Listing 10-73.Verify Update Having verified that your script works, you set about abstracting the script into a function (Listing [10-74](#Par221)). In [7]: def encode_target(_id):             """Encode the target for a single row as a boolean value. Takes a row _id."""             con, cur = connect_to_postgres()             cur.execute("""SELECT _id, income_label FROM adult where _id = {}""".format(_id))             this_id, income_label = cur.fetchone()             assert this_id == _id             greater_than_50k = (income_label == ' >50K')             cur.execute("""                 BEGIN;                 UPDATE adult                 SET target = {}                 WHERE _id = {};                 COMMIT;             """.format(greater_than_50k, _id))             con.close() Listing 10-74. encode_target Function In Listings [10-75](#Par223) and [10-76](#Par224), you test the new function and verify its success . In [8]: con, cur = connect_to_postgres()         cur.execute("""SELECT _id FROM adult WHERE target IS NULL;""")         this_id, = cur.fetchone()         encode_target(this_id)         con.close() Listing 10-75.Select a New Row with Null Target and Encode In [6]: con, cur = connect_to_postgres()         cur.execute("""         SELECT _id, income_label, target         FROM adult WHERE _id = {};         """.format(this_id))         print(this_id, cur.fetchone())         con.close()         11 (11, ' >50K', True) Listing 10-76.Verify Encoding

### 更新您的 Python 模块

In Figure [10-15](#Fig15), you navigate to lib/ using the notebook server, then within lib/ you select your postgres.py in order to update the module using the Jupyter Notebook server’s text interface. Next, you add the code in Listing [10-74](#Par221) to the file, as shown in Figure [10-16](#Fig16). Note that a check mark will appear next to the text file name when all current changes have been saved, as in Figure [10-17](#Fig17).![A439726_1_En_10_Fig15_HTML.jpg](img/A439726_1_En_10_Fig15_HTML.jpg) Figure 10-15.Open postgres.py for editing ![A439726_1_En_10_Fig16_HTML.jpg](img/A439726_1_En_10_Fig16_HTML.jpg) Figure 10-16.Latest version of postgres. py ![A439726_1_En_10_Fig17_HTML.jpg](img/A439726_1_En_10_Fig17_HTML.jpg) Figure 10-17.All changes saved for postgres.py Next, you will create a new notebook to use the encode_target function via your delayed job system to encode all of the rows in the adult table . You create a new notebook titled 20170619-Encode_target.ipynb. In Listing [10-77](img/#Par227), you begin the notebook with the project root design pattern. In Listing [10-78](#Par228), you import the functions you need from lib.postgres. In [1]: from os import chdir         chdir('/home/jovyan') Listing 10-77.The Project Root Design Pattern In [2]: from lib.postgres import connect_to_postgres, encode_target Listing 10-78.Import Functions from lib.postgres In Listing [10-79](#Par230), you see a new design pattern, the instantiation of a Queue from the rq library. The Queue is instantiated with a connection to a specific Redis server. As before, you use the name of your Redis service on the network created by Docker Compose, this_redis. In [3]: from redis import Redis         from rq import Queue         REDIS = Redis(host='this_redis')         Q = Queue(connection=REDIS) Listing 10-79.Create Connection to Redis and New Queue In Listing [10-80](#Par232), you put all of the pieces together. You create a new connection to PostgreSQL. You use a for-loop to pull the row _id for 100 rows from the adult table where the target has not yet been encoded. For each row, you add the encode_target function with an associated _id to the job queue using the .enqueue() function . Note that the argument to be passed (that is, the _id, to encode_target at runtime) is passed as a second argument to .enqueue(). In [4]: con, cur = connect_to_postgres()         for _ in range(100):             cur.execute("""SELECT _id FROM adult WHERE target IS NULL;""")             this_id, = cur.fetchone()             Q.enqueue(encode_target, this_id)         con.close() Listing 10-80.Add 100 Target Encoding Requests to Queue During at least one execution of this cell (you will need to run this particular cell over 300 times in order to encode the entire table unless modifications are made), I recommend using the browser-based monitor, as well as “tailing” the Docker Compose logs, to watch the Worker churn through these functions. The browser-based monitor will be available on the same IP address as your Jupyter Notebook server, but will be available on port 5000 (Figure [10-18](#Fig18)). Listing [10-81](#Par234) shows the “tailing” of the Docker Compose logs using the --follow flag. Each job process will pass through this log as it is executed.![A439726_1_En_10_Fig18_HTML.jpg](img/A439726_1_En_10_Fig18_HTML.jpg) Figure 10-18.Browser-based Queue and Worker monitor $ docker-compose logs --follow this_worker ... this_worker_1    | 01:43:04 *** Listening on default... this_worker_1    | 01:43:04 default: lib.postgres.encode_target(24) (914a8229-a876-438f-98d4-d6cd39a469b2) this_worker_1    | 01:43:04 default: Job OK (914a8229-a876-438f-98d4-d6cd39a469b2) this_worker_1    | 01:43:04 Result is kept for 500 seconds this_worker_1    | 01:43:04 this_worker_1    | 01:43:04 *** Listening on default... this_worker_1    | 01:43:04 default: lib.postgres.encode_target(24) (215ed343-0ca5-4f75-86e6-d8237e4a5983) this_worker_1    | 01:43:04 default: Job OK (215ed343-0ca5-4f75-86e6-d8237e4a5983) this_worker_1    | 01:43:04 Result is kept for 500 seconds this_worker_1    | 01:43:04 this_worker_1    | 01:43:04 *** Listening on default... this_worker_1    | 01:43:04 default: lib.postgres.encode_target(24) (1dfe0282-cf90-49de-a904-8a2ced103c50) this_worker_1    | 01:43:04 default: Job OK (1dfe0282-cf90-49de-a904-8a2ced103c50) this_worker_1    | 01:43:04 Result is kept for 500 seconds ... Listing 10-81.Tailing the Docker Compose Logs Finally, you track your work using git. In Listing [10-82](img/#Par236), you check the status of your project. In Listing [10-83](#Par236), you add and commit all of your recent work . $ git status On branch master Changes not staged for commit:   (use "git add <file>..." to update what will be committed)   (use "git checkout -- <file>..." to discard changes in working directory)         modified:   lib/postgres.py Untracked files:   (use "git add <file>..." to include in what will be committed)         ipynb/20170619-Develop_encoding_target_function.ipynb         ipynb/20170619-Encode_target.ipynb no changes added to commit (use "git add" and/or "git commit -a")      lib/ Listing 10-82.Check the Status of the Project $ git add -A $ git commit -m 'function and queueing for target encoding' [master 93f3033] function and queueing for target encoding  3 files changed, 319 insertions(+)  create mode 100644 ipynb/20170619-Develop_encoding_target_function.ipynb  create mode 100644 ipynb/20170619-Encode_target.ipynb Listing 10-83.Add All Files and Commit In Listing [10-84](#Par239), you display the current state of your project. $ tree . ├── data  │   └── adult.data ├── docker  │   ├── jupyter  │   │   └── Dockerfile  │   └── postgres  │       ├── Dockerfile  │       ├── get_data.sh  │       └── initdb.sql ├── docker-compose.yml ├── ipynb  │   ├── 20170611-Examine_Database_Requirements.ipynb  │   ├── 20170613-Initial_Database_Connection.ipynb  │   ├── 20170613-Verify_Database_Connection.ipynb  │   ├── 20170619-Develop_encoding_target_function.ipynb  │   └── 20170619-Encode_target.ipynb └── lib     ├── __init__.py     ├── postgres.py     └── __pycache__         ├── __init__.cpython-35.pyc         └── postgres.cpython-35.pyc Listing 10-84.Current Project Status

## 摘要

This chapter marks the conclusion of the book. In this chapter, you revisited the idea of interactive programming and saw the sketch of what a framework for interactive software development might look like. You defined the project root design pattern and software design pattern used to place Jupyter at the center of a well-structured interactive software application. You outlined steps for creating code modules via an interactive development process. Finally, you used Docker Compose and Redis to build a delayed job processing system into your application. Having finished this chapter, I hope that you are excited and prepared to begin building your own interactive applications. Footnotes [1](#Fn1_source)Noble, William Stafford; “A Quick Guide to Organizing Computational Biology Projects,” PLOS Computational Biology, July 31, 2009, [http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000424](http://journals.plos.org/ploscompbiol/article%3Fid=10.1371/journal.pcbi.1000424) .   [2](#Fn2_source) [http://mikegrouchy.com/blog/2012/05/be-pythonic-__init__py.html](http://mikegrouchy.com/blog/2012/05/be-pythonic-__init__py.html)   [3](#Fn3_source) [http://pandas.pydata.org](http://pandas.pydata.org)   [4](#Fn4_source) [http://archive.ics.uci.edu/ml/datasets/Adult](http://archive.ics.uci.edu/ml/datasets/Adult)   [5](#Fn5_source) [http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names](http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names)   [6](#Fn6_source) [https://ss64.com/bash/split.html](https://ss64.com/bash/split.html)   [7](#Fn7_source) [https://github.com/krallin/tini](https://github.com/krallin/tini)