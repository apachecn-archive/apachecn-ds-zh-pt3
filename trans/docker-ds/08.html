<html xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" lang="en" xml:lang="en" xsi:schemalocation="http://www.w3.org/2002/06/xhtml2/ http://www.w3.org/MarkUp/SCHEMA/xhtml2.xsd">
<head>
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
© Joshua Cook 2017
Joshua CookDocker for Data Science<a href="08.html" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">https://doi.org/10.1007/978-1-4842-3012-1_8</a>

<!--Begin Abstract--><h1 class="chaptertitle" xml:lang="en">8.数据存储</h1>

Joshua Cook<sup class="calibre5">1 </sup>
(1)Santa Monica, California, USA

 


<!--End Abstract-->In this chapter, let’s extend the discussion beyond the Jupyter Notebook server
       to explore open source data store technologies
       and how we can use Docker to simplify the process of working with these technologies. I propose that using Docker, it is possible to streamline the process to an extent that using a data store for even the smallest of datasets becomes a practical matter. I’ll show you a series of best practices for designing and deploying data stores, a set of practices that will be sufficient for working with all but the largest of data sets. Conforming to Docker best practice, you will work with Docker Hub official images throughout this chapter.
You will look at three data store technologies here: Redis
      , MongoDB
      , and PostgreSQL
      . Deploying and using these three technologies as services managed by Docker will require you to pay specific attention to two things:
<ol class="calibre7"><li class="listitem">1.使用卷管理 Docker 容器中的持久性</li>
<li class="listitem">2.码头集装箱之间的联网</li>
</ol>

    
With regard to data persistence, you will establish a simple-but-effective best practice using Docker volumes that will be repeated for each data store. With regard to networking, you will explore three different options: legacy links, deploying a service on its own AWS instance, and manually creating bridge networks. You will look at legacy links for running Redis, AWS for running Mongo, and manual network creation for running PostgreSQL, but to be clear, any of these techniques could be applied to any of the data stores. I am merely presenting them in this fashion for learning purposes. It is recommended that you absorb all three networking techniques and choose the best option for each project you find yourself working on.
You will be sourcing each of the data stores from their respective Docker Hub community pages.
<h2 class="heading2">序列化</h2>
A central task in the workflow
        
        
              
              
             of any data scientist is the storage, transmission, and reconstruction of data structures and object states. This process is known as serialization. It is a well-solved problem and you will have several tools at your disposal to manage this task. In this chapter, you will look at serialization in terms of converting objects in memory to their binary representation as well as the use of the popular JSON format for serialization as a text file. In Chapter <a href="09.html" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">9</a>, you will see a second format of text file serialization, the YAML format.
You will be serializing and deserializing primarily for the purposes of sharing objects and data across processes. In particular, you will be storing data in your databases, as well as caching objects in Redis for the purposes of using them in a separate notebook or process. Later, in Chapter <a href="09.html" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">9</a>, you will begin to define application architecture using code. In this case, you will be serializing your application configuration.
<h3 class="heading3">序列化格式和方法</h3>
This book places an emphasis
            
          
          
                
                
                
               on working in Python. As such, it will focus on two Python-specific methods for serializing data: pickling and serializing via bytestring. In addition, you will look at appropriate uses for two text-based approaches to serializing data: JSON and YAML. JSON (JavaScript Object Notation) is a machine-readable subset of the JavaScript programming language that has been adopted by the programming community as a human readable, language agnostic approach to serialization. YAML is an alternative solution to the exact same problem. Both JSON and YAML are able to use the standard primitive data types: integers, floating-point numbers, Booleans, and null values, in addition to strings. For providing larger structures, both make use of the associative array, often called the dictionary, and the ordered list, also known as the array, the vector, the list, or the sequence. A dictionary holds data using key-value pairs; a list
            
          
          
                
                
                
               holds data using a numerical index. The two mainly differ in syntax. JSON (Listing <a href="#Par10" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-1</a>) makes use of nested braces and brackets to define data structures, while YAML (Listing <a href="#Par11" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-2</a>) achieves the same purpose using white space. Note that in the following two examples, none of the keys used have any syntactical meaning.

              {'this_json' : 'is a JSON object',
 'a nested object' : {
  'obj_id' : 123,
  'object value' : 'temperamental',
  'is_nested' : true
  },
 'a list': [1,2,3,4],
 'a list of strings': ['green eggs', 'ham'],
 'last_used' : null
}
Listing 8-1.A Sample JSON Object
                    
                  
                  
                        
                        
                        
                      
                



            

              this_yaml: is a YAML object
a_nested_object:
  obj_id: 123
  object_value: 'temperamental'
  is_nested: true
a_list:
  - 1
  - 2
  - 3
  - 4
a_list_of_strings:
  - green eggs
  - ham
last_used: null
Listing 8-2.A Sample YAML Object



            

<h3 class="heading3">Python 中的二进制编码</h3>
The Python pickle module
          
          
                
                
              
          
                
                
                
               is the preferred method for serialization of Python objects and data to binary byte streams. There are a few fundamental differences between pickling data and serializing using JSON or YAML. As noted, both JSON and YAML are human readable. An object converted to a byte stream is not human readable. JSON and YAML serialized objects will be readable by a process run in any language, while a pickled object will only be readable in Python. Because a pickled object does not have to be concerned with interoperability, a wide variety of Python objects can be pickled, whereas only dictionaries can be serialized using JSON or YAML. For the data scientist, this includes but is not limited to the numpy array, the pandas DataFrame, or the sklearn Model. Over the next chapter, you will explore
            
          
          
                
                
                
               a variety of methods for encoding data to a binary byte stream using Python.


<h2 class="heading2">使用心得</h2>
Redis<a href="#Fn1" id="Fn1_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"><sup class="calibre6">1</sup></a> is an open source
        
        
              
              
            , in-memory data structure store. It stores data values of several different types associated to a given key. In your stack, you will use Redis for two purposes. First, it will serve as a cache for persisting objects beyond the lifespan of a Python process or in-between Jupyter Notebooks. Second, you will use Redis as a message broker in order to perform delayed job processing from your notebooks using the Python library named rq. In this chapter, you will address the first use case; a discussion of the second use case happens in Chapter <a href="10.html" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">10</a>.
<h3 class="heading3">拉丽丝的形象</h3>
You can retrieve the 
            redis image
            
          
          
                
                
                
               from the Docker Hub using docker pull, as in Listing <a href="#Par17" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-3</a>. The Docker Hub page<a href="#Fn2" id="Fn2_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"><sup class="calibre6">2</sup></a> for the redis image outlines much of what I will discuss in the next few pages including basic configuration, persistent storage, and connecting to Redis from another container.

              $ docker pull redis
Using default tag: latest
latest: Pulling from library/redis
6d827a3ef358: Already exists
787f13ab8ea9: Pull complete
...
Digest: sha256:1b358a2b0dc2629af3ed75737e2f07e5b3408eabf76a8fa99606ec0c276a93f8
Status: Downloaded newer image
                
              
              
                    
                    
                    
                   for redis:latest
Listing 8-3.Pull the redis Image from Docker Hub



            
Note
You already had the first layer of the redis image in your local Docker image cache and the pull command notifies you that layer 6d827a3ef358 “Already exists”. This is because Redis uses the same base image as Jupyter, that is, they both begin their respective Dockerfiles with FROM debian:jesse.

You can minimally verify that the image functions by running a Redis container in detached mode (-d) (Listing <a href="#Par20" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-4</a>) and sending a ping command to the server via the redis-cli (Listing <a href="#Par22" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-5</a>). To interface with Redis via the redis-cli you need to issue the command via a docker exec statement issued to the proper container. When the Redis server
            
          
          
                
                
                
               running in the container responds with PONG, you know that all is good.

              $ docker run -d redis
96d6ddb6d06f1422b11193ac84a18346f3be53fd7912dc38b6301c0573171647
Listing 8-4.Run a Redis Container in Detached Mode



            
In Listing <a href="#Par22" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-5</a>, you issue a command to the running Redis container via docker exec, making reference to the running container by the first four charactersof its container id, 96d6.

              $ docker exec 96d6 redis-cli ping
PONG
Listing 8-5.Ping the Redis Server



            
Finally, you shut down and remove your running Redis container (Listing <a href="#Par24" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-6</a>).

              $ docker stop 96d6
96d6
$ docker rm 96d6
                
              
              
                    
                    
                    
                  
            
96d6
Listing 8-6.Shut Down and Remove the Running Redis Container



            


<h2 class="heading2">Docker 数据量和持久性</h2>
In thinking about running the data stores
          
        
        
              
              
             using Docker, it is helpful to think of them as services (or microservices) that are being managed by Docker. Recall that containers themselves should be ephemeral. You should be able to start, stop, and discard containers at will. If the data in your data store is saved within the container, it will be lost each time a container is cycled. Just as when running a Jupyter Notebook server, persistence of data beyond the lifespan of a container will be mission critical for all of your data store containers. Let’s loosely define this idea of a “service” as a container and its persistent data.
Note
It is worth emphasizing that I am loosely defining the idea of a “service.” We are not working with the Docker tool service.

With notebooks, you persisted data by mounting a host directory as a data volume using the volume flag (-v) (Listing <a href="03.html#Par110" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">3-32</a>). With your data stores, you will address the issue by using data volumes.<a href="#Fn3" id="Fn3_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"><sup class="calibre6">3</sup></a> A data volume is a specially designed container specifically designed to persist data beyond the lifespan of associated containers. Now you can think of a service as the container and its linked volume (Figure <a href="#Fig1" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-1</a>). Because the data volume persists beyond the lifespan of a single container, you can start, stop
          
        
        
              
              
            , and remove the container, and attach a new container to the existing data volume, without any loss of data.<img src="Images/A439726_1_En_8_Fig1_HTML.jpg" alt="A439726_1_En_8_Fig1_HTML.jpg" class="calibre63"/>
Figure 8-1.Redis as a persistent service being managed by Docker



      
<h3 class="heading3">创建并查看新的数据卷</h3>
First, you create a new data volume
            
            
          
          
                
                
                
               (Listing <a href="#Par30" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-7</a>).

              $ docker volume create --name redis-dbstore
redis-dbstore
Listing 8-7.Create a New Redis Data Volume Container



            
Volumes exist apart from containers and thus are viewed independently using the docker volume ls command (Listing <a href="#Par32" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-8</a>).

              $ docker volume ls
DRIVER              VOLUME NAME
local               redis-dbstore
                
                
              
              
                    
                    
                    
                  
            
Listing 8-8.View Current Docker Volumes



            

<h3 class="heading3">将 Redis 作为一项持久服务推出</h3>
You will connect to the newly created volume
            
          
          
                
                
                
              
          
                
                
                
               at runtime in order to allow your Redis container to persist its data beyond its lifespan. In addition to connecting to the data volume, you will run the Redis container in detached mode (-d) and give the container a name (--name) (Listing <a href="#Par36" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-9</a>).
Note
You mount the docker volume to the /data directory in the Redis container. This location is specified in the Dockerfile used to define the redis image, which can be viewed on the Redis public page on Docker Hub.<a href="#Fn4" id="Fn4_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"><sup class="calibre6">4</sup></a> Care must be taken with each data store, as each will be looking for its data cache in a separate location (Table <a href="#Tab1" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-1</a>).Table 8-1.Location of Data Cache Within Container by Image


<table border="1" class="calibre17"><colgroup class="calibre18"><col class="calibre19"/>
<col class="calibre19"/>
</colgroup>
<thead class="calibre20"><tr class="header"><th class="calibre21">图像</th>
<th class="calibre21">数据缓存位置</th>
</tr>
</thead>
<tbody class="calibre22"><tr class="noclass"><td class="calibre23">使用心得</td>
<td class="calibre23">/数据</td>
</tr>
<tr class="noclass1"><td class="calibre23">蒙戈</td>
<td class="calibre23">/数据/数据库</td>
</tr>
<tr class="noclass2"><td class="calibre23">数据库</td>
<td class="calibre23">/var/lib/postgresql/data</td>
</tr>
</tbody>
</table>

          


              $ docker run -d --name this_redis -v redis-dbstore:/data redis
b216a67caedc934b09341cf1642e89079be09d52b607ce4ddecdeaae5b5ae704
Listing 8-9.Launch Redis with an Attached Volume



            
You can verify the persistence of data in the following manner. In Listing <a href="#Par38" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-10</a>, you create a new incr object on your Redis server using the redis-cli, and ping the incr twice more for good measure
            
          
          
                
                
                
              
          
                
                
                
              .

              $ docker exec this_redis redis-cli incr mycounter
1
$ docker exec this_redis redis-cli incr mycounter
2
$ docker exec this_redis redis-cli incr mycounter
3
Listing 8-10.Create a New incr Object via redis-cli
                



            
In Listing <a href="#Par40" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-11</a>, you shut down and remove the container named this_redis.

              $ docker stop this_redis &amp;&amp; docker rm this_redis
this_redis
this_redis
Listing 8-11.Shut Down and Remove this_redis
                



            
Next, in Listing <a href="#Par42" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-12</a>, you start up a new container using the same command as before. If the data has persisted, then a subsequent incr command
            
          
          
                
                
                
              
          
                
                
                
               will yield a 4 (Listing <a href="#Par43" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-13</a>).

              $ docker run -d --name this_redis -v redis-dbstore:/data redis
12fe7cea2e63aa2055585fd97b6b9205774a59bbf716672f71e5d75858c7cd72
Listing 8-12.Start Up a New Redis Instance for Use



            

              $ docker exec this_redis redis-cli incr mycounter
4
Listing 8-13.Issue an incr Command via redis-cli
                



            
You received a response of 4 because the volume you have created and attached to each container in turn has allowed you to persist the Redis data between container instances. You can think of this as starting and stopping Redis as a native process that reconnects to a stored file on disk each time it runs
            
          
          
                
                
                
              
          
                
                
                
              .

<h3 class="heading3">通过传统链接连接容器</h3>
Having solved the persistence
            
            
          
          
                
                
                
               issue, let’s now overcome the next hurdle: connecting to your container. The easiest way to connect to a container running on the same host machine is via the --link flag at runtime. This is done simply by adding the --link flag with a reference to a named running container to the docker run command issued to launch a new container. In Listing <a href="#Par46" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-14</a>, you launch a new Jupyter container with a link to your named Redis container, this_redis, as visualized in Figure <a href="#Fig2" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-2</a>.<img src="Images/A439726_1_En_8_Fig2_HTML.jpg" alt="A439726_1_En_8_Fig2_HTML.jpg" class="calibre64"/>
Figure 8-2.Connecting Redis and Jupyter on the same system



        

              $ docker run -d -v `pwd`:/home/jovyan --link this_redis jupyter/scipy-notebook
d6f09196bf85861df23eeb2f11bd68396287464d00febe27cda93024a3666251
Listing 8-14.Launch a New Jupyter Container Linked to the Running Redis Container



            
It is worth the effort to spend a moment studying
            
            
          
          
                
                
                
               the form of the connection created by this link. You will do so by opening a bash shell to the running Jupyter container and examining the environment (Listing <a href="#Par48" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-15</a>). Again, you use the shorthand of the first four characters of the container id d6f0 to facilitate your connection.

              $ docker exec -it d6f0 bash
jovyan@d6f09196bf85:∼$ env | grep THIS_REDIS
THIS_REDIS_PORT_6379_TCP=tcp://172.17.0.2:6379
THIS_REDIS_NAME=/determined_wilson/this_redis
THIS_REDIS_PORT=tcp://172.17.0.2:6379
THIS_REDIS_PORT_6379_TCP_PORT=6379
THIS_REDIS_ENV_REDIS_VERSION=3.2.8
THIS_REDIS_PORT_6379_TCP_PROTO=tcp
THIS_REDIS_ENV_GOSU_VERSION=1.7
THIS_REDIS_ENV_REDIS_DOWNLOAD_SHA1=6780d1abb66f33a97aad0edbe020403d0a15b67f
THIS_REDIS_ENV_REDIS_DOWNLOAD_URL=http://download.redis.io/releases/redis-3.2.8.tar.gz
THIS_REDIS_PORT_6379_TCP_ADDR=172.17.0.2
Listing 8-15.Explore the Environment of a Running Jupyter Container



            
Here, you use the env command line tool to display the defined environment. You then pipe (|) the output of the command to the grep tool matching on the pattern THIS_REDIS. Note that docker has defined numerous environment variables with the name THIS_REDIS. This environment
            
            
          
          
                
                
                
               variable will always be an all-caps version of the container’s name. Your container is named this_redis, and thus the environment variables will use THIS_REDIS.
You will be able to use any of these environment variables to facilitate your connection to Redis rather than needing to take note of an IP address each time you run the container.
In Listing <a href="#Par53" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-16</a>, you demonstrate this by sending a ping from the Jupyter container to the Redis container using the environment variable, THIS_REDIS_PORT_6379_TCP_ADDR. Note that you use the $ syntax to reference the environment
            
            
          
          
                
                
                
               variable.<a href="#Fn5" id="Fn5_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"><sup class="calibre6">5</sup></a>
        

              jovyan@d6f09196bf85:∼$ ping -c 4 $THIS_REDIS_PORT_6379_TCP_ADDR
PING 172.17.0.2 (172.17.0.2): 56 data bytes
64 bytes from 172.17.0.2: icmp_seq=0 ttl=64 time=0.470 ms
64 bytes from 172.17.0.2: icmp_seq=1 ttl=64 time=0.136 ms
64 bytes from 172.17.0.2: icmp_seq=2 ttl=64 time=0.120 ms
64 bytes from 172.17.0.2: icmp_seq=3 ttl=64 time=0.085 ms
--- 172.17.0.2 ping statistics ---
4 packets transmitted, 4 packets received, 0% packet loss
round-trip min/avg/max/stddev = 0.085/0.203/0.470/0.155 ms
Listing 8-16.Ping Redis from Jupyter



            
Warning
At the time of this writing, the --link flag is a deprecated legacy feature. Per the Docker documentation, “it may be eventually removed.” For our purposes, it provides such a straightforward method for connecting containers that I think it worth discussing, even though it has been deprecated.


<h3 class="heading3">与木星搭配使用 redis</h3>
This minimal connection
            
          
          
                
                
                
              
          
                
                
                
               is sufficient to verify that your containers are able to communicate with each other, but you will need to be able to interface with the redis server from within a Jupyter Notebook. By default, the jupyter/scipy-notebook image does not include the Redis Python library necessary to interface with a Redis server from within a Python process. While it is best practice to define a new image that includes the libraries that you wish to use, here you will use an ephemeral installation of the Redis library inside a running container to demonstrate how you might quickly interface with the Redis server from within a notebook.
In Listing <a href="#Par58" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-17</a> and Figure <a href="#Fig3" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-3</a>, you can see that the container does not in fact have the Python library Redis, after which you install the library
            
          
          
                
                
                
              
          
                
                
                
               using pip executed in a subprocess from the Jupyter notebook.<a href="#Fn6" id="Fn6_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"><sup class="calibre6">6</sup></a>
          <img src="Images/A439726_1_En_8_Fig3_HTML.jpg" alt="A439726_1_En_8_Fig3_HTML.jpg" class="calibre65"/>
Figure 8-3.Install Redis via a shell call from a Jupyter Notebook
                    
                  
                  
                        
                        
                        
                      
                  
                        
                        
                        
                      
                



        

              In [1]: import redis
          -----------------------------------------------------------------
          ImportError                     Traceback (most recent call last)
          &lt;ipython-input-1-6872e27f77ac&gt; in &lt;module&gt;()
          ----&gt; 1 import redis

          ImportError: No module named 'redis'
In [2]: !pip install redis
          Collecting redis
            Downloading redis-2.10.5-py2.py3-none-any.whl (60kB)
              100% |████████████████████████████████| 61kB 2.5MB/s ta 0:00:01
          Installing collected packages: redis
          Successfully installed redis-2.10.5
          You are using pip version 8.1.2, however version 9.0.1 is available.
          You should consider upgrading via the 'pip install --upgrade pip' command.

Listing 8-17.Install Redis via a Shell Call to a Jupyter Notebook



            

<h3 class="heading3">举个简单的例子</h3>
Having configured
            
          
          
                
                
                
              
          
                
                
                
               the jupyter/scipy-notebook container with the Python Redis library, you are ready to begin using the service. You can think of Redis as giving you the ability to read and write values to RAM, allowing you to persist these values beyond the lifespan of a process or between currently running processes. Each of the examples below demonstrates this by using Redis to share values between two running Jupyter Notebooks.
Figure <a href="#Fig4" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-4</a> demonstrates connecting two separate notebooks to the Redis service using the pattern outlined in Listing <a href="#Par61" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-18</a>.<img src="Images/A439726_1_En_8_Fig4_HTML.jpg" alt="A439726_1_En_8_Fig4_HTML.jpg" class="calibre66"/>
Figure 8-4.Instatiate a Redis connection from two separate notebooks



        

              In [1]: from redis import Redis
        from os import environ
        REDIS = Redis(host=environ['THIS_REDIS_PORT_6379_TCP_ADDR'])
Listing 8-18.Connect to the Redis Service



            
Next, in Figure <a href="#Fig5" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-5</a>, you set a key-value pair in Redis from the left notebook and retrieve the value in the right notebook, using the code patterns
            
          
          
                
                
                
              
          
                
                
                
               in Listing <a href="#Par63" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-19</a> and <a href="#Par64" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-20</a>, respectively.<img src="Images/A439726_1_En_8_Fig5_HTML.jpg" alt="A439726_1_En_8_Fig5_HTML.jpg" class="calibre67"/>
Figure 8-5.Store a key-value pair from one notebook to be retrieved by another



        

              In [2]: REDIS.set('foo', 42)
Listing 8-19.Set a Key-Value Pair in Redis



            

              In [2]: REDIS.get('foo')
Out[2]: b'42'
Listing 8-20.Get a Value from Redis



            
The power of what you have done here may not at first be obvious. You have shared a value from one notebook to another (i.e. one python process to another). Such a tool can be extraordinarily valuable
            
          
          
                
                
                
              
          
                
                
                
              .

<h3 class="heading3">跟踪笔记本之间的迭代过程</h3>
In Figure <a href="#Fig6" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-6</a>, you see how Redis can be used to track the development of an iterative process
            
            
          
          
                
                
                
              . You use the code pattern outlined in Listing <a href="#Par67" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-21</a> to mock the execution of an iterative process and the code pattern in Listing <a href="#Par68" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-22</a> to check the progress of the process while it is in the midst of execution
            
            
          
          
                
                
                
              .<img src="Images/A439726_1_En_8_Fig6_HTML.jpg" alt="A439726_1_En_8_Fig6_HTML.jpg" class="calibre68"/>
Figure 8-6.Track an iterative process across notebooks



        

              In [3]: import time
        def some_iterative_process():
            time.sleep(1)
In [4]: count = 0
        REDIS.set('count', 0)

        while count &lt; 30:
            some_iterative_process()
            count = REDIS.incr('count')

Listing 8-21.Mock an Iterative Process



            

              In [2]: REDIS.get('count')
Out[2]: 8
Listing 8-22.Get the Incrementor Value from Redis



            
The In [*] on the left notebook indicates that the process is currently running. You obtain the current count in the right notbook by retrieving the count from Redis. The count is continually updated at each iteration
            
            
          
          
                
                
                
               by the process being run on the right.

<h3 class="heading3">通过 JSON 转储传递字典</h3>
You might also pass a dictionary
            
            
          
          
                
                
                
               of values from one notebook to another using Redis. Figure <a href="#Fig7" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-7</a> demonstrates this with a dictionary containing potential model parameters. You use the code pattern in Listing <a href="#Par71" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-23</a> to define the dictionary and pass it to Redis. You use the pattern in Listing <a href="#Par72" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-24</a> to first load the object from Redis and then to convert it to a Python dictionary.<img src="Images/A439726_1_En_8_Fig7_HTML.jpg" alt="A439726_1_En_8_Fig7_HTML.jpg" class="calibre69"/>
Figure 8-7.Pass a dictionary via a JSON dump



        

              In [5]: import numpy as np
        import json
        model_params = {
            'C': list(np.logspace(-3,3,7)),
            'penalty': 'l1',
            'solver' : 'newton-cg'
        }

        REDIS.set('model_params', json.dumps(model_params))
Out[5]: True
                  
                  
                
                
                      
                      
                      
                    
              

Listing 8-23.Define a Dictionary and Pass It to Redis



            

              In [4]: REDIS.get('model_params')
Out[4]: b'{"C": [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0],
        "solver": "newton-cg", "penalty": "l1"}'
In [5]: import json
        json.loads(REDIS.get('model_params').decode())
Out[5]: {'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0],
        'penalty': 'l1',
        'solver': 'newton-cg'}
Listing 8-24.Load a Dictionary from Redis



            
In the receiving notebook, when you initially load the object from Redis, it is returned as a bytestring, as is denoted by the leading b when the string
            
            
          
          
                
                
                
               is displayed in Out[4]. Most objects returned by Redis will be returned as bytestrings, and you must take special care to handle them. Here, you wish the object to be a dictionary. To have the object load as a dictionary, you first use the .decode() function included as part of the Python bytes class. This converts the object to a string. You then pass this string to the json.loads() function, which converts
            
            
          
          
                
                
                
               the string to the dictionary you see displayed as Out[5].

<h3 class="heading3">将 Numpy 数组作为字节字符串传递</h3>

          numpy’s natural capacity for working
            
            
          
          
                
                
                
               with bytestrings makes it an excellent partner for Redis and it is very straightforward to pass numpy arrays and vectors. In Listings <a href="#Par78" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-25</a> and <a href="#Par79" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-26</a>, you encode and store, and then load and decode, a numpy array.
Note that in order to convert an array to a bytestring, you must first convert it to a vector. This is done using the .ravel() function included as part of the np.array class. This function converts an array of shape (n, m) to a vector of shape (n*m,). You then convert the vector to a bytestring using the .tostring() function.
To convert from a bytestring, you use the numpy function np.fromstring() before using the .reshape() function to convert the resulting vector back
            
            
          
          
                
                
                
               into an array. See Figure <a href="#Fig8" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-8</a>.<img src="Images/A439726_1_En_8_Fig8_HTML.jpg" alt="A439726_1_En_8_Fig8_HTML.jpg" class="calibre70"/>
Figure 8-8.Pass a numpy array as a bytestring



        
Throughout the process, you have had to keep track of the shape of the original array manually. In this case, you did so by storing the number of rows (n) and the number of columns (m) in Redis and then retrieving them when necessary.

              In [6]: import numpy as np
        A = np.array([
            [1,1,1],
            [2,2,2],
            [3,3,3]
        ])
        n,m = A.shape

        encoded_A = A.ravel().tostring()
        REDIS.set('encoded_A', encoded_A)
        REDIS.set('A_n', n)
        REDIS.set('A_m', m)
Out[6]: True
                  
                  
                
                
                      
                      
                      
                    
              

Listing 8-25.Encode a numpy Array as a bytestring and Pass it to Redis



            

              In [6]: import numpy as np
        A_bytestring = REDIS.get('encoded_A')
        A_encoded = np.fromstring(A_bytestring, dtype=int)
        n = int(REDIS.get('A_n').decode())
        m = int(REDIS.get('A_m').decode())
        A = A_encoded.reshape(n, m)
        A
Out[6]: array([[1, 1, 1],
               [2, 2, 2],
               [3, 3, 3]])
Listing 8-26.Load a numpy Array and Decode



            
Note
By default, np.fromstring() will convert a passed bytestring to a vector of floating point values. Since you passed a vector of integers, you must specify this type when converting from bytestring, as you have done with np.fromstring(A_bytestring, dtype=int).



<h2 class="heading2">MongoDB</h2>
Redis is a data structure store. It is largely concerned with storing individual object instances. MongoDB
        
        
              
              
             is a database. It is concerned with storing many instances of the same type. MongoDB calls each instance that it stores a document and stores these documents using a JSON-like format. This format also lends itself to working with the Python dictionary class.
As compared to other databases, MongoDB is known for its flexibility. Rather than dealing with complex schema in order to store object instances, you can quickly and easily add an object to a MongoDB collection without concerning yourself with the very idea of schema. Such an approach to databases is known as NoSQL, and MongoDB is perhaps the most widely used NoSQL database.
With regard to data persistence, you will use the same approach you used in setting up Redis. With regard to networking, you will take a slightly different approach. Rather than configuring MongoDB to run alongside a Jupyter service, you will configure Mongo to run on an AWS instance. The advantages of this come down to networking.
If you expose the necessary port in the Mongo container to the corresponding port on the host container, you no longer need to worry about networking. You simply connect to the IP address of the AWS machine. I have found this solution to be in practice slightly easier than configuring a bridge network to connect two containers on the same host system and does not use the now deprecated --link method previously discussed.
Note
As the recommended practice was for you to follow along in this book using a t2.micro, you may currently be using an AWS instance to do the work in this book. This is by design and I hope that this is still the case. The intent of the following section is for you to run MongoDB on a system that is apart from the one on which you are currently working. If you are currently working on an AWS instance, this means that you will now be working on two AWS instances. You will have these two instances communicate with each other over the open internet.

<h3 class="heading3">设置新的 AWS t2.micro</h3>
You did this work earlier in Chapter <a href="01.html" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">1</a>, so I will merely outline the steps necessary to bring a new t2.micro
            
          
          
                
                
                
               online:
<ol class="calibre7"><li class="listitem">1.从 AWS EC2 仪表板中，选择“启动实例”</li>
<li class="listitem">2.在“选择 AMI”选项卡上，选择 Ubuntu Server 16.04。</li>
<li class="listitem">3.在选择实例类型选项卡上，选择 t2.micro。</li>
<li class="listitem">4.在添加存储选项卡上，使用默认设置 8GB。</li>
<li class="listitem">5.在“配置安全组”选项卡上，选择“创建新的安全组”<ol class="calibre7"><li class="listitem">a.确认可以从任何地方通过端口 22 接受入站 SSH 流量。</li>
<li class="listitem">b.添加一个规则，接受来自任何地方的端口 2376 上的入站流量。此端口将允许您从 Docker Hub 获取图像。</li>
<li class="listitem">c.添加一个规则，接受来自任何地方的通过端口 27017 的入站流量。这是访问 MongoDB 的默认端口。</li>
</ol>

                

 </li>
<li class="listitem">6.检查并启动一个实例，注意确认您可以访问与您的 AWS 帐户一起存储的 SSH 密钥。</li>
</ol>

        
Note
You have added port 27017 to this Security Group so that you can access MongoDB on its default port over the open web. If Redis is configured on its own AWS instance, then port 6379 will need to be added to the Security Group. If PostgreSQL is configured on its own AWS instance, then 5432 will need to be added to the Security Group.


<h3 class="heading3">为 Docker 配置新的 AWS t2.micro</h3>
As before, you provision
            
          
          
                
                
              
          
                
                
                
               the new instance with Docker in order to do your work. Again, I will outline steps without a great deal of explanation. Code for this configuration is provided in Listing <a href="#Par103" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-27</a>. Readers seeking additional information are referred to the detailed description of this process in Chapter <a href="01.html" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">1</a>.
<ol class="calibre7"><li class="listitem">1.记下新配置的 AWS 实例的 IP 地址。</li>
<li class="listitem">2.使用该 IP 地址 SSH 到实例中。</li>
<li class="listitem">3.通过 shell 脚本安装 Docker。</li>
<li class="listitem">4.将 ubuntu 用户添加到 docker 组。</li>
<li class="listitem">5.注销并重新登录。</li>
</ol>

        

               (local) $ ssh ubuntu@255.255.255.255
(remote) $ curl -sSL https://get.docker.com/ | sh
(remote) $ sudo usermod -aG docker  ubuntu
Listing 8-27.Provision the AWS Instance with Docker



            
After this last command, log out of the ssh session to AWS and log back in to configure MongoDB
            
          
          
                
                
              
          
                
                
                
              .

<h3 class="heading3">拉蒙哥图像</h3>
As with the redis image, you will retrieve the mongo image
            
          
          
                
                
                
               from its public page on Docker Hub.<a href="#Fn7" id="Fn7_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"><sup class="calibre6">7</sup></a> The page also provides details about configuring Mongo. For your purposes, you are most interested in reading about the specifics of the default configuration. In Listing <a href="#Par107" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-28</a>, you pull the image.

               (local) $ ssh ubuntu@255.255.255.255
(remote) $ docker pull mongo
                
              
              
                    
                    
                    
                  
            
Listing 8-28.Pull the mongo Image



            

<h3 class="heading3">创建并查看新的数据卷</h3>
As with Redis, you will persist the data associated with your MongoDB by creating a data volume
            
          
          
                
                
                
               that will live beyond a container running Mongo. Again, you can think of MongoDB as a “service” comprised of the container running Mongo and the associated data volume (Figure <a href="#Fig9" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-9</a>). To set this up, you simply create a data volume for your Mongo data, as outlined in Listing <a href="#Par109" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-29</a>.<img src="Images/A439726_1_En_8_Fig9_HTML.jpg" alt="A439726_1_En_8_Fig9_HTML.jpg" class="calibre71"/>
Figure 8-9.MongoDB as a persistent service being managed by Docker
                    
                  
                  
                        
                        
                        
                      
                



        

              $ docker volume create --name mongo-dbstore
$ docker volume ls
DRIVER              VOLUME NAME
local               mongo-dbstore
local               redis-dbstore
Listing 8-29.Create and View a New Mongo Data Volume



            

<h3 class="heading3">将 MongoDB 作为持久服务启动</h3>
You connect to the newly created volume
            
          
          
                
                
                
               at runtime in order to allow your Mongo container to persist its data beyond its lifespan (Listing <a href="#Par113" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-30</a>).
Note
You mount the docker volume to the /data/db directory in the Mongo container. This location is specified in the Dockerfile used to define the mongo image, which can be viewed on the Mongo public page on Docker Hub.<a href="#Fn8" id="Fn8_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"><sup class="calibre6">8</sup></a>
          


              $ docker run -d --name this_mongo -v mongo-dbstore:/data/db -p 27017:27017 mongo

                  38a2f19d72a09851dc32cb874817a45274e888dd93aca01b5500cbfe9fb9364c
                
              
                  
                    
                    
                    
                  
                
Listing 8-30.Launch mongo with an Attached Volume



            

<h3 class="heading3">验证 MongoDB 安装</h3>
You can verify
            
          
          
                
                
                
               that you are running the mongo service by connecting to the running MongoDB via the MongoDB client, mongo, issued via docker exec (Listing <a href="#Par117" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-31</a>).
Warning
Both the image and the client share the same name, mongo. It is important to keep track of which is being referred to as you do your work. In the issuing of the command in Listing <a href="#Par117" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-31</a>, mongo refers to the command line client you are using to interface with the running MongoDB.

In Listing <a href="#Par117" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-31</a>, you connect and then insert a trivial document to a mongo collection. You are inserting the JSON object {"foo":1} into the collection test. You then search for the document you inserted using the .find() command
            
          
          
                
                
                
              .

              $ docker exec -it this_mongo mongo
MongoDB shell version v3.4.4
...
&gt; db.test.insert({"foo":1})
WriteResult({ "nInserted" : 1 })
&gt; db.test.find()
{ "_id" : ObjectId("591a00ee33e4717a80d8c92d"), "foo" : 1 }
                    
                    
                  
              
                    
                    
                    
                  
            
Listing 8-31.Connect to mongo and Insert a Document



            

<h3 class="heading3">通过 jupiter 使用 mongodb</h3>
As with Redis, you will need to be able to connect to MongoDB from within a Jupyter notebook
            
          
          
                
                
                
              . As before, you will need to install the necessary Python library, pymongo, and you will do this via a pip install issued through a notebook (Figure <a href="#Fig10" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-10</a>, Listing <a href="#Par119" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-32</a>).<img src="Images/A439726_1_En_8_Fig10_HTML.jpg" alt="A439726_1_En_8_Fig10_HTML.jpg" class="calibre72"/>
Figure 8-10.Install pymongo via a shell call from a Jupyter Notebook
                    
                  
                  
                        
                        
                        
                      
                



        

              In [1]: !pip install pymongo
        Collecting pymongo
          Downloading pymongo-3.4.0-cp35-cp35m-manylinux1_x86_64.whl (359kB)
            100% |████████████████████████████████| 368kB 700kB/s ta 0:00:01
        Installing collected packages: pymongo
        Successfully installed pymongo-3.4.0
Listing 8-32.Install pymongo via a shell Call from a Jupyter Notebook



            

<h3 class="heading3">MongoDB 结构</h3>
As a NoSQL database
            
          
          
                
                
                
              , MongoDB has a minimal approach to structure. MongoDB has three kinds of entities:
<ol class="calibre7"><li class="listitem">1.数据库</li>
<li class="listitem">2.该系列</li>
<li class="listitem">3.文件</li>
</ol>

        
These entities relate to each other in that databases hold collections which hold documents (Figure <a href="#Fig11" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-11</a>). Each document<a href="#Fn9" id="Fn9_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"><sup class="calibre6">9</sup></a> is a binary representation of a JSON data record. Documents are composed of key-value
            
          
          
                
                
                
               pairs where a value can be any of the BSON data types.<a href="#Fn10" id="Fn10_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"><sup class="calibre6">10</sup></a>
          <img src="Images/A439726_1_En_8_Fig11_HTML.jpg" alt="A439726_1_En_8_Fig11_HTML.jpg" class="calibre73"/>
Figure 8-11.MongoDB hierarchy



        

<h3 class="heading3">皮蒙戈</h3>

          pymongo
          <a href="#Fn11" id="Fn11_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"><sup class="calibre6">11</sup></a> is a Python module
            
          
          
                
                
                
               containing the MongoDB tools recommended for working with the database. You begin (Listing <a href="#Par129" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-33</a>) by instantiating a connection to MongoDB using pymongo.MongoClient. Here, you use the IP address of your AWS instance on which MongoDB is running
            
          
          
                
                
                
              .

              In [2]: from pymongo import MongoClient
        client = MongoClient('255.255.255.255', 27017)
Listing 8-33.Connect to MongoDB



            
Note
If using the default port of 27017 as you have done (Listing <a href="#Par113" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-30</a>), it is not necessary to specify the port.


          pymongo has a very useful “get or create” mechanism for both databases and collections. Databases and collections are accessed using either attribute-style (client.database_name) or dictionary-style (client['test-database']). If the database exists, this method will return a reference to the existing database or collection (“get”). If the database does not exists, this method will create the database or collection and then return a reference to it (“create”). The creation happens at the time of insertion of a document.
In Listing <a href="#Par133" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-34</a>, you display currently extant databases.

              In [3]: client.database_names()
Out[3]: ['admin', 'local', 'test']
Listing 8-34.Display Databases



            
Next, in Listing <a href="#Par135" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-35</a>, you create a reference to a new database and once more show databases. Note that the database named my_database does not yet exist
            
          
          
                
                
                
              .

              In [4]: db_ref = client.my_database
        client.database_names()
Out[4]: ['admin', 'local', 'test']
Listing 8-35.Create Database Reference and Display Databases



            
Next, in Listing <a href="#Par137" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-36</a>, you create a reference to a new collection in my_database and then show databases, as well as collections, associated with my_database. Note that the database my_database still does not yet exist.

              In [5]: coll_ref = db_ref.my_collection
        client.database_names(), db_ref.collection_names()
Out[5]: (['admin', 'local', 'test'], [])
Listing 8-36.Create Collection Reference and Display Databases and Collections



            
In Listing <a href="#Par139" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-37</a>, you create a new Python dictionary and insert the dictionary into your collection using the .insert_one() class function
            
          
          
                
                
                
              .

              In [6]: sample_doc = {"name":"Joshua", "message":"Hi!", 'my_array' : [1,2,3,4,5,6,7,9]}
        coll_ref.insert_one(sample_doc)
Out[6]: &lt;pymongo.results.InsertOneResult at 0x7efc749726c0&gt;
Listing 8-37.Insert a Document into a Collection



            
In Listing <a href="#Par141" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-38</a>, you show the databases as well as the collections associated with my_database. As can be seen, now your database and collection both exist as they were created on insertion.

              In [7]: client.database_names(), db_ref.collection_names()
Out[7]: (['admin',
          'local',
          'my_database',
          'test'],
         ['my_collection'])
Listing 8-38.Display Databases and Collections



            
In Listing <a href="#Par143" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-39</a>, you demonstrate an interesting behavior of MongoDB. You drop all elements
            
          
          
                
                
                
               from my_collection using the .drop() class function and then show the databases and the collections associated with my_database once more. Note that neither my_database nor my_collection continue to exist. In other words, databases and collections exist solely as containers for documents.

              In [8]: my_collection.drop()
        client.database_names(), db_ref.collection_names()
                    
                    
                  
              
                    
                    
                    
                  
            
Out[8]: (['admin', 'local', 'test'], [])
Listing 8-39.Drop All my_collection Documents and Display Databases and Collections



            

<h3 class="heading3">Mongo 和 Twitter</h3>
To demonstrate a simple usage for MongoDB
            
          
          
                
                
                
               with Jupyter, you will implement a basic Twitter streamer that inserts captured tweets into a MongoDB collection. Twitter data represents an ideal use case for the NoSQL MongoDB. Each tweet obtained via the Twitter API is received as an unstructured nested JSON object. Adding such an object to a SQL database would be a non-trivial task by any measure involving numerous foreign keys and JoinTables as the user seeks to manage each of the one-to-one, one-to-many, and many-to-one relationships built into the tweet. Adding such an object to Mongo, on the other hand, is a trivial task. MongoDB’s native Binary JSON (BSON) format was designed precisely to accept such an object.

<h3 class="heading3">获取 Twitter 证书</h3>
In order to follow
            
          
          
                
                
                
               along, you must obtain API credentials for accessing the Twitter API. This is done by creating a Twitter application.
In order to do this, follow these steps:
<ol class="calibre7"><li class="listitem">1.参观<a href="https://apps.twitter.com" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">https://apps.twitter.com</a>并签到。</li>
<li class="listitem">2.选择“创建新应用”(图<a href="#Fig12" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"> 8-12 </a>)。<img src="Images/A439726_1_En_8_Fig12_HTML.jpg" alt="A439726_1_En_8_Fig12_HTML.jpg" class="calibre74"/>图 8-12。创建新的 Twitter 应用程序</li>
<li class="listitem">3.为新应用程序命名、描述和网站。出于您的目的，这些响应的值是不相关的，尽管网站需要有一个有效的 URL 结构。</li>
<li class="listitem">4.同意开发者协议，点击“创建你的 Twitter 应用”(图<a href="#Fig13" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"> 8-13 </a>)。<img src="Images/A439726_1_En_8_Fig13_HTML.jpg" alt="A439726_1_En_8_Fig13_HTML.jpg" class="calibre75"/>图 8-13。同意开发者条款并创建应用程序</li>
</ol>

        
Once the new app
            
          
          
                
                
                
               is created, you will see details of your new app as shown in Figure <a href="#Fig14" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-14</a>.<img src="Images/A439726_1_En_8_Fig14_HTML.jpg" alt="A439726_1_En_8_Fig14_HTML.jpg" class="calibre76"/>
Figure 8-14.Your new Twitter app



        
Next, you will need to access your credentials on the “Keys and Access Tokens” tab. You will need a total of four values:
<ol class="calibre7"><li class="listitem">1.一个消费者密钥(API 密钥)(图<a href="#Fig15" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"> 8-15 </a> ) <img src="Images/A439726_1_En_8_Fig15_HTML.jpg" alt="A439726_1_En_8_Fig15_HTML.jpg" class="calibre77"/>图 8-15。消费者密钥和消费者秘密</li>
<li class="listitem">2.消费者秘密(API 秘密)(图<a href="#Fig15" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"> 8-15 </a></li>
<li class="listitem">3.一个访问令牌(图<a href="#Fig17" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"> 8-17 </a></li>
<li class="listitem">4.访问令牌秘密(图<a href="#Fig17" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"> 8-17 </a></li>
</ol>

        
The consumer key and consumer secret
            
          
          
                
                
                
               should be generated by default at the time of app creation. The access token and access token secret will need to be generated (Figure <a href="#Fig16" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-16</a>).<img src="Images/A439726_1_En_8_Fig16_HTML.jpg" alt="A439726_1_En_8_Fig16_HTML.jpg" class="calibre78"/>
Figure 8-16.Generate access tokens



          <img src="Images/A439726_1_En_8_Fig17_HTML.jpg" alt="A439726_1_En_8_Fig17_HTML.jpg" class="calibre38"/>
Figure 8-17.Access token and access token secret



        
Once these items have been obtained, enter the values into a cell in a new Jupyter Notebook
            
          
          
                
                
                
               as strings, as in Listing <a href="#Par159" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-40</a>. Note that the code pattern has been included for easy copy-and-paste, but readers will need to replace each None value with the appropriate credential value as a string.

              In [9]: CONSUMER_KEY = None
        CONSUMER_SECRET = None
        ACCESS_TOKEN = None
        ACCESS_SECRET = None
Listing 8-40.Load Twitter Credentials as Strings



            
In Listing <a href="#Par161" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-41</a>, you install the twitter library using a system call, as you have done previously with pymongo and redis.

              In [10]: !pip install twitter
         Collecting twitter
           Downloading twitter-1.17.1-py2.py3-none-any.whl (55kB)
             100% |████████████████████████████████| 61kB 932kB/s ta 0:00:011
         Installing collected packages: twitter
         Successfully installed twitter-1.17.1
Listing 8-41.Install the twitter Library



            
You next instantiate a twitter.OAuth object using the Python twitter module
            
          
          
                
                
                
               and the credentials you have just loaded (Listing <a href="#Par163" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-42</a>). You will use this object to facilitate your connection to Twitter’s API.

              In [11]: from twitter import OAuth
        oauth = OAuth(ACCESS_TOKEN, ACCESS_SECRET, CONSUMER_KEY, CONSUMER_SECRET)
Listing 8-42.Instatiate the twitter.OAuth Object



            

<h3 class="heading3">通过地理定位收集推文</h3>
For this example, you will be using Twitter’s Public Stream.<a href="#Fn12" id="Fn12_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"><sup class="calibre6">12</sup></a> Applications that are able to connect
            
          
          
                
                
                
               to a streaming endpoint will receive a sample of public data flowing through Twitter and will be able to do so without polling or concern of API rate limits. In other words, the Public Stream is a safe and sanctioned way to collect a sample of live public tweets. That said, even this sample will return a great deal of unordered data.
In order to provide a modicum of order to your Twitter stream, you will restrict incoming tweets using a geolocation<a href="#Fn13" id="Fn13_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"><sup class="calibre6">13</sup></a> bounding box,<a href="#Fn14" id="Fn14_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"><sup class="calibre6">14</sup></a> or bbox. You can easily obtain a bbox for a location of interest using the Klokantech BoundingBox Tool.<a href="#Fn15" id="Fn15_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"><sup class="calibre6">15</sup></a> In Figure <a href="#Fig18" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-18</a>, you obtain a bbox for Santa Monica, California in the United States, making sure to select CSV Raw as the copy and paste format
            
          
          
                
                
                
              .<img src="Images/A439726_1_En_8_Fig18_HTML.jpg" alt="A439726_1_En_8_Fig18_HTML.jpg" class="calibre79"/>
Figure 8-18.Obtain a bbox for Santa Monica, California



        
When you use a bbox to filter tweets, you will obtain only geolocated tweets falling within the bbox. Each bounding box should be specified as a pair of longitude and latitude
            
          
          
                
                
                
               pairs, with the southwest corner of the bounding box coming first, following geoJSON order (longitude, latitude).
Having obtained these values, you use them to define your bbox (Listing <a href="#Par172" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-43</a>), defining the CSV list of values as a string.

              In [12]: los_angeles_bbox = "-118.551346,33.96666,-118.443428,34.05056"
Listing 8-43.Define a bbox for Santa Monica, California



            
Finally, you instantiate a twitter.TwitterStream
          <a href="#Fn16" id="Fn16_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"><sup class="calibre6">16</sup></a> object you will use to collect tweets (Listing <a href="#Par176" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-44</a>). twitter.TwitterStream provides an interface to the Twitter Stream API in Python. The result of calling a method on this object is an iterator<a href="#Fn17" id="Fn17_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"><sup class="calibre6">17</sup></a> that yields tweets decoded from the Twitter stream as JSON objects
            
          
          
                
                
                
              .

              In [13]: from twitter import TwitterStream

         twitter_stream = TwitterStream(auth=oauth)
         twitterator = twitter_stream.statuses.filter(locations=los_angeles_bbox)

Listing 8-44.Instatiate a TwitterStream and an Associated Iterator



            
Following the instantiation of your twitterator, each subsequent next call will yield a tweet (Listing <a href="#Par178" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-45</a>), a massive, nested JSON object.

              In [14]: next(twitterator)
         {
          ...
          'created_at': 'Sun May 28 20:20:10 +0000 2017',
          ...
          'place': {'attributes': {},
           'bounding_box': {'coordinates': [[[-118.668404, 33.704538],
              [-118.668404, 34.337041],
              [-118.155409, 34.337041],
              [-118.155409, 33.704538]]],
            'type': 'Polygon'},
          ...
          },
          ...
          'quoted_status': {
          ...
           'text': "I'm in LA
                
              
              
                    
                    
                    
                   now and it's freakin' awesome ʿ",
          ...
          }
         }
Listing 8-45.Obtain the Next Tweet



            

<h3 class="heading3">将推文插入 Mongo</h3>
Twitter is a wonderful source
            
          
          
                
                
                
               of messy, “real” data. Wrangling it into a database is where MongoDB truly shines. Using your twitterator object and the .insert_one() class function this can be done in a single line of code (Listing <a href="#Par180" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-46</a>).

              In [14]: my_collection.insert_one(next(twitterator))
Out[14]: &lt;pymongo.results.InsertOneResult at 0x7f697c03cbd0&gt;
Listing 8-46.Insert the Next Tweet into my_collection
                



            
You can verify the tweet’s insertion and retrieve it with the .count() (Listing <a href="#Par182" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-47</a>) and .find_one() class functions (Listing <a href="#Par183" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-48</a>).

              In [15]: my_collection.count()
Out[15]: 1
Listing 8-47.Count Objects in my_collection
                



            

              In [15]: my_collection.find_one()
Out[16]: {'_id': ObjectId('592b57ae042cee05c85ecf1d'),
         ...
          'text': '@sccrphobia11 Cubs 4 Dodgers 9 All runs in game were scored by homers by both
                
              
              
                    
                    
                    
                  
            
         teams. 7 HRs in all through 7 innings.',
         ...
         }
Listing 8-48.Read One Object from my_collection
                



            


<h2 class="heading2">一种数据库系统</h2>
PostgreSQL<a href="#Fn18" id="Fn18_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"><sup class="calibre6">18</sup></a> is an open source, object-relational database system. I favor the use of PostgreSQL
        
        
              
              
             over other structured query language (SQL) databases because of its wide adoption by the industry as well as for its native array and binary object types. A major selling point of SQL databases is their capacity for working with structured data. The creation of these structures is in and of itself an art and well beyond the scope of this text. Herein, I will emphasize PostgreSQL’s natural aptitude for working with CSV files. I will also briefly explain PostgreSQL’s array and binary types as natural partners to our work in the numpy/scipy stack.
You will continue to use the docker volume tool to persist data beyond the lifespan of a container. You will look at yet a third approach to managing networking in Docker, exploring the docker network tool.
Note
The configuration of network connections using docker network should be considered an advanced technique. You may skip this section with no peril to your learning and connect to PostgreSQL using either of the techniques outlined above.
If you do attempt to configure their connection to PostgreSQL using docker network, I assume that you are working on the host system where you have been running Jupyter.

<h3 class="heading3">拉出 postgres 图像</h3>
As before, you retrieve the postgres image
            
          
          
                
                
                
               from its public page on Docker Hub.<a href="#Fn19" id="Fn19_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"><sup class="calibre6">19</sup></a> In Listing <a href="#Par191" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-49</a>, you pull the image.

              $ docker pull postgres
Using default tag: latest
latest: Pulling from library/postgres
...
Digest: sha256:a2e6e6012a9056fa7647df5746119768bdb0bf4e82bb04819d5a8e450968a967
Status: Downloaded newer image for postgres:latest
Listing 8-49.Pull the postgres Image



            

<h3 class="heading3">创建新的数据卷</h3>
Following the previous pattern
            
          
          
                
                
                
              , you can think of PostgreSQL as a “service” comprised of the container running postgres and an associate data volume (Figure <a href="#Fig19" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-19</a>). In Listing <a href="#Par193" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-50</a>, you create a new data volume.<img src="Images/A439726_1_En_8_Fig19_HTML.jpg" alt="A439726_1_En_8_Fig19_HTML.jpg" class="calibre80"/>
Figure 8-19.PostgreSQL as a persistent service being mangaged by Docker



        

              $ docker volume create --name pg-datastore
Listing 8-50.Create a New postgres Data Volume



            

<h3 class="heading3">将 PostgreSQL 作为持久服务启动</h3>
Again, you connect to the new data
            
          
          
                
                
                
               volume at runtime (Listing <a href="#Par196" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-51</a>).
Note
You mount the Docker volume to the /var/lib/postgresql/data directory in the Postgres container.


              $ docker run -d --name this_postgres -v pg-dbstore:/var/lib/postgresql/data -p 5432:5432 postgres
f9751d99f09d691dc3f04d246c502f1fa4ff3ae059632428a65affa3e5307f17
Listing 8-51.Display the Default Docker Networks
                    
                  
                  
                        
                        
                        
                      
                



            

<h3 class="heading3">验证 PostgreSQL 安装</h3>
You can verify your PostgreSQL service
            
          
          
                
                
                
               by connecting to the running container via the PostgreSQL client, psql, issued via docker exec (Listing <a href="#Par199" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-53</a>). Note that in order to do this you must specify a DBNAME and a USERNAME (Listing <a href="#Par198" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-52</a>). As defined in the postgres image, the default value for both of these is postgres.

              $ docker exec this_postgres psql --help
psql is the PostgreSQL interactive terminal.

Usage:
  psql [OPTION]... [DBNAME [USERNAME]]
...

Listing 8-52.Display psql Help



            

              $ docker exec -it this_postgres psql postgres postgres
                  
                
                
                      
                      
                      
                    
              
psql (9.6.3)
Type "help" for help.

postgres=# CREATE TABLE test ( _id INTEGER, name TEXT);
CREATE TABLE
postgres=# INSERT INTO test VALUES (1,'Joshua');
INSERT 0 1
postgres=# SELECT * FROM test LIMIT 1;
 _id |  name
-----+--------
   1 | Joshua
(1 row)

postgres=# \q
                  
                
                
                      
                      
                      
                    
              

Listing 8-53.Connect to postgres, Create a Table, and Insert and Select a record



            

<h3 class="heading3">码头集装箱网络</h3>
You can connect two containers
            
          
          
                
                
              
          
                
                
                
              , such as a postgres service and a jupyter service, using Docker networking. By default, the Docker installation creates three networks for you. You will not need to create any additional networks; however, you will need to know how the default networks function. Listing <a href="#Par201" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-54</a> shows the three networks created for you by using the docker network ls command.

              $ docker network ls
NETWORK ID          NAME                DRIVER
c9fced8bcbc9        bridge              bridge
3bec69979ce4        host                host
d8d9192909eb        none                null
Listing 8-54.Display the Default Docker Networks



            
In this text, you will only use the bridge network. It is the network that new containers connect to by default at runtime. It is possible to see this network on a Unix-variant host machine by running the ifconfig command
            
          
          
                
                
              
          
                
                
                
              , as seen in Listing <a href="#Par203" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-55</a>.

              $ ifconfig
...
docker0   Link encap:Ethernet  HWaddr 02:42:e4:f6:31:5a
          inet addr:172.17.0.1  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:e4ff:fef6:315a/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:49037 errors:0 dropped:0 overruns:0 frame:0
          TX packets:38968 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:31550630 (31.5 MB)  TX bytes:4105362 (4.1 MB)
...
Listing 8-55.Display the Host Network Configuration



            
You can connect to a running postgres container
            
          
          
                
                
              
          
                
                
                
               using a bash shell via the docker exec command, as seen in Listing <a href="#Par205" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-56</a>.

              $  docker exec -it this_postgres bash
root@f9751d99f09d:/#
Listing 8-56.Connect to this_postgres
                



            
From within the running container, you can see its network configuration by using the same ifconfig command (Listing <a href="#Par207" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-57</a>). Note that you will first have to install the tool ifconfig using the package manager apt.

              root@f9751d99f09d:/data# apt update
Get:1 http://security.debian.org jessie/updates InRelease [63.1 kB]
...
Fetched 9949 kB in 3s (2535 kB/s)

root@f9751d99f09d:/data# apt install net-tools
...

root@f9751d99f09d:/data# ifconfig
...
eth0      Link encap:Ethernet  HWaddr 02:42:ac:11:00:04
          inet addr:172.17.0.2  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:acff:fe11:4/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:1715 errors:0 dropped:0 overruns:0 frame:0
          TX packets:492 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:10299480 (9.8 MiB)  TX bytes:35243 (34.4 KiB)
...

root@f9751d99f09d:/data# exit
                  
                
                
                      
                      
                    
                
                      
                      
                      
                    
              

Listing 8-57.Container Network Configuration



            
You can see in Listing <a href="#Par207" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-57</a> that the Postgres container has an address of 172.17.0.2 on the network on which it is running. In Listing <a href="#Par203" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-55</a>, you saw that the docker bridge network exists on 172.17.0.1 on your host machine.
Note
This inspection of the network from within the container is not necessary for configuring the network. It is done solely for demonstration purposes.

Next, in Listing <a href="#Par211" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-58</a>, you inspect the bridge network using the docker network inspect tool.

              $ docker network inspect bridge
                
              
              
                    
                    
                  
              
                    
                    
                    
                  
            
[
    {
        "Name": "bridge",
        "Id": "c9fced8bcbc9279ec29d880199e20795777c0bf9b2e7578f0d594a03981ff524",
        "Scope": "local",
        "Driver": "bridge",
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": null,
            "Config": [
                {
                    "Subnet": "172.17.0.1/16"
                }
            ]
        },
        "Internal": false,
        "Containers": {
            "f9751d99f09d691dc3f04d246c502f1fa4ff3ae059632428a65affa3e5307f17": {
                "Name": "this_postgres",
                "EndpointID": "a9820c6df3120c7fc4a98f09372b1e51252ceb937aaa18f7a9eec001cc6e2760",
                "MacAddress": "02:42:ac:11:00:04",
                "IPv4Address": "172.17.0.2/16",
                "IPv6Address": ""
            }
        },
 ...
    }
]
Listing 8-58.Inspect the Bridge Network



            
Here you can see the bridge network with address 172.17.0.1 and the Postgres container running
            
          
          
                
                
              
          
                
                
                
               on the network on 172.17.0.2.

<h3 class="heading3">最低限度地验证 Jupyter-PostgreSQL 连接</h3>
Next, you will attempt
            
          
          
                
                
                
               to connect to the running Postgres container using a Jupyter Notebook. In Listing <a href="#Par214" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-59</a>, you launch a new Jupyter container using the docker run command.

              $ docker run -v `pwd`:/home/jovyan -p 8888:8888 jupyter/scipy-notebook
Listing 8-59.Launch a Jupyter Container



            
First, you identify the running Jupyter container by using the docker ps command, as shown in Listing <a href="#Par216" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-60</a>.

              $ docker ps
CONTAINER ID IMAGE    COMMAND             CREATED        STATUS       PORTS
f9751d99f09d  postgres  "docker-entrypoint.sh" 22 minutes ago  Up 22 minutes ...
cce1148863a2 jupyter/scipy-notebook                       "tini--start-notebo" 2 minutes ago  Up 2 minutes ...
Listing 8-60.Show Running Containers



            
As previously noted, when you launch a new container, by default it will be connected to the bridge network
            
          
          
                
                
                
              .
In Listing <a href="#Par219" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-61</a>, you verify this using the docker network inspect tool.

              $ docker network inspect bridge
docker network inspect bridge
[
    {
        "Name": "bridge",
        ...
        "Containers": {
            "12fe7cea2e63b622c7804d1df96fbe2afce25d014e850b4fdec4e2e5498fde1b": {
                "Name": "this_postgres",
                "EndpointID": "a9820c6df3120c7fc4a98f09372b1e51252ceb937aaa18f7a9eec001cc6e2760",
                "MacAddress": "02:42:ac:11:00:04",
                "IPv4Address": "172.17.0.2/16",
                "IPv6Address": ""
            },
            "cce1148863a22d11272ca031ded06139b2f0372d92aca269fd0d50234a30cf1c": {
                "Name": "hungry_cray",
                    
                    
                  
              
                    
                    
                    
                  
            
                "EndpointID": "cde785070465ea79d4d9296895cb09f5975ec06a22caca090fab789ca10b1d90",
                "MacAddress": "02:42:ac:11:00:03",
                "IPv4Address": "172.17.0.3/16",
                "IPv6Address": ""
            },
        },
     ...
]
Listing 8-61.Inspect Bridge Network



            
Here you see that the Postgres
            
          
          
                
                
                
               container is still running on 172.17.0.2 and the jupyter/scipy-notebook container is running on 172.17.0.3. More importantly, because both of these containers are on the same network, you will be able to connect to one from the other. In Listing <a href="#Par221" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-62</a>, you verify this by opening a bash shell to the jupyter/scipy-notebook container and verifying that you can ping the Postgres container at its IP on the bridge network
            
          
          
                
                
                
              .

              $ docker exec -it hungry_cray bash
jovyan@cce1148863a2:∼$ ping -c 3 172.17.0.2
PING 172.17.0.2 (172.17.0.2): 56 data bytes
64 bytes from 172.17.0.2: icmp_seq=0 ttl=64 time=0.138 ms
64 bytes from 172.17.0.2: icmp_seq=1 ttl=64 time=0.103 ms
64 bytes from 172.17.0.2: icmp_seq=2 ttl=64 time=0.116 ms
--- 172.17.0.2 ping statistics ---
3 packets transmitted, 3 packets received, 0% packet loss
round-trip min/avg/max/stddev = 0.103/0.119/0.138/0.000 ms
Listing 8-62.Ping the Postgres Container from the jupyter/scipy-notebook Container



            

<h3 class="heading3">通过名称连接容器</h3>
To use the bridge network
            
          
          
                
                
                
               in order to connect to another Docker container, you are going to need to identify the IP address of that container on the bridge network, which, as you have just seen, a fairly non-trivial process. For your purposes, it may be easier to create an additional network and connect your containers to that network. If you have been following to this point, you should have two containers currently running, a jupyter/scipy-notebook and a Postgres container, as shown in Listing <a href="#Par223" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-63</a>.

              $ docker ps
CONTAINER ID IMAGE      COMMAND               CREATED          ...  NAMES
f9751d99f09d  postgres    "docker-entrypoint.sh"  22 minutes ago...  this_postgres
cce1148863a2 jupyter/scipy-notebook                          "tini -- start-notebo" 2 minutes ago... hungry_cray
Listing 8-63.Show Running Containers



            
Using the docker network tool, it will be possible to create a network and to connect these running containers to that network. Once you have connected these containers to the new network, you will be able to connect
            
          
          
                
                
                
               to them by name (by this_postgres and hungry_cray).
First, you create a new bridge network to be used by your containers (Listing <a href="#Par226" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-64</a>) using the docker network create command.

              $ docker network create jupyter_bridge
b8146c1af3a91abe4c123b9234d372e098fd71f1f0facd3e8251da2e864253ee
Listing 8-64.Create a New bridge Network



            
You can inspect the new network
            
          
          
                
                
                
               using the docker network inspect command, as shown in Listing <a href="#Par228" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-65</a>.

              $ docker network inspect jupyter_bridge
[
    {
        "Name": "jupyter_bridge",
        "Id": "b8146c1af3a91abe4c123b9234d372e098fd71f1f0facd3e8251da2e864253ee",
        "Scope": "local",
        "Driver": "bridge",
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": {},
            "Config": [
                {
                    "Subnet": "172.18.0.0/16",
                    "Gateway": "172.18.0.1"                }
            ]
        },
        "Internal": false,
        "Containers": {},
        "Options": {},
        "Labels": {}
    }
]
Listing 8-65.Inspect the New bridge Network



            
Here you see the new network
            
          
          
                
                
                
               you have created. You can see that it has no containers currently connected. In Listing <a href="#Par230" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-66</a>, you connect the jupyter/scipy-notebook container, hungry_cray, and the Postgres container, this_postgres, to the new network
            
          
          
                
                
                
               you have created, after which you inspect the network once more.

              $ docker network connect jupyter_bridge hungry_cray
$ docker network connect jupyter_bridge this_postgres
$ docker network inspect jupyter_bridge
[
    {
        "Name": "jupyter_bridge",
    ...
        "Containers": {
            "cce1148863a22d11272ca031ded06139b2f0372d92aca269fd0d50234a30cf1c": {
                "Name": "hungry_cray",
                "EndpointID": "e01136684ea417200178a0afaaf634f39cd92f332cb91b34a81dd7f7fcbbfc43",
                "MacAddress": "02:42:ac:19:00:02",
                "IPv4Address": "172.25.0.2/16",
                "IPv6Address": ""
            },
            "f9751d99f09d691dc3f04d246c502f1fa4ff3ae059632428a65affa3e5307f17": {
                "Name": "this_postgres",
                "EndpointID": "ae9a95c157092da3adbcb251ff9c370820f6b7ad9f20a8df44cb971352af4cc9",
                "MacAddress": "02:42:ac:19:00:03",
                "IPv4Address": "172.25.0.3/16",
                "IPv6Address": ""
            }
        },
    ...
    }
]
Listing 8-66.Add the Running Containers to the New Bridge Network



            
Because you have connected these two containers to the same non-default bridge network
            
          
          
                
                
                
              , they will be able to resolve each other’s location by container name
            
          
          
                
                
                
              , as shown in Listing <a href="#Par232" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-67</a>.

              $ docker exec -it hungry_cray bash
jovyan@cce1148863a2:∼$ ping -c 3 this_postgres
PING this_redis (172.25.0.3): 56 data bytes
64 bytes from 172.25.0.3: icmp_seq=0 ttl=64 time=0.248 ms
64 bytes from 172.25.0.3: icmp_seq=1 ttl=64 time=0.082 ms
64 bytes from 172.25.0.3: icmp_seq=2 ttl=64 time=0.081 ms
--- this_redis ping statistics ---
3 packets transmitted, 3 packets received, 0% packet loss
round-trip min/avg/max/stddev = 0.081/0.137/0.248/0.078 ms
Listing 8-67.Ping the Postgres Container from the jupyter/scipy-notebook Container



            
Note
Containers that are solely linked by the default bridge network will not be able to resolve each other’s container name.


<h3 class="heading3">对 jupiter 使用 postgresql</h3>
You will use the same established pattern to connect to PostgreSQL
            
          
          
                
                
                
               from within a Jupyter notebook. You will first perform a !pip install of the psycopg2 library You will use to connect to the PostgreSQL database (Listing <a href="#Par235" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-68</a>).

              In [1]: !pip install psycopg2
        Collecting psycopg2
          Downloading psycopg2-2.7.1-cp35-cp35m-manylinux1_x86_64.whl (2.7MB)
            100% |████████████████████████████████| 2.7MB 333kB/s eta 0:00:01
        Installing collected packages: psycopg2
        Successfully installed psycopg2-2.7.1
Listing 8-68.Install psycopg2 via a Shell Call from a Jupyter Notebook



            

<h3 class="heading3">Jupyter、PostgreSQL、Pandas 和 psycopg2</h3>

          Pandas, as nearly every other technology
            
          
          
                
                
                
               referenced in this tome, is an open source library. pandas is the Python data analysis library. It plays well with the entire numerical Python stack from numpy to scikit-learn. It is intuitive and easy to use and has a place on the tool belt of every data scientist. Heck, it has even been known to make the R programmer more comfortable in Python land. Here, you will use pandas where appropriate to supplement psycopg2’s connection to PostgreSQL.

<h3 class="heading3">最小验证</h3>
You will start out by performing a minimal verification
            
          
          
                
                
                
               of your connection. You will
<ol class="calibre7"><li class="listitem">1.为您的工作导入几个库(清单<a href="#Par244" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"> 8-69 </a>)。</li>
<li class="listitem">2.实例化一个连接和一个附加到该连接的光标(清单<a href="#Par245" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"> 8-70 </a>)。</li>
<li class="listitem">3.使用光标执行查询(清单<a href="#Par246" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"> 8-71 </a>)。</li>
<li class="listitem">4.在熊猫中显示查询结果。数据帧(列表<a href="#Par247" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"> 8-72 </a>)。</li>
<li class="listitem">5.关闭连接(列表<a href="#Par248" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"> 8-73 </a>)。</li>
</ol>

        
Note
You import an additional module in Listing <a href="#Par244" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-69</a>, psycopg2.extras. You will use this module in Listing <a href="#Par245" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-70</a> to specify that you wish to use a special cursor type, the psycopg2.extras.RealDictCursor. Using this cursor type will allow you to easily pass the results of your query to a pandas.DataFrame, passing column names seamlessly from the database to the data frame.


              In [2]: import pandas as pd
        import psycopg2 as pg2
        import psycopg2.extras as pgex
Listing 8-69.Import Necessary Libraries



            

              In [3]: con = pg2.connect(host='this_postgres', user='postgres', database='postgres')
        cur = con.cursor(cursor_factory=pgex.RealDictCursor)
Listing 8-70.Instantiate Connection and Cursor



            

              In [4]: cur.execute("SELECT * FROM test;")
Listing 8-71.Execute Query
                    
                  
                  
                        
                        
                        
                      
                



            

              In [5]: pd.DataFrame(cur.fetchall())
Out[5]:         _id     name
        0       1       Joshua
Listing 8-72.Fetch All Results from the Query and Display in a pandas.DataFrame
                



            

              In [6]: con.close()
Listing 8-73.Close the Connection



            

<h3 class="heading3">将数据加载到 PostgreSQL</h3>
Loading data into PostgreSQL
            
          
          
                
                
                
               can be challenging. In later chapters, you will explore the use of Dockerfiles to load data and hold that this is the best practice. For the quick insertion of data, such as is necessary for exploring some minimal Jupyter-Postgres interaction in the remainder of this chapter, you will use simple SQL statements from Jupyter and manage your transactions manually via BEGIN and COMMIT.
In Listing <a href="#Par251" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-74</a>, you create a new table called from_jupyter_test with four data types: INTEGER, TEXT, DOUBLE PRECISION[], BYTEA. While readers are no doubt familiar with the first two data types, at this time I call special attention to the second two. The trailing brackets on the DOUBLE PRECISION[] type signifies that this is an array of DOUBLE PRECISION (floating-point) values. BYTEA is a binary type and you will use it to hold numpy arrays much as you did earlier with Redis.

              In [7]: con = pg2.connect(host='this_postgres', user='postgres', database='postgres')
        cur = con.cursor(cursor_factory=pgex.RealDictCursor)
        cur.execute("""
        BEGIN;
        CREATE TABLE from_jupyter_test (
            _id INTEGER,
            name TEXT,
            list DOUBLE PRECISION[],
            vector BYTEA
                
              
              
                    
                    
                    
                  
            
        );
        COMMIT;
        """)
Listing 8-74.Create a Table in PostgreSQL from Juptyer



            
In Listing <a href="#Par253" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-75</a>, you insert two rows into the database. Note the special handling of a list type on insertion. Note also that you have only inserted three values. By default these will align with the first three columns in the table.

              In [8]: cur.execute("""
        BEGIN;
        INSERT INTO from_jupyter_test VALUES (1, 'spam', '{1,2,3,4,5}');
        INSERT INTO from_jupyter_test VALUES (2, 'eggs', '{1,4,9,16,25}');
        COMMIT;
        """)
Listing 8-75.Insert Two Rows into PostgreSQL



            
Finally, you query the from_jupyter_test table and display the results in a pandas.
          
                DataFrame
                
                
              
          
                
                
                
               (Listing <a href="#Par255" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-76</a>), before closing the connection (Listing <a href="#Par256" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8-77</a>).

              In [9]: cur.execute("""
        SELECT * FROM from_jupyter_test;""")
        pd.DataFrame(cur.fetchall())
Out[9]:         _id     list                            name    vector
        0       1       [1.0, 2.0, 3.0, 4.0, 5.0]       spam    None
        1       2       [1.0, 4.0, 9.0, 16.0, 25.0]     eggs    None
Listing 8-76.Select All Rows from_jupyter_test and Display in a pandas.DataFrame
                



            

              In [10]: con.close()
Listing 8-77.Close the Connection
                    
                  
                  
                        
                        
                        
                      
                



            

<h3 class="heading3">PostgreSQL 二进制类型和数量</h3>
Like Redis, PostgreSQL has a native binary type
            
          
          
                
                
                
              , BYTEA, making it ideal for computational work with numpy. Converting to and from this type is a reasonably straightforward process. Here, you
<ol class="calibre7"><li class="listitem">1.查询 PostgreSQL 数据库中的所有列表(列表<a href="#Par264" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"> 8-78 </a>)并将它们显示为本地 Python 对象。</li>
<li class="listitem">2.将这些列表转换成 numpy.array 对象(清单<a href="#Par265" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"> 8-79 </a>)。</li>
<li class="listitem">3.使用 psycopg2 将 numpy.array 对象转换为 PostgreSQL 二进制对象。二进制(列表<a href="#Par266" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"> 8-80 </a>)。</li>
<li class="listitem">4.在数据库上执行两次 SQL 更新，将表中的向量值设置为各自的二进制对象(清单<a href="#Par267" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"> 8-81 </a>)。</li>
<li class="listitem">5.查询所有值并显示在熊猫中。数据帧(列表<a href="#Par268" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"> 8-82 </a>)。</li>
<li class="listitem">6.查询向量值，一次取一个，并将其转换回 numpy.array 对象(清单<a href="#Par269" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1"> 8-83 </a>)。</li>
</ol>

        

              In [11]: cur.execute("""
         SELECT list FROM from_jupyter_test;""")
         results = cur.fetchall()
         results
                
              
              
                    
                    
                    
                  
            
Out[11]: [{'list': [1.0, 2.0, 3.0, 4.0, 5.0]}, {'list': [1.0, 4.0, 9.0, 16.0, 25.0]}]
Listing 8-78.Query List Values and Display Results



            

              In [12]: import numpy as np
         ary_1 = np.array(results[0]['list'])
         ary_2 = np.array(results[1]['list'])
Listing 8-79.Convert Lists to numpy.array Objects



            

              In [13]: bin_ary_1 = pg2.Binary(ary_1)
         bin_ary_2 = pg2.Binary(ary_2)
Listing 8-80.Convert numpy.array Objects to Binary Objects



            

              In [14]: update_sql = """
         BEGIN;
         UPDATE from_jupyter_test
                  
                
                
                      
                      
                      
                    
              
         SET vector = {}
         WHERE _id={};
         COMMIT;
         """.format(pg2.Binary(bin_ary_1), 1)
         cur.execute(update_sql)

In [15]: update_sql = """
         BEGIN;
         UPDATE from_jupyter_test
         SET vector = {}
         WHERE _id={};
         COMMIT;
         """.format(pg2.Binary(bin_ary_2), 2)
         cur.execute(update_sql)

Listing 8-81.Perform SQL Updates



            

              In [16]: cur.execute("""
         SELECT * FROM from_jupyter_test;""")
         pd.DataFrame(cur.fetchall())
Out[16]:     _id     list                     name   vector
       0         1     [1.0, 2.0, 3.0, 4.0, 5.0]    spam    [b'\x00', b'\x00', ...
       1         2     [1.0, 4.0, 9.0, 16.0, 25.0]  eggs    [b'\x00', b'\x00', ...
Listing 8-82.Query and Display All Values



            

              In [17]: cur.execute("""
         SELECT vector FROM from_jupyter_test;""")
         result = cur.fetchone()
         result
                  
                
                
                      
                      
                      
                    
              
Out[17]: {'vector': &lt;memory at 0x7f8fd643b708&gt;}

In [18]: np.frombuffer(result['vector'])
Out[18]: array([ 1.,  2.,  3.,  4.,  5.])

In [19]: result = cur.fetchone()

In [20]: np.frombuffer(result['vector'])
Out[20]: array([ 1.,   4.,   9.,  16.,  25.])

Listing 8-83.Convert Vector Value to numpy.array
                



            


<h2 class="heading2">摘要</h2>
This is one of the longest chapters in the book and represents a significant departure from the material to this point. I introduced three data stores (Redis, MongoDB, and PostgreSQL) and discussed how to use them with Docker, especially with regard to data persistence and networking. I discussed ways in which numpy vectors can be serialized for storage in both Redis and PostgreSQL. I introduced Docker Volumes and Docker Networks.
This is a chapter that bears repeated readings and that I hope will serve as a reference for future projects. I hope that you complete the chapter with an understanding of when each of the three data stores might be used, and I hope you’re confident that you will be able to configure any one of them using this chapter as reference.
We are drawing near to the end of the text. In the last two chapters, we will make extensive use of these data stores, looking at how we can use Docker Compose to build larger applications and finally revisiting the idea of interactive programming and what it might look like to build software under this paradigm.

Footnotes
<a href="#Fn1_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">1</a>
                <a href="https://redis.io" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">
                    https://redis.io
                  </a>
              

 

<a href="#Fn2_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">2</a>
                  <a href="http://hub.docker.com/_/redis" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">
                      http://hub.docker.com/_/redis
                    </a>
                

 

<a href="#Fn3_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">3</a>
                <a href="https://docs.docker.com/edge/engine/reference/commandline/volume/" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">
                    https://docs.docker.com/edge/engine/reference/commandline/volume/
                  </a>
              

 

<a href="#Fn4_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">4</a>
                    <a href="https://hub.docker.com/_/redis/" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">
                        https://hub.docker.com/_/redis/
                      </a>
                  

 

<a href="#Fn5_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">5</a>
                  <a href="http://tldp.org/LDP/abs/html/ivr.html" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">
                      http://tldp.org/LDP/abs/html/ivr.html
                    </a>
                

 

<a href="#Fn6_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">6</a>
                  <a href="http://ipython.readthedocs.io/en/stable/interactive/python-ipython-diff.html?highlight=!ls#quick-overview" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">
                      http://ipython.readthedocs.io/en/stable/interactive/python-ipython-diff.html?highlight=!ls#quick-overview
                    </a>
                

 

<a href="#Fn7_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">7</a>
                  <a href="https://hub.docker.com/_/mongo/" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">
                      https://hub.docker.com/_/mongo/
                    </a>
                

 

<a href="#Fn8_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">8</a>
                    <a href="https://hub.docker.com/_/mongo/" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">
                        https://hub.docker.com/_/mongo/
                      </a>
                  

 

<a href="#Fn9_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">9</a>
                  <a href="https://docs.mongodb.com/manual/core/document/#bson-document-format" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">
                      https://docs.mongodb.com/manual/core/document/#bson-document-format
                    </a>
                

 

<a href="#Fn10_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">10</a>
                  <a href="https://docs.mongodb.com/manual/reference/bson-types/" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">
                      https://docs.mongodb.com/manual/reference/bson-types/
                    </a>
                

 

<a href="#Fn11_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">11</a>
                  <a href="http://api.mongodb.com/python/current/" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">
                      http://api.mongodb.com/python/current/
                    </a>
                

 

<a href="#Fn12_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">12</a>
                  <a href="https://dev.twitter.com/streaming/public" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">
                      https://dev.twitter.com/streaming/public
                    </a>
                

 

<a href="#Fn13_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">13</a>
                  <a href="https://dev.twitter.com/streaming/overview/request-parameters#locations" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">
                      https://dev.twitter.com/streaming/overview/request-parameters#locations
                    </a>
                

 

<a href="#Fn14_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">14</a>
                  <a href="http://wiki.openstreetmap.org/wiki/Bounding_Box" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">
                      http://wiki.openstreetmap.org/wiki/Bounding_Box
                    </a>
                

 

<a href="#Fn15_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">15</a>
                  <a href="http://boundingbox.klokantech.com" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">
                      http://boundingbox.klokantech.com
                    </a>
                

 

<a href="#Fn16_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">16</a>
                  <a href="https://github.com/sixohsix/twitter#the-twitterstream-class" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">
                      https://github.com/sixohsix/twitter#the-twitterstream-class
                    </a>
                

 

<a href="#Fn17_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">17</a>
                  <a href="https://docs.python.org/3/howto/functional.html?#functional-howto-iterators" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">
                      https://docs.python.org/3/howto/functional.html?#functional-howto-iterators
                    </a>
                

 

<a href="#Fn18_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">18</a>
                <a href="http://www.postgresql.org/about/" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">
                    www.postgresql.org/about/
                  </a>
              

 

<a href="#Fn19_source" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">19</a>
                  <a href="https://hub.docker.com/_/postgres/" class="calibre3 pcalibre pcalibre4 pcalibre3 pcalibre2 pcalibre1">
                      https://hub.docker.com/_/postgres/
                    </a>
                

 




</body>
</html>