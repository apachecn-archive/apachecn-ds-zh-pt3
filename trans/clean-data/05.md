

# 五、从网络上收集和清理数据

最常见和最有用的厨房工具之一是过滤器，也称为筛子、漏勺或 chinois，其目的是在烹饪过程中从液体中分离固体。在这一章中，我们将为我们在网上找到的数据建立过滤器。我们将学习如何创建几种类型的程序，帮助我们找到并保存我们想要的数据，同时丢弃我们不想要的部分。

在本章中，我们将:

*   理解设想 HTML 页面结构的两种选择，或者(a)作为我们可以从中寻找模式的行的集合，或者(b)作为包含我们可以识别和收集值的节点的树结构。
*   尝试三种解析网页的方法，一种使用逐行方法(基于正则表达式的 HTML 解析)，两种使用树结构方法(Python 的 BeautifulSoup 库和 Chrome 浏览器工具 Scraper)。
*   在一些真实世界的数据上实现所有这三种技术；我们将练习从张贴到网络论坛的消息中刮出日期和时间。

# 了解 HTML 页面结构

web 页面只是一个文本文件，其中包含一些特殊的标记**元素**(有时称为 HTML **标签**)，旨在向 web 浏览器指示页面在显示给用户时的外观，例如，如果我们希望某个特定的单词以强调的方式显示，我们可以用如下的`<em>`标签将其包围:

你必须遵循这些指示。

所有的网页都有这些相同的特征；它们由文本组成，并且文本可以包括标签。有两种主要的心智模型可以用来从网页中提取数据。这两种模式都有其有用的方面。在这一节中，我们将描述两个结构化的模型，然后在下一节中，我们将使用三种不同的工具来提取数据。

## 逐行分隔符模式

用最简单的方式思考网页，我们关注这样一个事实:有许多 HTML 元素/标签被用来组织和显示网页。如果我们想在这个简单的模型中从网页中提取有趣的数据，我们可以使用页面文本和嵌入的 HTML 元素本身作为**分隔符**。例如，在前面的例子中，我们可能决定我们想要收集在`<em>`标签内的所有东西，或者我们可能想要收集在`<em>`标签之前或者在`</em>`标签之后的所有东西。

在这个模型中，我们将网页看作是大量非结构化文本的集合，HTML 标签(或文本的其他特征，如重复出现的单词)有助于提供结构，我们可以用它来划分我们想要的部分。一旦我们有了分隔符，我们就有能力从垃圾中过滤出感兴趣的数据。

例如，下面是一个真实世界 HTML 页面的摘录，一个来自 Django IRC 频道的聊天日志。让我们考虑如何使用它的 HTML 元素作为分隔符来提取感兴趣的数据:

```
<div id="content">
<h2>Sep 13, 2014</h2>

<a href="/2014/sep/14/">← next day</a> Sep 13, 2014  <a href="/2014/sep/12/">previous day →</a>

<ul id="ll">
<li class="le" rel="petisnnake"><a href="#1574618" name="1574618">#</a> <span style="color:#b78a0f;8" class="username" rel="petisnnake">&lt;petisnnake&gt;</span> i didnt know that </li>
...
</ul>
...
</div>
```

给定这个示例文本，我们可以使用`<h2></h2>`标签作为分隔符来提取这个特定聊天日志的日期。我们可以使用`<li></li>`标签作为一行文本的分隔符，在这一行中，我们可以看到`rel=""`可以用于提取 chatter 的用户名。最后，从`</span>`结尾到`</li>`开头的所有文本似乎都是用户发送到聊天频道的实际行消息。

### 注意

这些聊天记录都可以在 Django IRC 日志网站上找到，http://django-irc-logs.com T2 T3。该网站还提供了日志的关键字搜索界面。前面代码中的省略号(`…`)表示本例中为简洁起见而删除的文本。

从这个杂乱的文本中，我们能够使用分隔符概念提取三个整洁的数据片段(日志日期、用户和行消息)。

## 树形结构模型

另一种想象文本网页的方式是由 HTML 元素/标签组成的树形结构，每个元素/标签都与页面上的其他标签相关。每个标签显示为一个**节点**，一个树由特定页面中的所有不同节点组成。出现在 HTML 中另一个标签内的标签被认为是**子标签**，而包围标签是**父标签**。在前面的 IRC 聊天示例中，HTML 摘录可以显示在树形图中，如下所示:

![The tree structure model](graphics/4276OT_05_01.jpg)

如果我们能够把我们的 HTML 文本想象成一个树形结构，我们就可以用编程语言来为自己构建一棵树。这允许我们根据元素名称或在元素列表中的位置提取所需的文本值。例如:

*   我们可能希望通过名称得到标签的值(给我来自`<h2>`节点的文本)
*   我们可能想要特定类型的所有节点(给我所有在`<ul>`内的`<li>`节点，它们在`<div>`内)
*   我们可能想要一个给定元素的所有属性(给我一个来自`<li>`元素的`rel`属性列表)

在本章的剩余部分，我们将通过一些例子来实践这两种心智模型——逐行和树形结构。我们将通过三种不同的方法来提取和清理 HTML 页面中的数据。



# 方法一——Python 和正则表达式

在这一节中，我们将使用一个简单的方法从 HTML 页面中提取我们想要的数据。这种方法基于这样的概念，即识别页面中的分隔符，并通过正则表达式使用模式匹配来提取我们想要的数据。

你可能还记得，当我们学习文本编辑器时，我们在第三章、*中试验了一下正则表达式(regex)。在这一章中，一些概念将是相似的，除了我们将写一个 Python 程序来寻找匹配的文本并提取它，而不是像我们在那一章中那样使用文本编辑器进行替换。*

在我们开始这个例子之前，最后要注意的是，尽管这个 regex 方法很容易理解，但是它确实有一些限制，这可能很重要，这取决于您的特定项目。我们将在本节的最后详细描述这种方法的局限性。

## 第一步——找到并保存一个网络文件以供实验

对于这个例子，我们将从 Django 项目中获取一个之前提到的 IRC 聊天日志。这些是公开可用的文件，具有相当规则的结构，因此它们是这个项目的理想目标。去 http://django-irc-logs.com/[的 Django IRC 日志存档](http://django-irc-logs.com/)找一个看起来对你有吸引力的日期。导航到其中一个日期的目标页面，并将其保存到您的工作目录。完成后，您应该有一个单独的`.html`文件。

## 第二步——查看文件，决定哪些内容值得提取

由于我们在[第 2 章](ch02.html "Chapter 2. Fundamentals – Formats, Types, and Encodings")、*基础——格式、类型和编码*中了解到`.html`文件只是文本，而[第 3 章](ch03.html "Chapter 3. Workhorses of Clean Data – Spreadsheets and Text Editors")、*整洁数据的主要工具——电子表格和文本编辑器*，使我们能够非常轻松地在文本编辑器中查看文本文件，这一步应该很容易。只需在文本编辑器中打开 HTML 文件并查看它。什么看起来已经成熟可以提取了？

当我查看我的文件时，我看到了一些我想摘录的东西。我马上看到每个聊天评论都有行号、用户名和评论本身。让我们计划从每个聊天行中提取这三样东西。

下图显示了在我的文本编辑器中打开的 HTML 文件。我已经打开了软换行，因为有些行相当长(在 TextWrangler 中，该选项位于菜单中的**视图** | **文本显示** | **软换行文本**)。在第 **29** 行周围，我们看到一个聊天行列表的开头，每一行都有我们感兴趣的三个项目:

![Step two – look into the file and decide what is worth extracting](graphics/4276OT_05_02.jpg)

因此，我们的工作是找到每一行看起来相同的特征，这样我们就可以预测地从每一个聊天行中提取出相同的三个项目。查看文本，下面是我们可以遵循的一些可能的规则，以准确地提取每个数据项，并进行最小的调整:

*   看起来我们想要的三个项目都在`<li>`标记中找到了，而它们本身又在`<ul id="ll">`标记中找到了。每个`<li>`代表一条聊天信息。
*   在该消息中，行号位于两个地方:它跟在字符串`<a href="#`后面，它在属性`name`后面的引号中。在所示的示例文本中，第一个行号是`1574618`。
*   在三个地方可以找到`username`属性，第一个是作为`li class="le"`的`rel`属性的值。在`span`标签内，`username`属性再次被发现作为`rel`属性的值，同样在`&lt`之间被发现；和`&gt`；符号。在示例文本中，第一个`username`是`petisnnake`。
*   该行消息位于`</span>`标签之后和`</li>`标签之前。在所示的例子中，第一行消息是`i didnt know that`。

现在我们有了在哪里找到数据项的规则，我们可以写程序了。

## 第三步——编写一个 Python 程序来提取有趣的部分，并保存到一个 CSV 文件中

下面是一小段代码，它以前面显示的格式打开一个给定的 IRC 日志文件，解析出我们感兴趣的三个部分，并将它们打印到一个新的 CSV 文件中:

```
import re
import io

row = []

infile  = io.open('django13-sept-2014.html', 'r', encoding='utf8')
outfile = io.open('django13-sept-2014.csv', 'a+', encoding='utf8')
for line in infile:
    pattern = re.compile(ur'<li class=\"le\" rel=\"(.+?)\"><a href=\"#(.+?)\" name=\"(.+?)<\/span> (.+?)</li>', re.UNICODE)
    if pattern.search(line):
        username = pattern.search(line).group(1)
        linenum = pattern.search(line).group(2)
        message = pattern.search(line).group(4)
        row.append(linenum)
        row.append(username)
        row.append(message)
        outfile.write(', '.join(row))
        outfile.write(u'\n')
        row = []
infile.close()
```

这段代码中最棘手的部分是`pattern`行。这一行构建模式匹配，文件的每一行都将与之进行比较。

### Tip

保持警惕。每当网站开发人员更改页面中的 HTML 时，我们都面临着精心构建的正则表达式模式不再有效的风险。事实上，在写这本书的几个月里，这个页面的 HTML 至少改变了一次！

每个匹配的目标组看起来是这样的:`.+?`。他们有五个人。其中三组是我们感兴趣的项目(`username`、`linenum`和`message`，而另外两组只是我们可以丢弃的垃圾。我们还将丢弃其余的网页内容，因为这与我们的模式完全不匹配。我们的程序就像一个筛子，上面正好有三个功能漏洞。好的东西会从洞里流出来，留下垃圾。

## 第四步——查看文件并确保它是整洁的

当我们在文本编辑器中打开新的 CSV 文件时，我们可以看到前几行如下所示:

```
1574618, petisnnake, i didnt know that 
1574619, dshap, FunkyBob: ahh, hmm, i wonder if there's a way to do it in my QuerySet subclass so i'm not creating a new manager subclass *only* for get_queryset to do the intiial filtering 
1574620, petisnnake, haven used Django since 1.5
```

这看起来是一个可靠的结果。您可能会注意到，第三列中的文本周围没有封装字符。这可能会成为一个问题，因为我们使用逗号作为分隔符。如果我们在第三列中有逗号呢？如果这让您担心，您可以在第三列周围添加引号，或者使用制表符来分隔列。为此，将第一个`outfile.write()`行改为使用`\t` (tab)作为连接字符，而不是逗号。您还可以通过`ltrim()`向消息中添加一些空白修整，以删除任何多余的字符。

## 使用正则表达式解析 HTML 的局限性

这个正则表达式方法起初看起来非常简单，但是它有一些限制。首先，对于新的数据清理器来说，正则表达式的设计和完善可能有点令人头疼。你一定要计划花大量的时间调试和编写大量的文档。为了帮助生成正则表达式，我肯定会推荐使用一个正则表达式测试器，比如[http://Pythex.org](http://Pythex.org)，或者只是使用你最喜欢的搜索引擎来找到一个。如果这是您正在使用的语言，请确保您指定了您想要一个 Python 正则表达式测试器。

接下来，你应该提前知道正则表达式完全依赖于将来保持不变的网页结构。因此，如果您计划按计划从网站收集数据，您今天编写的正则表达式明天可能就不工作了。它们只有在页面布局不变的情况下才起作用。在两个标记之间添加一个空格将导致整个正则表达式失败，并且极难排除故障。请记住，大多数时候你很少或根本无法控制网站的变化，因为你通常不是从你自己的网站上收集数据！

最后，在很多很多情况下，几乎不可能准确地编写正则表达式来匹配给定的 HTML 结构。Regex 功能强大，但并不完美或绝对可靠。关于这个问题，有趣的是，我向你推荐著名的栈溢出答案，这个答案已经被投票超过 4000 次:[http://stackoverflow.com/questions/1732348/](http://stackoverflow.com/questions/1732348/)。在这个回答中，作者幽默地表达了许多程序员的沮丧，他们一遍又一遍地试图解释为什么 regex 不是解析网页中不规则且不断变化的 HTML 的完美解决方案。



# 方法二——Python 和 BeautifulSoup

由于正则表达式有一些限制，我们的数据清理工具包中肯定需要更多的工具。在这里，我们描述如何使用名为 **BeautifulSoup** 的基于解析树的 Python 库从 HTML 页面中提取数据。

## 第一步——找到并保存一个实验文件

对于这个步骤，我们将使用与方法 1 相同的文件:来自 Django IRC 通道的文件。我们将搜索相同的三个项目。这样做可以很容易地比较这两种方法。

## 第二步——安装 BeautifulSoup

BeautifulSoup 目前在版本 4 中。这个版本适用于 Python 2.7 和 Python 3。

### 注意

如果您正在使用 Enthought Canopy Python 环境，只需在 Canopy 终端中运行`pip install beautifulsoup4`。

## 第三步——编写一个 Python 程序来提取数据

我们感兴趣的三个项目是在一组`li`标签中找到的，特别是那些带有`class="le"`的项目。在这个特定的文件中没有任何其他的`li`标签，但是让我们具体一点以防万一。以下是我们想要的项目以及在解析树中的位置:

*   我们可以从`rel`属性下的`li`标签中提取用户名。
*   We can get the `linenum` value from the `name` attribute in the `a` tag. The `a` tag is also the first item in the contents of the `li` tag.

    ### 注意

    记住数组是从零开始的，所以我们需要 0 项。

    在 BeautifulSoup 中，标签的 **内容**是解析树中该标签下的项目。其他一些软件包会称它们为**子**项目。

*   我们可以提取消息作为`li`标签中的第四个内容项(引用为数组项[3])。我们还注意到在每条消息的前面都有一个前导空格，所以我们想在保存数据之前去掉它。

下面是 Python 代码，它对应于我们希望从解析树中得到的内容:

```
from bs4 import BeautifulSoup
import io

infile  = io.open('django13-sept-2014.html', 'r', encoding='utf8')
outfile = io.open('django13-sept-2014.csv', 'a+', encoding='utf8')
soup = BeautifulSoup(infile)

row = []
allLines = soup.findAll("li","le")
for line in allLines:
    username = line['rel']
    linenum = line.contents[0]['name']
    message = line.contents[3].lstrip()
    row.append(linenum)
    row.append(username)
    row.append(message)
    outfile.write(', '.join(row))
    outfile.write(u'\n')
    row = []
infile.close()
```

## 第四步——查看文件并确保它是整洁的

当我们在文本编辑器中打开新的 CSV 文件时，我们可以看到前几行看起来与方法 1 中的一样:

```
1574618, petisnnake, i didnt know that 
1574619, dshap, FunkyBob: ahh, hmm, i wonder if there's a way to do it in my QuerySet subclass so i'm not creating a new manager subclass *only* for get_queryset to do the intiial filtering 
1574620, petisnnake, haven used Django since 1.5
```

就像正则表达式方法一样，如果担心最后一列中嵌入逗号，可以将列文本用引号括起来，或者只使用制表符来分隔列。



# 方法三——镀铬刮刀

如果你真的不想写一个解析数据的程序，有几个基于浏览器的工具，它们使用一个树形结构来允许你识别和提取你感兴趣的数据。我认为最简单易用的是一个 Chrome 扩展叫做**刮刀**，由一个叫做 **mnmldave** 的开发者创建(真实姓名:**戴夫·希顿**)。

## 第一步——安装刮刀镀铬延伸件

如果你还没有运行 Chrome 浏览器，请下载。确保获得正确的刮刀加长件；有几个扩展名具有非常相似的名称。我推荐使用开发者自己的 GitHub 网站，可以在 http://mnmldave.github.io/scraper/[的](http://mnmldave.github.io/scraper/)找到。这样你将能够拥有正确的刮刀工具，而不是试图使用 Chrome 商店进行搜索。从 http://mmldave.github.io/scraper 的[网站，点击链接安装谷歌商店的扩展，然后重启你的浏览器。](http://mmldave.github.io/scraper)

## 第二步——从网站上收集数据

将你的浏览器指向我们用来获取另外两个 web 数据提取实验数据的同一个 web URL，一个 Django IRC 日志。这里的例子和截图我一直用的是 2014 年 9 月 13 日的日志，所以我会去[http://django-irc-logs.com/2014/sep/13/](http://django-irc-logs.com/2014/sep/13/)。

在我的浏览器中，在我写作的时候，这个页面看起来是这样的:

![Step two – collect data from the website](graphics/4276OT_05_03.jpg)

我们对这个 IRC 日志中的三个项目感兴趣:

*   行号(从我们之前的两个实验中我们知道这是在 **#** 符号下面的链接的一部分)
*   用户名(位于 **<** 和 **>** 符号之间)
*   实际的行消息

Scraper 允许我们依次突出显示这三个项目，并将值导出到 Google 电子表格中，然后我们可以将它们重新组合成一个工作表，并导出为 CSV 格式(或者对它们做我们想做的任何事情)。下面是如何做到这一点:

1.  用鼠标突出显示要刮的物品。
2.  点击右键，从菜单中选择**刮相似…** 。在下面的例子中，我选择了用户名 **petisnnake** 作为我希望 Scraper 使用的用户名:![Step two – collect data from the website](graphics/4276OT_05_04.jpg)
3.  After selecting **Scrape similar**, the tool will show you a new window with a collection of all the similar items from the page. The following screenshot shows the entire list of usernames that Scraper found:![Step two – collect data from the website](graphics/4276OT_05_07.jpg)

    Scraper 根据一个示例用户名找到所有相似的项目。

4.  在窗口的底部，有一个标签为**导出到谷歌文档……**的按钮。请注意，根据您的设置，您可能需要点击以同意从 Scraper 中访问 Google Docs。

## 第三步——对数据列进行最终清理

一旦我们从页面中提取了所有的数据元素并存放在单独的谷歌文档中，我们将需要把它们合并到一个文件中，并做一些最后的清理。以下是提取行号后，但在我们清理它们之前，行号的外观示例:

![Step three – final cleaning on the data columns](graphics/4276OT_05_05.jpg)

我们对列 **A** 一点也不感兴趣，对领先的 **#** 符号也不感兴趣。用户名和行消息数据是相似的——我们想要它的大部分，但我们想删除一些符号，并将所有内容合并到一个单一的谷歌电子表格中。

使用我们在[第 3 章](ch03.html "Chapter 3. Workhorses of Clean Data – Spreadsheets and Text Editors")、*中的查找和替换技术来清理数据——电子表格和文本编辑器*(即删除 **#** 、 **<** 和 **>** 符号并将这些行粘贴到单个表中)，我们最终得到一个如下所示的整洁数据集:

![Step three – final cleaning on the data columns](graphics/4276OT_05_06.jpg)

Scraper 是从网页中提取少量数据的好工具。它有一个方便的谷歌电子表格界面，如果你不想写一个程序来做这项工作，它可以是一个快速的解决方案。在下一部分，我们将处理一个更大的项目。这可能非常复杂，以至于我们必须将本章中的一些概念实现到一个单一的解决方案中。



# 示例项目–从电子邮件和网络论坛中提取数据

Django IRC 日志项目非常简单。它旨在向您展示常用于从 HTML 页面中提取整洁数据的三种可靠技术之间的差异。我们提取的数据包括行号、用户名和 IRC 聊天消息，所有这些都很容易找到，几乎不需要额外的清理。在这个新的示例项目中，我们将考虑一个概念上类似的情况，但是这需要我们将数据提取的概念从 HTML 扩展到 web 上的另外两种类型的半结构化文本:Web 上托管的电子邮件和基于 Web 的论坛。

## 项目背景

我最近在做一项关于如何利用社交媒体提供软件技术支持的研究。具体来说，我试图发现某些类型的开发 API 和框架的软件开发组织是否应该将他们对开发人员的技术支持转移到 Stack Overflow，或者他们是否应该继续使用旧的媒体，如电子邮件和 web 论坛。为了完成这项研究，我比较了(除了别的以外)开发人员通过栈溢出获得他们的 API 问题的答案所花的时间，以及在 web 论坛和电子邮件群等旧的社交媒体上所花的时间。

在这个项目中，我们将研究这个问题的一小部分。我们将下载两种代表旧社交媒体的原始数据:来自网络论坛的 HTML 文件和来自 Google Groups 的电子邮件消息。我们将编写 Python 代码来提取发送到这两个支持论坛的消息的日期和时间。然后，我们将计算出哪些消息是对其他消息的回复，并计算一些关于每条消息得到回复所用时间的基本汇总统计数据。

### Tip

如果你想知道为什么我们不为这个示例项目中的问题的栈溢出部分提取数据，那就等到第 9 章、*栈溢出项目*吧。那一整章都致力于创建和清理栈溢出数据库。

该项目将分为两部分。在第一部分中，我们将从 Google Groups 托管的项目的电子邮件存档中提取数据，在第二部分中，我们将从另一个项目的 HTML 文件中提取数据。

## 第一部分——清除谷歌集团电子邮件中的数据

许多软件公司传统上使用电子邮件列表或混合电子邮件-网络论坛来为他们的产品提供技术支持。Google Groups 是这项服务的热门选择。用户既可以给这个小组发电子邮件，也可以在网络浏览器中阅读和搜索邮件。然而，一些公司已经不再通过谷歌团队(包括谷歌产品本身)向开发者提供技术支持，而是使用栈溢出。数据库产品 Google BigQuery 就是这样一个现在使用栈溢出的团体。

### 第一步——收集谷歌群组信息

为了研究 BigQuery Google Group 上的问题的响应时间，我首先创建了该组所有帖子的 URL 列表。你可以在我的 GitHub 网站上找到我的完整网址列表:[https://GitHub . com/Megan squire/stack paper 2015/blob/master/bigqueryggurls . txt](https://github.com/megansquire/stackpaper2015/blob/master/BigQueryGGurls.txt)。

一旦我们有了目标 URL 列表，我们就可以编写一个 Python 程序来下载驻留在这些 URL 中的所有电子邮件，并将它们保存到磁盘上。在下面的程序中，我的 URL 列表被保存为名为`GGurls.txt`的文件。包含了`time`库，所以我们可以在对 Google Groups 服务器的请求之间采用一个简短的`sleep()`方法:

```
import urllib2
import time

with open('GGurls.txt', 'r') as f:
    urls = []
    for url in f:
        urls.append(url.strip())

currentFileNum = 1
for url in urls:
    print("Downloading: {0} Number: {1}".format(url, currentFileNum))
    time.sleep(2)
    htmlFile = urllib2.urlopen(url)
    urlFile = open("msg%d.txt" %currentFileNum,'wb')
    urlFile.write(htmlFile.read())
    urlFile.close()
    currentFileNum = currentFileNum +1
```

该程序导致 667 个文件被写入磁盘。

### 第二步——从谷歌群组信息中提取数据

我们现在有 667 封电子邮件在不同的文件中。我们的任务是编写一个程序，一次读取一个，并使用本章中的一种技术提取我们需要的信息。如果我们偷看其中一封电子邮件，我们会看到许多**标题**，其中存储了有关电子邮件的信息，或者它的**元数据**。我们可以快速看到三个标题，它们标识了我们需要的元数据元素:

```
In-Reply-To: <ab71b72a-ef9b-4484-b0cc-a72ecb2a3b85@r9g2000yqd.googlegroups.com>
Date: Mon, 30 Apr 2012 10:33:18 -0700
Message-ID: <CA+qSDkQ4JB+Cn7HNjmtLOqqkbJnyBu=Z1Ocs5-dTe5cN9UEPyA@mail.gmail.com>
```

所有消息都有`Message-ID`和`Date`，但是`In-Reply-To`报头只会出现在回复另一条消息的消息中。一个`In-Reply-To`值必须是另一个消息的`Message-ID`值。

下面的代码显示了一个基于正则表达式的解决方案，用于提取`Date`、`Message-ID`和`In-Reply-To`(如果可用)值，并创建一些原始和回复消息的列表。然后，代码尝试计算消息与其回复之间的时间差:

```
import os
import re
import email.utils
import time
import datetime
import numpy

originals = {}
replies = {}
timelist = []

for filename in os.listdir(os.getcwd()):
    if filename.endswith(".txt"):
        f=open(filename, 'r')
        i=''
        m=''
        d=''
        for line in f:
            irt = re.search('(In\-Reply\-To: <)(.+?)@', line)    
            mid = re.search('(Message\-ID: <)(.+?)@', line)
            dt = re.search('(Date: )(.+?)\r', line)
            if irt: 
                i= irt.group(2) 
            if mid:
                m= mid.group(2)
            if dt:
                d= dt.group(2)
        f.close()
        if i and d:
            replies[i] = d
        if m and d:
            originals[m] = d

for (messageid, origdate) in originals.items():
    try:
        if replies[messageid]:
            replydate = replies[messageid]                
            try:
                parseddate = email.utils.parsedate(origdate)
                parsedreply = email.utils.parsedate(replydate)
            except:
                pass
            try:
                # this still creates some malformed (error) times
                timeddate = time.mktime(parseddate)
                timedreply = time.mktime(parsedreply)
            except:
                pass
            try:
                dtdate = datetime.datetime.fromtimestamp(timeddate)
                dtreply = datetime.datetime.fromtimestamp(timedreply)
            except:
                pass
            try:
                difference = dtreply - dtdate
                totalseconds = difference.total_seconds()
                timeinhours =  (difference.days*86400+difference.seconds)/3600
                # this is a hack to take care of negative times
                # I should probably handle this with timezones but alas
                if timeinhours > 1:
                    #print timeinhours
                    timelist.append(timeinhours)
            except:
                pass
    except:
        pass

print numpy.mean(timelist)
print numpy.std(timelist)
print numpy.median(timelist)
```

在这段代码中，最初的`for`循环遍历每条消息，并提取我们感兴趣的三段数据。(该程序不会将这些存储到单独的文件或磁盘中，但是如果您愿意，您可以添加此功能。)这部分代码还创建了两个重要的列表:

*   `originals[]`是原始消息列表。我们假设这些主要是向列表成员提出的问题。
*   `replies[]`是回复消息列表。我们假设这些主要是对另一封邮件中所提问题的回答。

第二个`for`循环处理原始消息列表中的每条消息，执行以下操作，如果有对原始消息的回复，尝试计算回复发送了多长时间。然后我们会保留一份回复时间的列表。

#### 提取代码

对于这一章，我们最感兴趣的是代码的清理和提取部分，所以让我们仔细看看这些代码行。这里，我们处理电子邮件文件的每一行，寻找三个电子邮件头:`In-Reply-To`、`Message-ID`和`Date`。我们使用正则表达式搜索和分组，就像我们在本章前面的方法 1 中所做的那样，来分隔标题并容易地提取下面的值:

```
for line in f:
    irt = re.search('(In\-Reply\-To: <)(.+?)@', line) 
    mid = re.search('(Message\-ID: <)(.+?)@', line)
    dt = re.search('(Date: )(.+?)\r', line)
    if irt: 
        i = irt.group(2) 
    if mid:
        m = mid.group(2)
    if dt:
        d = dt.group(2)
```

为什么我们决定在这里使用正则表达式而不是基于树的解析器？有两个主要原因:

1.  因为我们下载的电子邮件不是 HTML，所以不能简单地将它们描述为包含父节点和子节点的树。因此，BeautifulSoup 等基于解析树的解决方案并不是最佳选择。
2.  因为电子邮件标题是结构化的并且非常容易预测(特别是我们在这里寻找的三个标题,),所以正则表达式解决方案是可以接受的。

#### 程序输出

这个程序的输出是打印三个数字，估计这个 Google 群组上回复消息的平均值、标准偏差和以小时为单位的中值时间。当我运行这段代码时，我的结果如下:

```
178.911877395
876.102630872
18.0
```

这意味着发布到 BigQuery Google Group 的消息的平均响应时间约为 18 小时。现在让我们考虑如何从不同的来源提取相似的数据:网络论坛。你认为在网络论坛上回答问题会更快、更慢还是和谷歌群差不多？

## 第二部分——清理网络论坛中的数据

我们将为这个项目研究的网络论坛来自一家名为 **DocuSign** 的公司。他们也将他们的开发者支持转移到 Stack Overflow，但是他们有一个旧的基于 web 的开发者论坛的档案仍然在线。我在他们的网站上四处打探，直到我发现如何从那些旧论坛下载一些消息。这里展示的过程比 Google Groups 的例子更复杂，但是您将学到很多关于如何自动收集数据的知识。

### 第一步——收集一些指向 HTML 文件的 RSS

DocuSign 开发者论坛上有数千条信息。我们希望有一个所有这些消息或讨论线程的 URL 列表，以便我们可以编写一些代码来自动下载它们，并有效地提取回复时间。

要做到这一点，首先我们需要讨论的所有 URL 的列表。我发现 DocuSign 的旧 Dev-Zone 开发者网站的存档位于[https://community . DocuSign . com/t5/Dev Zone-Archives-Read-Only/CT-p/Dev _ Zone](https://community.docusign.com/t5/DevZone-Archives-Read-Only/ct-p/dev_zone)。

浏览器中的站点如下所示:

![Step one – collect some RSS that points us to HTML files](graphics/4276OT_05_08.jpg)

我们绝对不希望点击每一个论坛，然后点击每一条消息，并手动保存。那会花很长时间，而且会非常无聊。有没有更好的办法？

DocuSign 网站的**帮助**页面显示，可以下载一个**真正简单的聚合** ( **RSS** )文件，显示每个论坛的最新帖子和消息。我们可以使用 RSS 文件自动收集网站上许多讨论的 URL。我们感兴趣的 RSS 文件是那些只与开发者支持论坛相关的(不是公告或销售论坛)。这些 RSS 文件可从以下 URL 获得:

*   [https://community.docusign.com/docusign/rss/board?board.id =即将发布 _ 发布](https://community.docusign.com/docusign/rss/board?board.id=upcoming_releases)
*   [https://community.docusign.com/docusign/rss/board?board . id = DocuSign _ Developer _ Connection](https://community.docusign.com/docusign/rss/board?board.id=DocuSign_Developer_Connection)
*   [https://community.docusign.com/docusign/rss/board?board . id = Electronic _ Signature _ API](https://community.docusign.com/docusign/rss/board?board.id=Electronic_Signature_API)
*   [https://community.docusign.com/docusign/rss/board?board.id=Java](https://community.docusign.com/docusign/rss/board?board.id=Java)
*   [https://community.docusign.com/docusign/rss/board?board.id=php_api](https://community.docusign.com/docusign/rss/board?board.id=php_api)
*   [https://community.docusign.com/docusign/rss/board?board.id=dev_other](https://community.docusign.com/docusign/rss/board?board.id=dev_other)
*   [https://community.docusign.com/docusign/rss/board?Board . id = Ask _ A _ Development _ Question _ Board](https://community.docusign.com/docusign/rss/board?board.id=Ask_A_Development_Question_Board)

在 web 浏览器中访问列表中的每个 URL(如果时间紧迫，也可以只访问一个)。该文件是 RSS，看起来像带有标签的半结构化文本，类似于 HTML。在您的本地系统上将 RSS 保存为一个文件，并为每个文件指定一个`.rss`文件扩展名。在这个过程结束时，您应该最多有 7 个 RSS 文件，每个文件对应前面显示的 URL。

在每个 RSS 文件中，都有描述论坛上所有讨论主题的元数据，包括我们在这个阶段真正需要的数据:每个特定讨论主题的 URL。在文本编辑器中打开其中一个 RSS 文件，您将能够找到一个我们感兴趣的 URL 示例。它看起来像这样，在文件中，您会看到每个讨论主题都有一个这样的主题:

```
<guid>http://community.docusign.com/t5/Misc-Dev-Archive-READ-ONLY/Re-Custom-CheckBox-Tabs-not-marked-when-setting-value-to-quot-X/m-p/28884#M1674</guid>
```

现在，我们可以编写一个程序来循环遍历每个 RSS 文件，查找这些 URL，访问它们，然后提取我们感兴趣的回复次数。下一节将它分解成一系列更小的步骤，然后展示一个完成整个工作的程序。

### 第二步——从 RSS 中提取网址；收集和解析 HTML

在这个步骤中，我们将编写一个程序，让执行以下操作:

1.  打开我们在步骤 1 中保存的每个 RSS 文件。
2.  每当我们看到一个`<guid>`和`</guid>`标签对时，提取里面的 URL 并将其添加到一个列表中。
3.  对于列表中的每个 URL，下载该位置的任何 HTML 文件。
4.  读取 HTML 文件并从每条消息中提取原始发布时间和回复时间。
5.  像我们在第 1 部分中所做的那样，用平均值、中值和标准差计算发送一个回复需要多长时间。

下面是处理所有这些步骤的一些 Python 代码。我们将在代码清单的最后详细讨论提取部分:

```
import os
import re
import urllib2
import datetime
import numpy

alllinks = []
timelist = []
for filename in os.listdir(os.getcwd()):
    if filename.endswith('.rss'):
        f = open(filename, 'r')
        linktext = ''
        linkurl = ''
        for line in f:
            # find the URLs for discussion threads
            linktext = re.search('(<guid>)(.+?)(<\/guid>)', line)    

            if linktext:
                linkurl= linktext.group(2)
                alllinks.append(linkurl)
        f.close()

mainmessage = ''
reply = ''
maindateobj = datetime.datetime.today()
replydateobj = datetime.datetime.today()
for item in alllinks:
    print "==="
    print "working on thread\n" + item
    response = urllib2.urlopen(item)
    html = response.read() 
    # this is the regex needed to match the timestamp 
    tuples = re.findall('lia-message-posted-on\">\s+<span class=\"local-date\">\\xe2\\x80\\x8e(.*?)<\/span>\s+<span class=\"local-time\">([\w:\sAM|PM]+)<\/span>', html)	
    mainmessage = tuples[0]
    if len(tuples) > 1:
        reply = tuples[1]
    if mainmessage:
        print "main: "
        maindateasstr = mainmessage[0] + " " + mainmessage[1]
        print maindateasstr
        maindateobj = datetime.datetime.strptime(maindateasstr, '%m-%d-%Y %I:%M %p')
    if reply:
        print "reply: "
        replydateasstr = reply[0] + " " + reply[1]
        print replydateasstr
        replydateobj = datetime.datetime.strptime(replydateasstr, '%m-%d-%Y %I:%M %p')

        # only calculate difference if there was a reply 
        difference = replydateobj - maindateobj
        totalseconds = difference.total_seconds()
        timeinhours =  (difference.days*86400+difference.seconds)/3600
        if timeinhours > 1:
            print timeinhours
            timelist.append(timeinhours)

print "when all is said and done, in hours:"
print numpy.mean(timelist)
print numpy.std(timelist)
print numpy.median(timelist)
```

#### 计划状态

当程序工作时，它打印出状态信息，这样我们就知道它在做什么。状态消息如下所示，在 RSS 提要中找到的每个 URL 都有一个状态消息:

```
===
working on thread
http://community.docusign.com/t5/Misc-Dev-Archive-READ-ONLY/Can-you-disable-the-Echosign-notification-in-Adobe-Reader/m-p/21473#M1156
main: 
06-21-2013 08:09 AM
reply: 
06-24-2013 10:34 AM
74
```

在此显示中，74 表示线程中第一条消息的发布时间和线程中第一条回复时间之间的整数小时数(大约三天加两个小时)。

#### 程序输出

在它的结论中，这个程序打印出以小时为单位的平均值、标准差和中值回复时间，就像第 1 部分程序为 Google Groups 所做的一样:

```
when all is said and done, in hours:
695.009009009
2506.66701108
20.0
```

看起来 DocuSign 论坛的回复时间比 Google Groups 慢一点点。它报告的时间是 20 个小时，相比之下，谷歌集团用了 18 个小时，但至少这两个数字在相同的大致范围内。您的结果可能会改变，因为新消息一直在增加。

#### 提取代码

因为我们最感兴趣的是数据提取，所以让我们仔细看看发生这种情况的代码部分。下面是最相关的一行代码:

```
tuples = re.findall('lia-message-posted-on\">\s+<span class=\"local-date\">\\xe2\\x80\\x8e(.*?)<\/span>\s+<span class=\"local-time\">([\w:\sAM|PM]+)<\/span>', html)
```

就像我们之前的一些例子一样，这段代码也依赖正则表达式来完成它的工作。然而，这个正则表达式相当混乱。也许我们应该用 BeautifulSoup 写这个？让我们来看看我们试图匹配的原始 HTML，以便我们可以更多地了解这段代码试图做什么，以及我们是否应该以不同的方式来做这件事。下面是页面在浏览器中的截图。我们感兴趣的时代已经在截图上标注了:

![Extraction code](graphics/4276OT_05_09.jpg)

底层的 HTML 看起来像什么呢？这是我们的程序需要能够解析的部分。原来在 HTML 页面的几个地方都打印了原消息的日期，但是日期时间组合对于原文只打印一次，对于回复只打印一次。下面是显示这些内容的 HTML(为了便于查看，HTML 被压缩并删除了换行符):

```
<p class="lia-message-dates lia-message-post-date lia-component-post-date-last-edited" class="lia-message-dates lia-message-post-date">
<span class="DateTime lia-message-posted-on lia-component-common-widget-date" class="DateTime lia-message-posted-on">
<span class="local-date">‎06-18-2013</span>
<span class="local-time">08:21 AM</span>

<p class="lia-message-dates lia-message-post-date lia-component-post-date-last-edited" class="lia-message-dates lia-message-post-date">
<span class="DateTime lia-message-posted-on lia-component-common-widget-date" class="DateTime lia-message-posted-on">
<span class="local-date">‎06-25-2013</span>
<span class="local-time">12:11 AM</span>
```

对于 regex 来说，这是一个非常简单的问题，因为我们可以编写一个正则表达式，并为这两种类型的消息找到它的所有实例。在代码中，我们声明我们找到的第一个实例成为原始消息，下一个成为回复，如下所示:

```
mainmessage = tuples[0]
if len(tuples) > 1:
    reply = tuples[1]
```

我们可以使用基于解析树的解决方案，比如 BeautifulSoup，但是我们不得不面对这样一个事实，即两组日期的`span`类值是相同的，甚至父元素(`<p>`标记)也有相同的类。因此，这个解析树比本章前面的方法 2 要复杂得多。

如果你真的想尝试使用 BeautifulSoup 进行提取，我的建议是首先使用浏览器的开发工具查看页面的结构，例如，在 Chrome 浏览器中，你可以选择你感兴趣的元素——在本例中是日期和时间——并右键单击它，然后选择 **Inspect Element** 。这将打开一个开发人员工具面板，显示这段数据在整个文档树中的位置。每个 HTML 元素左边的小箭头表示是否有子节点。然后，您可以决定如何以编程方式在解析树中定位目标元素，并且可以制定一个计划来将它与其他节点区分开来。由于这个任务超出了这本入门书的范围，我将把留给读者作为练习。



# 总结

在这一章中，我们发现了一些可靠的技术，用于将感兴趣的数据与不想要的数据分开。当我们在厨师的厨房里制作肉汤时，我们用一个滤网来过滤我们不想要的骨头和蔬菜外壳，同时让我们想要的美味液体通过滤网上的孔流入我们的容器。当我们在数据科学厨房中从网页中提取数据时，同样的想法也适用。我们想设计一个清理计划，允许我们提取我们想要的，而留下其余的 HTML。

在此过程中，我们回顾了从 HTML 中提取数据时使用的两个主要心智模型，即逐行定界符方法和解析树/节点模型。然后，我们研究了三种可靠的、经过验证的解析 HTML 页面以提取所需数据的方法:正则表达式、BeautifulSoup 和基于 Chrome 的点击式抓取工具。最后，我们组织了一个项目，从真实世界的电子邮件和 HTML 页面中收集和提取有用的数据。

像电子邮件和 HTML 这样的文本数据并不难清理，但是二进制文件呢？在下一章，我们将探索如何从一个更加困难的目标中提取整洁的数据:PDF 文件。