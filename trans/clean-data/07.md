

# 第七章。RDBMS 清洗技术

家用冰箱都有搁板，大多数有一个或两个抽屉放蔬菜。但是，如果你曾经去过家庭整理店或与专业整理者交谈，你会发现还有许多额外的存储选项，包括鸡蛋容器、奶酪盒、汽水罐分配器、酒瓶架、剩菜标签系统，以及各种尺寸的可堆叠彩色编码垃圾桶。但是我们真的需要这些额外的东西吗？要回答这个问题，问问你自己这些问题，我经常使用的食物容易找到吗？食物占据了过多的空间吗？剩菜是否有明确的标签，以便我记得它们是什么以及我什么时候做的？如果我们的答案是*否*，组织专家说，容器和标签可以帮助我们优化存储，减少浪费，让我们的生活更轻松。

在我们的**关系数据库管理系统** ( **RDBMS** )中也是如此。作为经典的长期数据存储解决方案，RDBMS 是现代数据科学工具包的标准组成部分。但是很多时候，我们只是将数据存放在数据库中，很少考虑细节。在这一章中，我们将学习如何设计一个超越*两个架子和一个抽屉*的 RDBMS。我们将学习一些技术，确保我们的 RDBMS 优化存储，减少浪费，让我们的生活更轻松。具体来说，我们将:

*   了解如何在 RDBMS 数据中发现异常
*   学习几种策略来清理不同类型的有问题的数据
*   了解何时以及如何为已清理的数据创建新表，包括创建子表和查找表
*   了解如何记录管理您所做更改的规则

# 准备就绪

为了设置本章中的示例，我们将使用一个名为**sensition 140**的流行数据集。这个数据集是为了帮助了解 Twitter 上消息的积极和消极情绪而创建的。在本书中，我们并不真正关心情感分析，但是我们将使用这个数据集来练习在它已经被导入到关系数据库中之后清理数据。

为了让开始使用 Sentiment140 数据集，你需要一个 MySQL 服务器，就像早期的 Enron 例子一样。



# 第一步——下载并检查意见 140

我们希望使用的版本的 sensition 140 数据是在[http://help.sentiment140.com/for-students](http://help.sentiment140.com/for-students)的 sensition 140 项目中直接获得的两个文件的原始集合。这个 tweets 及其正负极性(或情绪，在 0、2 或 4 的范围内)的 ZIP 文件是由斯坦福大学的一些研究生创建的。自从这个文件被公开后，原始的 Sentiment140 文件被其他网站添加进来，成为许多更大的推文集合的一部分。对于这一章，我们将使用原始的 Sentiment140 文本文件，该文件可以作为前面网站的链接获得，也可以通过精确的路径到达[http://cs . Stanford . edu/people/Alec MgO/training and test data . zip](http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip)。

下载 ZIP 文件，将其解压缩，并使用文本编辑器查看其中的两个 CSV 文件。很快，您会注意到一个文件比另一个文件多很多行，但是这两个文件都有相同数量的列。数据以逗号分隔，每一列都用双引号括起来。每一栏的描述可以在前面部分链接的`for-students`页面上找到。



# 第二步–清理数据库导入

对于我们的目的——学习如何清理数据——将较小的文件加载到一个 MySQL 数据库表中就足够了。我们需要学习的所有东西，我们都可以用这个叫做`testdata.manual.2009.06.14.csv`的小文件来完成。

当我们查看数据时，如果我们试图将这个文件直接导入 MySQL，我们可能会注意到一些地方会出错。其中一个问题点位于文件的第 28 行:

```
"4","46","Thu May 14 02:58:07 UTC 2009","""booz allen""", 
```

你看到在关键字`booz`前面和单词`allen`后面的三个引号`"""`了吗？稍后在第 41 行出现了同样的问题，歌曲标题`P.Y.T`用双引号括起来:

```
"4","131","Sun May 17 15:05:03 UTC 2009","Danny Gokey","VickyTigger","I'm listening to ""P.Y.T"" by Danny Gokey…"
```

这些额外引号的问题是 MySQL 导入例程将使用引号来分隔列文本。这将产生一个错误，因为 MySQL 会认为这一行的列数比实际的多。

为了解决这个问题，在我们的文本编辑器中，我们可以使用 **Find and Replace** 将`"""`的所有实例替换为`"`(双引号)，将`""`的所有实例替换为`'`(单引号)。

### Tip

这些`""`也可能被完全移除，而对清洁工作几乎没有负面影响。要做到这一点，我们只需搜索`""`并用 nothing 替换它。但是如果你想贴近推文的原意，单引号(甚至是像`\"`这样的转义双引号)是替换字符的安全选择。

将这个清理后的文件保存为一个新文件名，类似于`cleanedTestData.csv`。我们现在准备将它导入 MySQL。



# 第三步——将数据导入 MySQL 的单个表中

为了将我们稍微干净一点的数据文件加载到 MySQL 中，我们需要重温一下从*将电子表格数据导入 MySQL* 部分的 CSV-to-SQL 技术，参见[第 3 章](ch03.html "Chapter 3. Workhorses of Clean Data – Spreadsheets and Text Editors")，*干净数据的主要工具——电子表格和文本编辑器*:

1.  从命令行中，导航到您保存在步骤 2 中创建的文件的目录。这是我们将要导入 MySQL 的文件。
2.  然后，启动你的 MySQL 客户端，连接到你的数据库服务器:

    ```
    user@machine:~/sentiment140$ mysql -hlocalhost -umsquire -p
    Enter password:
    ```

3.  输入您的密码，登录后，在 MySQL 内创建一个数据库来保存该表，如下:

    ```
    mysql> CREATE DATABASE sentiment140;
    mysql> USE sentiment140;
    ```

4.  接下来，我们需要创建一个表来保存数据。每一列的数据类型和长度应该代表我们匹配现有数据的最佳尝试。有些列是 varchars 类型的，每个列都需要一个长度。由于我们可能不知道这些长度应该是多少，我们可以使用我们的清洁工具来辨别一个合适的范围。
5.  If we open our CSV file in Excel (Google Spreadsheets will work just fine for this as well), we can run some simple functions to find the maximum lengths of some of our text fields. The `len()` function, for example, gives the length of a text string in characters, and the `max()` function can tell us the highest number in a range. With our CSV file open, we can apply these functions to see how long our varchar columns in MySQL should be.

    下面的截图展示了一种使用函数解决这个问题的方法。它显示了应用于列 **G** 的`length()`函数，以及用于列 **H** 但应用于列 **G** 的`max()`函数。

    ![Step three – import the data into MySQL in a single table](graphics/4276_07_01.jpg)

    列 **G** 和 **H** 展示了如何在 Excel 中获取一个文本列的长度，然后获取最大值。

6.  To calculate these maximum lengths more quickly, we can also take an Excel shortcut. The following array formula can work to quickly combine the maximum value with the length of a text column in a single cell—just make sure you press *Ctrl* + *Shift* + *Enter* after typing this nested function rather than just *Enter*:

    ```
    =max(len(f1:f498))
    ```

    这个嵌套函数可以应用于任何文本列，以获得该列中文本的最大长度，并且它只使用一个单元格来完成此操作，而不需要任何中间长度计算。

在我们运行这些函数之后，我们发现任何 tweets 的最大长度是 144 个字符。

## 检测和清洁异常

你可能想知道这个数据集中的一条推文怎么可能有 144 个字符长，因为 Twitter 限制所有推文的最大长度为 140 个字符。原来，在 sentiment140 数据集中， **&** 字符有时会被翻译成 HTML 等价代码`&amp`，但并不总是如此。也使用了一些其他的 HTML 代码，例如，有时候， **<** 字符变成了`&lt;`， **>** 变成了`&gt;`。因此，对于一些非常长的推文，这种只增加几个字符的做法很容易使这条推文超过 140 的长度限制。正如我们所知，这些 HTML 编码的字符并不是最初的人在推特上发布的内容，并且因为我们看到它们有时会发生，但不是一直都发生，我们将这些**数据异常称为**。

要清理这些，我们有两个选择。我们可以继续将杂乱的数据导入数据库并尝试清理，或者我们可以尝试先在 Excel 或文本编辑器中清理。为了显示这两种技术的区别，我们将在这里两者都做。首先，我们将在电子表格或文本编辑器中使用查找和替换来尝试转换下表中显示的字符。我们可以将 CSV 文件导入到 Excel 中，看看我们可以做多少清理工作:

| 

HTML 代码

 | 

用…替换

 | 

实例的计数

 | 

用于查找计数的 Excel 函数

 |
| --- | --- | --- | --- |
| `&lt;` | `<` | six | `=COUNTIF(F1:F498,"*&lt*")` |
| `&gt;` | `>` | five | `=COUNTIF(F1:F498,"*&gt*")` |
| `&amp;` | `&` | Twenty-four | `=COUNTIF(F1:F498,"*&amp*")` |

前两个字符交换与 Excel 中的**查找和替换**很好。`&lt`；和`&gt`；HTML 编码的字符被更改。看一看这样的文字:

```
I'm listening to 'P.Y.T' by Danny Gokey &lt;3 &lt;3 &lt;3 Aww, he's so amazing. I &lt;3 him so much :)
```

前面的内容变成如下文本:

```
I'm listening to 'P.Y.T' by Danny Gokey <3 <3 <3 Aww, he's so amazing. I <3 him so much :)
```

但是，当我们试图使用 Excel 查找`&amp;`并用`&`替换它时，您可能会遇到错误，如下所示:

![Detecting and cleaning abnormalities](graphics/4276_07_02.jpg)

某些操作系统和 Excel 版本有一个问题，我们选择了 **&** 字符作为替换。此时，如果我们遇到这个错误，我们可以采取几种不同的方法:

*   我们可以使用我们选择的搜索引擎来尝试找到这个错误的 Excel 解决方案
*   我们可以将 CSV 文本数据移动到文本编辑器中，并在那里执行查找和替换功能
*   我们可以勇往直前，把数据扔进数据库，尽管它有奇怪的`&amp`；字符，然后尝试在数据库内部清除它

通常，如果在数据库之外清理脏数据的可能性微乎其微，我会赞成不要将脏数据移入数据库。然而，由于这是关于数据库内部清理的一章，让我们继续将清理了一半的数据导入到数据库中，我们将继续清理`&amp`；当数据在表中时发出。

## 创建我们的表格

为了将我们清理了一半的数据移入数据库，我们将首先编写我们的`CREATE`语句，然后在我们的 MySQL 数据库上运行它。`CREATE`语句如下所示:

```
mysql> CREATE TABLE sentiment140 (
    ->   polarity enum('0','2','4') DEFAULT NULL,
    ->   id int(11) PRIMARY KEY,
    ->   date_of_tweet varchar(28) DEFAULT NULL,
    ->   query_phrase varchar(10) DEFAULT NULL,
    ->   user varchar(10) DEFAULT NULL,
    ->   tweet_text varchar(144) DEFAULT NULL
    -> ) ENGINE=MyISAM DEFAULT CHARSET=utf8;
```

### 注意

该语句使用简单快速的 MyISAM 引擎，因为我们预计不需要任何 InnoDB 特性，比如行级锁定或事务。关于 MyISAM 和 InnoDB 之间的差异，这里有一个关于何时使用每个存储引擎的便利讨论:[http://stack overflow . com/questions/20148/MyISAM-vs-InnoDB](http://stackoverflow.com/questions/20148/myisam-versus-innodb)。

您可能会注意到，代码仍然要求`tweet_text`列的长度为 144。这是因为我们无法用`&amp`清洁这些柱子；在里面编码。然而，这并没有让我太烦恼，因为我知道 varchar 列不会使用它们的额外空间，除非它们需要它。毕竟，这就是它们被称为 varchar 或可变字符列的原因。但是如果这个额外的长度真的困扰你，你可以在以后修改这个表，使这个列只有 140 个字符。

接下来，我们将使用 MySQL 命令行从该位置运行以下导入语句:

```
mysql> LOAD DATA LOCAL INFILE 'cleanedTestData.csv'
    ->   INTO TABLE sentiment140
    ->   FIELDS TERMINATED BY ',' ENCLOSED BY '"' ESCAPED BY '\'
    ->   (polarity, id, date_of_tweet, query_phrase, user, tweet_text);
```

这个命令将数据从我们清理的 CSV 文件加载到我们创建的新表中。成功消息如下所示，表明所有 498 行都已加载到表中:

```
Query OK, 498 rows affected (0.00 sec)
Records: 498  Deleted: 0  Skipped: 0  Warnings: 0
```

### Tip

如果您可以访问基于浏览器的界面，如 phpMyAdmin(或桌面应用程序，如 MySQL Workbench 或 Toad for MySQL)，所有这些 SQL 命令都可以在这些工具中非常容易地完成，而不必在命令行上键入，例如，在 phpMyAdmin 中，您可以使用 Import 选项卡并在那里上传 CSV 文件。只要确保数据文件按照*步骤二——数据库导入清理*中的程序进行清理，否则您可能会得到文件中有太多列的错误。这个错误是由引号问题引起的。



# 第四步–清洁&放大器；性格；角色；字母

在最后一步，我们决定推迟清洗`&amp`；字符，因为 Excel 给出了一个奇怪的错误。现在我们已经完成了*第三步——将数据导入到 MySQL 的单个表中*,我们的数据也导入到了 MySQL 中，我们可以非常容易地使用`UPDATE`语句和`replace()`字符串函数来清理数据。下面是获取`&amp`的所有实例所需的 SQL 查询；并用`&`来代替它们:

```
UPDATE sentiment140 SET tweet_text = replace(tweet_text,'&amp;', '&');
```

`replace()`功能的工作原理就像在 Excel 或文本编辑器中的查找和替换一样。我们可以看到 tweet ID 594，以前是说`#at&amp;t is complete fail`，现在是`#at&t is complete fail`。



# 第五步——清除其他神秘人物

当我们阅读`tweet_text`专栏时，我们可能已经注意到一些奇怪的推文，比如推文 IDs 613 和 2086:

```
613, Talk is Cheap: Bing that, I?ll stick with Google
2086, Stanford University?s Facebook Profile
```

`?`性格才是我们应该关心的。与我们前面看到的 HTML 编码的字符一样，这个字符问题也很可能是字符集之间先前转换的产物。在这种情况下，原始推文中可能有某种高位 ASCII 或 Unicode 撇号(有时被称为**智能引用**)，但当数据被转换为低位字符集(如普通 ASCII)时，这种特殊的撇号就被简单地更改为`?`。

根据我们计划如何处理数据，我们可能不想遗漏`?`字符，例如，如果我们正在执行字数统计或文本挖掘，将`I?ll`转换为`I'll`，将`University?s`转换为`University's`可能非常重要。如果我们认为这很重要，那么我们的工作就是检测错误发生的地方，然后设计一个策略将问号转换回单引号。当然，诀窍是我们不能仅仅用单引号字符替换`tweet_text`列中的每个问号，因为一些推文中有问号，应该不加理会。

为了定位问题字符，我们可以运行一些 SQL 查询，尝试使用正则表达式来定位问题。我们对出现在奇怪地方的问号感兴趣，比如紧随其后的字母字符。这里是使用 MySQL `REGEXP`特性的一个常规 SQL 表达式的初始过程。运行这个将会给我们一个大概的概念，问题问号可能存在于哪里:

```
SELECT id, tweet_text
FROM sentiment140
WHERE tweet_text
REGEXP '\\?[[:alpha:]]+';
```

这个 SQL 正则表达式要求后面紧跟一个或多个字母字符的任何问号字符。SQL 查询产生了六行，其中四行带有奇怪的问号，另外两行是**误报**。误报是符合我们模式的推文，但实际上不应该改变。两个误报是 id 为 **234** 和 **2204** 的推文。这两个包括问号作为合法 URL 的一部分。推文 **139** 、 **224** 、 **613** 和 **2086** 是**真阳性**，这意味着推文被正确检测为异常，应该被更改。所有的结果都显示在下面的 phpMyAdmin 截图中:

![Step five – clean other mystery characters](graphics/4276_07_03.jpg)

然而，Tweet **139** 很奇怪。在单词 **Obama** 前有一个问号，就好像一篇新闻文章的名字被引用了，但是在字符串的末尾没有匹配的引用(或者遗漏的引用)。这应该是另一个角色吗？这实际上也可能是一个假阳性，或者至少没有足够的真阳性来让我们修正它。当我们仔细查看这些推文时， **224** 在一个似乎不属于它的地方还有一个额外的奇怪问号。

如果我们要编写一个`replace()`函数来将有问题的问号改为单引号，我们将需要编写一个正则表达式，它只匹配真阳性，而不匹配任何假阳性。然而，由于这个数据集很小，并且只有四个真阳性——或者三个，如果我们决定 **139** 不需要清除——那么我们可以只手动清除真阳性。这是一个特别好的主意，因为我们对其他可能的问题有一些疑问，比如推文 **224** 中的额外问号。

在这种情况下，因为我们只有三个有问题的行，所以简单地对数据运行三个小的`UPDATE`命令会比试图创建完美的正则表达式更快。下面是处理推文 **224** (仅第一期)、 **613** 和 **2086** 的 SQL 查询:

```
UPDATE sentiment140 SET tweet_text = 'Life''s a bitch? and so is Dick Cheney. #p2 #bipart #tlot #tcot #hhrs #GOP #DNC http://is.gd/DjyQ' WHERE id = 224;

UPDATE sentiment140 SET tweet_text = 'Talk is Cheap: Bing that, I''ll stick with Google. http://bit.ly/XC3C8' WHERE id = 613;

UPDATE sentiment140 SET tweet_text = 'Stanford University''s Facebook Profile is One of the Most Popular Official University Pages - http://tinyurl.com/p5b3fl' WHERE id = 2086;
```

### 注意

请注意，我们必须在这些 update 语句中转义单引号。在 MySQL 中，转义字符要么是反斜杠，要么是单引号本身。这些示例将单引号显示为转义字符。

## 第六步——清洗枣子

如果我们看一下`date_of_tweet`列，我们会看到我们将它创建为一个简单的变量字符字段`varchar(30)`。这有什么错？嗯，假设我们想将推文从早到晚排序。现在，我们不能使用简单的 SQL `ORDER BY`子句并获得正确的日期顺序，因为我们将获得字母顺序。所有的星期五都在任何星期一之前，五月总是在六月之后。我们可以使用以下 SQL 查询对此进行测试:

```
SELECT id, date_of_tweet
FROM sentiment140
ORDER BY date_of_tweet;
```

前几行是有序的，但在第 28 行附近，我们开始看到一个问题:

```
2018  Fri May 15 06:45:54 UTC 2009
2544  Mon Jun 08 00:01:27 UTC 2009
…
3  Mon May 11 03:17:40 UTC 2009
```

`May 11`不在`May 15`或`June 8`之后。为了解决这个问题，我们需要创建一个新的列来清除这些日期字符串，并将它们转换成适当的 MySQL datetime 数据类型。我们在[第 2 章](ch02.html "Chapter 2. Fundamentals – Formats, Types, and Encodings")、*基础——格式、类型和编码*中的*数据类型*转换一节中了解到，当日期和时间被存储为本机**日期**、**时间**或**日期时间**类型时，MySQL 工作得最好。插入日期时间类型的格式看起来像这样:`YYYY-MM-DD HH:MM:SS`。但是这不是我们的数据在`date_of_tweet`栏中的样子。

有许多内置的 MySQL 函数可以帮助我们将杂乱的日期字符串格式化成自己喜欢的格式。通过这样做，我们将能够利用 MySQL 在日期和时间上执行数学运算的能力，例如，找出两个日期或时间之间的差异，或者按照日期或时间的顺序对项目进行适当的排序。

为了将我们的字符串转换成 MySQL 友好的 datetime 类型，我们将执行以下过程:

1.  更改表以包含一个新列，其目的是保存新的日期时间信息。我们可以将这个新列称为`date_of_tweet_new`或`date_clean`或其他名称，以便与原来的`date_of_tweet`列明显区分开来。执行此任务的 SQL 查询如下:

    ```
    ALTER TABLE sentiment140
    ADD date_clean DATETIME NULL
    AFTER date_of_tweet;
    ```

2.  对每一行执行更新，在此期间，我们将旧的日期字符串格式化为正确格式化的日期时间类型，而不是字符串，并将新值添加到新创建的`date_clean`列中。执行此任务的 SQL 如下:

    ```
    UPDATE sentiment140
    SET date_clean = str_to_date(date_of_tweet, '%a %b %d %H:%i:%s UTC %Y');
    ```

此时，我们有了一个已经填充了干净日期时间的新列。回想一下，旧的`date_of_tweet`列有缺陷，因为它没有正确地对日期进行排序。为了测试日期现在是否正确排序，我们可以按照新列的顺序选择数据:

```
SELECT id, date_of_tweet 
FROM sentiment140
ORDER BY date_clean;
```

我们看到这些行现在已经完美地排序了，5 月 11 日的日期排在第一位，没有任何日期是乱序的。

我们应该删除旧的`date`列吗？这取决于你。如果你担心你可能犯了一个错误，或者出于某种原因你可能需要原始数据，那么通过一切手段，保留它。但是如果您想删除它，只需删除该列，如下所示:

```
ALTER TABLE sentiment140 
DROP date_of_tweet;
```

您还可以创建一个包含原始列的 Sentiment140 表的副本作为备份。



# 第七步——分离用户提及、标签和网址

目前这种数据的另一个问题是在`tweet_text`列中隐藏了许多有趣的条信息，例如，考虑一个人在用户名前使用`@`符号将一条推文引向另一个人的注意的所有时间。这叫做推特上的**提**。统计一个特定的人被提及的次数或者他们与一个特定的关键词一起被提及的次数可能会很有趣。隐藏在一些推文中的另一个有趣数据是**标签**；例如，ID 为 2165 的推文使用`#jobs`和`#sittercity`标签讨论了工作和保姆的概念。

这条推文还包括一个外部的非 Twitter 的 URL。我们可以提取这些提及、标签和 URL，并分别保存在数据库中。

这项任务将类似于我们如何清理日期，但有一个重要的区别。对于日期，我们只有一个可能的日期修正版本，所以添加一个新列来保存新的、干净的版本就足够了。然而，对于提及、标签和 URL，我们可能在单个`tweet_text`值中有零个或多个标签，例如，我们之前看到的 tweet(ID 2165)中有两个标签，这条 tweet (ID 2223)也是如此:

```
HTML 5 Demos! Lots of great stuff to come! Yes, I'm excited. :) http://htmlfive.appspot.com #io2009 #googleio
```

这条推文没有提及，只有一个网址和两个标签。推文 13078 包括三个提及，但没有标签或网址:

```
Monday already. Iran may implode. Kitchen is a disaster. @annagoss seems happy. @sebulous had a nice weekend and @goldpanda is great. whoop.
```

我们将需要改变我们的数据库结构来保存这些新的信息——标签、URL 和提及——同时要记住，一条给定的 tweet 可能包含大量这些信息。

## 创建一些新表格

根据关系数据库理论，我们应该避免创建存储多值属性的列，例如，如果一条 tweet 有三个标签，我们不应该将所有三个标签放在一个列中。这条规则对我们的影响是，我们不能简单地复制之前用于日期清理问题的`ALTER`过程。

相反，我们需要创建三个新表:`sentiment140_mentions`、`sentiment140_urls`和`sentiment140_hashtags`。每个新表的主键将是一个合成的 ID 列，每个表将只包含另外两列:`tweet_id`，它将这个新表与原来的`sentiment140`表联系起来，以及 hashtag、提示语或 URL 的实际提取文本。这里有三个`CREATE`语句来创建我们需要的表格:

```
CREATE TABLE IF NOT EXISTS sentiment140_mentions (
  id int(11) NOT NULL AUTO_INCREMENT,
  tweet_id int(11) NOT NULL,
  mention varchar(144) NOT NULL,
  PRIMARY KEY (id)
) ENGINE=MyISAM DEFAULT CHARSET=utf8;

CREATE TABLE IF NOT EXISTS sentiment140_hashtags (
  id int(11) NOT NULL AUTO_INCREMENT,
  tweet_id int(11) NOT NULL,
  hashtag varchar(144) NOT NULL,
  PRIMARY KEY (id)
) ENGINE=MyISAM DEFAULT CHARSET=utf8;

CREATE TABLE IF NOT EXISTS sentiment140_urls (
  id int(11) NOT NULL AUTO_INCREMENT,
  tweet_id int(11) NOT NULL,
  url varchar(144) NOT NULL,
  PRIMARY KEY (id)
) ENGINE=MyISAM DEFAULT CHARSET=utf8;
```

### 注意

这些表不使用外键返回到原始的`sentiment140` tweet 表。如果你想补充这些，那当然是可以的。然而，为了学习如何清理这个数据集，这是不必要的。

现在我们的表已经创建好了，是时候用我们从`tweet_text column`中精心提取的数据来填充它们了。我们将分别处理每个提取案例，从用户提及开始。

## 提取用户提及

为了设计一个能够处理用户提及提取的程序，让我们首先回顾一下我们对推文中提及的了解:

*   用户提及总是以 **@** 符号开始
*   用户提及是紧跟在 **@** 符号之后的单词
*   如果 **@** 后有空格，则不是用户提及
*   用户名称本身没有空格
*   由于电子邮件地址也使用 **@** ，我们应该留意它们

使用这些规则，我们可以构建一些有效的用户提及:

*   @foo
*   @foobar1
*   @_1foobar_

我们可以构建一些无效用户提及的例子:

*   @ foo(跟在@后面的空格使其无效)
*   foo@bar.com(不承认 bar.com)
*   @foo bar(只会识别@foo)
*   @foo.bar(只会识别@foo)

### 注意

在这个例子中，我们假设我们不关心常规的`@mention`和。`@mention`，有时也叫点提。这些推文在`@`符号前有一个句号。它们被设计成向用户的所有关注者推送推文。

由于这个规则集比我们可以在 SQL 中有效执行的规则集更复杂，所以最好编写一个简单的小脚本，使用一些正则表达式来清理这些 tweets。我们可以用任何能连接到我们数据库的语言编写这种类型的脚本，比如 Python 或 PHP。正如我们在[第二章](ch02.html "Chapter 2. Fundamentals – Formats, Types, and Encodings")、*基础知识——格式、类型和编码*中使用 PHP 连接数据库一样，让我们在这里也使用一个简单的 PHP 脚本。这个脚本连接到数据库，在`tweet_text`列中搜索用户提及，并将任何找到的提及移动到新的`sentiment140_mentions`表中:

```
<?php
// connect to db
$dbc = mysqli_connect('localhost', 'username', 'password', 'sentiment140')
    or die('Error connecting to database!' . mysqli_error());
$dbc->set_charset("utf8");

// pull out the tweets
$select_query = "SELECT id, tweet_text FROM sentiment140";
$select_result = mysqli_query($dbc, $select_query);

// die if the query failed
if (!$select_result)
    die ("SELECT failed! [$select_query]" .  mysqli_error());

// pull out the mentions, if any
$mentions = array();
while($row = mysqli_fetch_array($select_result))
{
    if (preg_match_all(
        "/(?<!\pL)@(\pL+)/iu",
        $row["tweet_text"],
        $mentions
    ))
    { 
        foreach ($mentions[0] as $name)
        {
            $insert_query = "INSERT into sentiment140_mentions (id, tweet_id, mention) VALUES (NULL," . $row["id"] . ",'$name')";
            echo "<br />$insert_query";
            $insert_result = mysqli_query($dbc, $insert_query);
            // die if the query failed
            if (!$insert_result)
                die ("INSERT failed! [$insert_query]" .  mysqli_error());
        }
    }
}
?>
```

在对`sentiment140`表运行这个小脚本后，我们看到从 498 条原始推文中提取了 124 条独特的用户提及。关于这个脚本的一些有趣的事情包括，它将处理用户名中的 Unicode 字符，即使这个数据集碰巧没有任何 Unicode 字符。我们可以通过在`sentiment140`表的末尾快速插入一个测试行来测试这一点，例如:

```
INSERT INTO sentiment140 (id, tweet_text) VALUES(99999, "This is a @тест");
```

然后，再次运行脚本；您将看到一行被添加到了`sentiment140_mentions`表中，并且成功提取了`@тест` Unicode 用户提及。在的下一节，我们将构建一个类似的脚本来提取标签。

## 提取标签

标签有自己的规则，与用户提及略有不同。下面是一些规则的列表，我们可以用它们来确定某个东西是否是一个标签:

*   标签以`#`符号开始
*   标签是紧跟在`#`符号后面的单词
*   标签中可以有下划线，但是不能有空格和其他标点符号

将 hashtags 提取到它们自己的表中的 PHP 代码与用户提到的代码基本相同，除了代码中间的正则表达式。我们可以简单地将`$mentions`变量改为`$hashtags`，然后调整正则表达式如下:

```
if (preg_match_all(
        "/(#\pL+)/iu",
        $row["tweet_text"],
        $hashtags
    ))
```

这个正则表达式表示我们对匹配不区分大小写的 Unicode 字母字符感兴趣。然后，我们需要修改我们的`INSERT`行以使用正确的表和列名，如下所示:

```
$insert_query = "INSERT INTO sentiment140_hashtags (id, tweet_id, hashtag) VALUES (NULL," . $row["id"] . ",'$name')";
```

当我们成功运行这个脚本时，我们看到有 54 个 hashtags 被添加到了`sentiment140_hashtags`表中。更多的推文有多个标签，甚至比有多个用户提及的推文更多，例如，我们可以立即看到推文 174 和 224 都有几个嵌入的标签。

接下来，我们将使用同样的框架脚本并再次修改它来提取 URL。

## 提取网址

从文本中提取 URL 可以简单到只需查找以 *http://* 或 *https://* 开头的任何字符串，也可以复杂得多，这取决于文本字符串包含的 URL 类型，例如，一些字符串可能包含*file://*URL 或 torrent 链接，如 magnet URLs，或其他类型的异常链接。在我们的 Twitter 数据的例子中，我们稍微容易一些，因为包含在我们的数据集中的 URL 都以 HTTP 开头。因此，我们可以偷懒，只设计一个简单的正则表达式来提取 http://或 https://后面的任何字符串。这个正则表达式看起来就像这样:

```
if (preg_match_all(
        "!https?://\S+!",
        $row["tweet_text"],
        $urls
    ))
```

然而，如果我们在我们最喜欢的搜索引擎上搜索一下，我们会很容易发现一些令人印象深刻且有用的通用 URL 匹配模式，它们将处理更复杂的链接模式。这很有用的原因是，如果我们编写 URL 提取来处理更复杂的情况，那么如果我们的数据在未来发生变化，它仍然可以工作。

在[http://daring fireball . net/2010/07/improved _ regex _ for _ matching _ URLs](http://daringfireball.net/2010/07/improved_regex_for_matching_urls)网站上给出了一个非常详细记录的 URL 模式匹配例程。以下代码显示了如何修改我们的 PHP 代码，以便在 Sentiment140 数据集中使用此模式提取 URL:

```
<?php
// connect to db
$dbc = mysqli_connect('localhost', 'username', 'password', 'sentiment140')
    or die('Error connecting to database!' . mysqli_error());
$dbc->set_charset("utf8");

// pull out the tweets
$select_query = "SELECT id, tweet_text FROM sentiment140";
$select_result = mysqli_query($dbc, $select_query);

// die if the query failed
if (!$select_result)
    die ("SELECT failed! [$select_query]" .  mysqli_error());

// pull out the URLS, if any
$urls = array();
$pattern  = '#\b(([\w-]+://?|www[.])[^\s()<>]+(?:\([\w\d]+\)|([^[:punct:]\s]|/)))#';

while($row = mysqli_fetch_array($select_result))
{
    echo "<br/>working on tweet id: " . $row["id"];
    if (preg_match_all(
        $pattern,
        $row["tweet_text"],
        $urls
    ))
    {
        foreach ($urls[0] as $name)
        {
            echo "<br/>----url: ".$name;
            $insert_query = "INSERT into sentiment140_urls (id, tweet_id, url)
                VALUES (NULL," . $row["id"] . ",'$name')";
            echo "<br />$insert_query";
            $insert_result = mysqli_query($dbc, $insert_query);
            // die if the query failed
            if (!$insert_result)
                die ("INSERT failed! [$insert_query]" .mysqli_error());
        }
    }
}
?>
```

这个程序与我们之前编写的提示语提取程序几乎相同，除了两个例外。首先，我们将正则表达式模式存储在一个名为`$pattern`的变量中，因为它既长又复杂。其次，我们对数据库`INSERT`命令做了一些小的修改，就像我们对 hasthtag 提取所做的一样。

正则表达式模式的完整的逐行解释可以在它的原始网站上找到，但简短的解释是显示的模式将匹配任何 URL 协议，如 http://或 file://，它还试图匹配有效的域名模式和几个级别的目录/文件模式。如果您想查看它将匹配的各种模式以及一些肯定*不会*匹配的已知模式，源网站也提供了自己的测试数据集。



# 第八步–清理查找表

在*步骤七——分离用户提及、标签和 URL*部分，我们创建了新的表来保存提取的标签、用户提及和 URL，然后提供了一种通过`id`列将每一行链接回原始表的方法。我们遵循数据库规范化的规则，创建新的表格来表示推文和用户提及之间、推文和标签之间或者推文和 URL 之间的一对多关系。在这一步中，我们将继续针对性能和效率优化该表。

我们现在关注的栏目是`query_phrase`栏目。查看列数据，我们可以看到它包含重复出现的相同短语。这些显然是最初用于定位和选择现在存在于该数据集中的推文的搜索短语。在`sentiment140`表中的 498 条推文中，有多少查询短语被反复重复？我们可以使用下面的 SQL 来检测这一点:

```
SELECT count(DISTINCT query_phrase)
FROM sentiment140;
```

查询结果显示只有 80 个不同的查询短语，但是它们在 498 行中被反复使用。

在一个有 498 行的表中，这似乎不是一个问题，但是如果我们有一个非常大的表，比如有几亿行，我们应该关心这个列的两个问题。首先，反复复制这些字符串会占用数据库中不必要的空间，其次，搜索不同的字符串值非常慢。

为了解决这个问题，我们将创建一个查询值的**查找表**。每个查询字符串在这个新表中只存在一次，我们还将为每个查询字符串创建一个 ID 号。然后，我们将更改原始表，使用这些新的数值，而不是现在使用的字符串值。我们完成此任务的步骤如下:

1.  创建新的查找表:

    ```
    CREATE TABLE sentiment140_queries (
      query_id int(11) NOT NULL AUTO_INCREMENT,
      query_phrase varchar(25) NOT NULL,
      PRIMARY KEY (query_id)
    ) ENGINE=MyISAM DEFAULT CHARSET=utf8 AUTO_INCREMENT=1;
    ```

2.  用不同的查询短语填充查找表，并自动给每个短语一个`query_id`号:

    ```
    INSERT INTO sentiment140_queries (query_phrase)
    SELECT DISTINCT query_phrase FROM sentiment140;
    ```

3.  在原始表中创建一个新列来保存查询短语编号:

    ```
    ALTER TABLE sentiment140
    ADD query_id INT NOT NULL AFTER query_phrase;
    ```

4.  备份`sentiment140`表，以防下一步出错。每当我们在表上执行`UPDATE`时，做一个备份是个好主意。为了创建`sentiment140`表的副本，我们可以使用 phpMyAdmin 之类的工具轻松复制该表(使用**操作**选项卡)。或者，我们可以重新创建一个表的副本，然后将原始表中的行导入其中，如下面的 SQL 所示:

    ```
    CREATE TABLE sentiment140_backup(
      polarity int(1) DEFAULT NULL,
      id int(5)NOT NULL,
      date_of_tweet varchar(30) CHARACTER SET utf8 DEFAULT NULL ,
      date_clean datetime DEFAULT NULL COMMENT 'holds clean, formatted date_of_tweet',
      query_id int(11) NOT  NULL,
      user varchar(25) CHARACTER SET utf8 DEFAULT NULL,
      tweet_text varchar(144) CHARACTER SET utf8 DEFAULT NULL ,
      PRIMARY KEY (id)) ENGINE=MyISAM DEFAULT CHARSET=utf8;

    SET SQL_MODE='NO_AUTO_VALUE_ON_ZERO';
    INSERT INTO sentiment140_backup SELECT * FROM sentiment140;
    ```

5.  用正确的数字填充新列。为此，我们在文本列上将两个表连接在一起，然后从查找表中查找正确的数值，并将其插入到`sentiment140`表中。在下面的查询中，每个表都有一个别名，`s`和`sq` :

    ```
    UPDATE sentiment140 s
    INNER JOIN sentiment140_queries sq
    ON s.query_phrase = sq.query_phrase
    SET s.query_id = sq.query_id;
    ```

6.  删除`sentiment140`表中旧的`query_phrase`列:

    ```
    ALTER TABLE sentiment140
    DROP query_phrase;
    ```

此时，我们有一个创建短语列表的有效方法，如下所示。这些按字母顺序显示:

```
SELECT query_phrase
FROM sentiment140_queries
ORDER BY 1;
```

我们还可以通过在两个表之间执行连接来找出带有给定短语(`baseball`)的 tweets:

```
SELECT s.id, s.tweet_text, sq.query_phrase
FROM sentiment140 s
INNER JOIN sentiment140_queries sq
  ON s.query_id = sq.query_id
WHERE sq.query_phrase = 'baseball';
```

此时，我们有一个清理过的`sentiment140`表和四个新表来保存各种提取和清理过的值，包括标签、用户提及、URL 和查询短语。我们的`tweet_text`和`date_clean`列是干净的，我们有一个查询短语的查找表。

## 第九步——记录你做了什么

随着九个步骤的清洁和多种语言和工具的使用，毫无疑问，我们会犯错误，不得不重复一个步骤。如果我们必须向别人描述我们做了什么，我们几乎肯定会很难记住每件事的确切步骤和所有原因。

为了避免我们自己在这个过程中犯错误，我们有必要记录我们的清洁步骤。日志至少应按照执行顺序包含这些内容:

*   每个 SQL 语句
*   每个 Excel 函数或文本编辑器例程，包括截图，如果必要的
*   每个剧本
*   关于你为什么做每件事的笔记和评论

另一个好主意是在每个阶段创建表的备份，例如，我们在对`sentiment140`表执行`UPDATE`之前创建了一个备份，我们讨论了在创建新的`date_clean`列之后执行备份。备份很容易，如果您决定不需要备份的表，可以随时删除它。



# 总结

在这一章中，我们使用了一个样本数据集，一个名为 Sentiment140 的 tweets 集合，来学习如何在关系数据库管理系统中清理和操作数据。我们在 Excel 中执行了一些基本的清理过程，然后我们回顾了如何将数据从 CSV 文件中取出并放入数据库。此时，剩下的清理过程是在 RDBMS 内部执行的。我们学习了如何将字符串转换成适当的日期，然后我们从 tweet 文本中提取三种数据，最终将这些提取的值转移到新的、干净的表中。接下来，我们学习了如何创建当前存储效率低下的值的查找表，从而允许我们用高效的数字查找值更新原始表。最后，因为我们执行了很多步骤，并且因为我们所做的事情总是有可能出现错误或误解，所以我们回顾了一些策略来记录我们的清洁程序。

在下一章中，我们将把我们的视角从清理给我们的东西转移到准备清理的数据供他人使用。我们将学习一些最佳实践来创建需要最少他人清理的数据集。