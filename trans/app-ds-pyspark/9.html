<html lang="en">
<head><title>Deploying Machine Learning Models</title>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<link href="../css/springer_epub.css" rel="styleSheet" type="text/css"/>
</head>
<body>
 
<!--Begin Abstract--><h1 class="ChapterTitle" lang="en">9.部署机器学习模型</h1>

 
<!--End Abstract--><p>本章将指导您在部署机器学习模型时应遵循的最佳实践。我们将使用三种方法来演示生产部署，如下所示:</p>
<ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par3">使用 HDFS 对象和 pickle 文件的模型部署</p></li>
<li><p class="Para" id="Par4">使用 Docker 进行模型部署</p></li>
<li><p class="Para" id="Par5">实时评分 API</p></li>
</ul>

<p class="Para" id="Par6">在前一章中，您已经看到了第一种技术。在本节中，我们将更详细地介绍它。提供的代码是构建模型和评估结果的起始代码。</p>
<h2 class="Heading">起始代码</h2>
<p>这段代码只是一个模板脚本，供您加载<em class="EmphasisTypeItalic "> bank-full.csv </em>文件并构建模型。如果您已经有了模型，可以跳过这一部分。</p>
<pre>#default parameters
filename = "bank-full.csv"
target_variable_name = "y"

#load datasets
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
df = spark.read.csv(filename, header=True, inferSchema=True, sep=';')
df.show()

#convert target column variable to numeric
from pyspark.sql import functions as F
df = df.withColumn('y', F.when(F.col("y") == 'yes', 1).otherwise(0))

#datatypes of each column and treat them accordingly
# identify variable types function
def variable_type(df):
    vars_list = df.dtypes
    char_vars = []
    num_vars = []
    for i in vars_list:
        if i[1] in ('string'):
            char_vars.append(i[0])
        else:
            num_vars.append(i[0])
    return char_vars, num_vars

char_vars, num_vars = variable_type(df)
num_vars.remove(target_variable_name)

from pyspark.ml.feature import StringIndexer



from pyspark.ml import Pipeline

# convert categorical to numeric using label encoder option
def category_to_index(df, char_vars):

    char_df = df.select(char_vars)
    indexers = [StringIndexer(inputCol=c, outputCol=c+"_index", handleInvalid="keep") for c in char_df.columns]
    pipeline = Pipeline(stages=indexers)
    char_labels = pipeline.fit(char_df)
    df = char_labels.transform(df)
    return df, char_labels

df, char_labels = category_to_index(df, char_vars)
df = df.select([c for c in df.columns if c not in char_vars])

# rename encoded columns to original variable name
def rename_columns(df, char_vars):
    mapping = dict(zip([i + '_index' for i in char_vars], char_vars))
    df = df.select([F.col(c).alias(mapping.get(c, c)) for c in df.columns])
    return df

df = rename_columns(df, char_vars)

from pyspark.ml.feature import VectorAssembler

#assemble individual columns to one column - 'features'
def assemble_vectors(df, features_list, target_variable_name):
    stages = []
    #assemble vectors
    assembler = VectorAssembler(inputCols=features_list, outputCol="features")
    stages = [assembler]
    #select all the columns + target + newly created 'features' column
    selectedCols = [target_variable_name, 'features'] + features_list
    #use pipeline to process sequentially
    pipeline = Pipeline(stages=stages)
    #assembler model
    assembleModel = pipeline.fit(df)
    #apply assembler model on data



    df = assembleModel.transform(df).select(selectedCols)

    return df, assembleModel, selectedCols

#exclude target variable and select all other feature vectors
features_list = df.columns
#features_list = char_vars #this option is used only for ChiSqselector
features_list.remove(target_variable_name)

# apply the function on our DataFrame
df, assembleModel, selectedCols = assemble_vectors(df, features_list, target_variable_name)

#train test split – 70/30 split
train, test = df.randomSplit([0.7, 0.3], seed=12345)
train.count(), test.count()

# Random forest model
from pyspark.ml.classification import RandomForestClassifier

clf = RandomForestClassifier(featuresCol='features', labelCol="y")
clf_model = clf.fit(train)
print(clf_model.featureImportances)
print(clf_model.toDebugString)
train_pred_result = clf_model.transform(train)
test_pred_result = clf_model.transform(test)

# Validate random forest model
from pyspark.mllib.evaluation import MulticlassMetrics
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.sql.types import IntegerType, DoubleType

def evaluation_metrics(df, target_variable_name):
    pred = df.select("prediction", target_variable_name)
    pred = pred.withColumn(target_variable_name, pred[target_variable_name].cast(DoubleType()))
    pred = pred.withColumn("prediction", pred["prediction"].cast(DoubleType()))
    metrics = MulticlassMetrics(pred.rdd.map(tuple))
    # confusion matrix
    cm = metrics.confusionMatrix().toArray()
    acc = metrics.accuracy #accuracy
    misclassification_rate = 1 - acc #misclassification rate



    precision = metrics.precision(1.0) #precision
    recall = metrics.recall(1.0) #recall
    f1 = metrics.fMeasure(1.0) #f1-score
    #roc value
    evaluator_roc = BinaryClassificationEvaluator(labelCol=target_variable_name, rawPredictionCol="rawPrediction", metricName="areaUnderROC")
    roc = evaluator_roc.evaluate(df)
    evaluator_pr = BinaryClassificationEvaluator(labelCol=target_variable_name, rawPredictionCol="rawPrediction", metricName="areaUnderPR")
    pr = evaluator_pr.evaluate(df)
    return cm, acc, misclassification_rate, precision, recall, f1, roc, pr
    train_cm, train_acc, train_miss_rate, train_precision, \
        train_recall, train_f1, train_roc, train_pr = evaluation_metrics(train_pred_result, target_variable_name)
test_cm, test_acc, test_miss_rate, test_precision, \
        test_recall, test_f1, test_roc, test_pr = evaluation_metrics(test_pred_result, target_variable_name)

#Comparision of metrics
print('Train accuracy - ', train_acc, ', Test accuracy - ', test_acc)
print('Train misclassification rate - ', train_miss_rate, ', Test misclassification rate - ', test_miss_rate)
print('Train precision - ', train_precision, ', Test precision - ', test_precision)
print('Train recall - ', train_recall, ', Test recall - ', test_recall)
print('Train f1 score - ', train_f1, ', Test f1 score - ', test_f1)
print('Train ROC - ', train_roc, ', Test ROC - ', test_roc)
print('Train PR - ', train_pr, ', Test PR - ', test_pr)

# make confusion matrix charts
import seaborn as sns
import matplotlib.pyplot as plt




def make_confusion_matrix_chart(cf_matrix_train, cf_matrix_test):

    list_values = ['0', '1']

    plt.figure(1, figsize=(10,5))
    plt.subplot(121)
    sns.heatmap(cf_matrix_train, annot=True, yticklabels=list_values,
                                xticklabels=list_values, fmt="g")
    plt.ylabel("Actual")
    plt.xlabel("Pred")
    plt.ylim([0,len(list_values)])
    plt.title('Train data predictions')

    plt.subplot(122)
    sns.heatmap(cf_matrix_test, annot=True, yticklabels=list_values,
                                xticklabels=list_values, fmt="g")
    plt.ylabel("Actual")
    plt.xlabel("Pred")
    plt.ylim([0,len(list_values)])
    plt.title('Test data predictions')

    plt.tight_layout()
    return None
make_confusion_matrix_chart(train_cm, test_cm)

#Make ROC chart and PR curve
from pyspark.mllib.evaluation import BinaryClassificationMetrics

class CurveMetrics(BinaryClassificationMetrics):
    def __init__(self, *args):
        super(CurveMetrics, self).__init__(*args)

    def _to_list(self, rdd):
        points = []
        results_collect = rdd.collect()
        for row in results_collect:
            points += [(float(row._1()), float(row._2()))]
        return points

    def get_curve(self, method):
        rdd = getattr(self._java_model, method)().toJavaRDD()
        return self._to_list(rdd)

import matplotlib.pyplot as plt




def plot_roc_pr(df, target_variable_name, plot_type, legend_value, title):

    preds = df.select(target_variable_name,'probability')
    preds = preds.rdd.map(lambda row: (float(row['probability'][1]), float(row[target_variable_name])))
    # Returns as a list (false positive rate, true positive rate)
    points = CurveMetrics(preds).get_curve(plot_type)
    plt.figure()
    x_val = [x[0] for x in points]
    y_val = [x[1] for x in points]
    plt.title(title)

    if plot_type == 'roc':
        plt.xlabel('False Positive Rate (1-Specificity)')
        plt.ylabel('True Positive Rate (Sensitivity)')
        plt.plot(x_val, y_val, label = 'AUC = %0.2f' % legend_value)
        plt.plot([0, 1], [0, 1], color="red", linestyle="--")

    if plot_type == 'pr':
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.plot(x_val, y_val, label = 'Average Precision = %0.2f' % legend_value)
        plt.plot([0, 1], [0.5, 0.5], color="red", linestyle="--")

    plt.legend(loc = 'lower right')
    return None

plot_roc_pr(train_pred_result, target_variable_name, 'roc', train_roc, 'Train ROC')
plot_roc_pr(test_pred_result, target_variable_name, 'roc', test_roc, 'Test ROC')
plot_roc_pr(train_pred_result, target_variable_name, 'pr', train_pr, 'Train Precision-Recall curve')
plot_roc_pr(test_pred_result, target_variable_name, 'pr', test_pr, 'Test Precision-Recall curve')




</pre>

<h2 class="Heading">保存模型对象并创建分数代码</h2>
<p>到目前为止，我们已经构建了一个可靠的模型，并使用多个指标对其进行了验证。是时候保存模型对象以备将来评分和创建评分代码了。</p>
<ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par9">模型对象是 PySpark 或 Python 参数，包含来自训练过程的信息。我们必须确定复制结果所需的所有相关模型对象，而无需重新训练模型。一旦被识别，它们被保存在一个特定的位置(HDFS 或其他路径)，并在以后的评分中使用。</p></li>
<li><p class="Para" id="Par10">分数代码包含在新数据集上运行模型对象以产生分数的基本代码。请记住，分数代码仅用于评分过程。你不应该有一个分数代码的模型训练脚本。</p></li>
</ul>

<h3 class="Heading">模型对象</h3>
<p>在这个模型示例中，我们需要保存六个对象以备将来使用，如下所示:</p>
<ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par12"><code>char_labels</code>–该对象用于对新数据应用标签编码。</p></li>
<li><p class="Para" id="Par13"><code>assembleModel</code>–该对象用于组合向量，并使数据为评分做好准备。</p></li>
<li><p class="Para" id="Par14"><code>clf_model</code>–该对象是随机森林分类器模型。</p></li>
<li><p class="Para" id="Par15"><code>features_list</code>–该对象包含新数据中要使用的输入特征列表。</p></li>
<li><p class="Para" id="Par16"><code>char_vars</code>–字符变量</p></li>
<li><p class="Para" id="Par17"><code>num_vars</code>–数字变量</p></li>
</ul>

<p>下面的代码在提供的路径中保存了评分所需的所有对象和 pickle 文件。</p>
<pre>import os
import pickle

path_to_write_output = '/home/work/Desktop/score_code_objects'
#create directoyr, skip if already exists
try:
    os.mkdir(path_to_write_output)
except:
    pass




#save pyspark objects
char_labels.write().overwrite().save(path_to_write_output + '/char_label_model.h5')
assembleModel.write().overwrite().save(path_to_write_output + '/assembleModel.h5')
clf_model.write().overwrite().save(path_to_write_output + '/clf_model.h5')

#save python object
list_of_vars = [features_list, char_vars, num_vars]
with open(path_to_write_output + '/file.pkl', 'wb') as handle:
    pickle.dump(list_of_vars, handle)

</pre>

<h3 class="Heading">分数代码</h3>
<p>这是一个用于执行评分操作的 Python 脚本文件。如前所述，这个脚本文件不应该包含模型训练脚本。分数代码至少应该完成以下任务:</p>
<ol><li class="ListItem"><p class="Para" id="Par20">导入必要的包</p>
 </li>
<li class="ListItem"><p class="Para" id="Par21">读取新的输入文件以执行评分</p>
 </li>
<li class="ListItem"><p class="Para" id="Par22">读取从训练过程中保存的模型对象</p>
 </li>
<li class="ListItem"><p class="Para" id="Par23">使用模型对象执行必要的转换</p>
 </li>
<li class="ListItem"><p class="Para" id="Par24">使用分类器/回归对象计算模型得分</p>
 </li>
<li class="ListItem"><p class="Para" id="Par25">在指定位置输出最终分数</p>
 </li>
</ol>

<p>首先，我们将创建一个<em class="EmphasisTypeItalic "> helper.py </em>脚本，其中包含执行评分所需的所有代码。此处提供了要粘贴到文件中的代码:</p>
<pre>#helper functions – helper.py file
from pyspark.sql import functions as F
import pickle
from pyspark.ml import PipelineModel
from pyspark.ml.classification import RandomForestClassificationModel
from pyspark.sql.functions import udf
from pyspark.sql.types import IntegerType, DoubleType




# read model objects saved from the training process
path_to_read_objects = '/home/work/Desktop/score_code_objects'

#pyspark objects
char_labels = PipelineModel.load(path_to_read_objects + '/char_label_model.h5')
assembleModel = PipelineModel.load(path_to_read_objects + '/assembleModel.h5')
clf_model = RandomForestClassificationModel.load(path_to_read_objects + '/clf_model.h5')
#python objects
with open(path_to_read_objects + '/file.pkl', 'rb') as handle:
    features_list, char_vars, num_vars = pickle.load(handle)

#make necessary transformations
def rename_columns(df, char_vars):
    mapping = dict(zip([i + '_index' for i in char_vars], char_vars))
    df = df.select([F.col(c).alias(mapping.get(c, c)) for c in df.columns])
    return df

# score the new data
def score_new_df(scoredf):
    X = scoredf.select(features_list)
    X = char_labels.transform(X)
    X = X.select([c for c in X.columns if c not in char_vars])
    X = rename_columns(X, char_vars)
    final_X = assembleModel.transform(X)
    final_X.cache()
    pred = clf_model.transform(final_X)



    pred.cache()
    split_udf = udf(lambda value: value[1].item(), DoubleType())
    pred = pred.select('prediction', split_udf('probability').alias('probability'))
    return pred

</pre>
<p>创建文件后，可以使用下面的代码对新数据进行评分。您可以将脚本保存在一个<em class="EmphasisTypeItalic "> run.py </em>文件中。</p>
<pre># import necessary packages
from pyspark.sql import SparkSession
from helper import *

# new data to score
path_to_output_scores = '.'
filename = "score_data.csv"
spark = SparkSession.builder.getOrCreate()
score_data = spark.read.csv(filename, header=True, inferSchema=True, sep=';')

# score the data
final_scores_df = score_new_df(score_data)
#final_scores_df.show()
final_scores_df.repartition(1).write.format('csv').mode("overwrite").options(sep='|', header="true").save(path_to_output_scores + "/predictions.csv")

</pre>
<p class="Para" id="Par28">最终分数可在<em class="EmphasisTypeItalic "> final_scores_df </em>数据帧中获得，该数据集在最后一步保存为 csv 文件。你需要确保<em class="EmphasisTypeItalic "> run.py </em>和<em class="EmphasisTypeItalic "> helper.py </em>文件存在于同一个目录中。就是这样。我们有一个可靠的评分代码，可以随时部署到生产环境中。我们将在接下来的小节中讨论生产实现。</p>


<h2 class="Heading">使用 HDFS 对象和 Pickle 文件的模型部署</h2>
<p>这是在生产中部署模型的一种非常简单的方法。当您有多个模型要评分时，您可以手动提交您在上一步中创建的每个<em class="EmphasisTypeItalic "> run.py </em>脚本，或者创建一个调度程序来定期运行脚本。无论是哪种情况，下面的代码都将帮助您完成这项任务:</p>
<pre>spark-submit run.py

</pre>
<p class="Para" id="Par30">如前所述，<em class="EmphasisTypeItalic "> run.py </em>和<em class="EmphasisTypeItalic "> helper.py </em>文件应该在同一个目录下，这样代码才能运行。您可以配置前面的命令，在提交作业时指定<code>configurations</code>。当您处理大型数据集时,<code>configurations</code>非常有用，因为您可能需要使用执行器、内核和内存选项。您可以通过以下链接了解<code>spark-submit</code>和<code>configuration</code>选项:</p>
<p class="Para" id="Par31"><a href="https://spark.apache.org/docs/latest/submitting-applications.html">T2<code>https://spark.apache.org/docs/latest/submitting-applications.html</code></a></p>
<p class="Para" id="Par32"><a href="https://spark.apache.org/docs/latest/configuration.html">T2<code>https://spark.apache.org/docs/latest/configuration.html</code></a></p>

<h2 class="Heading">使用 Docker 进行模型部署</h2>
<p>让我们换个方式，使用 Docker 部署一个模型。与传统的评分方法(如<code>spark-submit</code>)相比，基于 Docker 的部署在很多方面都很有用。</p>
<ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par34">可移植性——你可以将你的应用移植到任何平台上，并立即使其工作。</p></li>
<li><p class="Para" id="Par35">容器——整个应用程序是独立的。让您的应用程序工作所需的所有代码和可执行文件都包含在容器中。</p></li>
<li><p class="Para" id="Par36">微服务架构——我们可以将传统的单片应用程序分离为微服务，而不是传统的单片应用程序。这样，我们可以根据需要修改或扩大/缩小单个微服务。</p></li>
<li><p class="Para" id="Par37">更快的软件交付周期–与持续集成和持续开发(CI/CD)流程兼容，因此应用程序可以轻松部署到生产环境中</p></li>
<li><p class="Para" id="Par38">当 Spark 后端改变时，数据科学家/工程师不必花费额外的时间来修复他们的代码/错误。</p></li>
<li><p class="Para" id="Par39">与虚拟机相比，Docker 消耗的内存更少，因此在使用系统资源方面更高效。</p></li>
</ul>

<p>让我们继续使用 Docker 实现我们的模型。在我们开始之前，我们应该在一个目录中有以下文件(图<a href="#Fig1"> 9-1 </a>)。</p>
<p><img alt="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig1_HTML.jpg" src="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig1_HTML.jpg" style="width:36.9em"/></p>
<p>图 9-1</p><p class="SimplePara">目录结构</p>


<p class="Para" id="Par41">现在，大多数文件对您来说应该很熟悉了。我们有两个如图所示的新文件:<em class="EmphasisTypeItalic "> Dockerfile </em>和<em class="EmphasisTypeItalic "> requirements.txt </em>。<em class="EmphasisTypeItalic ">Docker 文件</em>包含了创建 Docker 容器的脚本，<em class="EmphasisTypeItalic "> requirements.txt </em>文件包含了让代码工作所需的所有附加包。让我们看看文件内容。</p>
<h3 class="Heading">requirements.txt 文件</h3>
<p>此处提供了该文件的内容，以便您可以将它们复制并粘贴到文件中(图<a href="#Fig2"> 9-2 </a>)。</p>
<p><img alt="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig2_HTML.jpg" src="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig2_HTML.jpg" style="width:14.78em"/></p>
<p>图 9-2</p><p class="SimplePara">requirements.txt 文件</p>



<h3 class="Heading">Dockerfile</h3>
<p>我们使用基本 Docker 图像作为 PySpark Jupyter 笔记本图像。在图像内部，我们创建一个名为<code>/deploy/</code>的工作目录，我们将把所有的模型对象和文件复制并粘贴到这个目录中(图<a href="#Fig3"> 9-3 </a>)。</p>
<p><img alt="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig3_HTML.jpg" src="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig3_HTML.jpg" style="width:43.12em"/></p>
<p>图 9-3</p><p class="SimplePara">Dockerfile</p>


<p>这是在连续步骤中完成的。此外，我们通过执行<em class="EmphasisTypeItalic "> requirements.txt </em>文件来安装所需的包，如第 4 行所示。最后，我们使用上一节中使用的<code>spark-submit</code>命令来执行模型。需要注意的一点是，容器中暴露了端口号 5000。我们将在实时计分部分使用该端口。暂时忽略这个端口。这里提供了<em class="EmphasisTypeItalic "> Dockerfile </em>脚本:</p>
<pre>FROM jupyter/pyspark-notebook:latest
WORKDIR /deploy/
COPY ./requirements.txt /deploy/
RUN pip install -r requirements.txt
EXPOSE 5000
COPY ./file.pkl /deploy/
COPY ./run.py /deploy/
COPY ./helper.py /deploy/
COPY ./assembleModel.h5 /deploy/assembleModel.h5
COPY ./char_label_model.h5 /deploy/char_label_model.h5
COPY ./clf_model.h5 /deploy/clf_model.h5
ENTRYPOINT ["spark-submit", "run.py"]

</pre>

<h3 class="Heading">在 helper.py 和 run.py 文件中所做的更改</h3>
<p>在<em class="EmphasisTypeItalic "> helper.py </em>文件中，更改以下行:</p>
<pre>path_to_read_objects ='/deploy'

</pre>
<p>在<em class="EmphasisTypeItalic "> run.py </em>文件中，更改以下几行:</p>
<pre>path_to_output_scores = '/localuser'
filename = path_to_output_scores + "/score_data.csv"

</pre>
<p class="Para" id="Par47">这些变化反映了 Docker <code>(/deploy)</code>中适当的目录路径，以及我们将在 Docker 执行期间稍后<code>(/localuser)</code>挂载的卷。</p>

<h3 class="Heading">创建 Docker 并执行分数代码</h3>
<p>要创建 Docker 容器，请在终端/命令提示符下执行以下代码:</p>
<pre>docker build -t scoring_image .

</pre>
<p>一旦构建了映像，就可以使用下面的命令执行该映像。您需要从评分数据所在的目录中执行该命令。</p>
<pre>docker run -p 5000:5000 -v ${PWD}:/localuser scoring_image:latest

</pre>
<p class="Para" id="Par50">就是这样。您应该会在本地文件夹中看到<em class="EmphasisTypeItalic "> predictions.csv </em>文件。到目前为止，我们一直使用批量评分流程。现在让我们转向实时评分 API。</p>


<h2 class="Heading">实时评分 API</h2>
<p>实时评分 API 用于对您的模型进行实时评分。它使您的模型能够嵌入到 web 浏览器中，以便获得预测并根据预测采取行动。这对于进行在线推荐、检查信用卡批准状态等非常有用。应用程序非常庞大，使用 Docker 管理这样的 API 变得更加容易。让我们使用 Flask 来实时实现我们的模型(图<a href="#Fig4"> 9-4 </a>)。烧瓶脚本存储在<em class="EmphasisTypeItalic "> app.py </em>文件中，其内容如下。</p>
<p><img alt="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig4_HTML.jpg" src="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig4_HTML.jpg" style="width:43.12em"/></p>
<p>图 9-4</p><p class="SimplePara">烧瓶 API 就绪</p>


<h3 class="Heading">app.py 文件</h3>
<p>
 
</p>
<pre>#app.py file
from flask import Flask, request, redirect, url_for, flash, jsonify, make_response
import numpy as np



import pickle
import json
import os, sys
# Path for spark source folder
os.environ['SPARK_HOME'] = '/usr/local/spark'
# Append pyspark  to Python Path
sys.path.append('/usr/local/spark/python')

from pyspark.sql import SparkSession
from pyspark import SparkConf, SparkContext
from helper import *

conf = SparkConf().setAppName("real_time_scoring_api")
conf.set('spark.sql.warehouse.dir', 'file:///usr/local/spark/spark-warehouse/')
conf.set("spark.driver.allowMultipleContexts", "true")
spark = SparkSession.builder.master('local').config(conf=conf).getOrCreate()
sc = spark.sparkContext

app = Flask(__name__)

@app.route('/api/', methods=['POST'])
def makecalc():

    json_data = request.get_json()



    #read the real time input to pyspark df
    score_data = spark.read.json(sc.parallelize(json_data))
    #score df
    final_scores_df = score_new_df(score_data)
    #convert predictions to Pandas DataFrame
    pred = final_scores_df.toPandas()
    final_pred = pred.to_dict(orient='rows')[0]
    return jsonify(final_pred)

if __name__ == '__main__':

    app.run(debug=True, host='0.0.0.0', port=5000)

</pre>
<p>我们还将更改我们的<em class="EmphasisTypeItalic "> Dockerfile </em>来将<em class="EmphasisTypeItalic "> app.py </em>文件复制到<code>/deploy</code>目录。在最后一个模块中，我们运行了<em class="EmphasisTypeItalic "> run.py </em>文件。在这个模块中，由<em class="EmphasisTypeItalic "> app.py </em>文件负责。以下是更新后的<em class="EmphasisTypeItalic "> Dockerfile </em>内容:</p>
<pre>FROM jupyter/pyspark-notebook:latest
WORKDIR /deploy/
COPY ./requirements.txt /deploy/
RUN pip install -r requirements.txt
EXPOSE 5000
COPY ./file.pkl /deploy/
COPY ./run.py /deploy/
COPY ./helper.py /deploy/
COPY ./assembleModel.h5 /deploy/assembleModel.h5
COPY ./char_label_model.h5 /deploy/char_label_model.h5
COPY ./clf_model.h5 /deploy/clf_model.h5
COPY ./app.py /deploy/
ENTRYPOINT ["python", "app.py"]

</pre>
<p class="Para" id="Par54">好了，我们要进行实时评分了。我们将很快编写 UI 代码。让我们首先使用<code>Postman</code>应用程序测试代码。</p>
<p>使用以下代码重新创建 Docker 映像:</p>
<pre>docker build -t scoring_image .

</pre>
<p>这次您将不需要体积映射，因为我们将执行实时评分。您需要确保本地系统的端口号被映射到 Docker 端口号。</p>
<pre>docker run -p 5000:5000 scoring_image:latest

</pre>
<p class="Para" id="Par57">一旦收到这条消息，您就可以切换回 Postman API 并测试这个应用程序。如果你还没有的话，可以在网上下载 Postman 并安装。启动应用程序并进行必要的更改，如下所示。</p>

<h3 class="Heading">邮递员 API</h3>
<p>本节面向没有 Postman API 背景的读者。如果您已经熟悉这一部分，请跳过这一部分(图<a href="#Fig5"> 9-5 </a>)。</p>
<p><img alt="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig5_HTML.jpg" src="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig5_HTML.jpg" style="width:43.12em"/></p>
<p>图 9-5</p><p class="SimplePara">邮递员 API</p>


<p>您需要创建一个 API 实例，方法是单击左上角的<em class="EmphasisTypeItalic "> New </em>按钮，然后单击<em class="EmphasisTypeItalic ">Request Create a basic Request</em>以获得如图<a href="#Fig6"> 9-6 </a>所示的窗口。</p>
<p><img alt="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig6_HTML.jpg" src="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig6_HTML.jpg" style="width:26.58em"/></p>
<p>图 9-6</p><p class="SimplePara">创建新的 API 请求。</p>


<p>现在，让我们在 API 中进行配置，如图<a href="#Fig7"> 9-7 </a>所示。</p>
<p><img alt="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig7_HTML.jpg" src="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig7_HTML.jpg" style="width:43.12em"/></p>
<p>图 9-7</p><p class="SimplePara">配置</p>


<p>让我们通过导航到<em class="EmphasisTypeItalic ">标题</em>选项卡来添加一个标题列。通过这个选项，API 知道它正在寻找一个 JSON 输入(图<a href="#Fig8"> 9-8 </a>)。</p>
<p><img alt="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig8_HTML.jpg" src="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig8_HTML.jpg" style="width:43.12em"/></p>
<p>图 9-8</p><p class="SimplePara">JSON 标题对象</p>


<p>终于，你准备得分了。既然您正在为测试提供输入，让我们将 body 类型切换到<code>raw</code>并选择<code>JSON</code>作为文本选项(图<a href="#Fig9"> 9-9 </a>)。</p>
<p><img alt="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig9_HTML.jpg" src="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig9_HTML.jpg" style="width:43.12em"/></p>
<p>图 9-9</p><p class="SimplePara">JSON 选择</p>



<h3 class="Heading">使用 Postman API 进行实时测试</h3>
<p>使用以下 JSON 对象进行测试。注意前面的方括号。这用于传递 JSON 对象列表。</p>
<pre>[{"age":58,"job":"management","marital":"married","education":"tertiary","default":"no","balance":2143,"housing":"yes","loan":"no","contact":"unknown","day":5,"month":"may","duration":261,"campaign":1,"pdays":-1,"previous":0,"poutcome":"unknown"}]

</pre>
<p>将 JSON 对象复制粘贴到 API 体中，点击<em class="EmphasisTypeItalic ">发送</em>(图<a href="#Fig10"> 9-10 </a>)。</p>
<p><img alt="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig10_HTML.jpg" src="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig10_HTML.jpg" style="width:23.62em"/></p>
<p>图 9-10</p><p class="SimplePara">实时分数</p>


<p class="Para" id="Par65">我们现在可以开始为我们的应用程序构建 UI 组件了。</p>

<h3 class="Heading">增进</h3>
<p>我们将使用一个 Python 包<code>streamlit</code>来构建用户界面(UI)。<code>streamlit</code>很容易与我们现有的代码集成。使用<code>streamlit</code>的最大优势之一是你可以编辑 Python 代码，并立即看到 web 应用中的实时变化。另一个优点是它需要最少的代码来构建 UI。在开始之前，我们先来看看图<a href="#Fig11"> 9-11 </a>所示的目录结构。父目录是<code>real_time_scoring</code>。我们创建了两个子目录:<code>pysparkapi</code>和<code>streamlitapi</code>。我们将把到目前为止讨论过的所有代码移到<code>pysparkapi</code>目录中。UI 代码将驻留在<code>streamlitapi</code>目录中。</p>
<p><img alt="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig11_HTML.jpg" src="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig11_HTML.jpg" style="width:23.62em"/></p>
<p>图 9-11</p><p class="SimplePara">UI 目录结构</p>



<h3 class="Heading">streamlitapi 目录</h3>
<p class="Para" id="Par67">让我们导航到<code>streamlitapi</code>目录，并在其中创建以下三个文件。</p>
<p><em class="EmphasisTypeItalic ">坞站样式</em></p>
<pre>FROM python:3.7-slim
WORKDIR /streamlit
COPY ./requirements.txt /streamlit
RUN pip install -r requirements.txt
COPY ./webapp.py /streamlit
EXPOSE 8501
CMD ["streamlit", "run", "webapp.py"]

</pre>
<p><em class="EmphasisTypeItalic "> requirements.txt </em></p>
<pre>streamlit
requests

</pre>
<p class="Para" id="Par70">以下文件包含 UI 代码。</p>
<p><em class="EmphasisTypeItalic "> webapp.py </em></p>
<pre>import streamlit as st
import requests
import datetime
import json

st.title('PySpark Real Time Scoring API')

# PySpark API endpoint
url = 'http://pysparkapi:5000'
endpoint = '/api/'

# description and instructions
st.write('''A real time scoring API for PySpark model.''')

st.header('User Input features')

def user_input_features():
    input_features = {}
    input_features["age"] = st.slider('Age', 18, 95)
    input_features["job"] = st.selectbox('Job', ['management', 'technician', 'entrepreneur', 'blue-collar', \
     'unknown', 'retired', 'admin.', 'services', 'self-employed', \
           'unemployed', 'housemaid', 'student'])
    input_features["marital"] = st.selectbox('Marital Status', ['married', 'single', 'divorced'])
    input_features["education"] = st.selectbox('Education Qualification', ['tertiary', 'secondary', 'unknown', 'primary'])
    input_features["default"] = st.selectbox('Have you defaulted before?', ['yes', 'no'])
    input_features["balance"]= st.slider('Current balance', -10000, 150000)
    input_features["housing"] = st.selectbox('Do you own a home?', ['yes', 'no'])
    input_features["loan"] = st.selectbox('Do you have a loan?', ['yes', 'no'])
    input_features["contact"] = st.selectbox('Best way to contact you', ['cellular', 'telephone', 'unknown'])
    date = st.date_input("Today's Date")
    input_features["day"] = date.day
    input_features["month"] = date.strftime("%b").lower()
    input_features["duration"] = st.slider('Duration', 0, 5000)
    input_features["campaign"] = st.slider('Campaign', 1, 63)
    input_features["pdays"] = st.slider('pdays', -1, 871)
    input_features["previous"] = st.slider('previous', 0, 275)
    input_features["poutcome"] = st.selectbox('poutcome', ['success', 'failure', 'other', 'unknown'])
    return [input_features]

json_data = user_input_features()

submit = st.button('Get predictions')



if submit:
    results = requests.post(url+endpoint, json=json_data)
    results = json.loads(results.text)
    st.header('Final Result')
    prediction = results["prediction"]
    probability = results["probability"]
    st.write("Prediction: ", int(prediction))
    st.write("Probability: ", round(probability,3))

</pre>
<p>通过运行以下代码，您可以在部署到 Docker 容器之前测试该代码:</p>
<pre>streamlit run webapp.py

</pre>
<p class="Para" id="Par73">如前所述，<code>streamlit</code>的交互特性让你可以用最少的代码动态设计定制的布局。好的，我们都准备好了。这三个文件位于<code>streamlitapi</code>目录中。我们现在有两个 Docker 容器——一个用于 PySpark flask，另一个用于<code>streamlit</code> UI。我们需要在这两个容器之间创建一个网络，以便 UI 与后端 PySpark API 进行交互。让我们切换到父目录<code>real_time_scoring</code>。</p>

<h3 class="Heading">实时计分目录</h3>
<p class="Para" id="Par74">一个<em class="EmphasisTypeItalic "> docker-compose </em>文件用于组合多个 docker 容器并创建一个 Docker 网络服务。让我们看看文件的内容。</p>
<p><em class="EmphasisTypeItalic ">码头-化合物. yml </em></p>
<pre>version: '3'

services:
  pysparkapi:
    build: pysparkapi/
    ports:
      - 5000:5000
    networks:
      - deploy_network
    container_name: pysparkapi

  streamlitapi:
    build: streamlitapi/
    depends_on:
      - pysparkapi
    ports:
        - 8501:8501
    networks:
      - deploy_network
    container_name: streamlitapi

networks:
  deploy_network:
    driver: bridge

</pre>

<h3 class="Heading">执行 docker 合成. yml 文件</h3>
<p>让我们使用下面的代码来执行<code>docker-compose.yml</code>文件:</p>
<pre>docker-compose build

</pre>
<p>Docker 网络服务被建立。让我们使用图<a href="#Fig12"> 9-12 </a>所示的代码来执行 Docker 服务。在切换到浏览器之前，启动服务并等待调试器 pin 出现需要一些时间。</p>
<p><img alt="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig12_HTML.jpg" src="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig12_HTML.jpg" style="width:43.12em"/></p>
<p>图 9-12</p><p class="SimplePara">Docker 服务创建</p>



<h3 class="Heading">实时评分 API</h3>
<p>要访问 UI，您需要在浏览器中插入以下链接:</p>
<pre>http://localhost:8501/

</pre>
<p>如果你使用了本书中的相同代码，用户界面应该如图 9-13 所示。我们在图中只显示了部分 UI。您可以继续输入特性的值，然后单击<em class="EmphasisTypeItalic ">获得预测</em>以获得结果。</p>
<p><img alt="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig13_HTML.jpg" src="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig13_HTML.jpg" style="width:41.32em"/></p>
<p>图 9-13</p><p class="SimplePara">细流 api</p>


<p><img alt="../images/500182_1_En_9_Chapter/500182_1_En_9_Figa_HTML.jpg" src="../images/500182_1_En_9_Chapter/500182_1_En_9_Figa_HTML.jpg" style="width:41.32em"/></p>

<p>当你想终止这个服务时，你可以使用下面的代码(图<a href="#Fig14"> 9-14 </a>):</p>
<p><img alt="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig14_HTML.jpg" src="../images/500182_1_En_9_Chapter/500182_1_En_9_Fig14_HTML.jpg" style="width:23.62em"/></p>
<p>图 9-14</p><p class="SimplePara">杀死码头工人服务</p>


<pre>docker-compose down

</pre>
<p class="Para" id="Par81">就是这样。我们已经用一个很好的前端实时部署了我们的模型。这个部署最好的部分是 UI 和 PySpark 位于不同的 Docker 容器中。它们可以单独管理和定制，并可以在需要时与一个<em class="EmphasisTypeItalic "> docker-compose </em>文件组合。这使得软件开发生命周期过程更加容易。</p>


<h2 class="Heading">摘要</h2>
<p>
 
</p>
<ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par83">我们详细介绍了模型部署过程。</p></li>
<li><p class="Para" id="Par84">我们使用批处理和实时处理部署了一个模型。</p></li>
<li><p class="Para" id="Par85">最后，我们使用<code>streamlit</code> API 创建了一个定制的 UI 前端，并使用<em class="EmphasisTypeItalic "> docker-compose </em>与我们的 PySpark API 服务相结合。</p></li>
</ul>

<p class="Para" id="Par86">干得好！在下一章中，您将了解实验和 PySpark 代码优化。继续学习，敬请关注。</p>



</body>
</html>