# 八、机器学习流程和自动化流水线

将模型投入生产是数据科学领域最具挑战性的任务之一。这是许多组织中持续存在的最后一英里问题之一。尽管有许多管理工作流的工具，但随着组织的成熟，其需求会发生变化，管理现有的模型会成为一项艰巨的任务。当我们后退一步，分析为什么它如此具有挑战性时，我们可以看到这是因为大多数组织中存在的结构。有一个维护生产平台的工程团队。数据科学工具集和生产平台之间存在差距。一些数据科学工作可以在 Jupyter Notebook 中开发，很少考虑云环境。一些数据流是在本地创建的，规模有限。这类应用程序在大量数据的情况下往往会出问题。软件开发周期中存在的最佳实践与机器学习生命周期不太一致，因为涉及到各种各样的任务。该标准主要由组织中的数据科学团队定义。此外，该领域的快速发展在模型的管理和部署方面留下了真空。

本章的目标是探索可用于管理和部署数据科学流水线的工具。我们将向您介绍业内可用的一些工具，并通过一个示例演示如何构建自动化流水线。我们将使用我们迄今为止学到的所有概念来构建这些流水线。到本章结束时，您将学会如何构建机器学习模块，并通过自定义数据预处理使其自动化。

我们将在本章中讨论以下主题:

*   MLflow。我们将介绍 MLflow 及其组件和优点，然后进行演示。在前半部分，我们的重点将是概念的安装和引入。在下半部分，我们将重点介绍在 MLflow 中实现一个模型。

*   自动化机器学习流水线。我们将介绍在 PySpark 中设计和实现自动化 ML 框架的概念。本节的前半部分主要侧重于实现，后半部分侧重于从流水线中生成的输出。

## MLflow

有了从前面章节中获得的知识，我们知道如何建立单独的监督和非监督模型。很多时候，我们会在选择最佳模型之前尝试多次迭代。我们如何跟踪所有的实验和我们使用的超参数？我们如何在多次实验后重现一个特定的结果？当有多种部署工具和环境时，我们如何将一个模型转移到产品中？有没有一种方法可以更好地管理机器学习工作流，并与更广泛的社区共享工作？这些是您在 PySpark 中轻松构建模型后会遇到的一些问题。Databricks 已经建立了一个名为 MLflow 的工具，它可以在管理机器学习工作流的同时，优雅地处理前面的一些问题。MLflow 被设计为开放接口和开源。人们通常倾向于在实验和向模型添加新特性之间来回切换。MLflow 可以轻松处理这样的实验。它是围绕 REST APIs 构建的，可以被多种工具轻松使用。这个框架还使得添加现有的机器学习代码块变得容易。它的开源特性使得在不同的团队之间共享工作流和模型变得很容易。简而言之，MLflow 提供了一个管理机器学习生命周期的框架，包括实验、再现性、部署和中央模型注册。

MLflow 组件如图 [8-1](#Fig1) 所示，并在以下列表中描述。

![img/500182_1_En_8_Fig1_HTML.png](img/500182_1_En_8_Fig1_HTML.png)

图 8-1

MLflow 组件

*   **跟踪:**帮助记录、记录代码、配置、数据和结果。它还提供了一种查询实验的机制。

*   **项目:**ml flow 的这个组件有助于将机器学习代码捆绑成一种格式，以便在多个平台上复制。

*   **模型:**这有助于在不同的环境中部署机器学习代码。

*   **模型注册:**这个组件就像一个存储库，在其中存储、管理和搜索模型。

该框架为数据工程师和数据科学家合作并高效构建数据流水线打开了通道。它还提供了在许多不同的系统之间使用和写入数据的能力。它为处理数据湖和批处理流平台中的结构化和非结构化数据开辟了一个通用框架。

对于第一次使用的用户来说，这可能看起来很抽象。我们将通过一个例子向您展示这些组件是如何有用和实现的。出于说明的目的，让我们采用我们在第 [6](6.html) 章中使用的相同银行数据集。

### MLflow 代码设置和安装

在本节中，我们将介绍支持 MLflow 所需的代码更改及其安装。我们将在所有演示中使用 PySpark Docker 版本。启动和运行 MLflow 有两个主要步骤，如下所示:

*   代码/脚本更改

*   Docker 端更改和 MLflow 服务器安装

代码变化很小。我们必须在现有的机器学习代码中添加一些 MLflow 函数来适应 MLflow。

常规代码可以很好地构建简单的随机森林模型，但如果我想通过更改输入变量或任何超参数设置来了解接收器操作特性(ROC)/其他指标(准确性、错误分类等)，该怎么办呢？嗯，如果是一个单一的变化，我们可以在 Excel 中记录每次运行，但这不是跟踪我们实验的有效方法。作为数据科学专业人员，我们进行了数百次实验，尤其是在模型微调阶段。有没有更好的方法来捕获和注释这些结果和日志？是的，我们可以通过物流跟踪功能来实现。现在，让我们重写一个简单的随机森林模型代码，添加 MLflow 组件。代码中的更改以粗体突出显示。

Note

我们将把下面的代码保存为。py 并在安装 MLflow 后通过 Docker 运行。

```py
# Import libraries
import pyspark
from pyspark.sql import SparkSession
import mlflow
import mlflow.spark
import sys
import time

from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator

```

注意我们已经导入了与 MLflow 相关的库，允许用户使用 PySpark。注意 MLflow 支持多种语言和包，包括 Spark 和 Python，并扩展了对 scikitlearn 和 tensorflow 等包的支持。

```py
spark = SparkSession.builder.appName("mlflow_example").getOrCreate()

filename = "/home/jovyan/work/bank-full.csv"
target_variable_name = "y"
from pyspark.sql import functions as F

df = spark.read.csv(filename, header=True, inferSchema=True, sep=';')
df = df.withColumn('label', F.when(F.col("y") == 'yes', 1).otherwise(0))
df = df.drop('y')
train, test = df.randomSplit([0.7, 0.3], seed=12345)

for k, v in df.dtypes:
    if v not in ['string']:
        print(k)

df = df.select(['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous', 'label'])

from pyspark.ml.feature import VectorAssembler
from pyspark.ml import Pipeline

def assemble_vectors(df, features_list, target_variable_name):
    stages = []
    #assemble vectors

    assembler = VectorAssembler(inputCols=features_list, outputCol="features")
    stages = [assembler]
    #select all the columns + target + newly created 'features' column
    selectedCols = [target_variable_name, 'features']
    #use pipeline to process sequentially
    pipeline = Pipeline(stages=stages)
    #assembler model
    assembleModel = pipeline.fit(df)
    #apply assembler model on data
    df = assembleModel.transform(df).select(selectedCols)

    return df

#exclude target variable and select all other feature vectors
features_list = df.columns
#features_list = char_vars #this option is used only for ChiSqselector
features_list.remove('label')

# apply the function on our DataFrame
assembled_train_df = assemble_vectors(train, features_list, 'label')
assembled_test_df = assemble_vectors(test, features_list, 'label')

print(sys.argv[1])
print(sys.argv[2])

```

这些是我们打算在每次实验中改变的系统论点。当我们执行这个脚本时，我们将把它作为系统参数传递。这里我们用了`maxBins`和`maxDepth`。您可以自由地在脚本中包含您想要记录的任何其他变量更改。

```py
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator

maxBinsVal = float(sys.argv[1]) if len(sys.argv) > 3 else 20
maxDepthVal = float(sys.argv[2]) if len(sys.argv) > 3 else 3

```

在下面的步骤中，我们用 MLflow 初始化机器学习代码部分，因为我们感兴趣的跟踪变量在这个范围内。您可以根据用例的需要修改范围。

```py
with mlflow.start_run():
    stages_tree=[]
    classifier = RandomForestClassifier(labelCol = 'label',featuresCol = 'features',maxBins=maxBinsVal, maxDepth=maxDepthVal)
    stages_tree += [classifier]
    pipeline_tree=Pipeline(stages=stages_tree)
    print('Running RFModel')
    RFmodel = pipeline_tree.fit(assembled_train_df)
    print('Completed training RFModel')

    predictions = RFmodel.transform(assembled_test_df)
    evaluator = BinaryClassificationEvaluator()
    print("Test Area Under ROC: " + str(evaluator.evaluate(predictions, {evaluator.metricName: "areaUnderROC"})))

    mlflow.log_param("maxBins", maxBinsVal)
    mlflow.log_param("maxDepth", maxDepthVal)
    mlflow.log_metric("ROC", evaluator.evaluate(predictions, {evaluator.metricName: "areaUnderROC"}))
    mlflow.spark.log_model(RFmodel,"spark-model")

```

在前面的代码片段中，我们添加了`log_param`和`log_metric`来捕获我们想要跟踪的信息片段。还要注意，我们正在使用`mlflow.spark.log_model`函数记录模型，这有助于在 MLflow 后端保存模型。这是一个可选语句，但是如果您想从 MLflow 注册模型，这是很方便的。通过对代码进行最少的更改，我们能够使用 MLflow 组件来适应现有的模型。为什么这么重要？随着数百个实验在多天内进行，跟踪参数和指标的变化可能变得具有挑战性。我们需要将前面的代码保存为 Python 文件。我们将使用 Spark 提交来执行这个 PySpark 代码。在本图中，我们将其保存为*chapter 9 _ ml flow _ example . py*。

转到 MLflow 设置的第二部分，是时候使用下面的命令启动 Docker 容器了。回想一下，这与我们在第 [1](1.html) 章中演示的 PySpark Docker 图像相同。我们还必须将前面的 Python 文件保存在映射到 Docker 容器的同一个本地路径中。以下 Docker 启动代码及其本地映射适用于 Mac 对于 Windows，请适当更改您的路径。

```py
docker run -it -p 5000:5000 -v /Users/ramcharankakarla/demo_data/:/home/jovyan/work/ jupyter/pyspark-notebook:latest bash

```

Note

MLflow 默认使用端口 5000。

当我们进入 Docker 时，我们喜欢改变工作路径(`cd /home/jovyan/work/`)，因为我们所有的本地信息，包括 Python 文件 *Chapter9_mlflow_example.py，*都映射在那里。以下两个命令帮助您更改目录，并在 Docker 中查看工作目录中的可用内容。

```py
cd work
ls

```

确保在此路径中看到 Python 文件。通常，Docker 映像可能不包含 MLflow。使用`pip freeze|grep mlflow`来验证 MLflow 包在映像中是否可用是一个很好的做法。如果 MLflow 不可用，请使用以下命令添加包:

```py
pip install mlflow

```

下一步是初始化 MLflow 服务器。这个服务器是用户界面的后端，它捕获与实验相关的所有信息。可以使用以下命令启动它。请注意，我们是在后台运行这个命令的。

```py
mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root /home/jovyan/work --host 0.0.0.0 --port 5000 &

```

注意这里的参数包含后端- `store-uri`和`default-artifact-root`。如果我们要注册或记录任何型号，必须填写`Backend-store-uri`。我们在这里使用默认的 sqlite MLflow 数据库(db)进行演示。这可以是任何其他后端存储，包括基于文件和数据库的后端存储。这增强了 MLflow 的查询能力。工件根是工件存储的位置，适用于大型数据。这是所有与实验相关的人工制品所在的地方。默认情况下，`--` `default-artifact-root` `is set`到`./mlruns.`的位置您可以通过给定一个合适的位置来覆盖此设置。MLflow 支持不同的工件存储，包括:

*   分布式文件系统

*   网络文件系统

*   ftp 服务器

*   SFTP 服务器

*   谷歌云存储(GCS)

*   Azure Blob 存储

*   亚马逊 S3

在创建实验时指定工件 URI 是很重要的，否则客户机和服务器将引用不同的物理位置；即不同磁盘位置上的相同路径(表 [8-1](#Tab1) )。

表 8-1

存储类型

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

存储类型

 | 

URI 格式

 |
| --- | --- |
| 文件传送协议 | `ftp://user:pass@host/path/to/directory` |
| science for the people 为人类服务的科学 | `sftp://user@host/path/to/directory` |
| 网络文件系统 | `/mnt/nfs` |
| 分布式文件系统 | `hdfs://<host>:<port>/<path>` |
| 蔚蓝的 | `wasbs://<container>@<storage-account>.blob.core.windows.net/<path>` |
| 亚马逊 S3 | `s3://<bucket>/<path>` |

对于一些存储类型，还需要对其他设置进行身份验证。对于 S3，可以从环境变量`AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY`或 IAM 配置文件中获取凭证。

### MLflow 用户界面演示

在本节中，我们将通过上一节中讨论的一个示例来浏览 MLflow 用户界面。以下是运行我们之前保存的脚本的三种不同设置:

```py
MLFLOW_TRACKING_URI="http://0.0.0.0:5000" spark-submit --master local[*] /home/jovyan/work/Chapter9_mlflow_example.py 16 3  --spark_autolog True

MLFLOW_TRACKING_URI="http://0.0.0.0:5000" spark-submit --master local[*] /home/jovyan/work/Chapter9_mlflow_example.py 16 5  --spark_autolog True

MLFLOW_TRACKING_URI="http://0.0.0.0:5000" spark-submit --master local[*] /home/jovyan/work/Chapter9_mlflow_example.py 32 5  --spark_autolog True

```

一旦我们运行完前面的三个命令，我们就可以开始启动 MLflow 用户界面了。在任何网络浏览器上，导航至 [`http://0.0.0.0:5000`](http://0.0.0.0:5000) `.`您将看到如图 [8-2](#Fig2) 所示的 MLflow 用户界面。

![img/500182_1_En_8_Fig2_HTML.jpg](img/500182_1_En_8_Fig2_HTML.jpg)

图 8-2

MLflow 用户界面

如您所见，用户界面将所有信息格式化为实验和模型。我们可以使用与每个实验中的每次运行相关的注释选项进行任何注释或记录。这个框架使我们能够通过多次运行来管理和运行多个实验。我们还可以选择根据参数设置过滤运行。这在机器学习生命周期中非常有用，因为与软件开发生命周期不同，我们倾向于基于模型的稳定性和准确性在新旧版本之间迭代。

每次运行还有详细的行级信息。我们可以深入研究并比较许多实验中的信息。在我们的示例中，我们只使用了一个单一的度量，但是在真实的场景中，我们可能还希望捕获每次运行的准确性、错误分类、提升和 KS 统计数据。然后，我们可以根据基于业务需求的可接受的度量标准，对每个迭代进行比较和排序。

当我们从先前的执行中选择几个迭代时，观察到*比较*按钮是活动的；在图 [8-2](#Fig2) 中显示为灰色。当我们点击*比较*时，它将比较这两次迭代，并给出不同运行中度量如何变化的图表(图 [8-3](#Fig3) )。在前面的代码中，我们已经明确设置了日志指标。从 Spark 的版本 3 开始，有一个自动记录的选项。

![img/500182_1_En_8_Fig3_HTML.jpg](img/500182_1_En_8_Fig3_HTML.jpg)

图 8-3

运行的 MLflow UI 比较

观察图 [8-4](#Fig4) 中的窗口；MLflow 自动生成 runids。我们可以使用这个特性来比较一个实验中的多次运行。当我们有足够的数据点时，我们可以直观地挑选出表现最好的模型。我们还可以通过改变参数来观察模型性能如何变化。

![img/500182_1_En_8_Fig4_HTML.jpg](img/500182_1_En_8_Fig4_HTML.jpg)

图 8-4

MLflow UI 比较窗口

如果我们返回 MLflow 用户界面的主页并点击任何一次运行，它将为我们提供关于模型运行的更多信息，如图 [8-5](#Fig5) 所示。这个运行窗口还为我们提供了注册模型的便利特性。注册模型将使其可用于评分。注册模型后，还会有多个其他功能开放。

![img/500182_1_En_8_Fig5_HTML.jpg](img/500182_1_En_8_Fig5_HTML.jpg)

图 8-5

MLflow UI 运行窗口

点击工件部分的 *spark-model* 文件将打开下一个窗口。这将包含运行该模型所需的所有元数据信息，如图 [8-6](#Fig6) 所示。

![img/500182_1_En_8_Fig6_HTML.jpg](img/500182_1_En_8_Fig6_HTML.jpg)

图 8-6

MLflow UI 模型信息

如果你点击根文件夹，它会给出一个在元数据中注册模型的选项，如图 [8-7](#Fig7) 所示。

![img/500182_1_En_8_Fig7_HTML.jpg](img/500182_1_En_8_Fig7_HTML.jpg)

图 8-7

MLflow UI 模型注册

添加如图 [8-8](#Fig8) 所示的模型，将模型推入模型选项卡。您可以通过导航到顶部的模型选项卡来验证这一点(图 [8-9](#Fig9) )。

![img/500182_1_En_8_Fig9_HTML.jpg](img/500182_1_En_8_Fig9_HTML.jpg)

图 8-9

ml 流程模型选项卡

![img/500182_1_En_8_Fig8_HTML.jpg](img/500182_1_En_8_Fig8_HTML.jpg)

图 8-8

MLflow UI 注册窗口

单击型号名称，您将导航到一个包含详细型号信息的新窗口。新窗口还会给出将模型推入三种不同环境的选项，如图 [8-10](#Fig10) 所示。

![img/500182_1_En_8_Fig10_HTML.jpg](img/500182_1_En_8_Fig10_HTML.jpg)

图 8-10

MLflow 模型环境

*   脚手架

*   生产

*   存档

当我们从模型开发过渡到模型生命周期结束时，通过将模型移动到不同的环境中，这有助于有效地管理模型生命周期。

所有这些都很棒，但是我们如何从这些流水线中获得新数据呢？当我们点击运行或模型信息时，我们得到模型元数据信息的完整路径，如图 [8-7](#Fig7) 所示。我们可以使用这些元数据对任何新数据进行评分。有多种方法可以实现这一点。让我们编写一个简单的代码来准备评分的数据，我们将使用图 [8-10](#Fig10) 所示的 best_roc_model。我们将下面的代码保存为 Python 文件，并通过 Docker 中的 *spark-submit* 来运行它。

```py
#Importing necessary libraries
import mlflow
import mlflow.spark
import sys
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("mlflow_predict").getOrCreate()

filename = "/home/jovyan/work/bank-full.csv"
target_variable_name = "y"
from pyspark.sql import functions as F
df = spark.read.csv(filename, header=True, inferSchema=True, sep=';')
df = df.withColumn('label', F.when(F.col("y") == 'yes', 1).otherwise(0))
df = df.drop('y')
train, test = df.randomSplit([0.7, 0.3], seed=12345)

for k, v in df.dtypes:
    if v not in ['string']:
        print(k)

df = df.select(['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous', 'label'])

from pyspark.ml.feature import VectorAssembler
from pyspark.ml import Pipeline

# assemble individual columns to one column - 'features'
def assemble_vectors(df, features_list, target_variable_name):
    stages = []
    #assemble vectors
    assembler = VectorAssembler(inputCols=features_list, outputCol="features")
    stages = [assembler]
    #select all the columns + target + newly created 'features' column
    selectedCols = [target_variable_name, 'features']
    #use pipeline to process sequentially
    pipeline = Pipeline(stages=stages)
    #assembler model
    assembleModel = pipeline.fit(df)
    #apply assembler model on data
    df = assembleModel.transform(df).select(selectedCols)

    return df

#exclude target variable and select all other feature vectors
features_list = df.columns
#features_list = char_vars #this option is used only for ChiSqselector
features_list.remove('label')

# apply the function on our dataframe

assembled_test_df = assemble_vectors(test, features_list, 'label')

print(sys.argv[1])

# model information from argument
model_uri=sys.argv[1]
print("model_uri:", model_uri)
model = mlflow.spark.load_model(model_uri)
print("model.type:", type(model))
predictions = model.transform(assembled_test_df)
print("predictions.type:", type(predictions))
predictions.printSchema()
df = predictions.select('rawPrediction','probability', 'label', 'features')
df.show(5, False)

```

这段代码是一个简单的流水线。我们还可以灵活地将评分函数包装成一个简单的 udf。保存代码后，我们需要在 Docker 中运行以下命令来获得结果:

```py
spark-submit --master local[*] Chapter9_predict_spark.py /home/jovyan/work/0/29a3dfabb34140129ba5043be306a7a2/artifacts/spark-model

```

这将给出如图 [8-11](#Fig11) 所示的输出。

![img/500182_1_En_8_Fig11_HTML.jpg](img/500182_1_En_8_Fig11_HTML.jpg)

图 8-11

MLflow 模型评分输出

所有上述内容和代码都在 Spark 中指定。MLflow 还支持其他多种风格。我们可以在 mleap 风格中序列化前面的流水线，这是一个托管 Spark 流水线的项目，对于不需要任何分布式计算的较小数据集，没有 Spark 上下文。MLflow 还能够以 MLflow 项目格式将代码发布到 GitHub，使得任何人都可以轻松运行代码。

## 自动化机器学习流水线

机器学习的生命周期是一个迭代的过程。我们往往会来回调整参数、输入和数据流水线。随着数据管理流水线数量的增加，这很快会变得很麻烦。创建自动化流水线可以节省大量时间。从前面的章节中，我们已经学习了数据操作、算法和建模技术。现在，让我们将它们放在一起，创建一个自动化的 PySpark 流，它可以生成用于快速实验的基线模型。

对于这个实验，我们将使用来自 Kaggle ( [`https://www.kaggle.com/shrutimechlearn/churn-modelling#Churn_Modelling.csv`](https://www.kaggle.com/shrutimechlearn/churn-modelling%2523Churn_Modelling.csv) )的流失数据集。这个银行数据集包含关于客户属性和谁进行了交易的数据。我们的目标是根据给定的属性识别将流失的客户。表 [8-2](#Tab2) 列出了属性。

表 8-2

[计]元数据

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

圆柱

 | 

描述

 |
| --- | --- |
| 行号 | 标识符 |
| 客户号 | 客户的唯一 ID |
| 姓 | 客户的姓氏 |
| 信用评分 | 客户的信用评分 |
| 地理 | 客户所属的国家 |
| 性别 | 男性或女性 |
| 年龄 | 顾客年龄 |
| 任期 | 客户在银行工作的年数 |
| 保持平衡 | 客户的银行余额 |
| NumOfProducts | 客户正在使用的银行产品数量 |
| 哈斯卡尔 | 客户是否持有银行信用卡的二进制标志 |
| IsActiveMember | 客户是否为银行有效成员的二进制标志 |
| 估计销售额 | 客户的估计工资(美元) |
| 激动的 | 二进制标志:如果客户关闭了银行账户，则为 1；如果客户被保留，则为 0 |

### 流水线要求和框架

当设计任何流水线时，必须定义我们期望的输出和需求。首先，让我们定义我们认为必要的输出。由于我们正在处理一个二元目标，最好有以下输出:

*   **KS:** Kolmogorov-Smirnov 检验是一种区分商品和不商品的方法。越高越好..

*   **ROC:** (ROC)曲线是一种比较诊断试验的方法。这是真阳性率与假阳性率的关系图。越高越好。

*   **准确率:**(真阳性+真阴性)/(真阳性+真阴性+假阳性+假阴性)。越高越好。

现在，让我们列出自动化模型构建工具的需求。

*   模块应该能够自己处理缺失值和分类值。

*   我们也更喜欢模块来处理变量选择。

*   我们还希望这个模块在选择最终变量之前测试多种算法。

*   让该模块根据首选指标比较不同的算法，并为我们选择冠军和挑战者模型。

*   如果它可以通过前缀或后缀或变量名来删除某些变量，那就太方便了。这可以支持多次迭代和调整输入变量。

*   模块应该能够收集和存储所有的输出指标，并整理它们以生成文档供以后参考。

*   最后，如果该模块可以保存模型对象并自动生成评分代码，那么我们就可以部署所选的模型了。

我们如何编写这些需求？还记得我们在第 [3](3.html) (图 [8-12](#Fig12) )章中讨论的 CRISP–DM 框架吗？我们在这里使用这个框架作为我们的架构蓝图。

![img/500182_1_En_8_Fig12_HTML.png](img/500182_1_En_8_Fig12_HTML.png)

图 8-12

脆——DM

现在，这些是大量的需求，如果处理不当，会很快变得难以承受。我们定义了输出和需求，但是我们期望的输入是什么呢？可能是来自数据存储或平面文件的数据集。有了这些指定的信息，我们就可以通过将代码分解成逻辑步骤来开始一步一步地构建流水线。这将是一个代码密集的章节，你将能够使用这个模块并创建你自己的基线自动化模型。我们将在每个逻辑步骤中将任务分解成子任务。这些可以大致分为以下几类:

*   数据操作

*   特征选择

*   模型结构

*   度量计算

*   验证和绘图生成

*   型号选择

*   分数代码创建

*   整理结果

*   框架来处理所有前面的步骤

## 数据操作

在本节中，我们将定义一些处理数据的常用函数，如下所示:

*   缺失值百分比计算

*   输入数据的元数据分类

*   使用标签编码器处理分类数据

*   输入缺失值

*   重命名分类列

*   组合要素和标签

*   数据拆分为培训、测试和验证

*   组装向量

*   缩放输入变量

```py
from pyspark.ml.feature import StringIndexer
from pyspark.ml import Pipeline
from pyspark.sql import *
from pyspark.sql.types import *
from pyspark.sql import functions as F
from pyspark.ml.feature import IndexToString
from pyspark.sql.functions import col
from pyspark.ml.feature import StandardScaler
from pyspark.ml.feature import VectorAssembler

#    1\. Missing value calculation

def missing_value_calculation(X, miss_per=0.75):

    missing = X.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in X.columns])
    missing_len = X.count()
    final_missing = missing.toPandas().transpose()
    final_missing.reset_index(inplace=True)
    final_missing.rename(columns={0:'missing_count'},inplace=True)
    final_missing['missing_percentage'] = final_missing['missing_count']/missing_len
    vars_selected = final_missing['index'][final_missing['missing_percentage'] <= miss_per]
    return vars_selected

#    2\. Metadata categorization

def identify_variable_type(X):

    l = X.dtypes
    char_vars = []
    num_vars = []
    for i in l:
        if i[1] in ('string'):
            char_vars.append(i[0])
        else:
            num_vars.append(i[0])
    return char_vars, num_vars

#    3\. Categorical to Numerical using label encoders

def categorical_to_index(X, char_vars):
    chars = X.select(char_vars)
    indexers = [StringIndexer(inputCol=column, outputCol=column+"_index",handleInvalid="keep") for column in chars.columns]
    pipeline = Pipeline(stages=indexers)
    char_labels = pipeline.fit(chars)
    X = char_labels.transform(X)
    return X, char_labels

#    4\. Impute Numerical columns with a specific value

. The default is set to 0.

def numerical_imputation(X,num_vars, impute_with=0):
    X = X.fillna(impute_with,subset=num_vars)
    return X

#    5\. Rename categorical columns

def rename_columns(X, char_vars):
    mapping = dict(zip([i+ '_index' for i in char_vars], char_vars))
    X = X.select([col(c).alias(mapping.get(c, c)) for c in X.columns])
    return X

#    6\. Combining features and labels

def join_features_and_target(X, Y):

    X = X.withColumn('id', F.monotonically_increasing_id())
    Y = Y.withColumn('id', F.monotonically_increasing_id())
    joinedDF = X.join(Y,'id','inner')
    joinedDF = joinedDF.drop('id')
    return joinedDF

#    7\. Data splitting to training, testing, and validation

def train_valid_test_split(df, train_size=0.4, valid_size=0.3,seed=12345):

    train, valid, test = df.randomSplit([train_size, valid_size,1-train_size-valid_size], seed=12345)
    return train,valid,test

#    8\. Assembling vectors

def assembled_vectors(train,list_of_features_to_scale,target_column_name):

    stages = []
    assembler = VectorAssembler(inputCols=list_of_features_to_scale, outputCol="features")
    stages=[assembler]
    selectedCols = [target_column_name,'features'] + list_of_features_to_scale

    pipeline = Pipeline(stages=stages)
    assembleModel = pipeline.fit(train)

    train = assembleModel.transform(train).select(selectedCols)
    return train

#    9\. Scaling input variables

def scaled_dataframes(train,valid,test,list_of_features_to_scale,target_column_name):

    stages = []
    assembler = VectorAssembler(inputCols=list_of_features_to_scale, outputCol="assembled_features")
    scaler = StandardScaler(inputCol=assembler.getOutputCol(), outputCol="features")
    stages=[assembler,scaler]
    selectedCols = [target_column_name,'features'] + list_of_features_to_scale

    pipeline = Pipeline(stages=stages)
    pipelineModel = pipeline.fit(train)

    train = pipelineModel.transform(train).select(selectedCols)
    valid = pipelineModel.transform(valid).select(selectedCols)
    test = pipelineModel.transform(test).select(selectedCols)

    return train, valid, test, pipelineModel

```

## 特征选择

该模块列出变量清单，并选择最具预测性的前几个变量。它使用随机森林来识别顶级变量。

```py
import pandas as pd
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

# The module below is used to draw the feature importance plot
def draw_feature_importance(user_id, mdl_ltrl, importance_df):

    importance_df = importance_df.sort_values('Importance_Score')
    plt.figure(figsize=(15,15))
    plt.title('Feature Importances')
    plt.barh(range(len(importance_df['Importance_Score'])), importance_df['Importance_Score'], align="center")
    plt.yticks(range(len(importance_df['Importance_Score'])), importance_df['name'])
    plt.ylabel('Variable Importance')
    plt.savefig('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/' + 'Features selected for modeling.png', bbox_inches="tight")
    plt.close()
    return None

# The module below is used to save the feature importance as an Excel file
def save_feature_importance(user_id, mdl_ltrl, importance_df):
    importance_df.drop('idx',axis=1,inplace=True)
    importance_df = importance_df[0:30]
    importance_df.to_excel('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/' + 'feature_importance.xlsx')
    draw_feature_importance(user_id, mdl_ltrl, importance_df)
    return None

# The following module

is used to calculate the feature importance for each variable based on the Random Forest output. The feature importance is used to reduce the final variable list to 30.

def ExtractFeatureImp(featureImp, dataset, featuresCol):
    """
    Takes in a feature importance from a random forest / GBT model and maps it to the column names
    Output as a pandas DataFrame for easy reading
    rf = RandomForestClassifier(featuresCol="features")
    mod = rf.fit(train)
    ExtractFeatureImp(mod.featureImportances, train, "features")
    """

    list_extract = []
    for i in dataset.schema[featuresCol].metadata["ml_attr"]["attrs"]:
        list_extract = list_extract + dataset.schema[featuresCol].metadata["ml_attr"]["attrs"][i]
    varlist = pd.DataFrame(list_extract)
    varlist['Importance_Score'] = varlist['idx'].apply(lambda x: featureImp[x])
    return(varlist.sort_values('Importance_Score', ascending = False))

```

## 模型结构

在本模块中，我们定义了支持二元目标的多个算法函数，包括逻辑回归、随机森林、梯度推进和神经网络。

```py
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.classification import LogisticRegressionModel
# from sklearn.externals import joblib
import joblib

def logistic_model(train, x, y):
    lr = LogisticRegression(featuresCol = x, labelCol = y, maxIter = 10)
    lrModel = lr.fit(train)
    return lrModel

from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.classification import RandomForestClassificationModel

def randomForest_model(train, x, y):
    rf = RandomForestClassifier(featuresCol = x, labelCol = y, numTrees=10)
    rfModel = rf.fit(train)
    return rfModel

from pyspark.ml.classification import GBTClassifier
from pyspark.ml.classification import GBTClassificationModel

def gradientBoosting_model(train, x, y):
    gb = GBTClassifier(featuresCol = x, labelCol = y, maxIter=10)
    gbModel = gb.fit(train)
    return gbModel

from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.classification import DecisionTreeClassificationModel

def decisionTree_model(train, x, y):

    dt = DecisionTreeClassifier(featuresCol = x, labelCol = y, maxDepth=5)
    dtModel = dt.fit(train)
    return dtModel

from pyspark.ml.classification import MultilayerPerceptronClassifier
from pyspark.ml.classification import MultilayerPerceptronClassificationModel

def neuralNetwork_model(train, x, y, feature_count):
    layers = [feature_count, feature_count*3, feature_count*2, 2]
    mlp = MultilayerPerceptronClassifier(featuresCol = x, labelCol = y, maxIter=100, layers=layers, blockSize=512,seed=12345)
    mlpModel = mlp.fit(train)
    return mlpModel

```

### 度量计算

以下模块计算模型指标，包括 KS、ROC 和准确性。

```py
from pyspark.sql.types import DoubleType
from pyspark.sql import *
from pyspark.sql.functions import desc
from pyspark.sql.functions import udf
from pyspark.sql import functions as F
import sys
import time
# import __builtin__ as builtin
import builtins
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.feature import QuantileDiscretizer
import numpy

import numpy as np
from pyspark import SparkContext,HiveContext,Row,SparkConf

spark = SparkSession.builder.appName("MLA_metrics_calculator").enableHiveSupport().getOrCreate()
spark.sparkContext.setLogLevel('ERROR')
sc = spark.sparkContext

def highlight_max(data, color="yellow"):
    '''
    highlight the maximum in a Series or DataFrame
    '''
    attr = 'background-color: {}'.format(color)
    if data.ndim == 1:  # Series from .apply(axis=0) or axis=1
        is_max = data == data.max()
        return [attr if v else '' for v in is_max]
    else:  # from .apply(axis=None)
        is_max = data == data.max().max()
        return pd.DataFrame(np.where(is_max, attr, ''),index=data.index, columns=data.columns)

def calculate_metrics(predictions,y,data_type):
    start_time4 = time.time()

    # Calculate ROC

    evaluator = BinaryClassificationEvaluator(labelCol=y,rawPredictionCol='probability')
    auroc = evaluator.evaluate(predictions,{evaluator.metricName: "areaUnderROC"})
    print('AUC calculated',auroc)

    selectedCols = predictions.select(F.col("probability"), F.col('prediction'), F.col(y)).rdd.map(lambda row: (float(row['probability'][1]), float(row['prediction']), float(row[y]))).collect()
    y_score, y_pred, y_true = zip(*selectedCols)

    # Calculate Accuracy
    accuracydf=predictions.withColumn('acc',F.when(predictions.prediction==predictions[y],1).otherwise(0))
    accuracydf.createOrReplaceTempView("accuracyTable")
    RFaccuracy=spark.sql("select sum(acc)/count(1) as accuracy from accuracyTable").collect()[0][0]
    print('Accuracy calculated',RFaccuracy)

#     # Build KS Table
    split1_udf = udf(lambda value: value[1].item(), DoubleType())

    if data_type in ['train','valid','test','oot1','oot2']:
        decileDF = predictions.select(y, split1_udf('probability').alias('probability'))
    else:
        decileDF = predictions.select(y, 'probability')

    decileDF=decileDF.withColumn('non_target',1-decileDF[y])

    window = Window.orderBy(desc("probability"))
    decileDF = decileDF.withColumn("rownum", F.row_number().over(window))
    decileDF.cache()
    decileDF=decileDF.withColumn("rownum",decileDF["rownum"].cast("double"))

    window2 = Window.orderBy("rownum")
    RFbucketedData=decileDF.withColumn("deciles", F.ntile(10).over(window2))
    RFbucketedData = RFbucketedData.withColumn('deciles',RFbucketedData['deciles'].cast("int"))
    RFbucketedData.cache()
    #a = RFbucketedData.count()
    #print(RFbucketedData.show())

    ## to pandas from here

    print('KS calculation starting')
    target_cnt=RFbucketedData.groupBy('deciles').agg(F.sum(y).alias('target')).toPandas()
    non_target_cnt=RFbucketedData.groupBy('deciles').agg(F.sum("non_target").alias('non_target')).toPandas()
    overall_cnt=RFbucketedData.groupBy('deciles').count().alias('Total').toPandas()
    overall_cnt = overall_cnt.merge(target_cnt,on='deciles',how='inner').merge(non_target_cnt,on='deciles',how='inner')
    overall_cnt=overall_cnt.sort_values(by='deciles',ascending=True)
    overall_cnt['Pct_target']=(overall_cnt['target']/overall_cnt['count'])*100
    overall_cnt['cum_target'] = overall_cnt.target.cumsum()
    overall_cnt['cum_non_target'] = overall_cnt.non_target.cumsum()
    overall_cnt['%Dist_Target'] = (overall_cnt['cum_target'] / overall_cnt.target.sum())*100
    overall_cnt['%Dist_non_Target'] = (overall_cnt['cum_non_target'] / overall_cnt.non_target.sum())*100
    overall_cnt['spread'] = builtins.abs(overall_cnt['%Dist_Target']-overall_cnt['%Dist_non_Target'])
    decile_table=overall_cnt.round(2)
    print("KS_Value =", builtins.round(overall_cnt.spread.max(),2))
    decileDF.unpersist()
    RFbucketedData.unpersist()
    print("Metrics calculation process Completed in : "+ " %s seconds" % (time.time() - start_time4))
    return auroc,RFaccuracy,builtins.round(overall_cnt.spread.max(),2), y_score, y_pred, y_true, overall_cnt

```

### 验证和绘图生成

该模块生成各种图，包括 ROC、混淆矩阵和 KS。模型验证也是本模块的一部分。

```py
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from sklearn import metrics
import glob
import os
import pandas as pd
import seaborn as sns
from pandas import ExcelWriter
from metrics_calculator import *

# Generate ROC chart

def draw_roc_plot(user_id, mdl_ltrl, y_score, y_true, model_type, data_type):

    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score, pos_label = 1)
    roc_auc = metrics.auc(fpr,tpr)
    plt.title(str(model_type) + ' Model - ROC for ' + str(data_type) + ' data' )
    plt.plot([0, 1], [0, 1], 'r--')

    plt.plot(fpr, tpr, label = 'AUC = %0.2f' % roc_auc)
    plt.xlabel('False Positive Rate (1 - Specificity)')
    plt.ylabel('True Positive Rate (Sensitivity)')
    plt.legend(loc = 'lower right')
    print('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/' + str(model_type) + '/' + str(model_type) + ' Model - ROC for ' + str(data_type) + ' data.png')
    plt.savefig('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/' + str(model_type) + '/' + str(model_type) + ' Model - ROC for ' + str(data_type) + ' data.png', bbox_inches="tight")
    plt.close()

# Generate KS chart

def draw_ks_plot(user_id, mdl_ltrl, model_type):

    writer = ExcelWriter('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/' + str(model_type) + '/KS_Charts.xlsx')

    for filename in glob.glob('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/' + str(model_type) + '/KS ' + str(model_type) + ' Model*.xlsx'):
        excel_file = pd.ExcelFile(filename)
        (_, f_name) = os.path.split(filename)
        (f_short_name, _) = os.path.splitext(f_name)
        for sheet_name in excel_file.sheet_names:
            df_excel = pd.read_excel(filename, sheet_name=sheet_name)
            df_excel = df_excel.style.apply(highlight_max, subset=['spread'], color='#e6b71e')
            df_excel.to_excel(writer, f_short_name, index=False)
            worksheet = writer.sheets[f_short_name]
            worksheet.conditional_format('C2:C11', {'type': 'data_bar','bar_color': '#34b5d9'})#,'bar_solid': True
            worksheet.conditional_format('E2:E11', {'type': 'data_bar','bar_color': '#366fff'})#,'bar_solid': True
        os.remove(filename)
    writer.save()

# Confusion matrix

def draw_confusion_matrix(user_id, mdl_ltrl, y_pred, y_true, model_type, data_type):

    AccuracyValue =  metrics.accuracy_score(y_pred=y_pred, y_true=y_true)
    PrecisionValue = metrics.precision_score(y_pred=y_pred, y_true=y_true)
    RecallValue = metrics.recall_score(y_pred=y_pred, y_true=y_true)
    F1Value = metrics.f1_score(y_pred=y_pred, y_true=y_true)

    plt.title(str(model_type) + ' Model - Confusion Matrix for ' + str(data_type) + ' data \n \n Accuracy:{0:.3f}   Precision:{1:.3f}   Recall:{2:.3f}   F1 Score:{3:.3f}\n'.format(AccuracyValue, PrecisionValue, RecallValue, F1Value))
    cm = metrics.confusion_matrix(y_true=y_true,y_pred=y_pred)
    sns.heatmap(cm, annot=True, fmt="g"); #annot=True to annotate cells
    plt.xlabel("Predicted labels")
    plt.ylabel("True labels")
    print('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/' + str(model_type) + '/' + str(model_type) + ' Model - Confusion Matrix for ' + str(data_type) + ' data.png')
    plt.savefig('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/' + str(model_type) + '/' + str(model_type) + ' Model - Confusion Matrix for ' + str(data_type) + ' data.png', bbox_inches="tight")
    plt.close()

# Model validation

def model_validation(user_id, mdl_ltrl, data, y, model, model_type, data_type):

    start_time = time.time()

    pred_data = model.transform(data)
    print('model output predicted')

    roc_data, accuracy_data, ks_data, y_score, y_pred, y_true, decile_table = calculate_metrics(pred_data,y,data_type)
    draw_roc_plot(user_id, mdl_ltrl, y_score, y_true, model_type, data_type)
    decile_table.to_excel('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/' + str(model_type) + '/KS ' + str(model_type) + ' Model ' + str(data_type) + '.xlsx',index=False)
    draw_confusion_matrix(user_id, mdl_ltrl, y_pred, y_true, model_type, data_type)
    print('Metrics computed')

    l = [roc_data, accuracy_data, ks_data]
    end_time = time.time()
    print("Model validation process completed in :  %s seconds" % (end_time-start_time))
    return l

```

### 型号选择

该模块负责根据验证指标标准生成挑战者和冠军模型定义。它还生成包含所有模型信息的合并 Excel 输出文件。

```py
import pandas as pd
import joblib
import numpy as np
import glob
import os

def select_model(user_id, mdl_ltrl, model_selection_criteria, dataset_to_use):
    df = pd.DataFrame({},columns=['roc_train', 'accuracy_train', 'ks_train', 'roc_valid', 'accuracy_valid', 'ks_valid', 'roc_test', 'accuracy_test', 'ks_test', 'roc_oot1', 'accuracy_oot1', 'ks_oot1', 'roc_oot2', 'accuracy_oot2', 'ks_oot2'])
    current_dir = os.getcwd()
    os.chdir('/home/' + user_id + '/' + 'mla_' + mdl_ltrl)
    for file in glob.glob('*metrics.z'):
        l = joblib.load(file)
        df.loc[str(file.split('_')[0])] = l

    for file in glob.glob('*metrics.z'):
        os.remove(file)

    os.chdir(current_dir)
    df.index = df.index.set_names(['model_type'])
    df = df.reset_index()

    model_selection_criteria = model_selection_criteria.lower()
    column_to_sort = model_selection_criteria + '_' + dataset_to_use.lower()
    checker_value = 0.03

    if model_selection_criteria == 'ks':
        checker_value = checker_value * 100

    df['counter'] = (np.abs(df[column_to_sort] - df[model_selection_criteria + '_train']) > checker_value).astype(int) +                     (np.abs(df[column_to_sort] - df[model_selection_criteria + '_valid']) > checker_value).astype(int) +                     (np.abs(df[column_to_sort] - df[model_selection_criteria + '_test']) > checker_value).astype(int) +                     (np.abs(df[column_to_sort] - df[model_selection_criteria + '_oot1']) > checker_value).astype(int) +                     (np.abs(df[column_to_sort] - df[model_selection_criteria + '_oot2']) > checker_value).astype(int)

    df = df.sort_values(['counter', column_to_sort], ascending=[True, False]).reset_index(drop=True)

    df['selected_model'] = ''
    df.loc[0,'selected_model'] = 'Champion'
    df.loc[1,'selected_model'] = 'Challenger'

    df.to_excel('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/metrics.xlsx')
    return df

```

### 分数代码创建

该模块为生产部署生成伪分数代码。它链接建模过程中生成的所有模型对象，并将它们放在一个位置。这也包括数据操作和分类变量处理功能。只要模型对象保存在指向正确输入数据源的相同位置，该文件就可以独立运行。

```py
#Import the scoring features
import string

import_packages = """
#This is a pseudo score code for production deployment. It links to all your model objects created during the modeling process. If you plan to use this file, then change the "score_table" variable to point to your input data. Double-check the "home_path" and "hdfs_path" if you altered the location of model objects.
import os
os.chdir('/home/jovyan/work/spark-warehouse/auto_model_builder')
from pyspark import SparkContext,HiveContext,Row,SparkConf
from pyspark.sql import *
from pyspark.ml import Pipeline
from pyspark.ml.linalg import Vectors,VectorUDT
from pyspark.sql.functions import *
from pyspark.mllib.stat import *
from pyspark.ml.feature import *
from pyspark.ml.feature import IndexToString,StringIndexer,VectorIndexer
from sklearn.metrics import roc_curve,auc
import numpy as np
import pandas as pd
import subprocess
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml import Pipeline,PipelineModel
from pyspark.sql import functions as func
from datetime import *
from pyspark.sql import SparkSession,SQLContext
from pyspark.sql.types import *
from dateutil.relativedelta import relativedelta
from data_manipulations import *
from model_builder import *
import datetime
from datetime import date
import string
import os
import sys
import time
import numpy
spark = SparkSession.builder.appName("MLA_Automated_Scorecode").enableHiveSupport().getOrCreate()
spark.sparkContext.setLogLevel('ERROR')
sc = spark.sparkContext
"""

parameters = string.Template("""
user_id = '${user_id}'
mdl_output_id = '${mdl_output_id}'
mdl_ltrl = '${mdl_ltrl}'
#Since the hdfs and home path below are pointing to your user_id by default, to use this file for scoring, you need to upload the model objects in hdfs_path and home_path to the appropriate score location path (Could be advanl or any other folder path). You would need the following files to perform scoring

.
#hdfs_path  - all the files in the path specified below
#home_path - 'model_scoring_info.z'
hdfs_path = '/user/${user_id}' + '/' + 'mla_${mdl_ltrl}' #update score location hdfs_path
home_path = '/home/${user_id}' + '/' + 'mla_${mdl_ltrl}' #update score location home_path
""")

import_variables = """
from sklearn.externals import joblib
from pyspark.ml import Pipeline,PipelineModel
final_vars,id_vars,vars_selected,char_vars,num_vars,impute_with,selected_model,dev_table_name = joblib.load(home_path + '/model_scoring_info.z')
char_labels = PipelineModel.load(hdfs_path + '/char_label_model.h5')
pipelineModel = PipelineModel.load(hdfs_path + '/pipelineModel.h5')
"""

load_models = """
KerasModel = ''
loader_model_list = [LogisticRegressionModel, RandomForestClassificationModel, GBTClassificationModel, DecisionTreeClassificationModel, MultilayerPerceptronClassificationModel, KerasModel]
models_to_run = ['logistic', 'randomForest','gradientBoosting','decisionTree','neuralNetwork','keras']
load_model = loader_model_list[models_to_run.index(selected_model)]
model = load_model.load(hdfs_path + '/' + selected_model + '_model.h5')
"""

score_function = """
score_table = spark.sql("select " + ", ".join(final_vars) + " from " + dev_table_name) #update this query appropriately
def score_new_df(scoredf, model):
    newX = scoredf.select(final_vars)
    newX = newX.select(list(vars_selected))
    newX = char_labels.transform(newX)
    newX = numerical_imputation(newX,num_vars, impute_with)
    newX = newX.select([c for c in newX.columns if c not in char_vars])
    newX = rename_columns(newX, char_vars)
    finalscoreDF = pipelineModel.transform(newX)
    finalscoreDF.cache()

    finalpredictedDF = model.transform(finalscoreDF)
    finalpredictedDF.cache()
    return finalpredictedDF
ScoredDF = score_new_df(score_table, model)
"""

def selected_model_scorecode(user_id, mdl_output_id, mdl_ltrl, parameters):

    parameters = parameters.substitute(locals())
    scorefile = open('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/score_code_selected_model.py', 'w')
    scorefile.write(import_packages)
    scorefile.write(parameters)
    scorefile.write(import_variables)
    scorefile.write(load_models)
    scorefile.write(score_function)
    scorefile.close()
    print('Score code generation complete')

# # Generate individual score codes

def individual_model_scorecode(user_id, mdl_output_id, mdl_ltrl, parameters):

    loader_model_list = ['LogisticRegressionModel', 'RandomForestClassificationModel', 'GBTClassificationModel', 'DecisionTreeClassificationModel', 'MultilayerPerceptronClassificationModel', 'KerasModel']
    models_to_run = ['logistic', 'randomForest','gradientBoosting','decisionTree','neuralNetwork','keras']

    parameters = parameters.substitute(locals())
    for i in models_to_run

:
        try:
            load_model = loader_model_list[models_to_run.index(i)]

            write_model_parameter = string.Template("""
model = ${load_model}.load(hdfs_path + '/' + ${i} + '_model.h5')
            """).substitute(locals())

            scorefile = open('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/' + str(i[0].upper()) + str(i[1:]) + '/score_code_' + i + '_model.py', 'w')
            scorefile.write(import_packages)
            scorefile.write(parameters)
            scorefile.write(import_variables)
            scorefile.write(write_model_parameter)
            scorefile.write(score_function)
            scorefile.close()
        except:
            pass

    print('Individual Score code generation complete')

```

### 整理结果

这里使用了一个 zipfile 模块将前面模块生成的所有输出绑定到一个 zip 文件中。该文件可以很容易地导出，以便在其他地方部署模型。

```py
import os
import zipfile

def retrieve_file_paths(dirName):

  # set up filepaths variable
    filePaths = []

  # Read all directory, subdirectories, and file lists
    for root, directories, files in os.walk(dirName):
        for filename in files:
        # Create the full filepath by using os module.
            filePath = os.path.join(root, filename)
            filePaths.append(filePath)

    # return all paths
    return filePaths

# Declare the main function

def zipper(dir_name):
# Assign the name of the directory to zip

    # Call the function to retrieve all files and folders of the assigned directory
    filePaths = retrieve_file_paths(dir_name)

    # printing the list of all files to be zipped
    print('The following list of files will be zipped:')
    for fileName in filePaths:
        print(fileName)

    # writing files to a zipfile
    zip_file = zipfile.ZipFile(dir_name+'.zip', 'w')
    with zip_file:
        for file in filePaths:
            zip_file.write(file)

    print(dir_name+'.zip file is created successfully!')
    return(dir_name+'.zip')

```

### 结构

这个框架文件组合了前面的所有八个模块，将它们编排在一个逻辑流中，以创建所需的输出。它处理数据并执行变量选择。它构建机器学习算法，并在维持数据集上验证模型。它根据用户选择的统计数据选择最佳算法。它还为生产生成评分代码。

这个框架文件接收所有的输入。我们将所有前面的模块，包括这个框架文件，保存为一个 Python 文件。因为我们在这里处理客户流失建模数据集，所以我们在`data_folder_path.`中提供了输入 csv 文件的位置，所有其他所需的输入都在代码的第一个块中提供。所有这些代码都是为了适应 PySpark 的 Docker 版本而设计的。通过进行最小的修改，这也可以适用于任何集群执行。

```py
from pyspark import SparkContext,HiveContext,Row,SparkConf
from pyspark.sql import *
from pyspark.ml import Pipeline
from pyspark.ml.linalg import Vectors,VectorUDT
from pyspark.sql.functions import *
from pyspark.mllib.stat import *
from pyspark.ml.feature import *
from pyspark.ml.feature import IndexToString,StringIndexer,VectorIndexer
from sklearn.metrics import roc_curve,auc
import numpy as np
import pandas as pd
import subprocess
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml import Pipeline,PipelineModel
from pyspark.sql import functions as func
from datetime import *
from pyspark.sql import SparkSession,SQLContext
from pyspark.sql.types import *
from dateutil.relativedelta import relativedelta
import datetime

from datetime import date
import string
import os
import sys
import time
import numpy

spark = SparkSession.builder.appName("Automated_model_building").enableHiveSupport().getOrCreate()
spark.sparkContext.setLogLevel('ERROR')
sc = spark.sparkContext

import_data = False
stop_run = False
message = ''
filename = ''

user_id = 'jovyan'
mdl_output_id = 'test_run01' #An unique ID to represent the model
mdl_ltrl = 'chapter8_testrun' #An unique literal or tag to represent the model

input_dev_file='churn_modeling.csv'
input_oot1_file=''
input_oot2_file=''

dev_table_name = ''
oot1_table_name = ''
oot2_table_name = ''

delimiter_type = ','

include_vars = '' # user specified variables to be used
include_prefix = '' # user specified prefixes to be included for modeling
include_suffix = '' # user specified prefixes to be included for modeling
exclude_vars = 'rownumber,customerid,surname' # user specified variables to be excluded for modeling
exclude_prefix = '' # user specified prefixes to be excluded for modeling
exclude_suffix = '' # user specified suffixes to be excluded for modeling

target_column_name = 'exited'

run_logistic_model = 1
run_randomforest_model = 1
run_boosting_model = 1
run_neural_model = 1

miss_per = 0.75
impute_with = 0.0
train_size=0.7
valid_size=0.2
seed=2308

model_selection_criteria = 'ks' #possible_values ['ks','roc','accuracy']
dataset_to_use = 'train' #possible_values ['train','valid','test','oot1','oot2']

data_folder_path = '/home/jovyan/work/'
hdfs_folder_path = '/home/jovyan/work/spark-warehouse/'

####################################################################
######No input changes required below this for default run##########
####################################################################

if input_oot1_file=='':
    input_oot1_file=input_dev_file
if input_oot2_file=='':
    input_oot2_file=input_dev_file
# assign input files if the user uploaded files instead of tables.
if dev_table_name.strip() == '':
    dev_input_file = input_dev_file
    if dev_input_file.strip() == '':
        print('Please provide a development table or development file to process the application')
        stop_run = True
        message = 'Development Table or file is not provided. Please provide a development table or file name to process'

    import_data = True

    file_type = dev_table_name.split('.')[-1]
    out,err=subprocess.Popen(['cp',data_folder_path+dev_input_file,hdfs_folder_path],stdout=subprocess.PIPE,stderr=subprocess.PIPE).communicate()

if oot1_table_name.strip() == '':
    oot1_input_file = input_oot1_file

    out,err=subprocess.Popen(['cp',data_folder_path+oot1_input_file,hdfs_folder_path],stdout=subprocess.PIPE,stderr=subprocess.PIPE).communicate()

if oot2_table_name.strip() == '':
    oot2_input_file = input_oot2_file
    out,err=subprocess.Popen(['cp',data_folder_path+oot2_input_file,hdfs_folder_path],stdout=subprocess.PIPE,stderr=subprocess.PIPE).communicate()

ignore_data_type = ['timestamp', 'date']
ignore_vars_based_on_datatype = []

# extract the input variables in the file or table
if not stop_run:
    if import_data:
        df = spark.read.option("delimiter",delimiter_type).option("header", "true").option("inferSchema", "true").csv(hdfs_folder_path + dev_input_file)
        df = pd.DataFrame(zip(*df.dtypes),['col_name', 'data_type']).T
    else:
        df = spark.sql('describe ' + dev_table_name)
        df = df.toPandas()

    input_vars = list(str(x.lower()) for x in df['col_name'])
    print(input_vars)

    for i in ignore_data_type:
        ignore_vars_based_on_datatype += list(str(x) for x in df[df['data_type'] == i]['col_name'])

    if len(ignore_vars_based_on_datatype) > 0:
        input_vars = list(set(input_vars) - set(ignore_vars_based_on_datatype))

    input_vars.remove(target_column_name)

    ## variables to include
    import re
    prefix_include_vars = []
    suffix_include_vars = []

    if include_vars.strip() != '':
        include_vars = re.findall(r'\w+', include_vars.lower())

    if include_prefix.strip() != '':
        prefix_to_include = re.findall(r'\w+', include_prefix.lower())

        for i in prefix_to_include

:
            temp = [x for x in input_vars if x.startswith(str(i))]
            prefix_include_vars.append(temp)

        prefix_include_vars = [item for sublist in prefix_include_vars for item in sublist]

    if include_suffix.strip() != '':
        suffix_to_include = re.findall(r'\w+', include_suffix.lower())

        for i in suffix_to_include:
            temp = [x for x in input_vars if x.startswith(str(i))]
            suffix_include_vars.append(temp)

        suffix_include_vars = [item for sublist in suffix_include_vars for item in sublist]

    include_list = list(set(include_vars) | set(prefix_include_vars) | set(suffix_include_vars))

    ## Variables to exclude
    prefix_exclude_vars = []
    suffix_exclude_vars = []

    if exclude_vars.strip() != '':
        exclude_vars = re.findall(r'\w+', exclude_vars.lower())

    if exclude_prefix.strip() != '':
        prefix_to_exclude = re.findall(r'\w+', exclude_prefix.lower())

        for i in prefix_to_exclude

:
            temp = [x for x in input_vars if x.startswith(str(i))]
            prefix_exclude_vars.append(temp)

        prefix_exclude_vars = [item for sublist in prefix_exclude_vars for item in sublist]

    if exclude_suffix.strip() != '':
        suffix_to_exclude = re.findall(r'\w+', exclude_suffix.lower())

        for i in suffix_to_exclude

:
            temp = [x for x in input_vars if x.startswith(str(i))]
            suffix_exclude_vars.append(temp)

        suffix_exclude_vars = [item for sublist in suffix_exclude_vars for item in sublist]

    exclude_list = list(set(exclude_vars) | set(prefix_exclude_vars) | set(suffix_exclude_vars))

    if len(include_list) > 0:
        input_vars = list(set(input_vars) & set(include_list))

    if len(exclude_list) > 0:
        input_vars = list(set(input_vars) - set(exclude_list))

if not stop_run:

    final_vars = input_vars  # final list of variables to be pulled
    from datetime import datetime
    insertion_date = datetime.now().strftime("%Y-%m-%d")

    import re
    from pyspark.sql.functions import col

    # import data for the modeling
    if import_data:
        train_table = spark.read.option("delimiter",delimiter_type).option("header", "true").option("inferSchema", "true").csv(hdfs_folder_path + dev_input_file)
        oot1_table = spark.read.option("delimiter",delimiter_type).option("header", "true").option("inferSchema", "true").csv(hdfs_folder_path + oot1_input_file)
        oot2_table = spark.read.option("delimiter",delimiter_type).option("header", "true").option("inferSchema", "true").csv(hdfs_folder_path + oot2_input_file)
    else

:
        train_table = spark.sql("select " + ", ".join(final_vars + [target_column_name]) + " from " + dev_table_name)
        oot1_table = spark.sql("select " + ", ".join(final_vars + [target_column_name]) + " from " + oot1_table_name)
        oot2_table = spark.sql("select " + ", ".join(final_vars + [target_column_name]) + " from " + oot2_table_name)

    train_table = train_table.where(train_table[target_column_name].isNotNull())
    oot1_table = oot1_table.where(oot1_table[target_column_name].isNotNull())
    oot2_table = oot2_table.where(oot2_table[target_column_name].isNotNull())
    print (final_vars)

    oot1_table=oot1_table.toDF(*[c.lower() for c in oot1_table.columns])
    oot2_table=oot2_table.toDF(*[c.lower() for c in oot2_table.columns])
    print(oot1_table.columns)
    print(oot2_table.columns)
    X_train = train_table.select(*final_vars)
    X_train.cache()

    # apply data manipulations on the data - missing value check, label encoding, imputation

    from data_manipulations import *

    vars_selected_train = missing_value_calculation(X_train, miss_per) # missing value check

    vars_selected = filter(None,list(set(list(vars_selected_train))))
    print('vars selected')
    X = X_train.select(*vars_selected)
    print(X.columns)
    vars_selectedn=X.columns
    X = X.cache()

    Y = train_table.select(target_column_name)
    Y = Y.cache()

    char_vars, num_vars = identify_variable_type(X)
    X, char_labels = categorical_to_index(X, char_vars) #label encoding
    X = numerical_imputation(X,num_vars, impute_with) # imputation
    X = X.select([c for c in X.columns if c not in char_vars])
    X = rename_columns(X, char_vars)
    joinedDF = join_features_and_target(X, Y)

    joinedDF = joinedDF.cache()
    print('Features and targets are joined')

    train, valid, test = train_valid_test_split(joinedDF, train_size, valid_size, seed)
    train = train.cache()
    valid = valid.cache()
    test = test.cache()
    print('Train, valid and test dataset created')

    x = train.columns

    x.remove(target_column_name)
    feature_count = len(x)
    print(feature_count)

    if feature_count > 30:
        print('# No of features - ' + str(feature_count) + '.,  Performing feature reduction before running the model.')

    # directory to produce the outputs of the automation
    import os

    try:
        if not os.path.exists('/home/' + user_id + '/' + 'mla_' + mdl_ltrl):
            os.mkdir('/home/' + user_id + '/' + 'mla_' + mdl_ltrl)
    except:
        user_id = 'jovyan'
        if not os.path.exists('/home/' + user_id + '/' + 'mla_' + mdl_ltrl):
            os.mkdir('/home/' + user_id + '/' + 'mla_' + mdl_ltrl)

    subprocess.call(['chmod','777','-R','/home/' + user_id + '/' + 'mla_' + mdl_ltrl])

    x = train.columns

    x.remove(target_column_name)
    sel_train = assembled_vectors(train,x, target_column_name)
    sel_train.cache()

    # # Variable Reduction for more than 30 variables in the feature set using Random Forest

    from pyspark.ml.classification import  RandomForestClassifier
    from feature_selection import *

    rf = RandomForestClassifier(featuresCol="features",labelCol = target_column_name)
    mod = rf.fit(sel_train)
    varlist = ExtractFeatureImp(mod.featureImportances, sel_train, "features")
    selected_vars = [str(x) for x in varlist['name'][0:30]]
    train = train.select([target_column_name] + selected_vars)
    train.cache()

    save_feature_importance(user_id, mdl_ltrl, varlist) #Create feature importance plot and excel data

    x = train.columns
    x.remove(target_column_name)
    feature_count = len(x)
    print(feature_count)

    train, valid, test, pipelineModel = scaled_dataframes(train,valid,test,x,target_column_name)

    train = train.cache()
    valid = valid.cache()
    test = test.cache()
    print('Train, valid and test are scaled')
    print (train.columns)

    # import packages to perform model building, validation, and plots

    import time
    from validation_and_plots import *

    # apply the transformation done on training dataset to OOT 1 and OOT 2 using the score_new_df function
    def score_new_df(scoredf):
        newX = scoredf.select(*final_vars)
        #idX = scoredf.select(id_vars)

        print(newX.columns)
        newX = newX.select(*vars_selectedn)
        print(newX.columns)
        newX = char_labels.transform(newX)
        newX = numerical_imputation(newX,num_vars, impute_with)
        newX = newX.select([c for c in newX.columns if c not in char_vars])
        newX = rename_columns(newX, char_vars)

        finalscoreDF = pipelineModel.transform(newX)
        finalscoreDF.cache()
        return finalscoreDF

    # apply the transformation done on training dataset to OOT 1 and OOT 2 using the score_new_df function

    x = 'features'
    y = target_column_name

    oot1_targetY = oot1_table.select(target_column_name)
    print(oot1_table.columns)
    oot1_intDF = score_new_df(oot1_table)
    oot1_finalDF = join_features_and_target(oot1_intDF, oot1_targetY)
    oot1_finalDF.cache()
    print(oot1_finalDF.dtypes)

    oot2_targetY = oot2_table.select(target_column_name)
    oot2_intDF = score_new_df(oot2_table)
    oot2_finalDF = join_features_and_target(oot2_intDF, oot2_targetY)
    oot2_finalDF.cache()
    print(oot2_finalDF.dtypes)

    # run individual models

    from model_builder import *
    from metrics_calculator import *

    loader_model_list = []
    dataset_list = ['train','valid','test','oot1','oot2']
    datasets = [train,valid,test,oot1_finalDF, oot2_finalDF]
    print(train.count())
    print(test.count())
    print(valid.count())
    print(oot1_finalDF.count())
    print(oot2_finalDF.count())
    models_to_run = []

    if run_logistic_model:
        lrModel = logistic_model(train, x, y) #build model
        lrModel.write().overwrite().save('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/logistic_model.h5') #save model object
        print("Logistic model developed")
        model_type = 'Logistic'
        l = []

        try:
            os.mkdir('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/' + str(model_type))
        except:
            pass

        for i in datasets

:
            l += model_validation(user_id, mdl_ltrl, i, y, lrModel, model_type, dataset_list[datasets.index(i)]) #validate model

        draw_ks_plot(user_id, mdl_ltrl, model_type) #ks charts
        joblib.dump(l,'/home/' + user_id + '/' + 'mla_' + mdl_ltrl  + '/logistic_metrics.z') #save model metrics
        models_to_run.append('logistic')
        loader_model_list.append(LogisticRegressionModel)

    if run_randomforest_model:
        rfModel = randomForest_model(train, x, y) #build model
        rfModel.write().overwrite().save('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/randomForest_model.h5') #save model object
        print("Random Forest model developed")
        model_type = 'RandomForest'
        l = []

        try:
            os.mkdir('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/' + str(model_type))
        except:
            pass

        for i in datasets:
            l += model_validation(user_id, mdl_ltrl, i, y, rfModel, model_type, dataset_list[datasets.index(i)]) #validate model

        draw_ks_plot(user_id, mdl_ltrl, model_type) #ks charts
        joblib.dump(l,'/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/randomForest_metrics.z') #save model metrics
        models_to_run.append('randomForest')
        loader_model_list.append(RandomForestClassificationModel)

    if run_boosting_model:
        gbModel = gradientBoosting_model(train, x, y) #build model
        gbModel.write().overwrite().save('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/gradientBoosting_model.h5') #save model object
        print("Gradient Boosting model developed")
        model_type = 'GradientBoosting'
        l = []

        try:
            os.mkdir('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/' + str(model_type))
        except:
            pass

        for i in datasets

:
            l += model_validation(user_id, mdl_ltrl, i, y, gbModel, model_type, dataset_list[datasets.index(i)]) #validate model

        draw_ks_plot(user_id, mdl_ltrl, model_type) #ks charts
        joblib.dump(l,'/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/gradientBoosting_metrics.z') #save model metrics
        models_to_run.append('gradientBoosting')
        loader_model_list.append(GBTClassificationModel)

    if run_neural_model:
        mlpModel = neuralNetwork_model(train, x, y, feature_count) #build model
        mlpModel.write().overwrite().save('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/neuralNetwork_model.h5') #save model object
        print("Neural Network model developed")
        model_type = 'NeuralNetwork'
        l = []

        try:
            os.mkdir('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/' + str(model_type))
        except:
            pass

        for i in datasets:
            l += model_validation(user_id, mdl_ltrl, i, y, mlpModel, model_type, dataset_list[datasets.index(i)]) #validate model

        draw_ks_plot(user_id, mdl_ltrl, model_type) #ks charts
        joblib.dump(l,'/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/neuralNetwork_metrics.z') #save model metrics
        models_to_run.append('neuralNetwork')
        loader_model_list.append(MultilayerPerceptronClassificationModel)

    # model building complete. Let us validate the metrics for the models created

    # model validation part starts now

.
    from model_selection import *
    output_results = select_model(user_id, mdl_ltrl, model_selection_criteria, dataset_to_use) #select Champion, Challenger based on the metrics provided by user

    #print(type(output_results), output_results)

    selected_model = output_results['model_type'][0] #Champion model based on selected metric

    load_model = loader_model_list[models_to_run.index(selected_model)] #load the model object for Champion model
    model = load_model.load('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/' + selected_model + '_model.h5')

    print('Model selected for scoring - ' + selected_model)

    # Produce pseudo score for production deployment
    # save objects produced in the steps above for future scoring
    import joblib

    char_labels.write().overwrite().save('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/char_label_model.h5')
    pipelineModel.write().overwrite().save('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/pipelineModel.h5')

    save_list = [final_vars,vars_selected,char_vars,num_vars,impute_with,selected_model,dev_table_name]
    joblib.dump(save_list,'/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/model_scoring_info.z')

    # # Create score code

    from scorecode_creator import *
    selected_model_scorecode(user_id, mdl_output_id, mdl_ltrl, parameters)
    individual_model_scorecode(user_id, mdl_output_id, mdl_ltrl, parameters)

    message = message + 'Model building activity complete and the results are attached with this email. Have Fun'

    from zipper_function import *
    try:
        filename = zipper('/home/' + user_id + '/' + 'mla_' + mdl_ltrl)
    except:
        filename = ''

# clean up files loaded in the local path
if import_data:
    file_list = [dev_input_file, oot1_input_file, oot2_input_file]

    for i in list(set(file_list)):
        try:
            os.remove(data_folder_path + str(i))
        except:
            pass

# clean up files loaded in the hdfs path

if import_data:
    file_list = [dev_input_file, oot1_input_file, oot2_input_file]

    for i in list(set(file_list)):
        try:
            out,err=subprocess.Popen([ 'rm','-r','-f',hdfs_folder_path+str(i)],stdout=subprocess.PIPE,stderr=subprocess.PIPE).communicate()
        except:
            pass

```

为了执行，我们将使用以下命令打开 Docker:

```py
docker run -it -p 8888:8888 -v /Users/ramcharankakarla/demo_data/:/home/jovyan/work/ jupyter/pyspark-notebook:latest bash

```

在本地机器上，将所有文件复制到 Docker 公开的路径(例如，`/Users/ramcharankakarla/demo_data/).`我们已经创建了一个文件夹 *Chapter8_automator* ，并将所有需要的文件放在那里(表 [8-3](#Tab3) )。还将 *churn_modeling.csv* 数据集复制到同一个文件夹中。

表 8-3

自动化框架中的文件及其功能

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

组件

 | 

文件名

 |
| --- | --- |
| 数据操作 | data_manipulations.py |
| 特征选择 | feature_selection.py |
| 模型结构 | model_builder.py |
| 度量计算 | metrics_calculator.py |
| 验证和绘图生成 | validation_and_plots.py |
| 型号选择 | model_selection.py |
| 分数代码创建 | scorecode_creator.py |
| 整理结果 | zipper_function.py |
| 结构 | build_and_execute_pipe.py |

在运行这些框架文件之前，请确保在 Docker 中安装或检查以下软件包是否存在:

```py
pip install openpyxl

pip install xlsxwriter

```

要从 Docker 运行框架文件，请使用以下命令:

```py
spark-submit --master local[*] /home/jovyan/work/Chapter8_automator/build_and_execute_pipe.py

```

执行完成后，所有数据都将存储在`/home/jovyan/`中。

要制作文件和指标的本地副本并防止丢失任何工作，请使用以下命令。这将创建作品的本地副本。同样，如果需要，可以复制一个 zip 文件( *mla_chapter8_testrun.zip* )。

```py
cp -r mla_chapter8_testrun /home/jovyan/work/

```

干得好！我们已经成功构建了一个端到端自动化引擎，可以为您节省大量时间。让我们看看这个引擎产生的输出。

### 流水线输出

图 [8-13](#Fig13) 到 [8-15](#Fig15) 和表 [8-4](#Tab4) 到 [8-6](#Tab6) 说明了自动化流水线框架生成的输出。对于任何给定的二进制模型，都会生成以下输出。

表 8-6

神经网络测试

<colgroup><col class="tcol1 align-center"> <col class="tcol2 align-center"></colgroup> 
| - ![img/500182_1_En_8_Figb_HTML.jpg](img/500182_1_En_8_Figb_HTML.jpg) |

表 8-5

科尔莫戈罗夫-斯米尔诺夫统计量

<colgroup><col class="tcol1 align-center"> <col class="tcol2 align-center"></colgroup> 
| - ![img/500182_1_En_8_Figa_HTML.jpg](img/500182_1_En_8_Figa_HTML.jpg) |

表 8-4

组合指标

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"> <col class="tcol6 align-left"> <col class="tcol7 align-left"></colgroup> 
| 

型号类型

 | 

roc _ 火车

 | 

准确性 _ 训练

 | 

ks_train

 | 

roc_valid

 | 

准确性 _ 有效

 | 

ks _ 有效

 |
| --- | --- | --- | --- | --- | --- | --- |
| 神经网络 | 0.86171092 | 0.858369099 | Fifty-five point five | 0.857589 | 0.862000986 | Fifty-four point four three |
| 随机森林 | 0.838162018 | 0.860228898 | Fifty point five seven | 0.835679 | 0.856086742 | Fifty point four eight |
| 符号逻辑的 | 0.755744261 | 0.81230329 | Thirty-seven point one nine | 0.754326 | 0.807294234 | Thirty-seven point three one |
| 梯度推进 | 0.881999528 | 0.869384835 | Fifty-nine point four six | 0.858617 | 0.853622474 | Fifty-five point nine five |

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"> <col class="tcol6 align-left"></colgroup> 
| 

型号类型

 | 

roc _ 测试

 | 

准确性 _ 测试

 | 

ks _ 测试

 | 

roc _ boot 1

 | 

准确度 _oot1

 |
| --- | --- | --- | --- | --- | --- |
| 神经网络 | 0.813792 | 0.849134 | Forty-six point five eight | 0.856175 | 0.8582 |
| 随机森林 | 0.806239 | 0.840979 | Forty-four point three | 0.834677 | 0.8575 |
| 符号逻辑的 | 0.719284 | 0.805301 | Thirty-two point one nine | 0.751941 | 0.8106 |
| 梯度推进 | 0.816005 | 0.850153 | Forty-seven point two two | 0.870804 | 0.8643 |

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"> <col class="tcol6 align-left"></colgroup> 
| 

型号类型

 | 

ks _ oot1 型电脑

 | 

roc _ boot 2

 | 

准确度 _oot2

 | 

克 _ 奥特 2

 | 

选定型号

 |
| --- | --- | --- | --- | --- | --- |
| 神经网络 | Fifty-four point four three | 0.856175108 | 0.8582 | Fifty-four point four three | 冠军 |
| 随机森林 | Forty-nine point nine nine | 0.834676561 | 0.8575 | Forty-nine point nine nine | 挑战者 |
| 符号逻辑的 | Thirty-six point four nine | 0.751941185 | 0.8106 | Thirty-six point four nine |   |
| 梯度推进 | Fifty-seven point five one | 0.870803793 | 0.8643 | Fifty-seven point five one |   |

![img/500182_1_En_8_Fig15_HTML.jpg](img/500182_1_En_8_Fig15_HTML.jpg)

图 8-15

自动化模型——混淆矩阵

![img/500182_1_En_8_Fig14_HTML.jpg](img/500182_1_En_8_Fig14_HTML.jpg)

图 8-14

自动化模型 ROCs

![img/500182_1_En_8_Fig13_HTML.jpg](img/500182_1_En_8_Fig13_HTML.jpg)

图 8-13

自动化模型特征重要性

本模块旨在帮助您创建快速、自动化的实验，但绝不是要取代模型构建活动。

EXERCISE 8-2: BUILDING CUSTOM PIPELINES

**问题:**我们挑战你建立一个单独的流水线或集成代码流来适应连续的目标。

## 摘要

*   我们了解了模型管理和部署的挑战。

*   我们现在知道如何使用 MLflow 来管理实验和部署模型。

*   我们探索了如何为各种模型构建活动构建定制流水线。

*   我们看到了如何将流水线堆叠在一起以创建自动化流水线。

干得好！现在，您已经熟悉了将模型投入生产时将会用到的一些关键概念。这应该让您在构建数据准备流水线时，对如何管理模型生命周期有一个合理的想法。在下一章，我们将介绍一些在日常工作中有用的技巧、窍门和有趣的话题。