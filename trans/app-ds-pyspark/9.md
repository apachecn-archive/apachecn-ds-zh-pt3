# 9.部署机器学习模型

本章将指导您在部署机器学习模型时应遵循的最佳实践。我们将使用三种方法来演示生产部署，如下所示:

*   使用 HDFS 对象和 pickle 文件的模型部署

*   使用 Docker 进行模型部署

*   实时评分 API

在前一章中，您已经看到了第一种技术。在本节中，我们将更详细地介绍它。提供的代码是构建模型和评估结果的起始代码。

## 起始代码

这段代码只是一个模板脚本，供您加载 *bank-full.csv* 文件并构建模型。如果您已经有了模型，可以跳过这一部分。

```py
#default parameters
filename = "bank-full.csv"
target_variable_name = "y"

#load datasets
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
df = spark.read.csv(filename, header=True, inferSchema=True, sep=';')
df.show()

#convert target column variable to numeric
from pyspark.sql import functions as F
df = df.withColumn('y', F.when(F.col("y") == 'yes', 1).otherwise(0))

#datatypes of each column and treat them accordingly
# identify variable types function
def variable_type(df):
    vars_list = df.dtypes
    char_vars = []
    num_vars = []
    for i in vars_list:
        if i[1] in ('string'):
            char_vars.append(i[0])
        else:
            num_vars.append(i[0])
    return char_vars, num_vars

char_vars, num_vars = variable_type(df)
num_vars.remove(target_variable_name)

from pyspark.ml.feature import StringIndexer

from pyspark.ml import Pipeline

# convert categorical to numeric using label encoder option
def category_to_index(df, char_vars):

    char_df = df.select(char_vars)
    indexers = [StringIndexer(inputCol=c, outputCol=c+"_index", handleInvalid="keep") for c in char_df.columns]
    pipeline = Pipeline(stages=indexers)
    char_labels = pipeline.fit(char_df)
    df = char_labels.transform(df)
    return df, char_labels

df, char_labels = category_to_index(df, char_vars)
df = df.select([c for c in df.columns if c not in char_vars])

# rename encoded columns to original variable name
def rename_columns(df, char_vars):
    mapping = dict(zip([i + '_index' for i in char_vars], char_vars))
    df = df.select([F.col(c).alias(mapping.get(c, c)) for c in df.columns])
    return df

df = rename_columns(df, char_vars)

from pyspark.ml.feature import VectorAssembler

#assemble individual columns to one column - 'features'
def assemble_vectors(df, features_list, target_variable_name):
    stages = []
    #assemble vectors
    assembler = VectorAssembler(inputCols=features_list, outputCol="features")
    stages = [assembler]
    #select all the columns + target + newly created 'features' column
    selectedCols = [target_variable_name, 'features'] + features_list
    #use pipeline to process sequentially
    pipeline = Pipeline(stages=stages)
    #assembler model
    assembleModel = pipeline.fit(df)
    #apply assembler model on data

    df = assembleModel.transform(df).select(selectedCols)

    return df, assembleModel, selectedCols

#exclude target variable and select all other feature vectors
features_list = df.columns
#features_list = char_vars #this option is used only for ChiSqselector
features_list.remove(target_variable_name)

# apply the function on our DataFrame
df, assembleModel, selectedCols = assemble_vectors(df, features_list, target_variable_name)

#train test split – 70/30 split
train, test = df.randomSplit([0.7, 0.3], seed=12345)
train.count(), test.count()

# Random forest model
from pyspark.ml.classification import RandomForestClassifier

clf = RandomForestClassifier(featuresCol='features', labelCol="y")
clf_model = clf.fit(train)
print(clf_model.featureImportances)
print(clf_model.toDebugString)
train_pred_result = clf_model.transform(train)
test_pred_result = clf_model.transform(test)

# Validate random forest model
from pyspark.mllib.evaluation import MulticlassMetrics
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.sql.types import IntegerType, DoubleType

def evaluation_metrics(df, target_variable_name):
    pred = df.select("prediction", target_variable_name)
    pred = pred.withColumn(target_variable_name, pred[target_variable_name].cast(DoubleType()))
    pred = pred.withColumn("prediction", pred["prediction"].cast(DoubleType()))
    metrics = MulticlassMetrics(pred.rdd.map(tuple))
    # confusion matrix
    cm = metrics.confusionMatrix().toArray()
    acc = metrics.accuracy #accuracy
    misclassification_rate = 1 - acc #misclassification rate

    precision = metrics.precision(1.0) #precision
    recall = metrics.recall(1.0) #recall
    f1 = metrics.fMeasure(1.0) #f1-score
    #roc value
    evaluator_roc = BinaryClassificationEvaluator(labelCol=target_variable_name, rawPredictionCol="rawPrediction", metricName="areaUnderROC")
    roc = evaluator_roc.evaluate(df)
    evaluator_pr = BinaryClassificationEvaluator(labelCol=target_variable_name, rawPredictionCol="rawPrediction", metricName="areaUnderPR")
    pr = evaluator_pr.evaluate(df)
    return cm, acc, misclassification_rate, precision, recall, f1, roc, pr
    train_cm, train_acc, train_miss_rate, train_precision, \
        train_recall, train_f1, train_roc, train_pr = evaluation_metrics(train_pred_result, target_variable_name)
test_cm, test_acc, test_miss_rate, test_precision, \
        test_recall, test_f1, test_roc, test_pr = evaluation_metrics(test_pred_result, target_variable_name)

#Comparision of metrics
print('Train accuracy - ', train_acc, ', Test accuracy - ', test_acc)
print('Train misclassification rate - ', train_miss_rate, ', Test misclassification rate - ', test_miss_rate)
print('Train precision - ', train_precision, ', Test precision - ', test_precision)
print('Train recall - ', train_recall, ', Test recall - ', test_recall)
print('Train f1 score - ', train_f1, ', Test f1 score - ', test_f1)
print('Train ROC - ', train_roc, ', Test ROC - ', test_roc)
print('Train PR - ', train_pr, ', Test PR - ', test_pr)

# make confusion matrix charts
import seaborn as sns
import matplotlib.pyplot as plt

def make_confusion_matrix_chart(cf_matrix_train, cf_matrix_test):

    list_values = ['0', '1']

    plt.figure(1, figsize=(10,5))
    plt.subplot(121)
    sns.heatmap(cf_matrix_train, annot=True, yticklabels=list_values,
                                xticklabels=list_values, fmt="g")
    plt.ylabel("Actual")
    plt.xlabel("Pred")
    plt.ylim([0,len(list_values)])
    plt.title('Train data predictions')

    plt.subplot(122)
    sns.heatmap(cf_matrix_test, annot=True, yticklabels=list_values,
                                xticklabels=list_values, fmt="g")
    plt.ylabel("Actual")
    plt.xlabel("Pred")
    plt.ylim([0,len(list_values)])
    plt.title('Test data predictions')

    plt.tight_layout()
    return None
make_confusion_matrix_chart(train_cm, test_cm)

#Make ROC chart and PR curve
from pyspark.mllib.evaluation import BinaryClassificationMetrics

class CurveMetrics(BinaryClassificationMetrics):
    def __init__(self, *args):
        super(CurveMetrics, self).__init__(*args)

    def _to_list(self, rdd):
        points = []
        results_collect = rdd.collect()
        for row in results_collect:
            points += [(float(row._1()), float(row._2()))]
        return points

    def get_curve(self, method):
        rdd = getattr(self._java_model, method)().toJavaRDD()
        return self._to_list(rdd)

import matplotlib.pyplot as plt

def plot_roc_pr(df, target_variable_name, plot_type, legend_value, title):

    preds = df.select(target_variable_name,'probability')
    preds = preds.rdd.map(lambda row: (float(row['probability'][1]), float(row[target_variable_name])))
    # Returns as a list (false positive rate, true positive rate)
    points = CurveMetrics(preds).get_curve(plot_type)
    plt.figure()
    x_val = [x[0] for x in points]
    y_val = [x[1] for x in points]
    plt.title(title)

    if plot_type == 'roc':
        plt.xlabel('False Positive Rate (1-Specificity)')
        plt.ylabel('True Positive Rate (Sensitivity)')
        plt.plot(x_val, y_val, label = 'AUC = %0.2f' % legend_value)
        plt.plot([0, 1], [0, 1], color="red", linestyle="--")

    if plot_type == 'pr':
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.plot(x_val, y_val, label = 'Average Precision = %0.2f' % legend_value)
        plt.plot([0, 1], [0.5, 0.5], color="red", linestyle="--")

    plt.legend(loc = 'lower right')
    return None

plot_roc_pr(train_pred_result, target_variable_name, 'roc', train_roc, 'Train ROC')
plot_roc_pr(test_pred_result, target_variable_name, 'roc', test_roc, 'Test ROC')
plot_roc_pr(train_pred_result, target_variable_name, 'pr', train_pr, 'Train Precision-Recall curve')
plot_roc_pr(test_pred_result, target_variable_name, 'pr', test_pr, 'Test Precision-Recall curve')

```

## 保存模型对象并创建分数代码

到目前为止，我们已经构建了一个可靠的模型，并使用多个指标对其进行了验证。是时候保存模型对象以备将来评分和创建评分代码了。

*   模型对象是 PySpark 或 Python 参数，包含来自训练过程的信息。我们必须确定复制结果所需的所有相关模型对象，而无需重新训练模型。一旦被识别，它们被保存在一个特定的位置(HDFS 或其他路径)，并在以后的评分中使用。

*   分数代码包含在新数据集上运行模型对象以产生分数的基本代码。请记住，分数代码仅用于评分过程。你不应该有一个分数代码的模型训练脚本。

### 模型对象

在这个模型示例中，我们需要保存六个对象以备将来使用，如下所示:

*   `char_labels`–该对象用于对新数据应用标签编码。

*   `assembleModel`–该对象用于组合向量，并使数据为评分做好准备。

*   `clf_model`–该对象是随机森林分类器模型。

*   `features_list`–该对象包含新数据中要使用的输入特征列表。

*   `char_vars`–字符变量

*   `num_vars`–数字变量

下面的代码在提供的路径中保存了评分所需的所有对象和 pickle 文件。

```py
import os
import pickle

path_to_write_output = '/home/work/Desktop/score_code_objects'
#create directoyr, skip if already exists
try:
    os.mkdir(path_to_write_output)
except:
    pass

#save pyspark objects
char_labels.write().overwrite().save(path_to_write_output + '/char_label_model.h5')
assembleModel.write().overwrite().save(path_to_write_output + '/assembleModel.h5')
clf_model.write().overwrite().save(path_to_write_output + '/clf_model.h5')

#save python object
list_of_vars = [features_list, char_vars, num_vars]
with open(path_to_write_output + '/file.pkl', 'wb') as handle:
    pickle.dump(list_of_vars, handle)

```

### 分数代码

这是一个用于执行评分操作的 Python 脚本文件。如前所述，这个脚本文件不应该包含模型训练脚本。分数代码至少应该完成以下任务:

1.  导入必要的包

2.  读取新的输入文件以执行评分

3.  读取从训练过程中保存的模型对象

4.  使用模型对象执行必要的转换

5.  使用分类器/回归对象计算模型得分

6.  在指定位置输出最终分数

首先，我们将创建一个 *helper.py* 脚本，其中包含执行评分所需的所有代码。此处提供了要粘贴到文件中的代码:

```py
#helper functions – helper.py file
from pyspark.sql import functions as F
import pickle
from pyspark.ml import PipelineModel
from pyspark.ml.classification import RandomForestClassificationModel
from pyspark.sql.functions import udf
from pyspark.sql.types import IntegerType, DoubleType

# read model objects saved from the training process
path_to_read_objects = '/home/work/Desktop/score_code_objects'

#pyspark objects
char_labels = PipelineModel.load(path_to_read_objects + '/char_label_model.h5')
assembleModel = PipelineModel.load(path_to_read_objects + '/assembleModel.h5')
clf_model = RandomForestClassificationModel.load(path_to_read_objects + '/clf_model.h5')
#python objects
with open(path_to_read_objects + '/file.pkl', 'rb') as handle:
    features_list, char_vars, num_vars = pickle.load(handle)

#make necessary transformations
def rename_columns(df, char_vars):
    mapping = dict(zip([i + '_index' for i in char_vars], char_vars))
    df = df.select([F.col(c).alias(mapping.get(c, c)) for c in df.columns])
    return df

# score the new data
def score_new_df(scoredf):
    X = scoredf.select(features_list)
    X = char_labels.transform(X)
    X = X.select([c for c in X.columns if c not in char_vars])
    X = rename_columns(X, char_vars)
    final_X = assembleModel.transform(X)
    final_X.cache()
    pred = clf_model.transform(final_X)

    pred.cache()
    split_udf = udf(lambda value: value[1].item(), DoubleType())
    pred = pred.select('prediction', split_udf('probability').alias('probability'))
    return pred

```

创建文件后，可以使用下面的代码对新数据进行评分。您可以将脚本保存在一个 *run.py* 文件中。

```py
# import necessary packages
from pyspark.sql import SparkSession
from helper import *

# new data to score
path_to_output_scores = '.'
filename = "score_data.csv"
spark = SparkSession.builder.getOrCreate()
score_data = spark.read.csv(filename, header=True, inferSchema=True, sep=';')

# score the data
final_scores_df = score_new_df(score_data)
#final_scores_df.show()
final_scores_df.repartition(1).write.format('csv').mode("overwrite").options(sep='|', header="true").save(path_to_output_scores + "/predictions.csv")

```

最终分数可在 *final_scores_df* 数据帧中获得，该数据集在最后一步保存为 csv 文件。你需要确保 *run.py* 和 *helper.py* 文件存在于同一个目录中。就是这样。我们有一个可靠的评分代码，可以随时部署到生产环境中。我们将在接下来的小节中讨论生产实现。

## 使用 HDFS 对象和 Pickle 文件的模型部署

这是在生产中部署模型的一种非常简单的方法。当您有多个模型要评分时，您可以手动提交您在上一步中创建的每个 *run.py* 脚本，或者创建一个调度程序来定期运行脚本。无论是哪种情况，下面的代码都将帮助您完成这项任务:

```py
spark-submit run.py

```

如前所述， *run.py* 和 *helper.py* 文件应该在同一个目录下，这样代码才能运行。您可以配置前面的命令，在提交作业时指定`configurations`。当您处理大型数据集时,`configurations`非常有用，因为您可能需要使用执行器、内核和内存选项。您可以通过以下链接了解`spark-submit`和`configuration`选项:

[T2`https://spark.apache.org/docs/latest/submitting-applications.html`](https://spark.apache.org/docs/latest/submitting-applications.html)

[T2`https://spark.apache.org/docs/latest/configuration.html`](https://spark.apache.org/docs/latest/configuration.html)

## 使用 Docker 进行模型部署

让我们换个方式，使用 Docker 部署一个模型。与传统的评分方法(如`spark-submit`)相比，基于 Docker 的部署在很多方面都很有用。

*   可移植性——你可以将你的应用移植到任何平台上，并立即使其工作。

*   容器——整个应用程序是独立的。让您的应用程序工作所需的所有代码和可执行文件都包含在容器中。

*   微服务架构——我们可以将传统的单片应用程序分离为微服务，而不是传统的单片应用程序。这样，我们可以根据需要修改或扩大/缩小单个微服务。

*   更快的软件交付周期–与持续集成和持续开发(CI/CD)流程兼容，因此应用程序可以轻松部署到生产环境中

*   当 Spark 后端改变时，数据科学家/工程师不必花费额外的时间来修复他们的代码/错误。

*   与虚拟机相比，Docker 消耗的内存更少，因此在使用系统资源方面更高效。

让我们继续使用 Docker 实现我们的模型。在我们开始之前，我们应该在一个目录中有以下文件(图 [9-1](#Fig1) )。

![img/500182_1_En_9_Fig1_HTML.jpg](img/500182_1_En_9_Fig1_HTML.jpg)

图 9-1

目录结构

现在，大多数文件对您来说应该很熟悉了。我们有两个如图所示的新文件: *Dockerfile* 和 *requirements.txt* 。*Docker 文件*包含了创建 Docker 容器的脚本， *requirements.txt* 文件包含了让代码工作所需的所有附加包。让我们看看文件内容。

### requirements.txt 文件

此处提供了该文件的内容，以便您可以将它们复制并粘贴到文件中(图 [9-2](#Fig2) )。

![img/500182_1_En_9_Fig2_HTML.jpg](img/500182_1_En_9_Fig2_HTML.jpg)

图 9-2

requirements.txt 文件

### Dockerfile

我们使用基本 Docker 图像作为 PySpark Jupyter 笔记本图像。在图像内部，我们创建一个名为`/deploy/`的工作目录，我们将把所有的模型对象和文件复制并粘贴到这个目录中(图 [9-3](#Fig3) )。

![img/500182_1_En_9_Fig3_HTML.jpg](img/500182_1_En_9_Fig3_HTML.jpg)

图 9-3

Dockerfile

这是在连续步骤中完成的。此外，我们通过执行 *requirements.txt* 文件来安装所需的包，如第 4 行所示。最后，我们使用上一节中使用的`spark-submit`命令来执行模型。需要注意的一点是，容器中暴露了端口号 5000。我们将在实时计分部分使用该端口。暂时忽略这个端口。这里提供了 *Dockerfile* 脚本:

```py
FROM jupyter/pyspark-notebook:latest
WORKDIR /deploy/
COPY ./requirements.txt /deploy/
RUN pip install -r requirements.txt
EXPOSE 5000
COPY ./file.pkl /deploy/
COPY ./run.py /deploy/
COPY ./helper.py /deploy/
COPY ./assembleModel.h5 /deploy/assembleModel.h5
COPY ./char_label_model.h5 /deploy/char_label_model.h5
COPY ./clf_model.h5 /deploy/clf_model.h5
ENTRYPOINT ["spark-submit", "run.py"]

```

### 在 helper.py 和 run.py 文件中所做的更改

在 *helper.py* 文件中，更改以下行:

```py
path_to_read_objects ='/deploy'

```

在 *run.py* 文件中，更改以下几行:

```py
path_to_output_scores = '/localuser'
filename = path_to_output_scores + "/score_data.csv"

```

这些变化反映了 Docker `(/deploy)`中适当的目录路径，以及我们将在 Docker 执行期间稍后`(/localuser)`挂载的卷。

### 创建 Docker 并执行分数代码

要创建 Docker 容器，请在终端/命令提示符下执行以下代码:

```py
docker build -t scoring_image .

```

一旦构建了映像，就可以使用下面的命令执行该映像。您需要从评分数据所在的目录中执行该命令。

```py
docker run -p 5000:5000 -v ${PWD}:/localuser scoring_image:latest

```

就是这样。您应该会在本地文件夹中看到 *predictions.csv* 文件。到目前为止，我们一直使用批量评分流程。现在让我们转向实时评分 API。

## 实时评分 API

实时评分 API 用于对您的模型进行实时评分。它使您的模型能够嵌入到 web 浏览器中，以便获得预测并根据预测采取行动。这对于进行在线推荐、检查信用卡批准状态等非常有用。应用程序非常庞大，使用 Docker 管理这样的 API 变得更加容易。让我们使用 Flask 来实时实现我们的模型(图 [9-4](#Fig4) )。烧瓶脚本存储在 *app.py* 文件中，其内容如下。

![img/500182_1_En_9_Fig4_HTML.jpg](img/500182_1_En_9_Fig4_HTML.jpg)

图 9-4

烧瓶 API 就绪

### app.py 文件

```py
#app.py file
from flask import Flask, request, redirect, url_for, flash, jsonify, make_response
import numpy as np

import pickle
import json
import os, sys
# Path for spark source folder
os.environ['SPARK_HOME'] = '/usr/local/spark'
# Append pyspark  to Python Path
sys.path.append('/usr/local/spark/python')

from pyspark.sql import SparkSession
from pyspark import SparkConf, SparkContext
from helper import *

conf = SparkConf().setAppName("real_time_scoring_api")
conf.set('spark.sql.warehouse.dir', 'file:///usr/local/spark/spark-warehouse/')
conf.set("spark.driver.allowMultipleContexts", "true")
spark = SparkSession.builder.master('local').config(conf=conf).getOrCreate()
sc = spark.sparkContext

app = Flask(__name__)

@app.route('/api/', methods=['POST'])
def makecalc():

    json_data = request.get_json()

    #read the real time input to pyspark df
    score_data = spark.read.json(sc.parallelize(json_data))
    #score df
    final_scores_df = score_new_df(score_data)
    #convert predictions to Pandas DataFrame
    pred = final_scores_df.toPandas()
    final_pred = pred.to_dict(orient='rows')[0]
    return jsonify(final_pred)

if __name__ == '__main__':

    app.run(debug=True, host='0.0.0.0', port=5000)

```

我们还将更改我们的 *Dockerfile* 来将 *app.py* 文件复制到`/deploy`目录。在最后一个模块中，我们运行了 *run.py* 文件。在这个模块中，由 *app.py* 文件负责。以下是更新后的 *Dockerfile* 内容:

```py
FROM jupyter/pyspark-notebook:latest
WORKDIR /deploy/
COPY ./requirements.txt /deploy/
RUN pip install -r requirements.txt
EXPOSE 5000
COPY ./file.pkl /deploy/
COPY ./run.py /deploy/
COPY ./helper.py /deploy/
COPY ./assembleModel.h5 /deploy/assembleModel.h5
COPY ./char_label_model.h5 /deploy/char_label_model.h5
COPY ./clf_model.h5 /deploy/clf_model.h5
COPY ./app.py /deploy/
ENTRYPOINT ["python", "app.py"]

```

好了，我们要进行实时评分了。我们将很快编写 UI 代码。让我们首先使用`Postman`应用程序测试代码。

使用以下代码重新创建 Docker 映像:

```py
docker build -t scoring_image .

```

这次您将不需要体积映射，因为我们将执行实时评分。您需要确保本地系统的端口号被映射到 Docker 端口号。

```py
docker run -p 5000:5000 scoring_image:latest

```

一旦收到这条消息，您就可以切换回 Postman API 并测试这个应用程序。如果你还没有的话，可以在网上下载 Postman 并安装。启动应用程序并进行必要的更改，如下所示。

### 邮递员 API

本节面向没有 Postman API 背景的读者。如果您已经熟悉这一部分，请跳过这一部分(图 [9-5](#Fig5) )。

![img/500182_1_En_9_Fig5_HTML.jpg](img/500182_1_En_9_Fig5_HTML.jpg)

图 9-5

邮递员 API

您需要创建一个 API 实例，方法是单击左上角的 *New* 按钮，然后单击*Request Create a basic Request*以获得如图 [9-6](#Fig6) 所示的窗口。

![img/500182_1_En_9_Fig6_HTML.jpg](img/500182_1_En_9_Fig6_HTML.jpg)

图 9-6

创建新的 API 请求。

现在，让我们在 API 中进行配置，如图 [9-7](#Fig7) 所示。

![img/500182_1_En_9_Fig7_HTML.jpg](img/500182_1_En_9_Fig7_HTML.jpg)

图 9-7

配置

让我们通过导航到*标题*选项卡来添加一个标题列。通过这个选项，API 知道它正在寻找一个 JSON 输入(图 [9-8](#Fig8) )。

![img/500182_1_En_9_Fig8_HTML.jpg](img/500182_1_En_9_Fig8_HTML.jpg)

图 9-8

JSON 标题对象

终于，你准备得分了。既然您正在为测试提供输入，让我们将 body 类型切换到`raw`并选择`JSON`作为文本选项(图 [9-9](#Fig9) )。

![img/500182_1_En_9_Fig9_HTML.jpg](img/500182_1_En_9_Fig9_HTML.jpg)

图 9-9

JSON 选择

### 使用 Postman API 进行实时测试

使用以下 JSON 对象进行测试。注意前面的方括号。这用于传递 JSON 对象列表。

```py
[{"age":58,"job":"management","marital":"married","education":"tertiary","default":"no","balance":2143,"housing":"yes","loan":"no","contact":"unknown","day":5,"month":"may","duration":261,"campaign":1,"pdays":-1,"previous":0,"poutcome":"unknown"}]

```

将 JSON 对象复制粘贴到 API 体中，点击*发送*(图 [9-10](#Fig10) )。

![img/500182_1_En_9_Fig10_HTML.jpg](img/500182_1_En_9_Fig10_HTML.jpg)

图 9-10

实时分数

我们现在可以开始为我们的应用程序构建 UI 组件了。

### 增进

我们将使用一个 Python 包`streamlit`来构建用户界面(UI)。`streamlit`很容易与我们现有的代码集成。使用`streamlit`的最大优势之一是你可以编辑 Python 代码，并立即看到 web 应用中的实时变化。另一个优点是它需要最少的代码来构建 UI。在开始之前，我们先来看看图 [9-11](#Fig11) 所示的目录结构。父目录是`real_time_scoring`。我们创建了两个子目录:`pysparkapi`和`streamlitapi`。我们将把到目前为止讨论过的所有代码移到`pysparkapi`目录中。UI 代码将驻留在`streamlitapi`目录中。

![img/500182_1_En_9_Fig11_HTML.jpg](img/500182_1_En_9_Fig11_HTML.jpg)

图 9-11

UI 目录结构

### streamlitapi 目录

让我们导航到`streamlitapi`目录，并在其中创建以下三个文件。

*坞站样式*

```py
FROM python:3.7-slim
WORKDIR /streamlit
COPY ./requirements.txt /streamlit
RUN pip install -r requirements.txt
COPY ./webapp.py /streamlit
EXPOSE 8501
CMD ["streamlit", "run", "webapp.py"]

```

*requirements.txt*

```py
streamlit
requests

```

以下文件包含 UI 代码。

*webapp.py*

```py
import streamlit as st
import requests
import datetime
import json

st.title('PySpark Real Time Scoring API')

# PySpark API endpoint
url = 'http://pysparkapi:5000'
endpoint = '/api/'

# description and instructions
st.write('''A real time scoring API for PySpark model.''')

st.header('User Input features')

def user_input_features():
    input_features = {}
    input_features["age"] = st.slider('Age', 18, 95)
    input_features["job"] = st.selectbox('Job', ['management', 'technician', 'entrepreneur', 'blue-collar', \
     'unknown', 'retired', 'admin.', 'services', 'self-employed', \
           'unemployed', 'housemaid', 'student'])
    input_features["marital"] = st.selectbox('Marital Status', ['married', 'single', 'divorced'])
    input_features["education"] = st.selectbox('Education Qualification', ['tertiary', 'secondary', 'unknown', 'primary'])
    input_features["default"] = st.selectbox('Have you defaulted before?', ['yes', 'no'])
    input_features["balance"]= st.slider('Current balance', -10000, 150000)
    input_features["housing"] = st.selectbox('Do you own a home?', ['yes', 'no'])
    input_features["loan"] = st.selectbox('Do you have a loan?', ['yes', 'no'])
    input_features["contact"] = st.selectbox('Best way to contact you', ['cellular', 'telephone', 'unknown'])
    date = st.date_input("Today's Date")
    input_features["day"] = date.day
    input_features["month"] = date.strftime("%b").lower()
    input_features["duration"] = st.slider('Duration', 0, 5000)
    input_features["campaign"] = st.slider('Campaign', 1, 63)
    input_features["pdays"] = st.slider('pdays', -1, 871)
    input_features["previous"] = st.slider('previous', 0, 275)
    input_features["poutcome"] = st.selectbox('poutcome', ['success', 'failure', 'other', 'unknown'])
    return [input_features]

json_data = user_input_features()

submit = st.button('Get predictions')

if submit:
    results = requests.post(url+endpoint, json=json_data)
    results = json.loads(results.text)
    st.header('Final Result')
    prediction = results["prediction"]
    probability = results["probability"]
    st.write("Prediction: ", int(prediction))
    st.write("Probability: ", round(probability,3))

```

通过运行以下代码，您可以在部署到 Docker 容器之前测试该代码:

```py
streamlit run webapp.py

```

如前所述，`streamlit`的交互特性让你可以用最少的代码动态设计定制的布局。好的，我们都准备好了。这三个文件位于`streamlitapi`目录中。我们现在有两个 Docker 容器——一个用于 PySpark flask，另一个用于`streamlit` UI。我们需要在这两个容器之间创建一个网络，以便 UI 与后端 PySpark API 进行交互。让我们切换到父目录`real_time_scoring`。

### 实时计分目录

一个 *docker-compose* 文件用于组合多个 docker 容器并创建一个 Docker 网络服务。让我们看看文件的内容。

*码头-化合物. yml*

```py
version: '3'

services:
  pysparkapi:
    build: pysparkapi/
    ports:
      - 5000:5000
    networks:
      - deploy_network
    container_name: pysparkapi

  streamlitapi:
    build: streamlitapi/
    depends_on:
      - pysparkapi
    ports:
        - 8501:8501
    networks:
      - deploy_network
    container_name: streamlitapi

networks:
  deploy_network:
    driver: bridge

```

### 执行 docker 合成. yml 文件

让我们使用下面的代码来执行`docker-compose.yml`文件:

```py
docker-compose build

```

Docker 网络服务被建立。让我们使用图 [9-12](#Fig12) 所示的代码来执行 Docker 服务。在切换到浏览器之前，启动服务并等待调试器 pin 出现需要一些时间。

![img/500182_1_En_9_Fig12_HTML.jpg](img/500182_1_En_9_Fig12_HTML.jpg)

图 9-12

Docker 服务创建

### 实时评分 API

要访问 UI，您需要在浏览器中插入以下链接:

```py
http://localhost:8501/

```

如果你使用了本书中的相同代码，用户界面应该如图 9-13 所示。我们在图中只显示了部分 UI。您可以继续输入特性的值，然后单击*获得预测*以获得结果。

![img/500182_1_En_9_Fig13_HTML.jpg](img/500182_1_En_9_Fig13_HTML.jpg)

图 9-13

细流 api

![img/500182_1_En_9_Figa_HTML.jpg](img/500182_1_En_9_Figa_HTML.jpg)

当你想终止这个服务时，你可以使用下面的代码(图 [9-14](#Fig14) ):

![img/500182_1_En_9_Fig14_HTML.jpg](img/500182_1_En_9_Fig14_HTML.jpg)

图 9-14

杀死码头工人服务

```py
docker-compose down

```

就是这样。我们已经用一个很好的前端实时部署了我们的模型。这个部署最好的部分是 UI 和 PySpark 位于不同的 Docker 容器中。它们可以单独管理和定制，并可以在需要时与一个 *docker-compose* 文件组合。这使得软件开发生命周期过程更加容易。

## 摘要

*   我们详细介绍了模型部署过程。

*   我们使用批处理和实时处理部署了一个模型。

*   最后，我们使用`streamlit` API 创建了一个定制的 UI 前端，并使用 *docker-compose* 与我们的 PySpark API 服务相结合。

干得好！在下一章中，您将了解实验和 PySpark 代码优化。继续学习，敬请关注。