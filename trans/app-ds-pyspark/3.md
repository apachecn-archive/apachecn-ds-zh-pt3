# 3.效用函数和可视化

在这一章中，我们将深入探讨 PySpark 中可用的一些高级功能的实用性。我们鼓励您阅读前一章，并在您选择的任何数据集上尝试以下操作，以提高您的理解。本章将重点介绍窗口函数和其他主题，它们将有助于在大型数据集上创建和应用 Spark 程序。在本章结束时，我们还将介绍可视化和机器学习过程。这应该能让你轻松进入下一个机器学习章节。

在本章中，我们将讨论以下主题:

*   额外的数据操作

*   数据可视化

*   机器学习概述

## 额外的数据操作

让我们从回顾一些上一章没有涉及的数据操作开始。

### 字符串函数

出于本节的目的，让我们重新运行我们在第 [2](2.html) 章中执行的命令。

```py
# Rerunning the following command for recreating budget_cat variable

df_with_newcols = df.select('id','budget','popularity').\
withColumn('budget_cat', when(df['budget']<10000000,'Small').when(df['budget']<100000000,'Medium').otherwise('Big')).\
withColumn('ratings', when(df['popularity']<3,'Low').when(df['popularity']<5,'Mid').otherwise('High'))

```

如果我们想将`budget_cat`和`ratings`的值连接成一列，我们可以使用`concat`函数。在此基础上，让我们将新列的大小写改为小写，并使用`lower`和`trim`函数修剪掉任何空格(图 [3-1](#Fig1) )。

![img/500182_1_En_3_Chapter/500182_1_En_3_Fig1_HTML.jpg](img/500182_1_En_3_Chapter/500182_1_En_3_Fig1_HTML.jpg)

图 3-1

字符串函数的输出

```py
# Concatenating two variables

df_with_newcols=df_with_newcols.withColumn('BudgetRating_Category',concat(df_with_newcols.budget_cat,df_with_newcols.ratings))

# Changing the new variable to lowercase

df_with_newcols=df_with_newcols.withColumn('BudgetRating_Category',trim(lower(df_with_newcols.BudgetRating_Category)))

df_with_newcols.show()

```

### 注册数据帧

到目前为止，我们已经使用了数据帧及其功能。如果我们能够将 SQL 函数的强大功能应用到这些数据帧上，那不是很好吗？幸运的是，PySpark 有办法访问您的数据帧，并在其上运行 SQL 查询。所有的操作都将由 Spark 提供动力。为此，您必须使用`registerTempTable`函数将数据帧注册为临时表。这将创建一个内存中的表。请注意，该表仅在创建它的 Spark 会话中可用。数据以 hive 的列格式存储。这使您能够访问所有列，就像在常规 SQL 表中一样。您可以用不同的数据重写现有的临时表。

让我们将 DataFrame 注册为一个临时表，看看我们如何应用简单的 SQL 来实现所需的结果，而不使用任何函数(图 [3-2](#Fig2) )。

![img/500182_1_En_3_Chapter/500182_1_En_3_Fig2_HTML.jpg](img/500182_1_En_3_Chapter/500182_1_En_3_Fig2_HTML.jpg)

图 3-2

临时表上 Spark SQL 输出的输出

```py
# Registering temporary table

df_with_newcols.registerTempTable('temp_data')

# Applying the function to show the results

spark.sql('select ratings, count(ratings) from temp_data group by ratings').show(False,10)

```

### 窗口功能

什么是窗口功能？窗口函数有助于参考现有行跨一组行执行计算。当您想要计算与时间相关的分析函数时，这些函数非常有用。许多功能工程都围绕着窗口函数。如果你必须对窗口函数进行分类，你可以把它们放在三个不同的盒子里。聚合函数是将多行组合在一起形成单个值汇总的函数。这些功能可以是`sum`、`average`、`min`、`max`或`count`，仅举几例。排名和分析是另外两种类型的窗口函数。

当您使用窗口函数时，您必须指定三个重要组件:

*   Partition:这指定哪些行将包含在与该行相同的分区中。例如，如果您想按年份计算最高预算的电影，您可能希望在分区中包括年份。

*   Order:这指定您希望如何对分区内的行进行排序，升序还是降序。

*   框架:根据与当前位置的相对位置，指定要在当前行的上方/下方包含多少行。

PySpark 有一套内置的排名和分析函数。表 [3-1](#Tab1) 是一个简短且最有用的函数列表。

表 3-1

窗口函数的类型

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

类型

 | 

功能

 |
| --- | --- |
| 等级 | `rank` |
| `denseRank` |
| `percentRank` |
| `rowNumber` |
| `ntile` |
| 分析的 | `lead` |
| `lag` |
| `lastValue` |
| `firstValue` |
| `cumeDist` |

通过下面的插图，你会对这些概念有更好的理解。我们来计算一下人气的十分位数。我们说的*决定*是什么意思？如果你来自分析背景，你已经对十分位数有了欣赏。决策是将分布按升序或降序分成十个相等的桶的过程。这些用作评估预测模型性能的输入步骤。我们将在后面的章节中更深入地研究模型性能和验证统计。现在，让我们看看如何计算受欢迎程度的十分位数。

```py
# Importing the window functions

from pyspark.sql.window import *

# Step 1: Filtering the missing values
df_with_newcols=df_with_newcols.filter( (df_with_newcols['popularity'].isNotNull()) & (~isnan(df_with_newcols['popularity'])) )

# Step 2: Applying the window functions for calculating deciles
df_with_newcols = df_with_newcols.select("id","budget","popularity", ntile(10).over(Window.partitionBy().orderBy(df_with_newcols['popularity'].desc())).alias("decile_rank"))

# Step 3:Dispalying the values
df_with_newcols.groupby("decile_rank").agg(min('popularity').alias('min_popularity'),max('popularity').alias('max_popularity'),count('popularity')).show()

```

让我们来分析一下步骤 2 中发生了什么。我们使用`ntile`函数，它将变量分布分成 *n* 个桶。我们还指定如何使用一个分区来划分分布。这里，我们没有提到数据的任何分区。所以本质上它会使用全部数据。然后我们使用一个`orderBy`按照`popularity`值降序排列数据中的所有行。最后，`alias`函数用于重命名`decile`列。由于 Spark 在惰性评估上工作，所以这个命令的执行不会返回结果。在步骤 3，我们正在执行一个动作。当我们执行这一步时，执行基础计划并计算十分位数。

在第 3 步中，我们从分组的数据帧中收集所有我们感兴趣在`agg`函数中使用的统计数据。我们还将最小值、最大值和计数值添加到聚合函数中。我们期望在每个十分位数中看到大致相等数量的记录。我们还添加了计数来交叉验证这一事实。结果如图 [3-3](#Fig3) 所示。

![img/500182_1_En_3_Chapter/500182_1_En_3_Fig3_HTML.jpg](img/500182_1_En_3_Chapter/500182_1_En_3_Fig3_HTML.jpg)

图 3-3

流行设备的输出

我们现在对 10 个平均分布的桶的受欢迎程度的变化有了一个大致的概念。假设我想知道 1970 年第二受欢迎的电影。怎么用刚学的函数找到呢？

在开始之前，让我们使用内存中的数据帧。如果我已经覆盖了数据怎么办？好了，在这一点上，如果您已经覆盖了您的数据帧，您可以随时重新加载数据。但是请记住将变量转换为正确的数据类型。

让我们倒回去观察前面的步骤中发生了什么。首先，我们选择所需的列，并从发布日期中提取年份。您必须熟悉前面章节中的这一步。在第 4 步中，我们定义了窗口函数。我们想按年份划分数据。这样做的原因是我们想知道 1970 年的第二高收视率。这种划分将使我们能够按年份对数据中的所有受欢迎程度进行排名。在第 5 步中，我们使用了前面定义的`rank`函数和`window.partitionBy`函数来得到想要的结果。等等，我们还没说完呢！我们希望确定 1970 年的结果，因此我们将数据过滤到 1970 年，并对行秩为 2 的数据进行子集划分，如步骤 6 所示。结果如图 [3-4](#Fig4) 所示。

![img/500182_1_En_3_Chapter/500182_1_En_3_Fig4_HTML.jpg](img/500182_1_En_3_Chapter/500182_1_En_3_Fig4_HTML.jpg)

图 3-4

1970 年第二高的受欢迎程度

```py
# Step 1: Import the window functions

from pyspark.sql.window import *

# Step 2: Select the required subset of columns

df_second_best = df.select('id', 'popularity', 'release_date')

# Step 3: Create the year column from release date

df_second_best=df_second_best.withColumn('release_year',year('release_date')).drop('release_date')

# Step 4: Define partition function

year_window = Window.partitionBy(df_second_best['release_year']).orderBy(df_with_newcols['popularity'].desc())

# Step 5: Apply partition function

df_second_best=df_second_best.select('id', 'popularity', 'release_year',rank().over(year_window).alias("rank"))

# Step 6: Find the second best rating for the year 1970

df_second_best.filter((df_second_best['release_year']==1970) & (df_second_best['rank']==2)).show()

```

Let’s rewind and observe what’s happening in the preceding steps. First, we selected the required columns and extracted the year from the release date. You must be familiar with this step from the earlier section. In step 4, we defined our window function. We want to partition the data by year. The reason for doing this is we want to know the second highest rating in the year 1970\. This partition will give us the ability to rank all popularity ratings in the data by year. In step 5, we used a

我们在这里使用了`rank function`。`rank and denseRank. rank`在您的有序分区中给出了等级，而平局被赋予了相同的等级，这两者之间有一个微妙的区别。与平局相邻的级别被给予更高的级别。如果您有两个等级为 2 的行，下一个等级将设置为 4。在`denseRank`中，没有等级被跳过，它们是连续的。您必须根据您的需求来选择这些功能。

让我们更进一步。如果我们的问题是当年票房最高的电影和当年其他电影的收入有什么不同呢？为此，我们需要在确定当年票房最高的电影后，计算一部电影的收入差异。我们现在将演示如何实现这一点。结果如图 [3-5](#Fig5) 所示。

![img/500182_1_En_3_Chapter/500182_1_En_3_Fig5_HTML.jpg](img/500182_1_En_3_Chapter/500182_1_En_3_Fig5_HTML.jpg)

图 3-5

按年度列出的最高收入的收入差异输出

```py
# Step 1: Import the window functions

from pyspark.sql.window import *

# Step 2: Select the required subset of columns

df_revenue = df.select('id', 'revenue', 'release_date')

# Step 3: Create the year column from release date

df_revenue=df_revenue.withColumn('release_year',year('release_date')).drop('release_date')

# Step 4: Define partition function along with range
windowRev =Window.partitionBy(df_revenue['release_year']).orderBy(df_revenue['revenue'].desc()).rangeBetween(-sys.maxsize, sys.maxsize)

# Step 5: Apply partition function for the revenue differences

revenue_difference = (max(df_revenue['revenue']).over(windowRev) - df_revenue['revenue'])

# Step 6: Final data

df_revenue.select('id', 'revenue', 'release_year',revenue_difference.alias("revenue_difference")).show(10,False)

```

您会发现，从上一个示例到第 3 步，没有太大的变化。在步骤 4 中，除了`window.partitionBy`函数，我们还使用了`rangeBetween`函数。这是一个有助于在起点和终点(包括起点和终点)之间创建边界的功能。`rangeBetween`函数中的两个参数都表示包含该分区中可能的最低值和最高值。在第 5 步中，我们确定了`window`函数的最大值，并与行值相减。这给出了预期的输出，使用 step 6 显示。

窗口函数可以是一个很长的话题。但是前面的例子应该让您对如何使用这些函数的可能性有一个大致的了解。大多数 SQL 窗口函数都是 PySpark 中的内置函数。

### 其他有用的功能

在这一节中，我们将尝试揭示一些在前面几节中没有提到的其他有用的功能。基于用例，它们可能非常有效。

#### 收集列表

从前面的分析中我们知道，有些标题是重复的。假设我们想找到标题*失落的世界*重复出现的所有年份。我们有一个方便的函数来做这件事，叫做`collect_list`。出于演示的目的，我们将使用现有的数据帧。之前的输出如图 [3-6](#Fig6) 所示，以供参考。新的输出如图 [3-7](#Fig7) 所示。

![img/500182_1_En_3_Chapter/500182_1_En_3_Fig6_HTML.jpg](img/500182_1_En_3_Chapter/500182_1_En_3_Fig6_HTML.jpg)

图 3-6

单向频率的过滤和排序版本的输出

![img/500182_1_En_3_Chapter/500182_1_En_3_Fig7_HTML.jpg](img/500182_1_En_3_Chapter/500182_1_En_3_Fig7_HTML.jpg)

图 3-7

收集列表的输出

```py
# Step 1: Create the year column from release date

df=df.withColumn('release_year',year('release_date'))

# Step 2: Apply collect_list function to gather all occurrences

df.filter("title=='The Lost World'").groupby('title').agg(collect_list("release_year")).show(1,False)

```

如果我们中断步骤 2，我们将过滤掉我们感兴趣的标题，并根据标题对数据帧进行分组。这个特别的标题出现了六次。`collect_list`函数收集这个特定标题的发行年份，并显示在输出中。

#### 抽样

采样在分析中占有特殊的位置。简单地说，样本是总体的子集。好吧，什么是人口？群体是数据的整体。当考虑你有兴趣研究的人群时，你必须精确。例如，如果我们说我们的人口都是亚马逊的活跃用户，这将意味着亚马逊的所有用户，不分地区、语言或性别。在现实世界中，查找人口数据很困难，有时甚至不切实际。

我们用样本来推断人群中发生了什么。如果取样不正确，将对您的结果产生重大影响。如果我们必须对可用于抽样的方法进行广泛分类，您可以将它们分为概率性和非概率性技术。概率使用随机选择来选择样本。非概率技术基于主观判断。我们使用数据科学中的概率技术对样本做出明智的决策。概率方法中一些常用的抽样技术有简单随机抽样、系统随机抽样、分层随机抽样和整群抽样。

在简单随机抽样中，样本中的每个观测值都是随机选取的，因此所有观测值被选取的可能性都是相等的。简单随机抽样有两种变体。

*   简单随机抽样替换

    在这种方法中，一旦从总体中随机选择了一个观察值，它就会在选择第二个观察值之前被放回数据集中。换句话说，选择一个观察值不会影响其他观察值的结果。

*   无替换的简单随机抽样

    在这种方法中，一旦从总体中随机选择了一个观察值，在选择第二个观察值之前，不会将其放回数据集中。换句话说，一个观察的选择将极大地影响其他观察的结果。

在分层抽样中，您可以从可以划分为子总体的总体中选择样本。比方说，我们知道豪华车市场中拥有特定品牌汽车的人口比例。当我们选择样本时，我们可以使用这种技术来执行相同的比例。

现在让我们看看如何使用 PySpark 实现这些技术。

```py
# Simple random sampling in PySpark without replacement

df_sample = df.sample(False, 0.4, 11)
df_sample.count()

```

输出:

```py
17711

```

在前面的命令中，我们指定了三个参数。第一个参数 True/False 表示您是否希望进行样品替换。在这里，我们希望不替换，所以我们选择了`False`。第二个参数是分数。它表示您希望在样本中包含的人口比例。第三个参数是种子，它保证您每次运行这个代码片段时都得到相同的结果。

```py
# Simple random sampling in PySpark with replacement
df_sample = df.sample(True, 0.4, 11)
df_sample.count()

```

输出:

```py
17745

```

请注意，在这两种情况下，样本大小并不完全是我们要求的比例。这是因为 internally Spark 使用伯努利采样技术进行采样。因为样本总体中的每个元素都是单独选取的，所以样本大小不是固定的，而是遵循二项分布。

```py
# Stratified sampling in PySpark
df_strat = df.sampleBy("release_year", fractions={1959: 0.2, 1960: 0.4, 1961: 0.4}, seed=11)
df_strat.count()

```

输出:

```py
260

```

分层样本语法与简单随机样本稍有不同。请注意，我们使用的是`sampleBy`函数，而不是`sample.`函数。这里的第一个参数是您想要分层的列。在我们的例子中，我们使用发布年份。假设，我们知道 1960 年和 1961 年发行的电影数量是 1959 年的两倍；我们会用这个分数来定义我们的样本。这在第二个参数中表示。

#### 缓存和持久化

缓存是 Spark 最重要的概念之一。如果使用正确，在运行大型程序时可以节省大量时间。什么是*缓存*？这是一种将数据临时保存在内存中的方法。它在不同的操作系统、软件和编程语言中很常见。为什么有用？如果您可以将数据保存在内存中，您就不必在每次访问数据帧时都重复计算来获得这些结果。如果有效地使用缓存，您将节省昂贵的数据 I/O 操作的成本。你应该什么时候使用它？如果您要为多个操作多次访问一个数据帧，我们建议您使用缓存。如果只使用一次数据帧或者数据很容易计算，就不要使用它。

读取数据后，可以缓存数据帧上的第一个查询。第一个查询将以通常的速度运行。但是您将在后续操作中体验到增强的计算速度。高速缓存是懒惰的，这就是你在数据帧上的第一个操作需要时间的原因。您可以使用`cache`函数缓存任何数据帧。

```py
df.cache()

```

持久化具有与缓存相似的功能，或者可以看作是缓存的同义词。它们在存储级别设置上有所不同。Spark 有五种存储类型。这是从 Spark 文档中获得的。见表 [3-2](#Tab2) 。

```py
MEMORY_ONLY

```

将 RDD 作为反序列化的 Java 对象存储在 JVM 中。如果 RDD 不适合内存，一些分区将不会被缓存，并将在每次需要时动态地重新计算。这是默认级别。

```py
MEMORY_ONLY_SER

```

将 RDD 存储为序列化的 Java 对象(每个分区一个字节数组)。这通常比反序列化对象更节省空间，尤其是在使用快速序列化程序时，但读取时会占用更多 CPU 资源。

```py
MEMORY_AND_DISK

```

将 RDD 作为反序列化的 Java 对象存储在 JVM 中。如果 RDD 不适合内存，存储不适合磁盘的分区，并在需要时从那里读取它们。

```py
MEMORY_AND_DISK_SER

```

类似于 MEMORY_ONLY_SER，但是将不适合内存的分区溢出到磁盘，而不是在每次需要时动态地重新计算它们。

```py
DISK_ONLY

```

仅在磁盘上存储 RDD 分区。

缓存将使用`memory_only`。您可以使用`persist`函数对所有级别的存储使用 persist:

表 3-2

Spark 中的存储级别

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"> <col class="tcol6 align-left"></colgroup> 
| 

水平

 | 

使用的空间

 | 

中央处理机时间

 | 

在记忆中

 | 

在磁盘上

 | 

连载

 |
| --- | --- | --- | --- | --- | --- |
| 仅限内存 | 高的 | 低的 | 是 | 不 | 不 |
| 仅存储用户 | 低的 | 高的 | 是 | 不 | 是 |
| 内存和磁盘 | 高的 | 中等 | 部分的 | 部分的 | 部分的 |
| 内存和磁盘用户 | 低的 | 高的 | 部分的 | 部分的 | 是 |
| 仅磁盘 | 低的 | 高的 | 不 | 是 | 是 |

```py
df.persist()

```

如果您想手动删除存储级别中的数据，您可以使用`unpersist`功能:

```py
df.unpersist()

```

#### 保存数据

在所有的数据操作之后，您有多种选择来保存您的数据集，或者作为配置单元表或者作为文本文件。让我们首先看一个例子，看看我们如何使用`write`和`save`将输出数据帧保存为文本文件。在下面的命令中，我们可以指定输出文件的格式以及设置分隔符的选项。我们选择一个管道分隔符进行演示，但是您可以自定义并选择任何对您的数据集有意义的分隔符。

```py
df.write.format('csv').option('delimiter', '|').save('output_df')

```

假设我们正在一个集群上工作，并且有多个数据分区；我们可以使用`coalesce`函数将它们一起保存在一个文件中。在这种情况下，我们将以如下方式更改前面的命令:

```py
df.coalesce(1).write.format('csv').option('delimiter', '|').save('output_coalesced_df')

```

如果我们必须按任意列对数据进行分区以便于索引，我们可以在保存时使用函数`partitionBy,`来完成，如下所示:

```py
df.write.partitionBy('release_year').format('csv').option('delimiter', '|').save('output_df_partitioned')

```

如果您想将数据保存为配置单元表，您可以使用`saveAsTable`功能:

```py
df.write.saveAsTable('film_ratings')

```

#### 熊猫支持

是的，我们正在与 PySpark 合作。但是有时候你可能想把 PySpark 数据帧转换成 pandas 数据帧。您可以通过`toPandas()`功能来完成。警告:不要尝试在大型数据集上运行以下命令，因为您将遇到内存问题。这是因为您正在将分布式数据集复制到本地内存。一旦创建了 pandas 数据帧，新创建的 pandas 数据帧和 Spark 之间就没有联系了。

```py
# Pandas to PySpark

df_pandas=df.toPandas()

# Pandas to PySpark
df_py = spark.createDataFrame(df_pandas)

```

#### 连接

图 [3-8](#Fig8) 是所有连接的快速图示。在下一节中，您将了解每个连接及其工作原理。

![img/500182_1_En_3_Chapter/500182_1_En_3_Fig8_HTML.png](img/500182_1_En_3_Chapter/500182_1_En_3_Fig8_HTML.png)

图 3-8

连接的类型

到目前为止，连接是对任何数据帧进行的最频繁的操作之一。出于连接的目的，让我向您介绍另一个具有一些附加信息的数据集，我们将结合目前为止一直在处理的数据来处理它。参见表 [3-3](#Tab3) 。

表 3-3

附加数据的元数据

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

可变的

 | 

描述

 |
| --- | --- |
| 编号 | 电影的唯一标识符 |
| 投 | 演职人员名单 |
| 成人 | 指示电影是否是成人电影 |
| 董事 | 电影导演信息 |
| 投票计数 | 受欢迎程度的投票总数 |
| 口语 | 电影中使用的语言列表 |
| 海报 _ 路径 | 存储电影海报图像的路径 |
| 主页 | 电影的网页(如果存在) |
| imdb_id | 与电影相关的 IMDB ID |
| 体裁 | 电影类型列表 |
| 录像 | 网站上的视频指示器 |

让我们首先将这个数据作为一个新的 DataFrame 加载，并选择所有没有丢失 id 的行，如下所示:

```py
# Input parameters
file_location = "movie_data_part2.csv"
file_type = "csv"
infer_schema = "False"
first_row_is_header = "True"
delimiter = "|"

# Bring all the options together to read the csv file

df_p1 = spark.read.format(file_type) \
.option("inferSchema", infer_schema) \
.option("header", first_row_is_header) \
.option("sep", delimiter) \
.load(file_location)

# Cast identifer into the right datatype

df_p1 = df_p1.withColumn('id',df_p1['id'].cast("integer"))

# Filter missing values
df_p1=df_p1.filter((df_p1['id'].isNotNull()) & (~isnan(df_p1['id'])))

```

马上，您将在两个数据集中观察到的公共变量是 ID。我们将能够使用该标识符连接两个数据帧。PySpark 中有多种可用的连接，所以让我们逐一介绍一下。

*   **内部连接:**这将标识符上的两个数据帧连接起来，并且只保留公共的行和列(图 [3-9](#Fig9) )。

![img/500182_1_En_3_Chapter/500182_1_En_3_Fig9_HTML.jpg](img/500182_1_En_3_Chapter/500182_1_En_3_Fig9_HTML.jpg)

图 3-9

内部联接的输出列和计数

```py
# Inner Join

df.join(df_p1, df['id'] == df_p1['id'],'inner').printSchema()
df.join(df_p1, df['id'] == df_p1['id']).count()

```

注意到 Spark 已经处理了数据帧的连接，并包含了两次标识符。一个好的做法是，您可以用相同的名称重命名标识符，或者完全删除重复的标识符。

默认情况下，如果没有传递参数，Spark 会使用内部连接。

*   **Left/Left outer join:**Left join 合并标识符上的两个数据帧，保留左侧数据帧的所有行，并匹配与右侧数据帧共有的任何行。如果右边的数据帧中没有对应的行，Spark 将为所有列插入一个 null。

```py
# Left Join
df.join(df_p1, df['id'] == df_p1['id'],'left').count()

```

输出:

*   **Right/Right outer join:** 这将合并标识符上的两个数据帧，保留右侧数据帧的所有行，并匹配与左侧数据帧共有的任何行。如果左侧数据帧中没有对应的行，Spark 将为所有列插入一个 null。

```py
43998

```

```py
# Right Join
df.join(df_p1, df['id'] == df_p1['id'],'right').count()

```

输出:

*   **完全外部连接:**这将在标识符上连接两个数据帧，并保留具有相同标识符的左右数据帧的所有行。如果在任一数据帧中没有对等的行，Spark 将为所有列插入一个 null。

```py
43783

```

```py
# Outer Join
df.join(df_p1, df['id'] == df_p1['id'],'outer').count()

```

输出:

*   **左反联接:**这将标识符上的两个数据帧联接起来，并保留出现在右侧数据帧中的左侧数据帧的所有行。它也只保留左边的数据帧模式(图 [3-10](#Fig10) )。

```py
43998

```

![img/500182_1_En_3_Chapter/500182_1_En_3_Fig10_HTML.jpg](img/500182_1_En_3_Chapter/500182_1_En_3_Fig10_HTML.jpg)

图 3-10

左反联接的输出列和计数

```py
# Left Anti Join
df.join(df_p1, df['id'] == df_p1['id'],'left_anti').printSchema()
df.join(df_p1, df['id'] == df_p1['id'],'left_anti').count()

```

*   **左半连接:**这类似于内连接，除了它不会从右边的数据框中产生列(图 [3-11](#Fig11) )。

![img/500182_1_En_3_Chapter/500182_1_En_3_Fig11_HTML.jpg](img/500182_1_En_3_Chapter/500182_1_En_3_Fig11_HTML.jpg)

图 3-11

左反联接的输出列和计数

```py
# Left Semi Join
df.join(df_p1, df['id'] == df_p1['id'],'left_semi').printSchema()
df.join(df_p1, df['id'] == df_p1['id'],'left_semi').count()

```

*   **广播:**有些情况下，您必须连接一个大型数据集(数百万行)和一个小型数据集(数百行)。为了有效地加入它们，Spark 还有另一个名为`broadcast`的功能。较小的数据帧将在集群的所有节点中复制。这提高了效率，因为 Spark 不需要在更大的数据集中进行任何洗牌。下面是一个工作示例:

```py
# Broadcast Join

df.join(broadcast(df_p1), df['id'] == df_p1['id'],'left_semi').count()

```

输出:

```py
43998

```

#### 删除重复项

当处理真实世界的数据时，您最终可能会用到的另一个方便而有用的函数是`dropDuplicates`。您可以删除单行或多行的副本。假设我们想要删除数据帧中标题和年份相同的所有重复项；我们可以使用以下命令来实现这一点:

```py
# Dropping Duplicates
df.dropDuplicates(['title','release_year']).count()

```

输出:

```py
43643

# Original Data count

df.count()

```

输出:

```py
43998

```

## 数据可视化

坏消息是 Spark 中没有内置的数据可视化包。那我们在这里讨论什么？好的，我们知道 Python 有一个丰富的数据可视化包库，比如`matplotlib`和`seaborn`。我们可以将这些包与数据帧配合使用，并创建所需的绘图。

如果我们后退一步，我们会发现我们使用聚合数据主要是为了可视化。如果我必须理解一个连续变量，我能使用的最好的图是直方图。同样，对于分类数据，我希望看到组图。因此，我们可以使用 Spark 的能力来计算这些聚合，然后使用 Python 中提供的漂亮的可视化包来完成。

首先，我们来绘制人气数据直方图(图 [3-12](#Fig12) ):

![img/500182_1_En_3_Chapter/500182_1_En_3_Fig12_HTML.jpg](img/500182_1_En_3_Chapter/500182_1_En_3_Fig12_HTML.jpg)

图 3-12

受欢迎程度直方图

```py
# Step 1: Importing the required libraries

%matplotlib inline
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns

# Step 2: Processing the data in Spark. We can use the histogram function from the RDD
histogram_data = df.select('popularity').rdd.flatMap(lambda x: x).histogram(25)

# Step 3: Loading the Computed Histogram into a pandas DataFrame for plotting
hist_df=pd.DataFrame(list(zip(*histogram_data)), columns=['bin', 'frequency'])

# Step 4: Plotting the data
sns.set(rc={"figure.figsize": (12, 8)})
sns.barplot(hist_df['bin'],hist_df['frequency'])
plt.xticks(rotation=45)
plt.show()

```

让我们来分解一下每一步都发生了什么。首先，我们导入所需的库。我们正在做与步骤 2 中的数据相关的所有繁重工作。我们正在利用 RDD API 中可用的`histogram`函数来离散化分布并计算箱。在此过程中，我们请求它将我们的数据离散化为 25 个存储桶。在第三步中，我们在数据上创建一个熊猫数据框架，然后用它来绘图。我们在这里做了一次黑客攻击。我们没有直接使用 Python 中的直方图函数；相反，所有计算都在 Spark 中完成。我们在直方图数据的顶部使用一个条形图来查看分布情况。正如您所观察到的，由于尾部值的稀疏性，这种分布很难解释。

让我们试着过滤一下数据，看看是否能更好地了解分布情况。

```py
# Filtering the data to get a better understanding of data
df_fil=df.filter('popularity<22')

# Processing the data in Spark. We can use the histogram function from the RDD
histogram_data = df_fil.select('popularity').rdd.flatMap(lambda x: x).histogram(25)

# Loading the computed histogram into a pandas DataFrame for plotting
hist_df=pd.DataFrame(list(zip(*histogram_data)), columns=['bin', 'frequency'])

# Plotting the data
sns.set(rc={'figure.figsize':(11.7,8.27)})
sns.barplot(hist_df['bin'],hist_df['frequency'])
plt.xticks(rotation=25)
plt.title('Distribution of Popularity - Data is filtered')
plt.show()

```

除了过滤，我们重复了相同的步骤。现在我们看到一个好得多的分布(图 [3-13](#Fig13) )。

![img/500182_1_En_3_Chapter/500182_1_En_3_Fig13_HTML.jpg](img/500182_1_En_3_Chapter/500182_1_En_3_Fig13_HTML.jpg)

图 3-13

过滤后的流行度直方图

让我们试着画出另一个变量。假设我们想知道在我们的数据集中，1960 年到 1970 年之间有多少部电影上映。我们可以使用以下命令来完成(图 [3-14](#Fig14) ):

![img/500182_1_En_3_Chapter/500182_1_En_3_Fig14_HTML.jpg](img/500182_1_En_3_Chapter/500182_1_En_3_Fig14_HTML.jpg)

图 3-14

从 1960 年到 1970 年每年上映的电影数量

```py
# Step 1: Preparing the data using Spark functions and converting to pandas DataFrame

df_cat=df.filter("(release_year>1959) and (release_year<1971)").groupby('release_year').count().toPandas()

# Step 2: Sorting the values for display

df_cat=df_cat.sort_values(by=['release_year'], ascending=False)

# Step 3: Plotting the data
sns.set(rc={'figure.figsize':(11.7,8.27)})
sns.barplot(df_cat['release_year'],df_cat['count'])
plt.xticks(rotation=25)
plt.title('Number of films released each year from 1960 to 1970 in our dataset')
plt.show()

```

使用以下数据集进行练习 3-1: [`https://www.kaggle.com/kakarlaramcharan/tmdb-data-0920`](https://www.kaggle.com/kakarlaramcharan/tmdb-data-0920) 。

Exercise 3-1: Data Manipulations

**问题 1:** 根据受欢迎程度，找出 2010 年第二受欢迎的电影。

**问题 2:** 找出所有重复出现的书名，并显示它们重复出现的年份。

**问题 3:** 根据所有年份的受欢迎程度找出最受欢迎的电影。

**问题 4:** 可视化 2014 年和 2015 年电影的预算。

## 机器学习导论

在我们进入机器学习的世界之前，让我们了解一下分析的进展。分析是对数据或统计数据的计算分析。它用于发现、解释和交流有意义的数据模式。您可能会听到与分析相关的多个术语和术语，但归根结底，它们都是用来收集见解和做出明智决策的。

分析大致可分为以下几种(图 [3-15](#Fig15) ):

![img/500182_1_En_3_Chapter/500182_1_En_3_Fig15_HTML.png](img/500182_1_En_3_Chapter/500182_1_En_3_Fig15_HTML.png)

图 3-15

分析概述

*   **描述性:**这个分析领域专注于创建理解数据的报告和图表。

*   **预测:**这一分析领域与根据已经发生的事情估计将要发生的事情有关。它依靠历史数据进行预测。

*   **说明:**这个分析领域关注的是根据过去的数据必须做什么。这包括决策优化。

这些术语的使用方式经常会让新手感到困惑。当然，有重叠的地方。机器学习是计算机科学的一个子领域，被认为是人工智能的一个子集。*预测分析*指的是统计建模，是保险、银行和营销行业常见的术语。*机器学习*是一个更宽泛的术语，包含多个类别，包括以下内容:

*   **监督学习** **:** 在这种类型的学习中，向算法提供具有目标标签或事件的历史数据集。这个领域的研究是通过准确性的措施来验证的。这也包括任何时间序列建模。

*   **无监督学习** **:** 在这种类型的学习中，向算法提供没有目标标签或事件的历史数据集。这里的目的是识别数据中隐藏的模式或片段。

在讨论监督和非监督方法中的学习技术类型之前，让我们先了解一下数据类型。我们可以把数据大致分为定性数据和定量数据。

*   **定性**数据是无法测量的东西。这可以包括你的车的颜色，婚姻状况，教育程度等。这些变量也可以称为分类变量。我们可以进一步将定性数据分为以下几类:
    *   名义上的–当数据中有多个级别且没有自然排序时，您可以认为它是名义上的。例如，车辆的颜色、婚姻状况

    *   序数-当你的数据有一个固有的顺序时，你可以认为它是序数。例如教育。在教育中，你可以有多个层次，如低于高中，高中，研究生和研究生。这些级别有一个与之相关的等级。我们知道研究生是在光谱的高端，而不到高中是在光谱的低端。

*   **可以测量定量的**数据。这可以包括年龄、距离等。定量数据可以进一步分为离散数据和连续数据。
    *   离散-当数据有整数值时，你可以认为它是离散的；例如年龄(10，20)

    *   连续——当数据包含小数或十进制数据时，可以认为数据是连续的；例如距离(40.71 英里，59.82 英里)

我们有一章专门讲述无监督学习/聚类，在这一章中，你将学习不同的技术和方法(图 [3-16](#Fig16) )。

为什么知道这些数据类型很重要？因为它会帮助你决定什么类型的算法可以被应用。某些算法是为选定的数据类型设计的。在监督学习中，有两类不同的算法:分类和回归。

*   **分类** **:** 在这类机器学习中，目标/事件数据是离散的。示例包括预测可能流失的客户(0/1)或可能拖欠贷款的客户(0/1)。

*   **回归:**在这种类型的机器学习中，目标/事件数据是连续的。例如，根据给定的参数预测房子的价格($)，或者预测客户在给定月份的购买量。

我们将在后面的章节中详细探讨哪些算法可以应用到这些方法中。

*聚类*可以定义为将同类数据分组到一个数据集中。

![img/500182_1_En_3_Chapter/500182_1_En_3_Fig16_HTML.png](img/500182_1_En_3_Chapter/500182_1_En_3_Fig16_HTML.png)

图 3-16

机器学习的分类

大多数数据专业人员都遵循预测建模的标准方法。它被称为 CRISP–DM，即数据挖掘的跨行业标准(图 [3-17](#Fig17) )。它由以下六个主要里程碑组成:

![img/500182_1_En_3_Chapter/500182_1_En_3_Fig17_HTML.png](img/500182_1_En_3_Chapter/500182_1_En_3_Fig17_HTML.png)

图 3-17

CRISP–数据挖掘方法

1.  **业务理解** **:** 这是理解问题陈述并将其公式化为数据问题的第一个重要步骤。这来自于业务。

2.  **数据理解** **:** 这一步包括数据收集和描述性数据分析，以发现数据，熟悉感兴趣的特征。

1.  **数据准备** **:** 这包括整合所有数据特征，并从数据集创建附加特征/变量。

2.  建模:这包括应用适当的算法和循环，以在必要时创建更多的变量。

3.  **评估:**一旦建立了一个或多个模型，就根据验证标准集选择一个模型。我们有一个单独的章节来帮助你理解所有的模型评估和验证技术。我们还寻求来自企业的反馈，以确定模型是否实现了预期的功能。

4.  **部署:**这涉及到将模型投入生产，并使模型分数对业务中的各种涉众可用。

另一个在数据从业者中也很常见的标准框架是 SEMMA。SEMMA 是采样、探索、建模、修改和评估。这种方法是 SAS 在 CRISP–DM 之前引入的。这是一种以建模为中心的方法，不包括业务输入作为其过程的一部分。如果你对你的商业目标有一个清晰的定义，它在分散的组织中工作得很好。

## 摘要

*   我们探讨了创建窗口功能的可能性，并回答了具体的业务问题。

*   我们学习了什么是采样，如何实施采样，以及为什么采样如此重要。

*   我们学习了`cache`和`persist`函数的机制以及何时应该使用它们。

*   我们学习了可视化的简单技巧。

*   我们现在也具备了机器学习及其框架的基础知识。

干得好！在下一章，我们将介绍变量选择，并讨论不同的技术。当你被数据淹没时，你总是可以依靠变量选择章节。它将帮助您识别带有信息的顶级变量。下一章见！