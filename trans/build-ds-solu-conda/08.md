<title>B16589_06_ePub</title> <link href="css/style-JRserifv5.css" rel="stylesheet" type="text/css">

# *第六章*:克服 AI/ML 中的偏见

偏见在**人工智能** ( **AI** )中无处不在。这可能会导致一些看似无辜的事情，比如向主要由男性组成的*开发商*展示图像结果，向法官暗示某个种族的男性比其他人更有可能成为惯犯。你可能认为你不会有这个问题，但偏见可能会有许多形式，与你已经准备好处理它无关。

事实是，从数据集中完全消除所有偏见是不可能的。这大部分完全是无意的，只是由于缺乏可用的数据，但这并不重要。伤害还是可以造成的。你会看到信用评级、人脸检测等方面的偏见。

随着人工智能越来越多地融入社会的正常运作，它的影响将继续增长，并对人们产生非常现实的影响。我们不能声称对数据中的偏差和为使用它而创建的模型一无所知。有时，它可以从 YouTube 不推荐正确的视频开始，移动到摄像头检测深色肤色的人脸检测，并变得严重到让某人在监狱呆十年。当涉及到数据时，偏进、偏出会产生非常真实的后果。

在这本书最有影响力和最重要的一章中，我们将介绍一些存在偏见的最常见的例子。我们将用例子来定义每一个问题，然后告诉你如何克服它们。在最后一部分，我们将做一个练习，看看一个数据集，探索其中可能存在的偏差，以及我们可以做些什么来克服它。

在本章中，您将涉及以下主题:

*   定义偏见与歧视
*   克服不同类型的偏差——代理偏差、样本偏差、排除偏差、测量偏差和社会偏差
*   评估住房价格的案例研究

# 技术要求

本章需要遵循一些先决条件。它们如下:

*   Anaconda 个人版已安装。这包括康达和领航员。
*   从[https://github . com/packt publishing/Building-Data-Science-Solutions-with-Anaconda/tree/main/raw _ Data](https://github.com/PacktPublishing/Building-Data-Science-Solutions-with-Anaconda/tree/main/raw_data)下载数据集。

# 定义偏见与歧视

让我们首先确保我们对人工智能背景下的两个组成部分有一个清晰的理解——偏见和歧视。每个组件都有不同的方面，了解它们之间的区别很重要。

## AI/ML 偏差

AI/ML 偏差是指已经创建的模型显示出对某些群体或类别的偏好，而这些群体或类别不能反映世界的实际状态。

偏见在任何模型中都是不可避免的，它本身是无害的。假设你要写一篇关于最受欢迎的食物的论文，并对它们做一些分析。为此，你需要从你的朋友和家人那里收集他们的偏好数据。除此之外，想想你会回答的三种食物。那里有蔬菜吗？有埃塞俄比亚食物吗？土耳其有什么消息吗？也许不是。

这是一种偏见；除非你对全世界的人进行完全平均的抽样，否则你的数据中会包含偏见。当一个更严重的问题导致一个更严重的问题，即歧视时，这个更严重的问题就开始起作用了。

## AI/ML 的区分度

**歧视**是指基于模型输出的结果，某些无特权群体被置于更不利的劣势。这包括但不限于种族、年龄、性别、宗教或性取向。

歧视本身并不总是意味着一定有故意的恶意。以美国的安全带为例。如果你看看用来收集数据点的测试假人，你会发现它们代表了典型的美国男性。你可能会想，测试数据并没有说明女性的身体差异。这在很大程度上导致女性在车祸中受伤的可能性增加 47%，死亡的可能性增加 17%。

更多信息可以在这里找到:[https://bbc.in/3pAqDhJ](https://bbc.in/3pAqDhJ)。

### 性别数据差距

我们收集的大多数数据都是关于男性的。一个例子是我们在上一节刚刚讨论的崩溃数据。如果你看看过去收集数据的人主要是由谁组成的，这可能不会太令人惊讶。

另一个例子是，办公室平均温度低 5 度。为什么这是性别差距的一个例子？如果我们看看计算的来源，原因会变得更清楚，我们会看到它是基于几十年前一个普通男性的代谢率。这方面的缺陷可能看起来很明显，但女性通常比男性的静息代谢率(产生的热量)低，因此收集来计算理想舒适度的数据并不能反映总体情况。

我们可以猜测为什么使用这一标准，但事实是它不仅没有考虑整个劳动力的需求，而且是基于 154 磅男性的平均体重——远低于今天美国男性平均体重的近 200 磅。如果您使用的是用这些旧数据创建的任何人工智能模型，这将是模型漂移的一个例子，正如在 [*第 1 章*](B16589_01_ePub.xhtml#_idTextAnchor015) 、*理解人工智能/人工智能前景*、*模型如何变得陈旧–模型漂移*部分中讨论的那样。

可能是时候更新了。性别是确保平等代表性的重要领域，是受保护的阶层。现在让我们来看看在一个受保护的班级里意味着什么。

### 受保护的类别

理解您正在收集数据和构建模型的领域中的一些受保护的类是很重要的。根据 ThoughtCo(【https://bit.ly/3r7yns2】)的说法，**受保护阶层**是*受法律保护免受法律、惯例和政策因共同特征(如种族、性别、年龄、残疾或性取向)歧视的人群*。

对于不同国家的受保护类别，法律可能会有所不同，但在美国，以下是受保护的特征:

*   人种
*   宗教
*   民族起源
*   年龄(40 岁及以上)
*   性、性取向或性别认同
*   怀孕
*   家庭状况——对生孩子的歧视
*   无劳动能力
*   老兵身份
*   遗传信息

这并不意味着您永远无法获得包含上述群体信息的信息。相反，这可能是你需要包含的重要甚至关键信息。如果网飞打算向你推荐新的节目，那么年龄可能很重要，但如果它做错了，也没有坏处。然而，对于诸如为计算新冠肺炎病毒的风险而创建的模型之类的东西来说，年龄可能是生死之差。

更重要的是知道这些事情什么时候会对结果有害。让我们开始看看几种类型的偏见，从你如何在没有意识到的情况下将保护组添加到你的数据中开始。

# 克服代理偏见

有时你可以引入偏差,即使你没有任何直接链接到受保护类的特性或数据点。请记住，受保护的阶级是年龄、性别和宗教。这是通过代理介绍的。这归结为当前的数据与该组中的某个人密切相关，因为数据在某种程度上渗入了代理数据集。

在下图中，您可以看到代理偏差如何泄漏到数据中。在左边，您有完全有效的 **X** 和 **Y** 数据，但也有数据 **B** ，它是受保护类数据的形式。尽管来自 B 的数据不直接用于训练数据集中，但它是通过 X 数据集的代理引入的:

![Figure 6.1 – Proxy bias
](image/Figure_6.1.jpg)

图 6.1-代理偏差

让我们来看一些代理偏差的例子，使这一点更加具体。

## 代理偏见的例子

下面的列表包含了一些例子,这些例子可能会使代理偏差的定义更加清晰，并帮助您识别它何时会在现实世界中发生:

*   一个信用评分模型使用您的邮政编码拒绝覆盖范围。那个邮政编码有很多非裔美国人或其他少数民族人口。
*   您的 GPS 坐标显示，每周日上午 11 点，您都在教堂所在地。这可能与您的宗教信仰密切相关。
*   加入或受雇于某些团体，如汽车俱乐部、骄傲游行或成为护士。
*   众所周知，亚马逊创建了一个模型，利用过去的简历来发现谁可能是一个好的候选人。问题？据路透社报道，女性没有得到公平的机会:[https://reut.rs/3pBOihS](https://reut.rs/3pBOihS)。

他们发现，某些显示是女性申请人的指标会取消该人的资格，如果使用该工具，会使他们处于严重的不利地位。这些指标包括女子学院，甚至包括“女子象棋俱乐部”等。

其他单词的选择也与男性和女性有更密切的关系。

## 如何防止代理偏差

和所有类型的偏见一样，有很多方法可以避免它。以下是您可以采取的一些措施，以便让它远离您的数据和模型:

*   咨询该领域的主题专家，确保您了解特定领域的潜在代理偏见特征——性别、年龄、种族等。
*   删除您认为对代理偏见构成威胁的功能。
*   首先考虑你正在训练的模型是否应该被创建和使用，例如亚马逊将其用作招聘和雇佣工具的情况。
*   留下了可能有偏差的数据，但对模型创建过程造成了损失。这有时被误认为是一种公平的惩罚**。这种惩罚应用于损失函数，因此成为算法如何确定预测变好还是变坏的一部分。这种正则化有助于确保没有一个输入特征太重要。**

 **我们已经讨论了一些对某些群体的偏见可能潜入的方式。现在让我们来讨论一下什么时候应该在数据中表示的数据甚至没有做到那一步。

# 克服样本偏差

样本偏差是指数据的选择不能反映现实世界中的存在。这也被称为选择偏差。与许多类型的偏见一样，这可能完全无害，也可能非常有影响，具体取决于应用。

在下图中，您可以看到这种情况的可视化表示。左侧有一些假设的真实世界数据可能会有所帮助(表示为**输入 z** )，但是由于这样或那样的原因，这些数据没有包含在训练数据集中:

![Figure 6.2 – Sample bias
](image/Figure_6.2.jpg)

图 6.2–样本偏差

当我们把这些有价值的数据放在外面时，这对所有相关的人都是有害的。前面的图表比较抽象，所以让我们来看一些更具体的例子，看看样本偏差可能是什么样的。

## 样本偏差的例子

以下项目是可能存在样本偏差的示例。当然，这并不是一个详尽的列表，但有助于给出一个大概的概念:

*   训练视觉分类器来识别动物，但只包括陆地上老虎的图片，没有意识到它们是伟大的游泳者。当在野外发现它们时，缺乏包括它们游泳在内的训练数据会导致不良结果。
*   对于自动驾驶，您可能不会在训练数据中包括建筑区域。自动驾驶是一个众所周知的难题，在现实世界中会发生许多不同的情况，并且很难获得训练数据。当其他场景出现时，在阳光明媚的加州接受训练也可能被证明是具有挑战性的。这里，只从开发人员和员工生活和工作的地方采集数据是有偏见的。
*   在没有低于 32 华氏度的温度数据的情况下为风力涡轮机建立预测分析模型。最近，在德克萨斯州，气温下降到一位数后，该州大部分地区出现了大规模停电。如果任何模型都是为了预测涡轮机何时会发生故障，那么它们可能从来没有包含如此低温的数据。

样本偏差是更难发现的偏差之一，因为它是一个缺口和数据的缺乏，但希望之前的例子列表能让你更好地注意到它何时会出现。

现在让我们来看看其中一种最为臭名昭著的偏见，那就是种族和性别偏见。

## 种族/性别偏见

样本偏差的一个亚组是种族/性别偏差。具体来说，这是指数据遗漏了为了更好地展现整个人口中种族和性别的广度而应该包含的人口部分。

这比不在网上向你展示相关广告对社会有更大的影响。让我们来看一些例子，以便更好地理解这在哪里会成为一个问题。

### 种族/性别偏见的例子

现在，让我们来看看这种特定类型的偏见的几个例子:

*   面部识别没有包括足够多肤色较深的参与者，因此模型无法获得足够的信息进行训练。这导致无法检测到较暗的面孔。

性别差异是一项基于麻省理工大学论文的倡议，旨在将这一问题公之于众。来自微软、IBM 和 Face++的不同面部识别工具进行了比较，它们显示了深色女性和浅色男性之间的巨大差距。在最大的情况下，这是 33.8%的增量:

![Figure 6.3 – Bias in facial recognition from Gender Shades
](image/Figure_6.3.jpg)

图 6.3–性别差异导致的面部识别偏差

更多信息可以在这里找到:[http://gendershades.org/](http://gendershades.org/)。

*   公共浴室水槽中的洗手传感器没有检测到手颜色较深的人，因此没有打开。
*   安全带的设计没有考虑到女性往往坐得更近，质量比男性轻，导致受伤和死亡的几率更高。

## 如何防止样本偏差

解决这个问题的简单方法就是收集更多的数据，对吗？嗯，没那么快。虽然这很可能是解决问题的方法之一，但理解添加什么数据可能有点棘手。首先，有些地方你可能不太确定应该添加什么。

没有适用于所有情况的总括声明，但是有一些领域将适用于整个范围，这些领域包括:

*   获取一系列人口统计数据，包括种族、民族、性别和收入水平。

创建**合成特征**时要小心。这些是从其他数据集合中创建的要素。一个例子是基于其他属性，如长度、动作镜头、镜头中临时演员的数量和其他因素，量化一部电影是否是“史诗”。在训练模型时，这些其他属性(如电影的长度)将不会包含在最终输入中，因此它的重要性可能会被削弱。另一个例子是在后期制作中训练视觉问题时，减少或增加个人的对比度和照明。这可能是一个好主意，但对于数据中更多样化的个体来说，这不是一个好的替代品。

*   针对可能出现的情况进行模拟训练。例如，对于自动驾驶，创建一个类似视频游戏的环境，可以在可能出现但很少出现的情况下训练模型，因此很难收集正确的训练数据。
*   让不同背景的人加入你的团队。这在我们所涉及的几乎每一个案例中都有帮助，但是当你完全忽略了某些群体时，这尤其有用。这允许团队从不同的角度或方法思考问题。

使用前面的技术并不能保证你会做得很好，但是这是很好的第一步。

我们已经看到数据甚至不包括在结果数据集中，但是从不同的角度来看，删除有用的数据也是同样的问题。

# 克服排斥偏见

排除偏见是当你选择删除被认为无用的信息。人工智能的优势之一是，它可以为你没有意识到存在的事情找到模式或关系。如果个人或团队对某个主题没有很好的领域知识，因此对他们没有意识到有价值的项目不屑一顾，这种情况会更常见。

如果数据科学家认为他们对一个领域足够了解，能够围绕这个领域建立模型，那么一个额外的危险就出现了。这可能与**催款-克鲁格效应**密切相关，这是一种潜在的*认知偏差*，在这种情况下，某个特定领域技能低下的人高估了自己的能力。你不知道你不知道什么，这意味着当你对一个领域陌生时，你甚至意识不到它的许多方面是你知识的空白。相反，你可以让在某个领域知识丰富的人认为他们的技能水平很低，因为他们只知道一个话题有多广泛。

在下图中，您可以看到这种偏差的表现。所有有帮助和有用的数据都被捕获在训练集中，但是随后决定没有必要或没有必要在以下方面对模型进行训练:

![Figure 6.4 – Exclusion bias
](image/Figure_6.4.jpg)

图 6.4–排除偏差

这种偏见可能会令人沮丧，因为你可以看到一些有价值的数据已经准备好被使用，但当它可以被用作**功能 3** 时，它却被简单地删除了。目前，我们的数据集中只有**特征 1** 和**特征 2** 。

让我们看看这在现实世界中可能是什么样子。

## 排除偏差的例子

排除偏差的一些例子如下:

*   从销售数据中删除区号
*   当观察学生在考试中的表现时，从大学的课堂中拿出`professor`字段
*   不包括那些没有毕业的学生的数据，当他们在职业生涯后期看这些学生的工资时，也被称为生存偏差

让我们更详细地看看第三个例子。在前面的章节中，我们看到在某些领域，比如工程，学生比他们的同龄人挣得多。但是我们试图回答什么问题呢？如果我们试图完全告知即将入学的新生选择哪个专业，那么包括高比例的学生不能从该专业毕业的信息可能是至关重要的。

这也可能是故意的。如果它表明学生退学后挣的一样多，那么这些学生可能会从这些信息中得出结论，所讨论的专业可能不是很有用。

## 如何防止排除偏差

在某些方面，排斥偏见可能更加恶意或有害。而选择偏差可能更无知，因为不知道应该放入什么东西，而排除偏差的信息是存在的，并且有一个有意的行动来消除它。

接下来，我们将看到如何仍然可以输入所有正确的数据，但是如果您使用不同的方法获取数据，仍然会有问题。

# 克服测量偏差

测量偏差是指收集的数据与现实世界中收集的数据不同。这将是一个问题，因为模型不理解现实世界可能如何工作的细微差别。怎么可能呢？它只知道你告诉它的东西。

下图显示了这可能是什么样子。您可以在顶部看到使用了 **X** 、Y 和 **Z** 训练数据。在下面，您可以看到真实世界的数据( **A** 、 **B** 和 **C** )，这些数据被输入到从训练数据集创建的模型中。它类似于训练数据，但是您可以看到它看起来有些不同，与预期的不太一样:

![Figure 6.5 – Measurement bias
](image/Figure_6.5.jpg)

图 6.5–测量偏差

拥有与真实世界不同的训练数据可能是一个大问题。直到很久以后，在你的模型已经投入生产很长一段时间之后，你才可能认为是一个问题，那时不准确预测的损害可能已经造成了。

## 测量偏差的例子

您可能会发现测量偏差是一个问题的几个场景如下:

*   测量电视的图像清晰度，但人们比第一次测试时坐得更靠近屏幕。
*   从传感器收集运动数据来探测地震。在培训阶段，传感器要敏感得多，但在实际生产中，执行力度要弱得多。
*   使用毫秒作为时间单位进行测量的传感器，但您的模型需要秒。
*   在一种类型的摄像机上训练的自动驾驶模型，但是量产车具有不同的放置和摄像机分辨率。

现在我们知道了测量偏差是什么样的，让我们继续讨论一些可以从一开始就阻止它发生的方法。

## 如何防止测量偏差

只需确保生产中使用的工具和测量数据的方式相同，就可以防止测量偏差。这说起来容易做起来难，因为维修之类的事情可能会发生，也许技术人员会使用不同的部件。总的来说，你可以做一些事情来帮助消除这种偏见:

*   确保您与了解真实世界数据中使用的传感器和设备的人一起工作
*   了解数据点是如何收集的，并确定这是否与收集和测量真实世界数据的过程相同
*   一旦在现场部署了模型，就收集反馈和见解，以查看该模型和在培训环境中看到的模型之间是否存在重大差异

希望您现在已经准备好使用以前的工具，以避免由于在野外发现您的测量偏差而不得不进行更昂贵的更改或更新。

# 克服社会人工智能偏见

根据 Lexalytics 的一篇文章([https://www . lex alytics . com/lex ablog/bias-in-ai-machine-learning](https://www.lexalytics.com/lexablog/bias-in-ai-machine-learning))，**社会 AI 偏见**是指当一个 AI 以反映社会不容忍或制度歧视的方式表现时。乍一看，算法和数据本身似乎没有偏见，但它们的输出强化了社会偏见。

下图是抽象意义上的社会偏见的一瞥。您可以看到有一些好的数据被引入，但随之而来的是一些零碎的数据。这种畸形的数据代表了某种缺陷，使我们无法对我们试图建模的世界状态有一个正确的认识:

![Figure 6.6 – Societal bias
](image/Figure_6.6.jpg)

图 6.6-社会偏见

这些支离破碎的有缺陷的数据位将会被烘焙到任何基于这些数据训练的模型中，除非我们对此做些什么。这种偏差的一个独特之处在于，一旦数据已经收集并组合到训练数据集中，它就不可见了。

## 社会偏见的例子

了解一些关于这种偏见的真实例子总是有帮助的，以确保我们在处理可能包含这一问题的数据时做好准备。接下来，您将看到这种情况的几种表现方式:

*   **犯罪数据**:派遣执法人员的最佳地点应该是一个地区犯罪率高的地方，这是有道理的。这将确保有快速的反应时间，并阻止实施非法行为。

为什么会有这样的问题？一个问题出现了，因为在有更多警察积极寻找他们的地区有更多的犯罪被报道。在一个警察寻找超速者的地方，你会发现更多的人超速行驶。

这个地区更多的犯罪数据导致更多的警察被派去。这就是所谓的**正向强化**循环。这是当结果鼓励更多的行为，导致*期望的*结果。

*   招聘数据:如果你把目前在一个组织工作的人看作是你应该在他们之后招聘的人的例子，这将导致创建更多相同的团队。如果你过分迎合现有的刻板印象，男护士或女开发人员将会很少。这种类型的偏见可能更难梳理，但如果我们想尽可能取得最好的结果，并帮助事情向前发展，我们必须意识到这一点。

现在，我们已经讨论了最后一种类型的偏差。这还不是你在野外能找到的所有东西的详尽清单，但是你现在已经很好地意识到如何更好地准备自己。

让我们把刚刚学到的东西付诸实践，使用最近在现实世界中发生的事情作为基本案例。我们将看一个例子，看看我们是否学到了足够的知识，能够批判性地思考可能存在的差距和偏见。

# 在示例中寻找偏差

在下面的例子中，发现数据中的偏差对业务有重大影响。

住房数据公司 Zillow 最近退出了 iBuying 业务。Zillow 是一家总部位于美国的公司，为普通消费者提供房屋信息。iBuying 是用于即时购买的术语，涉及 Zillow 直接购买房产，然后出售获利(理论上)。Zillow 发现他们的估计(或 zestimates)相差很大，这导致该公司撤出了该领域。也许我们能找到原因。

在这种情况下，我们将试图找到 bias 在诸如 zestimate 之类的东西中进入系统的位置。为了给你一个工作框架，我们将通过之前讨论的步骤和每种类型的偏见来思考它。这一点很重要，因为除非你看到，否则你可能不会立即跳到某一种偏见。这是一个问题，因为每个人都倾向于寻找某样东西而不是其他东西。

一个重要的注意事项是，Zillow 没有揭示他们的模型是如何创建的，因此没有办法审计他们是如何确切地得出他们的结果的。以下假设示例仅用于探索目的。

在看下面的例子并思考可能遗漏了什么之前，请尝试仔细思考以下每个潜在偏见的例子。您会看到每一项都涵盖了我们在本章前面介绍过的一种偏差类型，以及属于每一种类型的示例:

*   代理偏差:
    *   邮政编码:可能会有某些地区被考虑在内，包括代表不足的群体占人口比例较高的地区。与其他领域相比，这些领域可能被低估了。
*   样本/选择偏差:
    *   犯罪数据:那些看起来“很少”犯罪的地区，但也可能是因为那个地区的犯罪没有被报告，因为警察局不在附近。
*   排除偏差:
    *   建造质量:所用木材的年代最初被包含在数据集中，但因为被认为意义不大而被剔除。
    *   颜色:我们知道有购买各种产品的数据，你会看到各种产品搭配颜色卖的更好。可能有几栋房子的某种颜色已经非常过时了，比如石灰绿？
*   测量偏差:
    *   不正确的数据:填写了不同数量的卧室。
    *   学校:学校并不像收视率显示的那么好。学校评级来自一个有其他几个输入的数字。送孩子去一所学校的父母认为这所学校好的方式可能会有很大的不同。这也可能引入代理偏差。
*   社会偏见:
    *   社会偏见根深蒂固:模型知道人口统计数据，这可能会在少数民族人口较多的地区根深蒂固地形成旧观念和偏见。

考虑到这一点，我们来看一个关于住房数据的示例数据集。在本例中，我们添加了真实数据和合成数据的混合数据，以证明像这样的数据集和域可能会产生的潜在偏差。这些数据在很大程度上基于这里找到的数据集:[https://www.kaggle.com/harlfoxem/housesalesprediction](https://www.kaggle.com/harlfoxem/housesalesprediction):

![Figure 6.7 – The housing dataset
](image/Figure_6.7.jpg)

图 6.7–住房数据集

让我们看一下`bathrooms`字段，看看是否可能存在一些问题。在我们尝试发现数据集中的偏差时，请遵循以下步骤:

1.  先从以下链接下载数据集:[https://github . com/packt publishing/Building-Data-Science-Solutions-with-Anaconda/tree/main/raw _ Data/KC _ house _ Data](https://github.com/PacktPublishing/Building-Data-Science-Solutions-with-Anaconda/tree/main/raw_data/kc_house_data)。
2.  导入所需的库，并将 CSV 文件作为 pandas DataFrame:

    ```
    import pandas as pd
    import matplotlib.pyplot as plt
    df = pd.read_csv('kc_house_data.csv')
    ```

3.  您决定更多地查看`bathrooms`字段，因为您认为由于可能值的分布很广，可能会出现测量错误。制作一个条形图，查看该字段中不同值的计数(注意，末尾的额外`;`告诉 Jupyter 笔记本只绘制它，不会告诉您任何更多信息):

    ```
    count = df['bathrooms']
    count.value_counts().plot(kind='bar');
    ```

以下是上述代码的输出:

![Figure 6.8 – The bathroom count
](image/Figure_6.8.jpg)

图 6.8–浴室数量

1.  使用这个，你可以看到浴室四分之一的大幅度摆动和增量可能表明这是如何测量的一些问题。

现在，让我们看看不同房屋是否有一个好的表示，或者在`race`栏获取这些数据是否有一些偏差。让我们抓住几个较大的邮政编码，并检查按种族分列的情况。

我们已经导入了 DataFrame，所以我们需要做的就是将我们需要的两列分组并检查它们的大小:

```
df.groupby(['zipcode','race']).size()
```

您应该在 Jupyter 笔记本中看到下面的输出，它显示了我们的数据集，该数据集由两个`zipcode`和`race`字段分组:

![Figure 6.9 – The dataset grouped by feature
](image/Figure_6.9.jpg)

图 6.9–按特征分组的数据集

根据[的 https://www . Washington-demographics . com/98001-demographics #:~:text = Race %20% 26% 20 ethylation % 202019，% 25)% 20 和% 20 西班牙裔%20(11.4%25](https://www.washington-demographics.com/98001-demographics#:~:text=Race%20%26%20Ethnicity%202019,%25)%20and%20Hispanic%20(11.4%25) )，谷歌快速搜索后发现，第一行显示的 **98001** 邮政编码由 58.7%的白人和 13%的亚裔组成。在我们的数据集中，它显示了完全不同的构成，因为几乎所有的值都代表白人家庭。去检查这些数据是如何被检索的可能是一个好主意(这超出了本书的范围)。

所有这些潜在的领域和更多的领域是一个很好的理由，为什么任何房地产公司都应该确保他们密切关注如何评估这些东西，以减少偏见。

在另一个广泛使用的关于住房的例子中，`scikit-learn`包附带了波士顿住房数据集，作为他们的玩具数据集之一。现在越来越多的人知道，这个数据集将不再被使用，因为它将特定种族的因素纳入了数据。在`scikit-learn`文档中，你可以看到他们通知你改用加州数据集，波士顿数据集将在未来版本中被移除。

# 总结

谈到数据和人工智能，你可能听说过“垃圾进，垃圾出”这句话。在这一章中，你看到我们也应该同样认真地对待短语*偏向于，偏向于*。

我们研究了一些主要领域，在这些领域中偏差可能会渗入到我们的数据中，我们必须着眼于尽早发现这一点。在这个过程中的某些时候，已经太晚了。偏见和歧视会对现实世界产生影响，从招聘和车辆安全到围绕社会规范的持续不公正做法。

你有几个选择来确保你尽你所能来避免这种偏见，例如拥有领域知识或咨询那些拥有领域知识的人，并让来自不同背景的人查看数据(或者更好的是，在你的团队中)。

还存在许多其他类型的偏见，并且承认，我们甚至没有意识到的事情是值得关注的领域。同样需要注意的是 [*第一章*](B16589_01_ePub.xhtml#_idTextAnchor015) *中谈到的漂移，理解 AI/ML 的景观，*在这里发挥作用。现实世界在变化，这些变化应该反映在你陈旧的生产模式中。一开始可能不存在的偏见很可能在久而久之变得很明显。

既然您已经对偏见在哪里起作用有了基本的了解，那么是时候开始研究模型本身了。从线性回归到随机森林，有很多技术可以使用，确保为您的问题选择正确的模型方法是至关重要的。在下一章，我们将更详细地探讨如何选择正确的模型。**