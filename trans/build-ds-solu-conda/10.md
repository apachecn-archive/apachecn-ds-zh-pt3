

# 八、处理常见数据问题

快速评估数据缺陷并纠正它们的能力是按时完成任务还是落后的关键。在本章中，我们将为您提供识别这些问题的工具，您会发现这些问题存在于行业中的许多数据中。

我们首先来看看什么时候会有太多的数据。这可能是一个问题，因为要素之间的相关性非常高，从而使模型变得复杂。您将看到如何找到这些信息，然后删除有问题的条目。

在那之后，我们将检查如何去除空白的、空的或**非数字的** ( **南**)数据，这些数据会把水搅浑。这个问题造成了没有附加值的空白。

我们还会看看当你有分类值时该怎么做。有些时候你需要维护类别之间的关系，有些时候你想要混淆它们，这样你的模型就不会推导出任何不必要的关系。我们将探索和检查这两者。

接下来介绍限制上限，我们将讨论它们是什么以及它们是如何发生的，还将讨论它们如何在数据中产生泡沫，这些泡沫需要在您可以有效地使用它之前进行整理。

最后，我们将了解数据时间操作的基础，您可以使用它来分割和分离数据段，以获取您需要的部分，例如从时间戳中提取月份。

在本章中，我们将讨论以下主题:

*   处理过多的数据
*   查找并纠正不正确的数据条目
*   使用一键编码处理分类值
*   特征缩放
*   使用日期格式

我们开始吧！

# 技术要求

有几件事你需要从本章中得到最大的收获。它们如下:

*   Anaconda 发行版。这包括康达和领航员。你可以从以下网址下载:[https://www.anaconda.com/products/distribution](https://www.anaconda.com/products/distribution)
*   一个有`scikit-learn`、`pandas`和`matplotlib`的康达环境。
*   一个 Jupyter 笔记本来执行所有的编码部分。您也可以使用任何选择的 IDE，甚至命令行，但前提是您将在笔记本上工作。

设置好之后，我们可以看看我们的第一个主题——如何处理额外的数据。

# 处理过多的数据

的确，数据越多通常越好，但情况并非总是如此。很多时候，拥有额外的数据会对结果产生负面影响。这样的例子在 [*第一章*](B16589_01_ePub.xhtml#_idTextAnchor015) ，*理解人工智能/人工智能领域*中有所涉及，其中一位父亲给了他的孩子一个额外的例子，说明什么是老虎，但这个额外的例子实际上是一只美洲豹。这些额外的信息会变成对训练集的负面补充，并为您的模型创建更差的学习结果。

你怎么会知道这些？了解数据。这将是本章、本书以及现实世界中的一个共同主题。如果你不从那里开始，那么其他一切都更具挑战性。这类似于能够理解偏差，在 [*第六章*](B16589_06_ePub.xhtml#_idTextAnchor146) 、*克服 AI/ML 中的偏差*中讨论过。

虽然有时候你不会或者不能完全掌握数据，但是你可以使用工具来帮助你。您可以使用的第一个线索是训练数据集的某些要素是否彼此高度相关。

## 检查特征相关性

特征相关性是检查数据中是否存在潜在重叠的重要指标。pandas 给你一个简单的方法，使用`corr()`函数。现在让我们来看一个例子，使用一个数据集来显示美国大学橄榄球运动中各个大学的招生排名。

数据集将由以下要素组成，这些要素在数据集中的列中表示。

*   **排名**:根据`247Sports.com`确定的招募点数，队伍的降序
*   Ave :基于 100 分制的每个新兵的平均排名
*   **积分**:该校所有招聘人员的总积分
*   **胜率**:2021 年学校总胜率

在你的 Jupyter 笔记本中，首先，确保 panda s 已经安装并读取了 NCAA 数据。**代表**全国大学生体育协会**:**

```
import pandas as pd 
df_ncaa = pd.read_csv('ncaa_data.csv')
```

然后，使用`corr()`函数，它将检查给定数据集中所有列之间的相关性。输出将是一个表格，显示每列从–1.0 到 1.0 的分数，显示它与其他列的相关性:

```
df_ncaa.corr()
```

这段代码将输出相关表，从中可以很容易地看到潜在特性之间的关系:

![Figure 8.1 – The NCAA correlation matrix
](image/Figure_8.1.jpg)

图 8.1-NCAA 相关矩阵

在这个表中，您可以看到 **1** 出现在列与其自身相关的所有地方。这是意料之中的，如下表所示:

*   **1** :完全正相关。一个例子可能是你的汽车每周的燃料成本和所用燃料的升/加仑数(假设每升/加仑的成本不变)。
*   **0** :无关联。一点关系都没有。你可能会从法国的气温和德克萨斯某一天的图书购买量中看到这一点。
*   **-1** :完全负相关。当你看汽车的速度和到目的地的距离时，这可能会发生。

这就是所谓的**皮尔逊相关系数**，在现实世界中很难看到之前的精确数字。如果你这样做了，那么你会想要重新检查你没有意外地引用和比较数字本身或者有一些其他的错误。更有可能的是，在这些数字之间会有一些十进制数字，表明相关性的相对强度。

取皮尔逊相关的绝对值，任何大于 0 . 3 的都被认为有一定的相关性，任何大于 0 . 8 的都有很强的相关性。0.95 以上强得令人难以置信。

利用我们刚刚学到的知识，让我们看看是否有任何列对于我们的数据集是冗余的。看一看**分**排，可以看出排名和平均有很强的相关性，分别为 **-0.91** 和 **.96** 。如果您考虑从点数中收集的平均值，就很容易理解为什么会出现这种情况，并且排名是对总招募点数最高的团队的直接排序。

所有这些列都需要吗？大概不会。安全的做法是去掉衍生变量，只保留事实的来源，即原始点总数。让我们现在做那件事。

### 删除 pandas 中不必要的列

回想一下，我们可以使用`drop()` 命令简单地删除列，该命令可以在 pandas 数据帧上调用。第一个参数是列的名称，也称为标签。这将是你传入的字符串列表。第二个告诉 pandas 你要降列，是二维表中的 *y* 轴，官方称为`1`轴。`0`轴是用来删除行的。

这里，我们删除了`Rank`和`Ave`列:

```
df_ncaa_slim = df_ncaa.drop(['Rank','Ave'],axis=1)
```

之后，您可以检查以确认我们只剩下了我们想要的数据:

```
 df_ncaa_slim.head()
```

`head()`方法将向我们展示新的精简数据集，如图*图 8.2* 所示:

![Figure 8.2 – The trimmed down NCAA dataset
](image/Figure_8.2.jpg)

图 8.2-修剪后的 NCAA 数据集

您可以在这里看到，这允许我们只使用对我们有用的功能/列。

你并不总是想要或者需要删除相关性高的东西；让我们来看看我们可能不想删除这些列的几种方式。

### 移除列时要小心

小心著名的*相关性不等于因果关系*的说法。这意味着仅仅因为两件事之间可能有关联，它们之间可能就没有真正的关系。

在我们早期的例子中，我们发现我们的两个特征之间有直接的联系，但是情况可能并不总是这样。以下面这个我最喜欢的*相关性不等于因果关系*的例子为例——更高的冰淇淋销量与更多的鲨鱼袭击相关。

商店应该停止销售冰淇淋以防止袭击吗？大概不会。在这种情况下，可能的原因是夏季的相关因素，导致更多的人用冰淇淋降温和去海滩。

有些情况下，即使存在相关性，保持数据分离仍然是有用的。以一所房子的房间数和卧室数为例。卧室的数量是房间总数的一个子集，也具有相关性。在这种情况下，两者兼而有之可能是有价值的。很多时候，卧室的数量会达到极限，即使新的房间如剧院、书房或游戏室增加了可用房间的总数。

你还需要注意，一些算法能够处理所谓的多重共线性，你不需要去尝试和执行你自己的分析。随机森林和套索回归等方法不会受到这种分析的太大影响。查看此分析以了解数据并可能对其进行调整可能是有价值的，但如果显示了任何关系，请小心不要认为您需要删除潜在的有价值的数据而矫枉过正。

有时候，有太多数据的反面，那就是当你看到空白或丢失项目的时候。让我们看看在这种情况下我们有什么选择。

### 使用缺失值

空白值是许多数据集的一部分，并且有许多不同的方式可以实现。可能某个值从数据库中被删除，记录某个值时出现了问题，或者甚至可能是某个值不适用的合法情况。

**南**代表**不是数字**。这是一个特殊的 T4 国家，熊猫用它来告诉我们某个值丢失或不可用。这是从 NumPy 继承的，您可能会看到它们显示为 NAN、NaN 或 NaN——它们都是等效的。我们将在本书中使用 NaN。

熊猫的钠值

熊猫从 1.0 版本开始，对于缺失值的实验值为 **NA** 。这样做的原因是为了确保是一个跨所有数据类型的本机和统一的缺失值，但是在编写本文时(版本 1.4.2)，这仍然会在没有警告的情况下发生变化，因此，本章不会对此进行过多讨论。

我们将讨论两种情况，一种是不需要 NaN，另一种是不应该有数据的有效情况。首先，我们需要找到这些值。

## 检测 NaN 值

通过使用`isnull()`方法获取所有为`null`的元素并用`sum()`对结果求和，有一种快速的方法可以检测出我们正在处理的数据是否有 NaN 值。下面的代码将做到这一点:

```
df_ncaa.isnull().sum()
```

第一部分`df_ncaa.isnull()`，获取`df_ncaa`数据帧并返回一个新的布尔数组，每个字段保存相应的缺失元素的`True`或`False`。然后，新的布尔数组被送入`sum()`，它将每一列的值相加，`False`等于`0`，而`True`等于`1`。在这种情况下，将它分成不同的命令没有太大意义，这也是为什么它保留为一个命令的原因。

这将输出数据集中每一列中出现的 NaN 值的数量，如下图所示:

![Figure 8.3 – The total NaNs in the NCAA dataset
](image/Figure_8.3.jpg)

图 8.3-NCAA 数据集中的总 nan

我们现在可以看到在**上届冠军**栏下有很多，在其他栏中只有 **2** 。我们可以深入研究一下，看看哪一行有这些 nan。

## 处理有效的 NaN 值

认为一些 nan 是有效的可能看起来有些违背直觉，但是和往常一样，想想你正在工作的数据集的上下文。能保证每个大学都拿过冠军吗？一点也不。在这种情况下，拥有一个空白的条目是有效的。我们不想对此做任何事情。

## 处理无效的 NaN 值

另一种情况是当存在需要移除的值时。我们可以通过先忽略 **Last Championship** 列来检查哪些值有 nan，因为我们知道它有许多有效的 nan:

```
df = df_ncaa.iloc[:,0:-1]
```

然后，我们使用屏蔽技术来查找有 nan 的行:

```
df[df.isna().any(axis=1)]
```

您将看到有两个条目只包含 nan，我们可以推断这显然是错误的，如以下输出所示:

![Figure 8.4 – The NaN rows in our data
](image/Figure_8.4.jpg)

图 8.4-我们数据中的 NaN 行

然后，我们可以使用内置方法删除 NaNs 值，但是如果我们这样做，它将在我们的 **Last Championship** 列中获取具有 NaNs 的元素，我们知道这些元素是有效的。我们不想仅仅因为学校没有赢得冠军就丢弃这些数据。

我们可以通过设置参数`how='all'`来解决这个问题，即删除一行的标准是当所有的值都是 NaN 时。在这种情况下，我们可以假设数据输入有错误。根据我们之前的发现，第 28 行和第 46 行符合这个标准:

```
ncaa_clean = df_ncaa.dropna(how='all')
```

再次检查 NaN 值，您可以看到我们已经从除了 **Last Championship** 列之外的所有内容中删除了所有 NaN 项:

![Figure 8.5 – The updated NaN values in the NCAA dataset
](image/Figure_8.5.jpg)

图 8.5-NCAA 数据集中更新的 NaN 值

我们现在已经清理了 NaN 值，这给了我们更多的信心，我们已经有了创建模型所需的数据。

# 查找并纠正数据条目

在计算机时代，人的失误总是会起作用。不幸的是，那些错误的击键会在我们负责处理的数据集中显现出来。这将出现在从医疗信息到汽车服务记录的所有东西中。

您可以用几种方法检查异常情况；一种是简单地将项目组合在一起，并查看哪些项目在该组的其他项目中脱颖而出。回顾我们的大学足球数据集，我们想确认学校的会议都是正确的。

我们可以简单地调用`Conference`列，它将位于熊猫系列对象中。这个对象有很多方法可以访问，但是我们感兴趣的是 pandas 的`Series.value_counts()`方法。

让我们用它来检查是否有单独的会议:

```
df_ncaa_error.Conference.value_counts()
```

这将显示以下内容:

![Figure 8.6 – A count by conference 
](image/Figure_8.6.jpg)

图 8.6-按会议统计

我们可以看到有一些只有一个记录。从我们对会议的了解中，我们知道**太阳带**和**西部**(山西)是有效的，但是 **SEV** 是一个突出的问题。召唤那个特定的会议将会让我们看到哪个团队举行。

## 按条件检索特定的熊猫商品

熊猫有一个特别有用的能力,让我们根据数据框中简单到复杂的条件抓取特定的项目。在我们当前的例子中，我们需要检查哪个元素将 **SEV** 作为会议。这就像使用布尔掩码一样简单，如第 4 章 、*使用 Jupyter 笔记本和 NumPy* 中所讨论的。首先，使用满足我们所寻找的条件来创建一个掩码:

```
mask = df_ncaa_error['Conference'] == 'SEV'
```

然后，将该掩码应用于完整的数据帧，以给出我们需要的答案:

```
df_ncaa_error[mask]
```

然后，这个面具会隐藏我们不想看到的所有东西，同时显示我们关注的区域:

![Figure 8.7 – Using a mask to show bad data
](image/Figure_8.7.jpg)

图 8.7–使用掩码显示错误数据

我们知道(或者一些快速研究将告诉我们)A & M 是 **SEC** 的一部分，因此我们可以确信值 **SEV** 是一个输入错误。我们可以使用`loc`方法轻松地进行这种修正，这种方法不仅可以通过索引定位一个项目，还可以获取特定的列。

我们已经准备好了面具，所以我们可以在这里再次使用它。我们还想指定我们只需要**会议**列，并且我们将把它设置为正确的**秒**会议。我们将使用同样的`loc`方法打印出结果。这将允许我们验证它是否已被纠正:

```
df_ncaa_error.loc[mask, 'Conference'] = 'SEC'
df_ncaa_error.loc[7]
```

结果如下:

![Figure 8.8 – Using a mask to verify the updated data
](image/Figure_8.8.jpg)

图 8.8–使用掩码验证更新的数据

现在很清楚，我们已经成功地纠正了这个问题，并可以继续使用我们的清理数据集和正确的会议设置。

会议本身是一个分类值，除非一切都是数字，否则人工智能模型不会很好地工作。进行这种转换时，需要考虑一些事情。

# 使用一键编码处理分类值

机器学习和统计可以很好地确定数字之间的关系。但是，如果你有一个特征是绝对的，没有关系呢？一个**分类特征**的定义是当变量是一个标签或具有离散可能性的类别，比如颜色、动物王国或城市。

当您拥有这种类型的数据时，一个选择是使用**一次性编码**。这是将分类值转换为一组 1 和 0 的过程，以便模型可以将它们解释为独立的，但不能推断它们之间存在关系。这也防止了某些类别有优劣之分的推论。

您可以在下图中看到这种情况的示例。假设您正在查看弹力球的销售数据，其中一个特征是颜色。有三种颜色——红色、蓝色和绿色。这在下表中表示为数据:

![Figure 8.9 – The ball color categories
](image/Figure_8.9.jpg)

图 8.9–球颜色类别

由于这些类别只是简单的文本，所以不能直接解释，所以您可以使用一次性编码来表示这些数据，这种方式可以用来训练一个模型。如果将每种颜色展开到各自的列中，可以使用一个由 1 和 0 组成的二进制指示器来表示每个离散的类别。我们的球示例如下表所示:

![Figure 8.10 – A one-hot encoding example
](image/Figure_8.10.jpg)

图 8.10–一个热点编码示例

在这里，你可以看到球的颜色可以简单地通过查看 whic h 列有 **1** 来确定。对于**球 1** ，**红色**列是热的(表示为 **1** )，其余的有一个虚拟变量，这仅仅意味着我们在那些点上放了零。因此，只有一个列会成为*热列*，这就是编码类型的名称。当我们讨论如何在 pandas 中使用一键编码数据集时，将会探讨虚拟变量的术语。

## 熊猫一键编码

让我们从使用熊猫的球颜色示例中获得一次性编码结果。我们将首先根据*图 8.9* 重新创建我们的简单数据框架，并添加一个球，以便更容易地查看结果:

```
import pandas as pd
df = pd.DataFrame({
    'name': ['ball_1', 'ball_2','ball_3', 'ball_4'],
    'color': ['red', 'blue', 'green', 'blue']
    })
df
```

您将看到数据框，它与前面的表格非常相似:

![Figure 8.11 – A one-hot encoding example setup
](image/Figure_8.11.jpg)

图 8.11–一次性编码设置示例

接下来，我们将使用 pandas `get_dummies`函数对我们的结果进行一次热编码。正如我们所知，一键编码在扩展的数据帧中生成大部分虚拟值(零)，这就是这个名字的由来。`column`参数指定了我们应该编码的内容，前缀只是一个字符串，它将被添加到前面以便于阅读:

```
one_hot_df = pd.get_dummies(df, prefix=['color'], columns = ['color'])
one_hot_df
```

这样做的结果将是我们的一次性编码结果。注意 **ball_4** 的色域与 **ball_2** 的色域相同，因为它们都是**蓝色**:

![Figure 8.12 – A one-hot encoding results
](image/Figure_8.12.jpg)

图 8.12–一键编码结果

这个结果是我们所期望的，但是在决定使用一键编码时，有一些事情需要考虑。

### 何时不使用一键编码

一键编码并不总是你想要的方法，你需要考虑一些事情。

当有异常大量的类别或独特的元素时，您需要保持谨慎。回想一下只有三种颜色的球的例子。如果不是三种可能性，而是一万种可能性呢？那么，如果我们从两个颜色特征开始，分别代表原色和次原色，会怎么样呢？您会将数据集从几列大规模扩展到 20，000 列！这将显著降低训练或分析模型所需的处理时间。

当关系开始起作用时，还有另一种类型的编码你可能要考虑，那就是序数编码。我们接下来会谈到这一点。

## 序数编码

事实上，对于确实有关系的东西，最好不要一次性编码。在它们之间具有固有的排序的变量被称为**顺序变量**。示例包括测试中的等级或调查中的满意度。这些，实际上确实有主观的*更好和更差*的方面给他们，作为一个 *D* 成绩不如一个 *A* ，而*非常不满意*不如*非常满意*。

如果您有顺序变量，最好以保持这种关系不变的格式保留它们，即使它们是分类评级。如果数量级与试图解决的问题相关，那么它们可能不是一次性编码的可行候选。

例如，下面显示了一项大学调查的结果。捕捉到的只是学生的专业和他们对学院的满意度。评级范围从 1 到 5:

![Figure 8.13 – Simple survey results
](image/Figure_8.13.jpg)

图 8.13-简单调查结果

在这种情况下，您可以对专业进行一次性编码，但您不会想在答案中使用这种方法。从 1 到 5 的答案有一个客观的排序，比如 5 比 4 好。我们不想因为一键编码而失去上下文。

# 特征缩放

当你处理大量的数据时，偏差越大，就越难训练出一个好的模型。关于偏差的这个问题有很多原因，我们现在不讨论，但是我们将在第 9 章 *的 [*中的“缩放数据”部分更深入地讨论缩放技术，使用 scikit-learn* 构建回归模型。但你应该知道，有时你会遇到已经有人对数据进行了缩放的数据集。](B16589_09_ePub.xhtml#_idTextAnchor225)*

你不可能总是知道一个数据集来自哪里，所以你可能不知道为什么做出一个特定的决定。

这些数据可能来自同事、Kaggle 竞赛，或者只是包含在`scikit-learn`中的一个示例数据集，就像我们现在使用的这个。这是在 [*第二章*](B16589_02_ePub.xhtml#_idTextAnchor036) 、*分析开源软件*中使用的同一个加州训练数据集，我们假设你已经有了`y_test`和`y_predict`设置。如果没有，回头参考 [*第二章*](B16589_02_ePub.xhtml#_idTextAnchor036) 、*解析开源软件*。

让我们绘制训练数据集，看看我们从哪里开始。以下所有内容都与前面的示例相同，只是增加了`alpha`参数，该参数允许一个浮点数来指示我们希望图形中的某些内容有多透明或不透明。这将让我们更清楚地看到密度。通常，`.2`是一个好的起始值:

```
import matplotlib.pyplot as plt
plt.title('Actual vs Predicted California Housing price') 
plt.xlabel('Actual price') 
plt.ylabel('Predicted price') 
plt.scatter(y_test,y_predict,alpha=.2)
plt.plot([0, 5], [0, 5],"r-") 
plt.show()
```

前面的代码块将输出下面的图，显示预测值与实际房屋值的对比，其中较暗的点表示密度较大:

![Figure 8.14 – A data anomaly from plotting the California dataset
](image/Figure_8.14.jpg)

图 8.14–绘制加州数据集时的数据异常

如你所见，这张数据图在右边显示了一个奇怪的异常。你认为所有那些房子的实际价格都完全一样吗？大概不会。一个问题是，实际价格上限为 5.0，但模型可能(正确地)预测这些单位高于缩放后的 5.0。

另一个问题是规模本身。你很难在任何地方找到五美元左右的房子。让我们从另一种可视化数据的方式开始，解开可能发生的事情。

## 创建熊猫直方图

通过使用熊猫的内置直方图特征，我们可以看到是否有一些极值可能是一个问题。

首先，让我们构建一个值的直方图，看看它们是否遵循正态分布。一个**直方图**是一种显示数据的方式，其中值被放入相等范围的桶中，然后你计算每个桶中有多少项。这也被称为**宁滨**。考虑到我们对房屋价格的各种可能性，我们将使用大量的箱子。箱越多，每个箱中的范围越小。

我们将使用加州数据集的 targe t ，然后调用`hist()`函数，将 bin 号设置为`100`:

```
target_value.hist(bins = 100)
```

利用这一点，我们看到了我们所期望的。许多房屋的价格都列在同一个桶中，如下图最右边所示:

![Figure 8.15 – The California home price histogram's original data
](image/Figure_8.15.jpg)

图 8.15-加州房价直方图的原始数据

随意摆弄`bins`数字来查看结果图。让我们展示一个只有`5`个仓的快速极端例子:

```
target_value.hist(bins = 5)
```

以下是结果图，与*图 8.15* 相同，但由于只有五个箱子可放东西，箱子尺寸更大:

![Figure 8.16 – The California home price histogram with a small bin number
](image/Figure_8.16.jpg)

图 8.16-带有小 bin 号的加州房价直方图

很明显，如果我们从这一小部分数据开始，我们将会失去有价值的洞察力，即在数据的高端存在异常。

关于在图 8.15 的*中 100 个面元的直方图上看到的异常，这是在设定的限制下封顶数据的一个缺点。你牺牲了一些价格的明确性来换取你需要训练的确切目标！其中一些房子可能是上限的两到三倍，但没有办法知道。*

发生这种情况有很多原因。也许有一项民意调查有多个选择，或者也许收集数据的研究人员认为最好将数据打包，以便更容易处理。无论是哪种情况，我们正在失去纯粹的信息，需要看看我们是否能够纠正这种情况。

在我们做其他事情之前，我们需要一些客观的方法来衡量我们将要做的事情是否真的改善了模型。有几种方法可以解决回归问题。

## 使用 R2 评分评估模型

用默认数据训练这个模型的好坏的一个方法是使用 R2 分数。R2 分数是一个指标，表明我们的模型与我们简单地取目标值的平均值并假设每栋房子都以此价格出售相比表现如何:

*   **R2 = 0.0** :该模型的表现与简单地对每个因变量取平均值完全相同——在这种情况下，就是房价。
*   **R2 = 1.0** :模型与数据完美吻合。这可能是不可取的，因为这可能意味着模型过拟合训练数据，并且可能在以前没有见过的数据上表现不佳。
*   R2 < 0.0 :这个模型的表现比你只取所有东西的平均值要差。这表明模型需要大量工作，数据需要大量工作，或者两者都需要。

让我们快速计算一下 R2 的分数。我们将在 [*第 9 章*](B16589_09_ePub.xhtml#_idTextAnchor225) 、*用 scikit-learn 建立回归模型*中更详细地介绍`r2`分数和其他评估模型准确性的方法。

我们将讨论与前几章相同的创建模型的步骤，因此我们将跳过导入 California 测试数据集和`train/test`分割的步骤。参见 [*第二章*](B16589_02_ePub.xhtml#_idTextAnchor036) ，*分析开源软件*，复习模型创建。

在这里，我们将重点关注模型训练和获得 R2 分数:

1.  首先，创建一个模型，正如我们之前所做的:

    ```
    from sklearn.linear_model import LinearRegression
    linear_regressor = LinearRegression() 
    y_predict = linear_regressor.fit(X_train,y_train).predict(X_test)
    ```

2.  然后，使用`sklearn`，通过使用预测值并将它们与我们从完整数据集:

    ```
    from sklearn.metrics import r2_score
    r2_score(y_test, y_predict)
    ```

    中获得的`test`值进行比较，我们可以轻松获得`r2`分数

这将给出`0.626`的结果，这没问题，但是让我们看看在本章的后面我们是否可以改进。

## 使用 MSE 分数评估模型

除了`r2`分数，另一种常用的方法是用均方差来度量回归模型。**均方误差** ( **MSE** )就是一个简单的等式，在这个等式中，你取实际值与预测值的平方有多远的平均值。实际点到预测点的原始距离就是误差，因此得名 *MSE* 。

下图显示了它的方程式:

![Figure 8.17 – The MSE equation
](image/Figure_8.17.jpg)

图 8.17–MSE 方程

以下是上一个等式的组成部分:

*   **Yi** =实际值
*   **ŷi**=预测值
*   **n** =数据点的总数

MSE 会给你一个分数，分数越低越好，大致如下:

*   **MSE = 0** :所有数据点都有一个完美的预测。类似于得到一个完美的 R2 分数，这可能非常可疑，因为它可能表明数据有点太吻合了。
*   MSE = > 0 :你的预期目标并不完美，而且随着目标越高越差。这种测量的一个缺陷是，虽然你可以在完全相同的数据集上比较两个模型，但没有一个范围从*非常好的*到*糟糕的*。在一个问题或场景中，20 的 MSE 可能是不可思议的，但是在另一个场景中，相同的值可能与`true`值相差甚远。这完全取决于目标值的范围。

使用`sklearn`计算 MSE 也很简单，假设您使用与之前 R2 相同的`y_test`和`y_predict`变量，您可以简单地使用`mean_squared_error`方法来计算它，如下所示:

```
from sklearn.metrics import mean_squared_error
mean_squared_error(y_test, y_predict)
```

这将给出预期的 MSE，在我们的例子中如下:

```
0.61222
```

这些 R2 和 MSE 值都很好，但是还有一种方法可以评估我们的结果，这就是 MAE。

## 使用 MAE 评分评估模型

**平均绝对误差** ( **MAE** )与 MSE 非常相似，除了对于每个数据点，你取误差的绝对值而不是平方它。这样做的好处是，在处理较大的数字时，可以防止形成大量的数字，在平方小于 1 的数字时，也可以防止出现倾斜的较小数字。

方程式如下:

![Figure 8.18 – The MAE equation
](image/Figure_8.18.jpg)

图 8.18–MAE 方程

以下是上一个等式的组成部分:

*   **yi** =实际值
*   **xi** =预测值
*   **n** =数据点的总数:

我们可以使用以下代码，使用与之前相同的值再次计算 MAE:

```
from sklearn.metrics import mean_absolute_error
print(f"MAE is: {mean_absolute_error(y_test, y_predict)}")
```

这给了我们 0.5117 的分数，但我们认为，如果我们专注于消除中值收入的上限值，我们可以改善数据点。但是我们应该做些什么呢？

## 克服封顶值的限制

一种解决方案是简单地删除所有达到上限的数据。这可能是最安全的方法，以确保我们不包括任何有上限的数据，但事实上，可能会更高。

### 基于条件删除熊猫行

在我们的例子中，`y_train`是代表房价的训练数据。我们需要清除超过五个阈值的数据，因为我们知道这是达到上限的数据。

将该标准设置为要检查的条件，然后使用该条件创建每个元素是否满足该标准的布尔表示。使用以下代码来实现这一点:

```
target_cond = target_value < 5.0
target_cond
```

您将会看到这为您提供了布尔掩码，如下所示:

```
0        True
1        True
2        True
         ... 
20637    True
20638    True
20639    True
```

因为我们知道训练特征由于该数据与目标值匹配而具有相同的行数，所以我们现在可以将该掩码应用于我们的训练特征:

```
filtered_training_data = training_data[target_cond]
```

这将为您提供一个 19，648 行× 6 列的数据集来进行训练。

我们还要检查新目标值的直方图，以确保我们已经移除了顶部上限。我们将调用与前面相同的直方图方法，只是使用目标的新子组:

```
target_value[target_cond].hist(bins = 100)
```

如您所见，我们已经移除了末尾的大列。这是我们想要和期望的:

![Figure 8.19 – The California home price histogram filtered
](image/Figure_8.19.jpg)

图 8.19-过滤后的加州房价直方图

现在，让我们用新数据以同样的方式训练一个模型，并再次检查 R2 和 MSE 分数:

```
X_train, X_test, y_train, y_test = train_test_split(filtered_training_data, target_value[target_cond], test_size = 0.2, random_state=1) 
linear_regressor = LinearRegression() 
y_predict = linear_regressor.fit(X_train,y_train).predict(X_test)
print(f"Filtered Prediction")
print(f"R2 is: {r2_score(y_test, y_predict)}")
print(f"MSE is: {mean_squared_error(y_test, y_predict)}")
```

我们将看到，这为我们提供了这个新数据集的两个分数:

```
Filtered Prediction
R2 is: 0.5159223449106703
MSE is: 0.4582190705946869
```

让我们使用已知的值来比较新旧值:

![Figure 8.20 – Comparing the R2 and MSE scores
](image/Figure_8.20.jpg)

图 8.20–比较 R2 和 MSE 分数

你可能会注意到一些奇怪的事情。我们认为不是特别好的原始数据集具有稍微好一点的 **R2** 分数，但是过滤后的数据集具有好得多的 **MSE** 分数！

我们应该从中得出三个结论:

*   清理数据并不总是保证更好的模型。一个模型是否能够准确地预测某件事，有大量的因素在起作用，改变一个方面可能会产生不可预见的影响。
*   查看多个评分指标是一个好主意。没有一个明确的方法可以知道一个模型是好是坏，如果你只是看着`r2`的分数，你可能甚至不会尝试去改进。相反，如果你只是使用 MSE 作为衡量标准，那么你可能会失去这样的洞察力，即删除这些额外的数据可能不是你认为的本垒打。
*   有时候，你会犯错误。你认为你对数据的洞察力可能是错误的。也许，在这种情况下，房屋显示了准确的数据，人们只是非常犹豫要不要把他们的房子标在 50 万美元的价格上，这类似于人们出于心理原因更可能购买价格为 19.99 美元而不是 20.00 美元的产品。

最后，过滤数据的 MSE 大约提高了 25%,这是一个显著的进步。我们应该坚持这个。较高的分数可能是因为与总数相比，处于上限价格的房屋相对较少，但这些高销售数字提高了平均水平。

然而，如果我们想弄清楚我们最初是如何得到这个价格上限的呢？您可能想要尝试获取原始数据集或者收集更多关于用于构建它的过程的信息。

## 恢复原始数据集

你可以自己简单地回到原始数据集并从那里开始。如果你是从同事那里、网上或其他渠道得到这些信息的，那么追查起来可能会很简单。

如果数据集是由您认识的人创建的，您也可以与他们交谈，看看是收集方法还是后来的数据处理达到了这个上限。

存储库中可能还有文档，告诉您使用了什么缩放以及如何反转它。可能有更高级的东西压缩了更高的数字，因此你可能已经失去了逆转比例的能力。

# 使用日期格式

日期和时间经常出现在数据集中，可能会带来一些独特的数据问题，成为数据科学家的一大难题。世界上有许多格式，这些格式因国家和系统而异。例如，美国常用月/日/年格式(mm/dd/yyyy)，但在欧洲，你更可能看到日/月/年(dd/mm/yyyy)。

Python 有一个内置的`datetime`对象，但是我们也将使用 pandas 的内置`datetime`类型。这将允许我们轻松地对它们执行一些不同的操作，包括获取月份值、指定特定的格式以及其他操作。

时区也起了作用。对于何时发生什么，世界上有许多不同的规则。这是 UTC 变得更加普遍的一个原因。UTC 是一套标准，无论您的具体时区是什么，都可以使用。

### 在 pandas 中指定日期字段

调出日期的最简单方法是在读取数据时指定日期。假设您使用的是`read_csv()`方法，您可以使用`parse_dates`参数告诉 pandas 哪一列包含数据。我们将使用我们的大学数据集和大学最后一次赢得冠军的日期。

读入 CSV 文件，然后检查`info()`以验证第二列现在作为`datetime`对象被正确提取:

```
df_date_format = pd.read_csv('date_format.csv', parse_dates=[1])
df_date_format.info()
```

您将看到第二列被列为一个`datetime`对象，正如所料。注意 **64** 只是说明这个对象是 64 位；这是熊猫内部运作的一部分，你不需要担心:

```
Data columns (total 2 columns): 
#   Column  Non-Null Count  Dtype         -
--  ------  --------------  -----          
0   Price   12 non-null     int64          
1   Date    12 non-null     datetime64[ns]
dtypes: datetime64[ns](1), int64(1)
```

现在，假设我们只需要年份，因为月份和日期对于理解数据或回答我们想要的问题并不重要。我们可以通过从这个`data`对象创建一个只包含年份的新列来做到这一点。

您可以调用`dt`来访问`datetime`方法，并从这些方法中调用 year:

```
df_date_format['year'] = df_date_format['Date'].dt.year
df_date_format.head()
```

这将向您显示以下内容:

![Figure 8.21 – Pulling out the year from pandas' datetime object
](image/Figure_8.21.jpg)

图 8.21–从熊猫的日期时间对象中提取年份

正如你现在看到的，年份在末尾单独出现在一个干净的列中。

### 使用 to_datetime 方法将字符串转换为日期

你不会总是有好处得到一个干净的数据格式的日期，但有一个快速和简单的方法来转换它们与熊猫。`to_datetime()`方法将接受一个类似数组的单个项目或 DataFrame 对象，并将其转换成一个`datetime`对象。

一个简单的例子向您展示了它的易用性。让我们从如下日期列表开始:

```
dates = pd.Series(['09/10/1956', '7/05/1957', '9/08/1981', '06/10/1983', '07/13/1987', '9/14/1990', '5/02/1992', '10/08/1994'])
```

然后，我们简单地传入这个列表和`infer_datetime_format=True`参数，用下面的代码表示我们想从输入数据中推断出格式是什么:

```
dates_df = pd.to_datetime(dates, infer_datetime_format=True)
```

现在，我们将获得一个 DataFrame，它具有传入的相同元素，但由`datetime`类型组成，我们可以根据需要对其进行操作和更改。

在我们拥有一个`datetime`对象之后，我们可以执行许多其他操作。更完整的列表，请看这里的官方文档:[https://pandas . pydata . org/docs/reference/API/pandas . to _ datetime . html](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html)。

# 总结

你看到的每一种情况和数据集都将是独一无二的；但是，你和他们遇到的问题不会。在本章中，您看到了在您将要使用的数据集上会反复出现的问题。

我们已经看到，高度相关的要素拥有过多的数据会带来怎样的问题，以及如何找到并消除这种相关性。我们使用了大学招聘分数和排名的例子，但你可以很容易地在现实世界中找到其他的，如房价-你可能有每平方英尺的价格，但也有这些作为单独的功能。

处理分类数据很常见，但归根结底，机器学习模型需要数字才能工作。我们看到，有些时候我们希望保持分类值之间的关系，比如一个评级系统，而有些时候我们不希望这样。我们看到了当我们不想保留关系时，如何使用一次性编码来编码这些类别。

处理缺失值或空白值是另一个问题，我们研究了如何隔离那些显示为 nan 的项目。我们看到有些不是问题，而有些会造成问题。对于那些可能成为问题的元素，我们检查了如何删除只包含 NaN 值的元素，以便我们可以避免将来出现类似的问题。

接下来，我们看了看如何通过在分类列中找到自己突出的条目来隔离具有不正确数据的字段。当一个条目不是预期的时候，这可以给我们一个指示；当条目的数量足够多时，一个类别中只有一个实体的可能性很小。

我们还看到了如何通过查看直方图来检测上限值，以及 pandas 如何让您有条件地删除元素。我们看到了这如何改变用于评估模型的`r2`和 MSE 分数，以及如何在给其中任何一个太大的权重时使用运动警告。

最后，我们快速浏览了一下如何使用`datetime`对象，以及如何提取`datetime`的特定组成部分，比如月份或年份。

希望有了这些工具，您将能够决定何时需要清理数据，从而更快地纠正错误。现在，您可以加快纠正上述问题的速度，从而更快地实现解决问题和创建模型的最终目标。

在我们的下一章中，我们将把本书到目前为止学到的许多东西放在一起，使用`scikit-learn`创建一个更大的回归模型，并能够可视化结果。**