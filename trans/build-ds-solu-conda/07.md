

# 五、清理和可视化数据

根据 Anaconda 最新的*数据科学状态报告*(【https://bit.ly/3F2D8YM】T2)，作为一名数据科学家，你 39%的时间将花在数据准备或清理上。这可能不足为奇，但是能够正确地设置问题对于能够从数据中获得好的答案是至关重要的。

数据很少会以完美的形式出现在你面前，即使这样，你也可能想要操纵它来回答不同的问题。能够快速找到一般的统计数据、发现并删除坏的列，以及就地改变字段都是需要的。

在正确的形式下，可视化是一个关键的工具，不仅可以向那些可能关心它的人展示你的发现，还可以在这个数据探索阶段作为你自己的指南。清理和可视化是齐头并进的，很多时候你会看到数据的某些方面在看到后需要调整。本章将教你如何清理和可视化数据。

在本章中，您将了解以下主题:

*   使用熊猫清理数据
*   使用 Matplotlib 可视化数据

# 技术要求

要学习本章内容，您需要与前面章节相同的基本设置，包括以下内容:

*   Anaconda 发行版已安装。默认情况下，这包括 conda、Navigator 和 Jupyter 笔记本。

进入[https://github . com/fivethirtyeight/data/tree/master/college-majors](https://github.com/fivethirtyeight/data/tree/master/college-majors)下载数据集。或者，如果你不想下载整个库，去 https://data.fivethirtyeight.com/的[搜索*选择大学专业的经济指南*。在右侧，您会看到一个带箭头的按钮，用于下载数据集。](https://data.fivethirtyeight.com/)

# 用熊猫清理数据

在处理数据时，最重要的一个方面是确保它是您需要的正确格式。除了获得足够的数据，这可能是训练精确模型的最重要的部分。在本节中，我们将介绍导入 CSV 文件的步骤，然后了解如何分析和清理它，以确保它为我们做好准备。

我们要看的例子是各种美国大学专业的数据，以及它与薪酬的关系。对我们正在研究的领域有一个大致的了解是至关重要的，这是一个你可能已经掌握的领域。本数据集由优秀的 FiveThirtyEight 网站提供，更多信息可以在这里找到:[https://github . com/FiveThirtyEight/data/tree/master/college-majors](https://github.com/fivethirtyeight/data/tree/master/college-majors)。

我们的目标是看看我们是否可以利用这些数据来判断我们是否应该选择另一个专业。我们甚至会发现上大学终究不是个好主意。我们还会回答一些问题，比如，学生们会选择收入潜力最高的专业吗？

让我们首先确保安装了 pandas。

## 在您的 conda 环境中安装 pandas

要安装 pandas，我们将使用 conda 或 Anaconda Navigator，这取决于您分别想要命令行还是可视化显示。我们将从一个新的环境开始，以确保一切都处于一个新的状态，并且您不需要担心搞乱任何早期的包版本:

1.  创建一个名为`ch_5`的新环境。关于这方面的复习，请参见 [*第 2 章*](B16589_02_ePub.xhtml#_idTextAnchor036) 、*分析开源软件*中的*创建 Conda 虚拟环境*部分
2.  激活`ch_5`环境。
3.  安装`pandas`。
4.  通过 Anaconda Navigator 在该环境中启动 Jupyter 笔记本。

进入 Jupyter 笔记本后，您就可以开始使用数据集了。

## 使用 CSV

在专业环境中以 **CSV** 的形式获取数据是很常见的。CSV 格式是一种简单、廉价且非常便携的格式，是存储结构化数据的普遍方法。pandas 提供了一种用`read_csv`函数导入 CSV 的简单方法。

在其原始形式中，所有 CSV 都是用逗号分隔值的文本(因此缩写为)。包含三列两行数据的 CSV 如下所示:

```
Column 1, Column 2, Column 3
data1, data2, data3
data4, data5, data6
```

Microsoft Excel 和 Google Sheets 等工具对此进行解释，以显示我们熟悉的常见表格格式，如下所示:

![Figure 5.1 – An example of a CSV file in a table representation
](image/Figure_5.1.jpg)

图 5.1–表格形式的 CSV 文件示例

### 结构化与非结构化数据

结构化数据是具有清晰的格式和模式的数据，您可以对其进行解析。这通常是有意为您创建的数据，以便您能够处理和解析。典型结构化数据的示例包括:

*   战斗支援车
*   JSON
*   数据库表

另一种类型是非结构化的，你可以猜到需要多做一点工作来处理。这些数据可以有结构化的元数据，但是原始形式本身没有容易获取的模式。

一些例子包括如下:

*   图片
*   录像
*   pdf

我们将在本章和整本书中关注结构化数据。

让我们从下载解决这个问题所需的数据开始。

### 下载 CSV 数据

前往[https://github . com/packt publishing/Building-Data-Science-Solutions-with-Anaconda/blob/main/chapter 05/College/all-ages _ dept . csv](https://github.com/PacktPublishing/Building-Data-Science-Solutions-with-Anaconda/blob/main/Chapter05/College/all-ages_dept.csv)下载 CSV 文件。将这个文件保存到你创建 Jupyter 笔记本的同一个目录。

请注意，FiveThirtyEight 的原始数据与我们将要使用的略有不同。为了清理一些东西，我添加、调整和删除了一些项目。如果您直接从 FiveThirtyEight GitHub 库下载 CSV 文件，您会看到不同的结果。

### 用熊猫读取 CSV 数据

在 pandas 中处理 CSV 文件很简单。首先，确保导入熊猫本身，然后使用带有 CSV 文件名作为参数的`read_csv`函数。这假定您将该文件与当前使用的 Jupyter 笔记本放在同一个文件夹中。如果不是，你需要给出完整的路径。在第一个单元格中，键入以下内容:

```
import pandas as pd
df_raw = pd.read_csv('college/all-ages_dept.csv')
```

运行之后，在下一个单元格中，只需用下面的检查前几行:

```
df_raw.head()
```

这将为您提供前五行数据的表格，其格式如下图所示:

![Figure 5.2 – The head() function
](image/Figure_5.2.jpg)

图 5.2–head()函数

默认情况下，标题名称将是第一行。您可以用`header = none`覆盖它并将所有行作为数据。我们希望保持原样，因此我们将保留默认值。

readcsv()中的可选分隔符参数

在`readcsv()`方法中，有一个常用的输入，就是`sep`。这是`separator`的缩写，它表示熊猫是否应该基于除默认逗号之外的任何东西来拆分字段。虽然从技术上来说它不是一个 CSV，但是您经常会看到用分号(`;`)来分隔字段。这个输入会让熊猫知道它，这样它就不会搞混了。

## 分析和清理数据

无论您在处理什么数据集,我都建议您首先执行一系列步骤:

1.  理解数据
2.  删除不必要的列
3.  删除重复行

每次使用`head()`函数查看数据时，您想要运行的第一件事是`info()`。运行这个程序将向您显示有价值的信息，比如列、数据类型，以及对我们来说最重要的`null`值。

现在运行该命令，您将看到`Dept_Head`字段的数字比**非空计数**列中的其他字段的数字小，如下图所示:

![Figure 5.3 – Using the info() command on a DataFrame
](image/Figure_5.3.jpg)

图5.3–在数据帧上使用 info()命令

这表明存在大量缺失值。让我们在下一节更深入地探讨这个问题。

## 处理缺失数据

缺失的价值观可能是一个无声的杀手。即使您可能会看到数千行，如果缺少值，您仍然不能创建最好的模型。让我们通过对空值进行计数来获得一个更精确的空值数目，这样我们就可以对事情有一个清楚的了解。使用以下方法合计每列的总数:

```
In [10]: df_raw.isnull().sum()
Out[10]:
[...]
Median     7
P75th      0
Dept_Head  73
dtype: int64
```

如您所见，`Dept_Head`和`Median`都有缺失值。根据上下文，您可以选择几条路线。一个是删除这个专栏，另一个是让对它们可能是什么做出一个有根据的猜测(称为猜测)。现在让我们都来看看。

### 移除列

最直接的方法可能是完全删除该列。在某个点上，具有许多空值的特征/列对于进行数据分析或训练模型来说是低效的。确切的阈值超出了本章的范围，但是要注意，如果超过一半的元素是空白的，那么它可能没有用。

如果您已经知道该列可能没有什么价值，这也可能是一条路径-例如，如果您正在查看每个人口的预期寿命以确定哪些因素在其中起作用，您可能会发现拥有国家动物的字段没有什么用处。一些国家可能没有，但这可能与你认为一些国家的平均年龄更低的真正原因没有任何关系。

虽然删除列的原因有很多，但它们都归结为对最终结果没有任何影响，但是被训练的模型不会知道这一点，并且会尝试将其融入到解释中。当字段为下列情况之一时，可能会需要删除列:

*   唯一 id
*   名称
*   网站
*   电子邮件

在提到的所有例子中，你会发现高水平的独特值，这对于训练模型来说不是一个有价值的补充。现在让我们看看如何评估当这个列可能是一个很好的删除候选时。

#### 计算列中唯一值的数量

在查看新数据集时，发现列是否包含所有唯一值是应该采取的第一个行动。如果这是一个数字值，那么它不会像预期的那样成为一个大问题，但是在某一点上，如果你正在处理许多独特的分类项目，那么你可能会看到一些可能更好的东西。

我们感觉到`Dept_Head`列可能有大量的唯一值，所以让我们通过获得唯一值的百分比来找出这一点。我们可以通过将唯一项的数量除以总数来实现这一点。我们现在真的不关心南人:

1.  First, let's use the `count()` function. This DataFrame function will count the number of non-NaN items:

    ```
    num_dept = df_raw['Dept_Head'].count()
    ```

    count() ve rsus 形状[0]

    注意，`count()`将返回非 NaN 项的计数，`shape[0]`将返回数据帧的长度。在这种情况下，这就是我们想要的，但在另一种情况下，如果您试图找出有多少实际项目，您可能想要使用`dataframe.shape[0]`方法。

2.  接下来，我们还需要它拥有的唯一值的计数。找到这个值并打印出来，以便进行快速的完整性检查:

    ```
    num_dept_unique = df_raw['Dept_Head'].nunique()
    print(num_dept_unique)
    ```

您应该会看到以下输出:

```
172
```

1.  最后，我们使用一个简单的等式，将唯一项的数量除以总数量乘以 100，得到我们的百分比:

    ```
    print((num_dept_unique/num_dept)*100)
    ```

这给了你一个非常可怕的 100%的数字，这意味着名单上的每个名字都是唯一的。我们知道从中不会收集到任何有用的信息，所以我们放弃吧。

回到我们的`college`数据集，您可以通过简单地使用`drop`函数来删除它。如果你想直接改变数据帧而不需要创建一个变量来保存结果，你可以使用`inplace`参数。这将告诉熊猫，它应该突变(或改变)数据帧，所以确保这是你想要的结果。如果您希望原始数据集保持不变，那么您可以省去这个参数，并将输出保存到您选择的变量中。稍后，我们将在*创建数据帧的深层副本*部分更详细地讨论这一点。

下面将采用后一种方法，因此如果我们愿意，我们可以稍后引用`df_raw`数据集:

```
df = df_raw.drop('Dept_Head', 1, inplace =False)
```

现在，`df`变量将保存一个没有`Dept_Head`列的新数据帧。让我们通过调用 DataFrame 的`columns`属性来检查该数据集的前几行，以确保情况确实如此:

```
df.columns
Index(['Major_code', 'Major', 'Major_category', 'Total', 'Employed',        'Employed_full_time_year_round', 'Unemployed', 'Unemployment_rate',        'Median', 'P25th', 'P75th'],       dtype='object')
```

你总是想对此保持谨慎。很多时候，一些你从未想过会有关联的东西通过两者之间的轻微联系提供了一些洞察力。

如果删除一列不是一个有意义的选项，您可以看看下一个选项来处理丢失的数据，这是一个关于它可能是什么的计算猜测。现在让我们详细讨论一下。

### 输入值

另一个选择是对丢失的值进行最佳猜测。**输入**是用替代值替换缺失数据的过程。有许多方法可以做到这一点，但一些常见的方法是使用平均值，中间值，模式，或向前/向后填充。请注意，您可能已经猜到，您只能计算数值数据的平均值/中值。该模式也可以用于分类数据。

我们先来关注一下`Median`一栏。我们知道这是这里信息的关键部分，所以我们不想放弃它。在我们看我们想要做什么之前，让我们缩小列中的一个特定元素，以便能够确认我们已经在完成时纠正了丢失的值。

### 访问特定元素

在熊猫中有两种主要的方式来访问一个特定的—`Loc`和`iloc`:

*   `Loc`:基于标签位置的检索。您可以使用一个设定的字符串(或数字)来查找一个项目，如果该索引不变，它将总是返回相同的元素，即使事情发生了变化。这可以被认为是一个关键值。
*   `Iloc`:这是一种基于整数位置的方法，根据索引来检索项目。如果您重新排列实际数据，这将为相同的输入检索不同的元素。换句话说，使用`iloc[4]`将总是检索第五个(从 0 开始的)元素。这更像是对数组中第四个元素的随机访问。

现在让我们检查一个 NaN 元素，稍后，我们将再次检查它是否已被更新:

```
In [10]: df['Median'].iloc[170]
Out[10]:  nan
```

如果我们想缩小特定行中的,我们也可以简单地显示所有有 NANs 的情况。现在让我们来学习如何做。

### 显示所有 NAN 值

另一个选择是挑出有问题的元素。您可以通过指定问题中的列，然后结合掩码使用`isna()`函数来找到。对于`Median`列，它将如下所示:

```
df[df['Median'].isna()]
```

这将为您提供如下所示的表格。请注意，还有更多列，我们需要向右滚动才能看到:

![Figure 5.4 – Viewing all NaNs with the isna() function
](image/Figure_5.4.jpg)

图 5.4–使用 isna()函数查看所有 nan

既然我们可以看到空白的列，我们必须弄清楚用什么来填充列。在我们研究一些方法之前，让我们先看看如何创建数据帧的副本，这样我们就可以根据需要进行更改，而不必担心进行永久更改。

## 创建数据帧的深层副本

可能有些时候你想创建一个数据帧的副本，其中一种可能是尝试不同的估算方法。保留原始表单会非常方便，以防在发现您尝试做的事情不如预期的那样好时，您想要重新开始。这是之前使用`inplace =False/True`参数完成的。

你的第一反应可能是简单地做一些如下的事情:

```
df_wrong_copy = df_raw
```

事实上，这两个变量都指向计算机内存中完全相同的东西。对我们的`df_wrong_copy`数据帧的任何编辑或改变都会改变`df_raw`数据帧。这是一种 Python 设计，因此没有大量耗时的复制操作，也没有大量大型数据集实例占用的内存。

在我们的例子中，我们认为我们想要这样，所以我们需要做的就是通过在数据帧上使用`copy()`函数告诉我们想要一个副本，比如`df`，如下所示:

```
new_df = df.copy()
```

我们现在有了新的`new_df`数据框架，我们可以对其执行任何操作或变异，而不用担心会改变原来的数据框架。

现在我们有了这些知识，让我们看看我们可以采用的几种估算方法。

### 输入–向前填充/回填

当您有时序数据，其中元素与之前或之后的项目有关系时，向前填充和回填在场景中最有效。直接在丢失数据之前或之后的值被用作填充空白点的值。

一个很好的例子是，如果你有温度数据时，你正在检查一个即将到来的风暴。读数出了问题，你有几分钟的空白数据。在这种情况下，你可以相当肯定的是，实际数据将与你收到的最后读数相似。

在下面的例子中，你认为缺少的值应该是什么？

![Figure 5.5 – Impute empty values
](image/Figure_5.5.jpg)

图 5.5–估算空值

直观地，你可以根据周围的时间段预测 **56** 或 **57** 度可能是空位置的数据。在这种情况下，向前填充可用于**空锁 2** 以使 *57* ，且回填可用于使**空锁 1** *56* 。

这就是`fillna()`方法的用武之地，它是*填充 NAs* 的缩写。它附带了几个选项，可以让您填充 NAs，并帮助您删除数据集中所有那些烦人的 nan 或空值。其中两个选项是`ffill`和 `bfill`。

您可以使用以下内容分别调用正向填充和反向填充:

```
data_after_impute = df.fillna(method='ffill')
 _impute = df.fillna(method='bfill')
```

使用这些函数将返回填充了空值的新数据集。

出于我们的目的，我们想要创建数据集的副本，然后使用这个向前填充。我们将对第一行执行此操作，然后，使用新的数据帧，更新**中值**列中的所有项目，以填充值:

```
df_ffill = df_raw.copy()
df_ffill['Median'] = df_ffill['Median'].fillna(method='ffill')
```

我们可以使用前面的单个元素进行快速的完整性检查，看看它是否仍然是一个 NaN:

```
df_ffill['Median'].iloc[170]
```

现在这向我们展示了以下内容:

```
72000.0
```

这表明已经处理好了。你也可以检查一下中所有带有`df_ffill['Median'].isna().sum()`的 NaNs 的总和，它显示你有`0`。

回填与顺填是完全相同的过程；我们只是替换了方法。考虑到这一点，我们将跳过这个例子。

除了向前填充和回填之外，还有另一种基本而有效的方法来计算和替换空值，我们将在接下来介绍。

### 输入-平均值

另一种可能性是获取一列的平均值，并使用该值替换所有的空值。在某些情况下，这可能不是一个很好的选择，但如果您有一个较低的标准偏差和一组非时间序列的数据，这可能是一个正确的选择。

回头看看我们的`college`数据集中的`Median`列，我们可以看到这可能是替换那些值的更好的选择。前后的行彼此没有关系，所以只选择它们周围的值来填充 p 可能没有多大意义。让我们用平均值来填充它。

我们将从创建数组的副本开始，然后寻找列的平均值。我们将这个值保存为`mean_value`。提醒一下，`Median`栏只是该专业的工资中位数:

```
In [1]:  df_mean_impute = df_raw.copy()
mean_val = df_mean_impute['Median'].mean()
   mean_value
Out[1]: 50000 
```

接下来，我们将使用该值和`fillna()`方法用计算出的平均值替换所需的值:

```
In [2]: df_mean_impute['Median'] = df_mean_impute['Median'].fillna(mean_val)
```

接下来，我们将检查我们之前找到的特定的项，即`null`，并确保它已如预期的那样被替换为，意思是:

```
In [10]:  df_mean_impute ['Median'].iloc[170]
Out[10]: 56886.06
```

最后，我们将检查它是否已经被我们之前计算的值正确替换，并且不再是`nan`。我们再次使用`isnull()`函数来检查所有的数据，以查看空值 h 已经被处理了:

```
In [11]:    df_mean_impute.isnull().sum()
Out[11]:
[…]
Median    0
P75th    0
Dept_Head  73
dtype: int64
```

我们现在看到,`Median`列的所有错误都已得到纠正，我们正在顺利前进。

#### 均值估算的缺点

当你有一个分类变量时，这种方法就不起作用了。毕竟**动物科学**和**计算机科学**是什么意思？这不是你可以用非数字数据来计算的。

另一个问题是，它没有考虑任何背景。一般来说，计算机科学专业的工资比文科学位的工资要高，但是通过简单地取平均值，额外的信息就被冲走了。

现在让我们看看我们的另一个选择——KNN。

### input-kn

计算缺失值的另一个选项实际上是另一种机器学习技术， **k 近邻** ( **KNN** )。这很好地满足了我们的目的，因为我们只是试图预测一个目标变量。KNN 方法稍微复杂一点，因为您需要经历一些步骤，这些步骤在正常的数据科学流程中可以找到，但在更简单的方法中找不到，例如我们前面看到的简单使用均值估算方法。现在，我们将在一个更高的水平上检查事情；然而，我们将在第 7 章 、*选择最佳人工智能算法*中更详细地介绍 KNN 和其他算法。我们还将在第 8 章 、*处理常见数据问题*中深入讨论 one-hot 编码和缩放的下两个步骤。

我们将从创建数据帧的副本开始，因为我们将对此进行大量更改。这些变化将从删除几列开始，这是纯粹的统计操作，将与`Median`工资高度相关。另一个是`Major`，对于每一行都是唯一的，我们更关心的是`Major_category`:

```
df_knn_impute = df.copy()
df_knn_impute.drop(['Major','P25th','P75th'], axis = 1, inplace = True)
```

#### 一次性编码数据

一键编码的想法很简单——你想把分类特征变成数字。当处理数据时，这是一种非常常见的情况，因为我们的 ML 算法有一个非常困难(有时不可能)的任务来处理类别，这是由于在引擎盖下使用的数学是多么沉重。

幸运的是，pandas 给了我们一个简单的方法来设置这些虚拟变量，这就是一键编码过程。我们只需要告诉它我们想要编码哪些列，要查看什么数据集，以及我们想要在结果列标题上显示什么前缀:

```
columns_to_encode = ['Major_category']
one_hot_df = pd.get_dummies(df_knn_impute, prefix=['one_hot'], columns = columns_to_encode) 
one_hot_df
```

您将看到类似下图的结果:

![Figure 5.6 – A result from one-hot encoding
](image/Figure_5.6.jpg)

图 5.6–一键编码的结果

现在，有了我们的分类数据框架，我们需要缩放数据，这样我们的 KNN 算法就不会过多关注数据中的较大数字。

#### 缩放数据

在运行实际估算之前，我们的最后一步是缩放数据。一般来说，这不是一个正常的估算步骤，但是由于 KNN 过程，这是特别需要的。我们将使用一个简单的叫做`MinMax`的方法。这是一个数学运算，将所有数值调整到 0 到 1 之间。这将大大降低我们的 KNN 模型错误地过分强调较大数字的风险。

首先，我们需要导入它，然后我们只需要创建一个`MinMaxScaler`的实例。在您的 IDE、Jupyter 笔记本或 Python 编辑器工具中输入以下内容:

```
from sklearn.preprocessing import MinMaxScaler
min_max_scaler = MinMaxScaler() 
```

现在，我们将实际上使缩放器适合数据，然后将结果放回到一个漂亮的 pandas 数据帧结构中，以便我们可以容易地看到它:

```
scaled_one_hot_df = min_max_scaler.fit_transform(one_hot_df)
df_scaled_X = pd.DataFrame(scaled_one_hot_df,columns=one_hot_df.columns)
```

我们现在已经得到了我们的数据——热编码、缩放并删除了一些不必要的列；我们现在可以对缺失数据进行实际估算。

#### 执行估算操作

现在我们已经准备好了来执行我们的主要步骤，让我们导入所需的估算值，并使用`nan`的参数创建一个实例，以表明我们想要使用该类型来校正值，并且我们想要查看 KNN 的三个最近的邻居，以确定缺失的值应该是什么:

```
from sklearn.impute import KNNImputer
imputer = KNNImputer(missing_values=np.nan, n_neighbors=3)
```

然后，我们将在前面几节中处理的数据帧输入到这个估算对象中，告诉它替换那些丢失的值。然后，我们将它放入 DataFrame 表单中。使用下面两行代码来完成所有这些工作:

```
imputed_df = imputer.fit_transform(df_scaled)
knn_df = pd.DataFrame(imputed_df,columns=df_scaled_X.columns)
```

现在，通过使用下面的代码，您将会看到我们不会剩下任何 nan:

```
knn_df.isna().any()
```

以下是上述代码的结果:

![Figure 5.7 – The KNN impute result
](image/Figure_5.7.jpg)

图 5.7-KNN 估算结果

您还会注意到，一次性编码的结果仍然存在。这对能见度来说不是最好的。如果我们想将这个数据返回到我们的原始数据集，我们可以简单地使用这个列代替原始数据集:

```
df_knn_impute['Median'] = knn_df['Median']
```

之前我们看了位置 170 的元素。我们可以再看看这个项目，看看在我们目前的情况下会有什么:

```
knn_df['Median'].iloc[170]
```

我们会得到`0.3288`的缩放版回归。我们将在第 8 章 、*处理常见数据问题*中更详细地介绍如何反转缩放比例，但是反转这个会给你`64600.0`作为中值。

到目前为止，我们已经接触了很多命令。让我们在这里添加一个快速指南，可以根据需要参考 pandas 的操作。

### 熊猫小抄

在熊猫行动中，80/20 法则开始发挥作用——20%的方法将是你 80%的时间使用的方法。以下是您将使用的最常用方法的快速参考。

对于给定的任何示例，我们将假设存在以下数据帧:

```
df2 = pd.DataFrame(
    [[1, 2], [4, 5]],
    index= ['row1', 'row2'],
    columns= ['col1', 'col2'])
```

下面是方法及其解释的列表:

*   `df.apply(lambda)`:对数据采用并应用 lambda 函数。
*   `pd.read_csv()`:读入 CSV 文件。
*   `df.to_datetime()`:转换成一个`datetime`对象。
*   `df.drop_duplicates(inplace = True)`:从数据帧中删除所有重复的行。
*   `groupby()`:按指定列分组。
*   `df.sort_values(by='col1', ascending = False)`:指定数据帧在哪些列上进行排序。
*   `df.fillna(method= {'backfill/bfill/pad/ffill/None)`:使用提供的方法填写所有 NaN 值。
*   `pd.isnull(df)`:检测是否有缺失值。
*   `df.loc['row1'] and df.iloc[0]`:分别按列标签或整数索引搜索特定元素。
*   `df.query ('col1 > col2')`:使用布尔(`True` / `False`)表达式执行基本查询。在我们的示例中，它将返回第一列的值大于第二列的任何行。

要获得更完整的方法列表，以及冗长的指南，可以查看这里找到的官方文档:[https://pandas . pydata . org/pandas-docs/stable/user _ guide/index . html](https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html)。

我们经历了一段旅程，整理了我们在这里发现的大量数据。现在，是时候深入研究一下，看看数据告诉我们什么了。让我们学习如何将我们正在看的东西形象化，看看我们在哪里可以找到一些关于人们如何选择专业的见解。

# 使用 Matplotlib 进行可视化

像本书中讨论的许多其他事情一样，有许多软件包可以处理任何特定的领域。对于可视化的工作，Matplotlib 是最广泛使用的工具之一。不仅显示简单的图形非常容易，而且还有许多高级选项可供您使用。它与 panda DataFrames 配合得非常好，并且已经成为数据科学中使用最广泛的软件包之一。

让我们从一个简单的如何显示一个图的例子开始。

几乎每次想要展示一个情节时，都应该采取一些基本步骤:

1.  准备数据
2.  绘制数据
3.  定制情节
4.  展示剧情

我们将遍历所有这些，但是我们已经完成了用 pandas 清理数据一节中第一步的大部分工作。让我们把这些数据分组，集中在专业类别上。

## 准备绘图数据

让我们利用现有的数据框架,找出哪个专业的平均工资最高，以及它们之间的关系。类别把我们的专业分成不同的类别，比如工程和商业。首先，让我们开始对数据进行分组。

### 利用 groupme()函数

我们将使用`groupby()`方法按照`major`类别进行分组。分组是一个简单的概念，但也可能是一个复杂的操作。它将尝试根据您想要的列来组合 DataFrame 的所有实例。为此，您还需要提供一种聚合技术，如`sum`或`mean`。让我们看几个简单的例子来说明这是如何工作的。

在下图中，我们看到一个简单的数据集，其中包含四只动物的年龄。如果您想简单地查看每种动物的平均年龄的汇总，您可以对**动物**列应用`groupme`方法，并使用汇总的均值 s 类型，如下图所示:

![Figure 5.8 – A Groupby().mean() example on animals
](image/Figure_5.8.jpg)

图 5.8–分组依据()。动物的平均()例子

这可能看起来不是很强大，但是想象一下如果有成百上千行数据。你可以看到这是一个非常有用的工具。

现在，让我们回到我们的大学专业工作来回答这个关于专业的问题。让我们采取同样的想法，在我们的例子中，我们将按`Major_category`分组，而不是按动物分组。一件有趣的事情是，您需要通过调用`groupby()`方法上的适当函数来告诉 pandas 您想要做什么聚合操作。这是因为`groupby()`代码实际上会给你一个`DataFrameGroupBy`对象。现在细节可能对你来说不太重要，但是要知道如果你想做任何有用的事情，你需要在那个对象上调用另一个方法。在这种情况下，我们要的意思是:

```
df_major_cat = df_raw.groupby('Major_category').mean()
```

您将会看到，我们已经将项目汇总到预期的组中，并且每个数字字段都取了平均值:

![Figure 5.9 – Groupby() using mean
](image/Figure_5.9.jpg)

图 5.9–使用平均值的 Groupby()

这是我们要求它做的事情的预期结果，但是如果我们不想对所有事情都使用平均值呢？举个例子，简单地在**就业**栏中取平均就业人数没有太大意义。我们想要总数。要做到这一点，我们可以采取稍微不同的方法，这种方法更有效。

在下面的例子中，我们将使用`agg()`方法，它允许我们为每一列指定一种聚合方法。我们仍将使用工资的平均值和总数。我们还可以给`Median`列起一个更具描述性的`Median_Salary`名称:

```
df_major_cat = df.groupby('Major_category').agg(
    Median_Salary = ('Median','mean'),
    Total = ('Total', 'sum')
)
```

这给了我们一个更有可能的数字，1805865 艺术专业，正如我们在图 5.10 中看到的:

![Figure 5.10 – Groupby() using different aggregation techniques
](image/Figure_5.10.jpg)

图 5.10–使用不同聚合技术的 Groupby()

现在，让我们根据我们的主要标准来做一个 q 快速重新排序。

### 利用 sort()函数

`sort()`让我们能够根据我们认为最重要的东西来订购物品。我们可以很容易地选择我们的列，然后指定我们是希望项目升序(从最小到最大)还是降序(从最大到最小)。Ascending being】是默认参数，所以如果我们想换个方式，我们需要把它变成`False`。

使用`inplace()`命令，不必将该数据帧重新分配给另一个变量:

```
df_major_cat.sort_values(by=' Median_Salary ', ascending=False, inplace=True)
```

既然我们已经像我们想要的那样建立了我们的数据，让我们继续可视化。

## 绘图数据

现在，是绘制图表的时候了！我们把做成条形图吧。 *x* 轴为`major`类， *y* 轴为`median salary`。你会发现下面代码中奇怪的一点是`df_major_cat.index`的使用。这是 major，但是因为它是默认使用的索引，所以我们不能像调用其他普通列一样调用它。使用以下代码开始我们的绘图:

```
plt.bar(df_major_cat.index,df_major_cat[' Median_Salary '])
```

在我们真正展示这个图表之前，我们还会添加一些东西。让我们自定义它，使它更容易阅读。

## 定制剧情

除非易于阅读，否则图表是没有用的。幸运的是，你可以做很多事情来让你自己和你的读者更容易解析。其中一些如下:

*   旋转 *x* 或 *y* 标签:`xticks(rotation=<degrees>)`其中度数应替换为所需的文本角度——90°表示垂直，45°表示更倾斜的外观。
*   添加标签:`plt.ylabels('labelName')`和`plt.xlabels('labelName')`
*   改变颜色:`color='blue'`
*   改变字体大小:`(fontsize=15)`
*   包含一个图例:给你的图`(label="Major")`添加标签，然后`plt.legend()`将能够智能地拾取这个图例并显示它。

我们将用来创建该图的完整代码如下。你可以在图 5.11 中的代码后看到这个图:

```
plt.bar(df_major_cat.index,df_major_cat['Median_Salary'],color='blue', label="Major")
plt.xticks(rotation=80)

plt.xlabel('Major')
plt.ylabel("Median Salary")
plt.title('Barchart - Median Salary vs Major ',fontsize=15)
plt.legend()
plt.show()
```

稍后您将会看到该图的某些部分(如图例)可能并不需要，但它展示了您可以根据需要对此进行扩展。

## 显示情节

这是最容易的第一步，因为所有具有挑战性的工作已经完成。最后一步是调用`show()`函数来显示图形本身:

```
plt.show()
```

这将导致实际的绘图显示为工资在 *y* 轴上，专业在 *x* 轴上，如下所示:

![Figure 5.11 – The median pay by college major
](image/Figure_5.11.jpg)

图 5.11-按大学专业划分的工资中位数

由此看来，工程似乎是一个相当不错的选择，而如果你的主要目标是退休后富有，社会工作可能不是最好的选择。

记住，Jupyter 笔记本会显示这个图，如果你只是在最后一行的单元格中有它，但是在纯代码中，你需要使用`show()`来显式地测试 ell `matplotlib`你正在尝试做什么。这类似于在普通编码环境中而不是在 Jupyter 笔记本中使用 Python 时，需要调用`print()`方法来显示变量。

## 绘制散点图和多项式回归线

所以，现在我们知道了什么是高薪专业，但是人们会被高薪专业吸引吗？我们可以画出这两个数据点，看看学生们是否利用这些数据来做决定。我们需要使用 NumPy，所以导入它。接下来，创建 *x* -和*y*-轴数据点。 *x* 将是学生总数， *y* 将是我们之前清理的工资中位数:

```
import numpy as np
x = df_major_cat['Total']
y = df_major_cat['Median']
```

接下来，我们将创建散点图并标记两个轴以便于识别:

```
plt.scatter(x,y)
plt.xlabel('Number of Students')
plt.ylabel('Median')
```

最后，我们将利用 NumPy 及其`polyfit`和`poly1d`方法。这两种方法都使多项式的处理变得更加容易，其中 c 可用于绘制多项式回归线:

```
poly = np.polyfit(x, y, 1)
p = np.poly1d(poly)
plt.plot(x, poly(x))
```

我们现在可以看到回归线试图遵循模式的图。我们看到了什么？嗯，中等收入和学生数量之间似乎存在轻微的负相关关系:

![Figure 5.12 – Median pay versus the student count
](image/Figure_5.12.jpg)

图 5.12–工资中位数与学生人数

这可能意味着一些事情，但总的来说，我认为可以肯定地说，孩子们没有试图选择那些往往能让他们从大学毕业后受益最大的专业。

# 总结

在本章中，我们看到了如何获取数据集，然后在继续清理之前分析它包含的内容。我们了解了 pandas 如何为我们提供许多强大的工具，使我们能够快速获取 CSV 数据、计算基本统计数据，并使用向前填充和回填等功能清理缺失值等问题。

然后，我们研究了如何使用 Matplotlib 为底层数据带来一些视觉效果，以创建条形图和散点图。该工具是一个重要的组成部分，有助于更好地理解您所拥有的数据，并轻松地将信息和分析传达给其他同事。

这两个工具，pandas 和 Matplotlib，是你会反复使用的。我们现在配备了 Conda、Jupyter 笔记本、NumPy、pandas 和 Matplotlib。仅仅使用这些工具，你就已经能够回答现实世界中的许多问题了，比如，*是否有更多的学生选择高薪专业？*尽管我们可以得到这些简单的答案，但我们还没有完成。

在本书的其余部分，我们将看看如何使用本章描述的过程中产生的干净数据来训练 AI 模型，但有一个重要的组件我们需要首先看看。人工智能模型中的偏差可能是一个巨大的问题，在我们一头扎进训练它们之前理解它是至关重要的。

人工智能中的偏差通常来自用于训练的数据，所以让我们在下一章中使用我们迄今为止学到的所有知识，因为我们了解了更多关于偏差的信息，它看起来像什么，以及我们如何减轻和消除它。