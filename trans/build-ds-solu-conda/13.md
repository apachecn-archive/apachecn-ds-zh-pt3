

# 十、可解释的 AI——使用 LIME 和 SHAP

让我们快速地表演一个场景。你刚做完年度体检，坐在医生的办公室里。医生检查了你的检查结果，然后漫不经心地提到你有 95%的可能在下个月患心脏病。你的下一个问题是什么？“为什么？”你问。“我不确定，”医生回答。

在这种情况下，你对医生的回答会有什么反应？这样够好了吗，或者你想要更多的信息？如果你像大多数人一样，你可能想知道你是否可以做些什么来防止它，或者也许你的血液工作搞混了，你没有什么好害怕的。

随着 AutoML 和其他低代码和无代码选项的出现，人工智能模型的使用在未来几年将会增加，这些选项允许那些技术专业知识较少的人创建模型。你会看到很多模型，但是你需要能够解释事物是如何形成的。偏差一旦进来，就很难消除。Zillow 可能有很多偏差，这些偏差被融入到他们的工作中，但你永远不会知道，因为它是隐藏的(Zillow 没有公布其模型是如何创建的)。除了制定法律要求公司披露一定程度的可解释性之外，客户、同事和其他人需要知道你是如何得出结论的。可解释性是一个好的结果得以持续的原因。

在这一章中，我们将看看一个可能产生真正影响的场景，心脏病发作，看看你如何向潜在的商业伙伴、同事甚至政府机构解释它是如何产生的以及使用了哪些数据。我们将会看到如何移除围绕它的所谓黑箱，以获得关于如何找出它是如何得出结论的具体答案。这可能是至关重要的，因为我们知道哪些因素是我们真正应该关心的，以确保我们正确关注患者和社区的健康。

在这一章中，我们将看看如何用那些被设计成可以理解的算法，以及那些可能更强大但更像是黑盒的算法来实现这一点。

我们还将介绍两种最流行的方法来实现**事后可解释性**，它们是在模型被训练后完成的解释，可以用于许多模型类型。

在本章中，我们将讨论以下主题:

*   理解解释的价值
*   探索可由设计解释的模型
*   用 LIME 解释模型结果
*   与 SHAP 一起解释模型结果

让我们快速地了解一下执行本章所有内容所需的要求。

# 技术要求

为了阅读本章，我们需要一些东西。

首先是安装了 Anaconda 发行版。正如我们所知，这包括 Python、conda、Navigator 以及数据科学中使用的许多其他包。

我们将使用以下软件包:

*   SHAP
*   LIME
*   熊猫
*   NumPy
*   Scikit-learn

您应该创建一个新的 conda 环境来安装所有这些包。建议在开始的时候全部安装，但是你也可以在本章的相关部分安装。

在你有了这些东西之后，我们可以看看为什么我们首先要关心模型的解释。

# 理解诠释的价值

如果用户要理解、适当信任并有效管理这一代人工智能伙伴，可解释的人工智能将是必不可少的。

这是 DARPA 在他们 2016 年的*可解释人工智能(XAI)*([https://bit.ly/3JU9yql](https://bit.ly/3JU9yql))报告中的展望。

不管你是否同意人工智能被用于军事领域，它正在被用于这个领域。在这个领域和许多其他领域，能够知道为什么会取得某些成果是至关重要的。人工智能不仅用于军方扮演的更传统的角色，还用于试图辨别飞机等资源从一个军事基地转移到另一个军事基地时对军事供应链的三阶和四阶影响。

## 知道解释和说明的区别

了解什么样的特性能给你带来你想要的结果是很有价值的，能够解释为什么它很重要甚至更有价值。能够解释某事的一个组成部分是能够解释模型正在做什么，但这不是全部。这一章更侧重于解释方面，因为**可解释的人工智能** ( **XAI** )的领域不仅仅是知道什么特征影响你的模型的结果。有大量的心理学、数学和其他领域远远超出了显而易见的范围。

**解释**就是能够解析信息，从而知道事情发生的原因。您也许能够从浏览器中解读错误信息，了解 Wi-Fi 已关闭。**可解释性**能够解释这意味着什么以及为什么它很重要，比如试着向你不懂电脑的朋友解释为什么无线网络关闭意味着他们不能浏览互联网。

XAI 非常有用的其他例子如下:

*   一个人比犯同样罪行的其他人被判更长的刑期
*   一个求职者被一个人工智能/人工智能系统拒绝，该系统分析了他的简历，认为它不值得
*   对银行财务系统的审计，以确保他们准确、一致和公平地发现欺诈

下图涉及了 XAI 域中的不同方面。你会发现这个领域不仅仅是解释。

![Figure 10.1 – DARPA scope of XAI
](image/Figure_10.1.jpg)

图 10.1-XAI DARPA 范围

你应该从这张图表中得到的启示是，能够解释人类正在发生的事情是一个复杂的问题，没有一个单一的问题空间。

还有很多其他的原因让你想合并 XAI。一些领域如下:

*   合法的
*   道德的
*   商业
*   模型性能

我们将进一步探讨这些，但首先，让我们从法律可能扮演的角色开始。

## 寻找可解释性的法律理由

在未来的某个时候，我们预计展示模型是如何构建的将不仅仅是一件好事，而且将是部署到业务中的一项要求。随着消费者要求知道他们的数据在哪里的压力越来越大，将有一种趋势是知道这些数据是如何被使用的。在撰写本文时，一些国家已经提出了法律，如果他们不允许探测他们的算法，可能会导致高管入狱。

GitHub 已经创建了一个名为 Co-Pilot 的自动代码生成工具。如果作为训练数据输入其中的一些代码有许可证，规定你必须使任何公开使用的东西都可用，这意味着什么？软件的许可是否包括它何时被吸收到人工智能算法中？现在还没有一个明确的答案，这是一个模糊的法律领域，但很可能是一个单一的许可证法律裁决导致这是一个巨大的问题。

有一个所谓的代理规则，根据 GDPR 法律，受某些法律保护的用于培训的数据可能处于危险之中。这是你需要注意的另一个方面。

在我们上一章的葡萄酒场景中，也许我们需要确保酒精含量非常高不是一个明确的属性，我们试图通过一个假设的法律在一个国家增加。了解这些数据可能是能否在该国销售我们的优质葡萄酒的关键。

## 审视可解释性的道德原因

还有道德义务需要考虑。我们在 [*第六章*](B16589_06_ePub.xhtml#_idTextAnchor146) ，*克服 AI/ML 中的偏差*中深入讨论了偏差，这在这里也起了作用。尽管我们可能试图避免偏差，我们仍然应该能够解释一个决定的推理来自哪里。当你判了更长的刑期，拒绝了某人的房屋贷款，或者拒绝了一份工作申请，你就有了一个新的责任去发现除了黑箱方法之外的原因。

## 寻找可解释性的商业原因

我们将触及的下一个原因是商业案例。虽然有时候结果本身就说明了问题，但为什么一家公司应该相信你的模式，答案不应该仅仅是“嗯，因为”确保你能交付想要的价值，很多时候是一种信任，而这种信任是建立在拥有正确答案的基础上的。

就葡萄酒而言，专注于任何一个方面都是非常昂贵的，所以我们需要把它做好。如果我们做出错误的决定，我们可能会失业，失去我们家族世代相传的葡萄园。

在这一章的大部分时间里，我们将使用一个医学例子，因为其结果可能比酿酒场景更有意义。

还有一些实际的原因，意味着你将创建一个更好的模型。我们现在就开始吧。

## 查看模型可解释性的改进原因

有时，有简单的方法来创建训练更好的模型，使用你从一个可解释的模型中找到的信息，从能够暴露你的数据中的问题开始。

### 了解您的数据中是否存在问题

当你理解了是什么特征导致了结果是什么样的，你将能够通过回到数据收集点并可能获得更多该类型的数据，确保数据的准确性，甚至尝试不同的插补方法，更有效地迭代解决问题。

### 了解较差的模型性能

你可以了解为什么模型表现不佳。举例来说，这可能会揭示一个您认为已经一次性编码但却被视为序数的特征。或者，您可能会发现，您认为正在进行的标准化并没有按照您预期的方式真正压缩数据。

### 识别特征工程的机会

您可能会发现某个事件的日期与您期望的结果之间有很高的相关性，因此您可能希望将原始数据分成几天或几小时，以反馈到模型中进行再培训。

### 了解未来状态

使模型更加透明和易于理解，可以更容易理解当引入模糊数据或新数据点时，模型可能会如何表现。了解一种新疫苗在未来如何影响年轻患者可能会非常有用。

### 检查准确性和可解释性之间的权衡

我们在 [*第七章*](B16589_07_ePub.xhtml#_idTextAnchor169)*选择最佳 AI 算法*中看到，某些算法在可解释性的度量上有好有坏。这里有一个简单的表格供参考:

![Figure 10.2 – Ratings for various algorithms
](image/Figure_10.2.jpg)

图 10.2–各种算法的评级

一些具有更高解释能力的算法在某种程度上更准确。有许多模型证明了这一点，但就像数据科学中的几乎所有东西一样，也有一些警告。注意精度栏上的星号。虽然这是一个客观的说法，像神经网络这样的东西比决策树更难解释，但不明确的是它总是会给你带来更好的结果。对于拥有较小数据集或者与**自然语言处理** ( **NLP** )或计算机视觉无关的问题，使用更多可解释的模型可能会给你更好或非常接近的结果，同时能够用更少的工作来解释。

下图显示了不同算法的相对能力，基于它们的解释难易程度和准确性。请注意，这并不是适用于所有数据集和这些算法子家族的普遍真理:

![Figure 10.3 – Graph of algorithm accuracy versus interpretability
](image/Figure_10.3.jpg)

图 10.3–算法准确性与可解释性的关系图

你可以看到黑盒算法，如随机森林，在左上方，表明它们通常更准确，但可解释性较差。

由于黑盒算法并不总是更好，我们将从开箱即可解释的模型开始。让我们在下一部分来看看。

# 理解可通过设计解释的模型

在 [*第七章*](B16589_07_ePub.xhtml#_idTextAnchor169) *选择最佳 AI 算法*中，我们提到了更复杂的算法类型，如神经网络，尽管它们提供的好处很少，但仍经常被使用。遵循**吻**原则(**保持简单，笨蛋**)，你应该倾向于尽可能保持简单。不仅其他模型可能更简单、更容易解释，而且它们也提供了一些奇妙的结果。简单不代表低人一等。

我们已经在这本书里看到了许多模型，它们能够理解这些结果是如何在没有任何特殊技术的情况下实现的。我们现在将讨论的算法如下:

*   决策树
*   线性/逻辑回归
*   KNN

正如本章前面提到的，我们将使用一个医学例子。这个数据集是一个二元分类器，用于判断某人是否有患心脏病的风险。对于本章，我们将把数据准备和其他步骤排除在外，这样我们就可以专注于本章的重点——可解释性。不过，我们将提供一个快速数据字典，因为它是可解释等式的可解释部分的一个非常重要的部分。

我们将在这里介绍常见的模型创建步骤，但是如果您只想查看 LIME 结果，可以随意跳过它们。

### 设置我们的问题

以下是我们将在模型中使用的特性的描述。您不需要记住它们，但可以浏览它们，并在需要时用作参考:

*   **年龄**:患者的年龄，以岁为单位。
*   **性别**:患者的性别(男:男，女)。
*   **胸部彩绘型**:
    *   -TA:典型心绞痛
    *   -ATA:非典型心绞痛
    *   -午睡:非心绞痛疼痛
    *   -ASY:无症状
*   **静息血压**:静息血压(mmHg)。
*   **胆固醇**:血清胆固醇(mm/dl)。
*   **FastingBS** :空腹血糖:大于 120 mg/dl 为 1，否则为 0。
*   **静息心电图**:静息心电图结果:
    *   正常:正常
    *   ST:有 ST-T 波异常
*   **LVH** :显示可能或明确的左心室肥厚。
*   **MaxHR** :最大心率。
*   心绞痛是一种由于流向心脏的血液减少而引起的胸痛。这是如果这种情况发生在人们健身的时候。
*   **Oldpeak** :相对于休息的运动。
*   **ST_Slope** :最大运动 ST 段的斜率(上:上坡，平:平，下:下坡)。
*   **心脏病**:如果患者有心脏病。1:心脏病，0:正常。这是目标特征。

现在，我们已经有了一个要检查的参考，让我们设置数据:

1.  首先，导入所有需要的库，这样我们就可以专注于后面的实际工作:

    ```
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import OneHotEncoder
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.model_selection import cross_val_score
    from sklearn.model_selection import StratifiedKFold
    from sklearn.metrics import f1_score
    import shap

    from sklearn.linear_model import LogisticRegression
    from sklearn.linear_model import LinearRegression
    from sklearn.ensemble import RandomForestClassifier
    ```

2.  接下来，我们导入我们的数据集:

    ```
    df =  pd.read_csv('heart.csv')
    ```

3.  我们需要一次性编码一些分类值。用下面的代码做这件事:

    ```
    categorical_columns = ['Sex','ChestPainType','RestingECG','ExerciseAngina','ST_Slope']
    prefix = ['Sex_','ChestPainType_','RestingECG_','ExerciseAngina_','ST_Slope_']
    one_hot_df = pd.get_dummies(df, prefix =prefix, columns = categorical_columns, drop_first=True)
    ```

4.  然后我们需要分离自变量和因变量，或者输入特征和目标特征。我们只需将目标特征保存到`y`，然后将减去目标特征的原始特征保存到`X` :

    ```
    y = one_hot_df['HeartDisease']
    X = one_hot_df.drop(['HeartDisease'],axis=1)
    ```

5.  然后，我们将把这个数据分成训练集和测试集:

    ```
    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=33)
    ```

6.  为了确保我们得到一个合适的模型，我们将通过缩放来压缩数据:

    ```
    min_max_scaler = MinMaxScaler()
    minmax_scaled_X = min_max_scaler.fit_transform(X_train)

    df_minmax_scaled_X = pd.DataFrame(minmax_scaled_X,columns=X.columns)
    df_minmax_scaled_X.head()
    ```

此时，我们的数据已被扩展并准备就绪。我们将首先使用决策树中的数据。

## 解读决策树

在我们使用上一节的医疗数据之前，让我们看看决策树是如何工作的，以及设计函数如何解释算法。我们将用一个例子来说明母亲是如何决定让孩子熬夜还是睡觉的。下图显示了一个简单的二进制决策树，该决策树映射出某些特征，以获得最终的二进制分类器/决定，即他们是应该睡得更久还是应该上床睡觉:

![Figure 10.4 – Binary decision tree
](image/Figure_10.4.jpg)

图 10.4–二元决策树

在现实世界中，你可能找不到父母会在冰箱上清楚地说明这些事情，但你可以看到这会让他们的决定更容易理解。它们更容易解释吗？也许吧，但是你会注意到第一个节点是基于他们的孩子是否是女性。行动偏差！他们可能无法解释。这带来了另一点，你可以解释假设的模型，但也许你不能解释它。虽然这些父母可能不会制造头条新闻，但你可以看到这可能会在一个潜在的更严重的情况下带来偏差的核心问题。

随机森林可解释性与决策树

你可能想知道为什么随机森林不容易解释，如果它们只是由一堆决策树组成的话。这是一个很好的问题，我们将在稍后的*用 LIME 解释模型结果*中触及这个问题。

这也是一个**分类和回归树** ( **推车**)的例子。通过使用一些简单的工具，我们可以得到一个真实模型的视觉效果，就像之前的模型一样，我们现在就来探讨一下。

## 绘制决策树

为了绘制一棵树，你可以使用 **Graphviz** 作为一种方法，但是新的`plot_tree`要容易得多。首先，你不必转换文件类型。

现在让我们制作一个简单的决策树模型，然后用图表表示出来:

1.  首先，导入所需的库:

    ```
    from sklearn.tree import plot_tree, export_text
    ```

2.  接下来，创建您的模型。我们将保持深度很小:

    ```
    dt_clf = tree.DecisionTreeClassifier(random_state=33, max_depth = 2)
    dt_clf = dt_clf.fit(df_minmax_scaled_X,y_train)
    ```

3.  最后，我们绘制结果图。设置`dots-per-inch(dpi)=300`和`Rounded=True`是简单的 Comet 选择:

    ```
    plt.figure(dpi=300)
    plot_tree(dt_clf, class_names = True, feature_names=X.columns.values, filled = True, proportion = True, rounded = True)
    ```

该图应该如下所示:

![Figure 10.5 – Decision tree graph
](image/Figure_10.5.jpg)

图 10.5-决策树图

在下一节中，我们将从设计上可解释的算法转移到所谓的黑盒模型和技术上的工作，使它们更具可解释性。

# 用 LIME 解释模型的结果

现在我们开始转向黑盒模型。它们变得越来越普遍，因为它们在该领域的热门领域中表现出了功效，如 NLP、视觉问题以及大量数据输入产生惊人结果的各种其他领域。这些领域不会有任何进展，因此我们需要找到一种方法，在事后使用事后可解释性来解释这些模型。

我们要看的第一种方法是**局部可解释的模型不可知解释** ( **时间**)，它假设如果你放大甚至一个复杂的非线性关系，你会在局部水平找到一个线性关系。然后，它将尝试通过创建类似于我们所关心的记录的合成记录来学习这种局部线性关系。通过创建这些输入略有变化的点/记录，它可以根据模型的输出计算出每个要素的影响。顾名思义，它的模型不可知能力使它成为一个非常通用的方法，因为它可以用于任何类型的模型。它使用这种方法创建一个代理模型，用来近似我们关心的黑盒模型。模型是用来解释模型的。

作为概念的例子，让我们以洗衣服的场景为例。在我们的场景中，你不知道洗衣机需要什么样的设置，你想知道在特定情况下什么会影响你洗涤的时间长度和你使用的肥皂量。你不知道洗衣机里发生了什么，一旦你关上它，它就是一个黑匣子。你看到的都是那些衣服，肥皂，一个时间输入进去，然后脏的或者干净的衣服出来。一种方法是对每个功能(在这种情况下，肥皂量和洗涤时间)进行微调，并检查结果。这也被称为**干扰**特征输入，因为你正在调整初始点。

让我们看一张可能更有意义的图片。为了简单起见，我们假设这是一个二元分类问题，衣服要么是干净的(用蓝色表示)，要么是脏的(较暗的灰色区域)。样品的重量由 x 或圆圈的大小表示:

![Figure 10.6 – LIME global versus local. Local results assume linear relationships
](image/Figure_10.6.jpg)

图 10.6-LIME 全球与本地对比。局部结果假设线性关系

查看*图 10.6* 中的示例，在左侧我们可以看到全局视图，然后我们也可以放大查看右侧显示的单个记录。莱姆的假设是，我们可以放大并发现线性关系，即使你可能在全局空间中看到非线性模式。

在我们假设的洗衣机示例中，我们正在进行真实世界的测试，但实际的 LIME 算法不会这样做。它围绕从现实世界中捕获的数据点创建合成数据点。我们也可以在我们的例子中这样做，但是实际记录被赋予更多价值的原因很简单，如果我们正在创建合成数据，那么它是有用的，但是你不能假设它会像我们知道的来自真实世界的记录一样实用。因此，我们通过给它一个比真实记录小的乘数来对冲我们的赌注。

让我们看一个例子，使用一个随机森林和我们之前的心脏病数据。

## 创建一个 LIME 示例

对于这个例子，我们将专注于获得输出的解释，而不是训练完美的模型:

1.  训练随机森林:

    ```
    rf = RandomForestClassifier(random_state=33)
    rf.fit(df_minmax_scaled_X,y_train)
    ```

2.  使用 LIME 解释器。

我们现在将开始使用来自 LIME 的第一个函数。我们希望将它用于表格数据，因此使用了`LimeTabularExplainer()`函数。我们传入我们想要查看的值:

```
explainer = lime.lime_tabular.LimeTabularExplainer(X_test.values,feature_names = X_train.columns,class_names=['Heart Disease','No Heart Disease'])
```

1.  选择要解释的记录。我们用这个，一个 52 岁的男性:

    ```
    X_test.loc[[180]]
    ```

您将看到这一行的输出:

![Figure 10.7 – Output of the preceding code
](image/Figure_10.7.jpg)

图 10.7–前面代码的输出

我们将保存实际的值以备后用:

```
target_exp = X_test.loc[[118]].values[0]
```

1.  接下来，我们从*步骤 2* 中取出解释器对象，并挑选出我们想要更多信息的记录或实例。我们传入我们的目标并使用`predict_proba`函数，它给出了我们预测的类的概率:

    ```
    exp = explainer.explain_instance(target_exp, rf.predict_proba, num_features=20)
    ```

2.  最后，我们将用下面的代码在笔记本中显示结果:

    ```
    exp.show_in_notebook()
    ```

您将看到类似如下的输出:

![Figure 10.8 – LIME interpretation
](image/Figure_10.8.jpg)

图 10.8–LIME 解释

在上图的左侧，您将看到每个职业的机会百分比。在这种情况下，有 66%的风险患心脏病。患者会想知道为什么，而你将能够告诉他们 **ST_Slope** 是平的(并且恰当地说 **ST_Slope_UP** 不是真的)，这些都是重要的因素。另外， **Oldpeak** 高于 **1.50** 也是一个因素。

尽管这不是一个万无一失的方法；有一些问题，我们将在接下来讨论。

## 权衡 LIME 的弊端

并非所有关于 LIME 的东西都是完美的，因为没有一种方法能保证我们需要的结果。在下面的列表中，看看我们在使用它时应该注意的一些事情:

*   它有助于解释，但不是完整的解释。正如我们在*图 10.8* 中看到的，年龄似乎不是影响结果的一个因素。这些信息能帮助你向病人解释结果吗？几乎没有。这会引发更多的问题。这是问题设置的错误，算法的错误，还是数据的错误？不深入挖掘很难知道。
*   创建的数据点可能无效。考虑到合成点被创建和评估，所创建的人工点可能不能很好地表示实际数据的分布。你可能有一个负的洗涤时间，或者一个 150 岁的病人，正如我们所知，这是不现实的。
*   可能分布不均匀或不公平。与上一点类似，合成值的随机性不会创建一个在输入要素间均匀分布值的集合。这可能是法律、合规或审计方面的问题。
*   The bias can be hidden by malicious intent.

    参考

    在康奈尔大学的一篇论文中，*愚弄莱姆和 SHAP:对事后解释方法的对抗性攻击*表明，你可以采用一个包含有意偏差的模型，并使用意图技术来蒙蔽莱姆对潜在偏差的认识。论文的完整代码和链接可以在这里找到:【https://bit.ly/3MdWPQH[。](https://bit.ly/3MdWPQH)

*   它关注的是单个的局部结果，假设该区域中存在线性关系，但这可能不是真的。它不会给你任何关于全球关系或其重要性的见解。

现在你应该已经很好地理解了什么是 LIME，以及它如何使用合成特性的力量来揭示黑盒模型中可能发生的事情。现在让我们来看一种方法，它采用一些相同的想法，并以稍微不同的方式应用它们，从而赋予你全球以及本地的重要性。

# 用 SHAP 解释模型的结果

LIME 问世后不久，又推出了另一个工具来帮助解释 AI 模型，**SHapley Additive explaintions**(**SHAP**)。SHAP 的主要功能是，如果您对不同的输入特征进行排列，您可以确定每个特征对结果的重要性。这听起来像是 LIME，那是因为它从中获得了灵感,同时也使用了一些新概念。然而，有一些关键的区别，我们将会解释。

## 避免与 Shapley 值混淆

这种方法基于 Shapley 值，但不是一回事。SHAP 是以沙普利博弈论为基础的迭代。SHAP 有多种形式，如 kernel 和 TreeSHAP。此外，还有全球性的解释，SHAP 允许，这又是建立在什么沙普利价值观允许。

让我们看一个例子来更清楚地说明 SHAP 是如何使用的。

假设你是一个二对二篮球联盟的球员。这只是在一个普通的篮球环境中玩的同样的游戏，但是为了简单起见，每队少了三名球员。你试图根据每个球员对球队得分能力的贡献(而不仅仅是个人得分)来找出他们应该得到多少报酬。所以，你只需记录你队中三名球员的不同组合得了多少分。

如果您采用两个玩家的所有组合，您将有三种不同的方式来创建两人团队，如下图中的**组合**部分所示。输出代表总点数，越高越好。

![Figure 10.9 – Player combinations and the total output
](image/Figure_10.9.jpg)

图 10.9-玩家组合和总输出

从这张图表中我们能收集到什么？你可能注意到的一件事是，**玩家 2** 似乎对输出有重大影响，因为他们不参与的一个组合的分数比其他两个低得多。如果我们想要更多的数据点，我们可以单独播放它们。SHAP 技术还有更多的东西，但这是核心思想。

您也可以将此视为一种获得线性回归系数所代表的相同信息的方法。

有几种不同的方法来使用 SHAP，如 KernelSHAP 和 TreeSHAPE。我们将在下一个例子中使用 TreeSHAPE。

根据 Christoph Molnar 在他的精彩著作*可解释机器学习*中的说法，SHAP 是实用的，因为它满足四个要点:

*   **可加性**:对于一个游戏，有许多游戏的组合支付，你可以计算所有实例的平均值，这将是整个集合的 Shapley 值。对于随机森林，这意味着您可以获得所有单个决策树的平均值，这将获得整个随机森林的该特性的值。
*   **Dummy** :任何对预测值没有影响的特征，其 SHAP 值应该为 0，表示它不会影响任何东西。在我们的篮球例子中，如果你有一套仓鼠服装作为球员，它可能对球队的总得分没有影响。
*   **效率**:特性贡献必须与我们关心的目标特性采用相同的单位。基本上，如果你得到一个看似无意义的 *+1.32 度*关于一个特征如何影响一栋房子的美元价格，相对于它是+232，000 美元的形式，它几乎是无用的。
*   **对称性**:如果两个特征对所有组合的贡献完全相等，那么它们的贡献必须相同。在我们的例子中，如果两个玩家做出相同的贡献。

所有这些属性结合起来使这种方法非常有用。既然我们对这种技术有了更多的了解，让我们来看一个例子。

## 创建 SHAP 范例

让我们深入一些代码，看看这看起来会是什么样子。我们将使用本章前面训练的随机森林。如果您尚未创建此模型，请参考上一节:

1.  首先，我们想要获取数据的子集，以便减少所需的训练时间。我们使用另一种机器学习算法`kmeans`，它将创建 k 个样本。我们将使用 50 个样本。在您的 Jupyter 笔记本中键入以下代码，创建该数据子集:

    ```
    X_train_summary = shap.kmeans(X_train, 50)
    ```

2.  接下来，我们将创建主解释器对象，告诉它结果的概率。随机森林模型的`proba`属性保存了结果的概率。我们还会传入我们在上一步中创建的训练数据的摘要:

    ```
    exp = shap.KernelExplainer(rf.predict_proba, X_train_summary)
    ```

3.  接下来，我们将使用这个解释器来调用它的`shap_values`方法。我们传入一个我们想要解释的数据点。在这种情况下，它位于索引位置 180:

    ```
    shap_values = exp.shap_values(X_test.iloc[[180]])
    ```

您将看到类似这样的输出，显示它已经完成了计算:

![Figure 10.10 – Progress bar for SHAP values
](image/Figure_10.10.jpg)

图 10.10-SHAP 值的进度条

1.  我们需要初始化 JavaScript 以允许可视化首先发生，您可以简单地在单元格

    ```
    shap.initjs()
    ```

    中使用`initjs()`方法来完成
2.  最后，我们将把它形象化。我们传入与之前创建的`shap_values`方法进行比较的参考值:

    ```
    shap.force_plot(exp.expected_value[0], shap_values[0], X_test.iloc[[847]])
    ```

我们完事了。现在剩下的就是查看结果，结果应该显示在您的单元格中。

## 看着 SHAP 的成绩

您应该会看到如下所示的输出:

![Figure 10.11 – SHAP result
](image/Figure_10.11.jpg)

图 10.11-SHAP 结果

目标特征的基础值是 **.44** ，在任意一侧，您都会发现这个特定记录的不同特征是如何影响结果的。例如， **ST_Slope_UP** 为 **1** (或真)为这个特定的结果增加了很多。

SHAP 是一个相当不可思议的图书馆，我们只是触及了其中的一些好处。你可以在 GitHub 页面上找到更多的例子和功能:[https://bit.ly/37q4Kvq](https://bit.ly/37q4Kvq)。

像 LIME 或任何其他方法一样，使用这种技术也有一些缺点。有些你会从我们的 LIME 列表中认出来。

## 权衡 SHAP 的弊端

因为它需要对所有可能的特征进行排列，所以计算量会很大。在*图 10.11* 中，功能的数量相对较少，但是随着功能的增加，组合的数量会呈指数增长。有 2k 种可能的特征组合。这意味着有了三个特征(就像我们的篮球例子)，如果我们尝试了每一个组合，总共有八个组合，包括不玩任何一个。如果我们增加到 10 个玩家，那就是 1024 种排列；而有了 20，就是 1073741824 种组合！这是假设一次可以玩的人数没有限制。

SHAP 假设随机噪声的影响将被忽略。当 SHAP 想要忽略一个特性时，它不能简单地删除它，因为模型总是需要一些东西。SHAP 将会看到当要素被移除时会发生什么，并通过替换随机垃圾数据或给定数据集中的数据来创建这种噪声。这是可行的，因为模型将确定它对结果的重要性为零，因此被忽略。

既然我们已经看到了 SHAP 可能存在的问题，那么让我们缩小范围，看看我们应该如何看待 XAI 的总体情况。

你也可能因为错误的原因做出正确的决定。为什么这很有价值的一个例子是，有一个视觉分类器的实验，它被训练来区分狗(哈士奇)和狼。在训练部分，有 100%的准确率。但是在现实世界的例子中，它很难做到。为什么？因为所有狼的训练例子都有雪作为背景。当模型被要求对背景中有雪的哈士奇进行分类时，假设中的这一缺陷就暴露出来了。假设背景中的雪=狼。如果数据科学家知道这一点，他们就能够采取措施来纠正这个问题，比如在雪地里引入更多的哈士奇图片，以及在温暖天气下的狼。

## 通过解释和 XAI 的缺点进行思考

在 1997 年的电影《Gattaca》中，当一个孩子出生时，父母被告知患心脏病的可能性、攻击性倾向以及其他可能不为人知的特征。如果我们能解释导致这些预测的模型，我们能明智地使用这些知识吗？这就是应该如何使用 XAI 吗？这是一个广泛的问题，我们必须问自己，但正因为我们可以使用我们已经谈到的工具来更好地理解模型，我们如何使用这些信息可能是一个更难的问题。AI/ML 还有其他的部分和方面，旅程不会就此停止。

我们介绍的技术并不是保证 AI/ML 所有问题都得到解决的万无一失的方法。这无助于确保前面收集和选择数据的步骤是以公正的方式进行的，或确保正确选择用于缺失数据的插补方法。这是假设模型和数据是无偏的过程中的一步。我们只是在这个过程中的一步，所以我们应该始终全面地看待整个过程，以确保我们朝着正确的方向前进。

这也不是故事的结尾。看看哈士奇/狼的例子，一个天真的数据科学家可能会看到斯诺的解释，并推断出这个模型有多聪明。他们可能认为狼总是在雪地里，所以没有做任何事情来检查这一点或确认已经存在多年的模型应该被审计。

另一个问题是没有正式的定义。你可以进入所谓的 **p-hacking** 。这是基于寻找可以被认为是静态显著的皮尔逊相关性或数据关系，即使事实并非如此。

你也可能因为错误的原因做出正确的决定。在一个为什么原因有价值的例子中，有一个视觉分类器的实验，它被训练来区分一种狗(哈士奇)和一只狼。

也可以有偏差烘烤。人们想要他们能理解的清晰的解释。诚然，哈士奇/狼的例子已经在许多场景中使用，正是因为这个错误很容易找到，但正如本书中的许多事情一样，当你进入真实的世界时，事情会变得复杂得多。在一个假设的例子中，癌症诊断可能表明这是因为这个人没有每三年进行一次体检。这对你有意义吗？也许不是。是否应该因此而忽视它？答案可能是肯定的，也可能是否定的。模型如何得出这个结论的可见性可能会让医生有理由怀疑模型训练非常差，一个完全健康的 30 岁的人不应该担心。

尽管存在所有这些问题，但有必要指出，在人工智能出现之前的日子里(甚至是现在)，围绕如何应用日常方面的逻辑存在很大的不确定性。你能告诉我信用评分是如何得出的吗？或者 20 年前医生是如何决定你是否需要治疗对线问题的？AI/ML 不应该获得免费通行证，但世界上有许多我们一直在处理的事情都是黑箱，没有明确的方法来获得更好的解释。

# 总结

通过这一章，你深入了解了可解释性和可解释性是如何融入一个健康的模型和健壮的数据科学工作流程的。我们看到了它们的重要性，不仅仅是因为创造了一个伟大的模型，还有商业、道德和法律方面的原因。

我们回顾了前几章的算法，如决策树，发现它们不仅在准确性上有很大优势，而且在被创建它们的数据科学家解释的能力上也有很大优势。

后来，我们看到，尽管有人建议应该首先考虑更简单的模型，但黑盒模型是非常常见的，因此我们仍然应该能够解释随机森林之类的模型。考虑到这一点，您看到了 LIME 如何成为一个很好的工具，通过假设在放大全局空间时可以找到线性关系，将黑盒变成其自身的更透明版本。

最后，我们检查了 SHAP，它建立在沙普利值和博弈论的基础上，通过创建不同的特征群组来计算对独立变量的贡献。

虽然对 XAI 来说没有灵丹妙药，但能够解释一个模型是向能够解释模型所做的预测迈出的一大步。当涉及到医疗诊断、监狱服刑或财务审计等领域时，能够给出更好的解释不仅仅是额外的一步，如果我们作为数据科学家能够以公平透明的方式证明将模型保持在现代生活的中心是至关重要的。

在下一章中，我们将涵盖书中的最后一组技能，即如何使用 Scikit-learn 的管道轻松迭代您的模型及其超参数。此外，我们将了解如何对所有模型进行版本化，以便保存并与其他人共享。