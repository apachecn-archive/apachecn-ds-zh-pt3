

# 7

# 特征工程

> 人们带着他们的数据作为数据科学家来找我。然后我的工作就变成了一个数据处理员，一个心理咨询师。
> 
> –匿名

[*第六章*](Chapter_6.xhtml#_idTextAnchor008)*值插补*看着填补缺失值。在 [*第 5 章*](Chapter_5.xhtml#_idTextAnchor007) 、*数据质量*中，我们谈到了标准化和缩放，它们调整值以人工适应某些数字或分类模式。这两个较早的主题都接近本章的主题，但这里我们更直接地关注基于原始数据集的*合成要素*的创建。插补是对可能缺失的值进行合理猜测的问题，而特征工程是改变数据的*表示形式*，但是是以确定性的方式，并且通常是信息保留的方式(例如，可逆的)。综合特征的一个简单例子是前一章中身体质量指数(体重指数)的构建。

我们有很多方法可以转换数据。在一个简单的例子中，我们可以将日期时间的数字或字符串表示形式转换为本地表示形式，这使得许多操作更加容易。对于字符串，我们可能会产生规范的表示和/或将它们视为类别(也称为**因子**)。此外，一个字符串通常可以包含几条有意义但独立的信息，这些信息作为单独的变量处理会更有用。对于数值，有时将它们转换成不同的范围，从而转换成顺序值，有时有助于揭示被过多精度所混淆的模式。当然，量子化不在可逆变换之列；但是良好的实践继续推荐版本化数据和脚本化转换以实现可重复性。

虽然单个要素表示中的数据类型变化很重要，但我们有时也希望使用[参数空间](Glossary.xhtml#_idTextAnchor095)和数据集的维度来执行一些更系统的操作。一键编码是一种简单的转换，将单个分类特征转换为多个数值字段；对于特定的统计或建模技术，这通常是需要的。多项式特征是合成特征，它以某种方式组合了多个原始特征，通常可以揭示在单变量特征中看不到的有意义的相互作用。

在分解中执行完全系统的转换。**主成分分析** ( **PCA** )等技术以信息保持的方式变换整个参数空间。就其本身而言，这种转换不会增加或损失任何信息，但这通常与*维度缩减*相结合，其中大部分信息只能从这些转换维度的子集收集。根据您的目的，这样的转换可能会使模型更容易处理和/或质量更好。

***

在我们开始本章的章节之前，让我们运行我们的标准设置代码。

```
from src.setup import *

%load_ext rpy2.ipython

%%R 

library(tidyverse) 
```

本章使用 scikit 中的功能——比其他章节了解得更全面。我在这里使用 scikit-learn 演示的所有内容当然也可以通过其他方式完成。在为机器学习模型准备数据的过程中，scikit-learn 恰好内置了大量用于特征工程的工具。scikit-learn 提供的 API 是一致的和设计良好的，所以总的来说，它当然值得称赞，但本章的目标是解释底层概念。

# 日期/时间字段

> 时间是孩子们玩得很漂亮的游戏。
> 
> 赫拉克利特

**概念**:

*   组合时间戳组件
*   数据框中的日期/时间操作
*   时间增量
*   重复的时间戳(选择与平均)
*   重采样和分组
*   缺失时间戳处的插值

作为一个日期编码的例子，它并不像我们希望的那样立即有用，让我们回到本书其他地方使用的温度读数。对于其他地方的不同目的，我们简单地提供了一个`read_glarp()`函数，该函数执行少量的数据清理。对于这一部分，我们将从原始数据中进行一些类似的操作。

温度数据由几个文件组成，每个文件包含不同自动温度计的测量值，该温度计(通常)每三分钟读取一次读数。查看其中一个，我们看到内容是这样排列的:

```
%%bash

zcat data/glarp/outside.gz | head -5 
```

```
2003 07 25 16 04 27.500000

2003 07 25 16 07 27.300000

2003 07 25 16 10 27.300000

2003 07 25 16 13 27.400000

2003 07 25 16 16 27.800000 
```

这些文件没有标题，但是有几列对应于您可以直观地解析为 2003 年和 2004 年的日期。对于这种特定的格式，我们可以以空格分隔或固定宽度的方式读入文件。这里我们用熊猫作为空格分隔的文件来读。

```
temps = pd.read_csv('data/glarp/outside.gz', 

                    sep=' ', header=None, 

                    names=['year', 'month', 'day', 

                           'hour', 'minute', 'degrees'])

temps.head(5) 
```

```
 Year   month   day   hour   minute   degrees

0    2003       7    25     16        4      27.5

1    2003       7    25     16        7      27.3

2    2003       7    25     16       10      27.3

3    2003       7    25     16       13      27.4

4    2003       7    25     16       16      27.8 
```

这个外部温度数据集中的特殊问题是次要的。然而，仍然有足够多的时间序列允许我们使用许多最常用的技术，这些技术是你在处理时间序列数据时需要的。本节中的示例都使用了 Pandas，但是其他数据框库(无论何种语言)通常都具有类似的功能。

## 创建日期时间

我们想要的所有信息都可以在数据框中找到，但是让我们把它变得更有用。许多 Pandas 操作使用 DateTime 索引特别方便，所以我们将它作为索引。

```
ts_fields = ['year', 'month', 'day', 'hour', 'minute']

temps.index = pd.to_datetime(temps[ts_fields])

temps.drop(columns=ts_fields, inplace=True)

temps 
```

```
 degrees

2003-07-25 16:04:00         27.5

2003-07-25 16:07:00         27.3

2003-07-25 16:10:00         27.3

2003-07-25 16:13:00         27.4

                ...          ...

2004-07-16 15:19:00         16.9

2004-07-16 15:22:00         16.8

2004-07-16 15:25:00         16.8

2004-07-16 15:28:00         16.4

169513 rows × 1 columns 
```

尽管从表面上看，这些数据似乎是按时间序列顺序排列的，但是有许多行，而且可能并不总是这样。一般来说，我们希望按时间顺序保存数据，以便进行各种操作，包括制作可能代表数据的图表。我们可以简单地将时间序列(本例中的索引)排序为一个[幂等](Glossary.xhtml#_idTextAnchor057)运算，但是在我们这样做之前，让我们检查一下这个目标是否已经达到。

```
temps.index.is_monotonic_increasing 
```

```
False 
```

我们可以通过观察连续行之间的步长差异来探究这一点——在熊猫中用`Timedelta`来表示。

```
increments = temps.index.to_series().diff()

increments[increments < pd.Timedelta(minutes=0)] 
```

```
2003-10-26 01:01:00   -1 days +23:03:00

dtype: timedelta64[ns] 
```

该指数不是单调的，有一个向后跳转(它比当年实际夏令时调整早一个小时，但据推测仍然相关)。我们应该考虑这样一个事实，即按某些字段值排序的数据不一定以实际的磁盘格式来表示。许多格式，如 SQL 数据库，执行各种优化，可以忽略排序假设，除非强加。在我们进一步研究之前，让我们按照数据的`DateTimeIndex`对数据进行显式排序。

```
temps.sort_index(inplace=True)

temps.index.is_monotonic_increasing 
```

```
True 
```

## 强加规律性

正如您在 [*第 5 章*](Chapter_5.xhtml#_idTextAnchor007) 、*数据质量*中的一个练习中可能已经确定的那样，在一般的“每三分钟”模式中，我们预期会出现时间戳缺失的情况。让我们首先核实这种差距确实存在，然后加以补救，以产生一个更有规律的时间序列。我们在这里所做的显然与价值估算有关；它的不同之处在于“发明”了整行，而不仅仅是单个数据点。回想一下，在不到一年的时间里，三分钟的增量加起来大约是 170，000 个预期观察值。

```
increments = temps.index.to_series().diff()

gaps = increments[increments > pd.Timedelta(minutes=3)]

gaps 
```

```
2003-07-26 19:28:00   0 days 00:06:00

2003-07-27 09:10:00   0 days 00:06:00

2003-07-29 08:28:00   0 days 00:06:00

2003-07-29 11:43:00   0 days 00:06:00

                            ...      

2004-07-05 19:55:00   0 days 07:36:00

2004-07-06 09:28:00   0 days 00:06:00

2004-07-06 16:28:00   0 days 00:06:00

2004-07-14 04:04:00   0 days 00:06:00

Length: 160, dtype: timedelta64[ns] 
```

因此，我们的测量确实存在一些差距。他们并不多；只有大约千分之一的测量值与间隔超过三分钟的时间增量相邻。然而，我们确实看到，虽然大多数差距是单次测量的损失，即六分钟而不是预期的三分钟，但在一些地方存在更大的差距。存在一些大的差距；最长的一天多。其他的以小时或分钟为单位。

```
with show_more_rows():

    print(gaps.sort_values(ascending=False).head(15)) 
```

```
2003-12-11 03:04:00   1 days 13:48:00

2004-04-28 00:31:00   0 days 13:06:00

2004-07-05 19:55:00   0 days 07:36:00

2003-12-18 09:25:00   0 days 06:33:00

2003-12-06 09:25:00   0 days 06:24:00

2003-12-29 08:46:00   0 days 06:03:00

2003-12-11 14:19:00   0 days 04:42:00

2004-04-04 03:01:00   0 days 01:03:00

2004-06-30 18:13:00   0 days 00:33:00

2003-11-24 08:04:00   0 days 00:30:00

2003-10-11 17:13:00   0 days 00:27:00

2003-12-13 17:10:00   0 days 00:15:00

2004-06-30 03:07:00   0 days 00:12:00

2004-06-22 10:16:00   0 days 00:12:00

2004-07-02 09:22:00   0 days 00:12:00

dtype: timedelta64[ns] 
```

一个典型的小间隙看起来像下面的例子。在`2003-07-26 19:25:00`错过了一个我们通常期望出现的观察。这是缺失的数据，但通过其相对于可预测序列的隐含缺失，而不是用一些标记明确标记。

```
temps.loc['2003-07-26 19:22:00':'2003-07-26 19:28:00'] 
```

```
 degrees

—————————————————————————————————

2003-07-26 19:22:00         27.5

2003-07-26 19:28:00         27.1 
```

我们还可以寻找测量值之间的差距过短的情况。他们很少，但是看到那几个就会指向另一个问题。

```
small_steps = increments[increments < pd.Timedelta(minutes=3)]

small_steps.sort_values(ascending=False) 
```

```
2003-10-03 12:04:00   0 days 00:02:00

2003-12-24 15:10:00   0 days 00:00:00

2003-10-26 01:01:00   0 days 00:00:00

2003-10-26 01:07:00   0 days 00:00:00

                            ...      

2003-10-26 01:52:00   0 days 00:00:00

2003-10-26 01:55:00   0 days 00:00:00

2003-10-26 01:58:00   0 days 00:00:00

2003-10-26 01:31:00   0 days 00:00:00

Length: 22, dtype: timedelta64[ns] 
```

时间戳中的小间隙的数量只有 22，但是更具体地说，除了一个之外，所有的小间隙都是实际的零时间增量，也就是说是重复的日期时间值。两分钟而不是预期的三分钟的一个间隙将使得观察的间隔稍微不规则，因为后面的点将从预期位置偏移一分钟。

作为一个领域判断，我们将决定一分钟的差异对于我们对数据所做的任何分析或建模都不重要。然而，这是一个我们需要做出的判断，不会对每个数据集都通用。特别是，当我们相对于下面的缺失观测值进行调整时，我们也将移动许多观测值的估算测量时间。对于与特定时间而非数据变化模式相关的事件，这种转变可能是不可接受的。

下一个单元格也是一个展示 Pandas API 的一个好特性的机会。我们将查看两分钟间隙周围的一片数据，但该片的末端是数据本身并未实际出现的时间。Pandas 足够聪明，能够知道时间顺序，并选择在特定日期时间之间*的所有索引值，即使端点本身不存在。我们也能够将日期时间写成实际的日期时间对象或字符串(以几种猜测的字符串格式中的任何一种；在可能的情况下，使用 ISO-8601 始终是最佳选择)。*

```
temps.loc['2003-10-03 11:57':'2003-10-03 12:08'] 
```

```
 degrees

——————————————————————————————————

2003-10-03 11:58:00         13.0

2003-10-03 12:02:00         12.8

2003-10-03 12:04:00         12.8

2003-10-03 12:07:00         12.8 
```

## 重复的时间戳

在这里，我们遇到了另一个问题，这个问题在时间序列数据中并不少见。我们数据中的一小部分行由相同的时间戳索引。幸运的是，在这个数据集中，170，000 行中只有 41 行有问题，所以这里几乎任何方法都可以。请注意，在许多情况下，附加列可能是显式或隐式键的一部分。例如，如果其他位置的温度与外部温度相加，一个整洁的数据框可以将该位置作为分类列包含在内；在这种情况下，我们通常会期望许多重复的时间戳，但是每个类别/位置只有一个。

```
# Show all rows that are part of duplicate set

# Other 'keep' options will drop some or all duplicates

temps[temps.index.duplicated(keep=False)] 
```

```
 degrees

——————————————————————————————————

2003-10-26 01:01:00          1.9

2003-10-26 01:01:00          0.9

2003-10-26 01:07:00          1.9

2003-10-26 01:07:00          1.1

                ...          ...

2003-10-26 01:58:00          0.1

2003-12-24 15:10:00          6.4

2003-12-24 15:10:00         20.9

2003-12-24 15:10:00          6.4

41 rows × 1 columns 
```

大多数复制品有一个很小的值差，一摄氏度或更少。然而，奇怪的事情在`2003-12-24 15:10:00`发生了。在同一时刻记录了三个不同的值，其中两个是 6.4 摄氏度，但剩下的一个是 20.9 摄氏度。我们对科罗拉多州 12 月室外温度的领域知识和数据本身的模式可能会让我们放弃这个明显的异常值。很有可能，由于几个仪器都在记录，其中大多数是在一个加热的房子里，这个 20.9 的读数是用不同的温度计测量的换位。

我们的一个选择是使用熊猫的方法。它让我们可以选择保留第一行，保留最后一行，或者删除所有不明确的行。我们没有一个明确的基础来决定这些选项，但在这种情况下，没有一个是有害的，因为重复的频率相对较低。例如:

```
no_dups = (temps

             .reset_index()  # De-dup on named column

             .drop_duplicates(keep='first', subset='index')

             .set_index('index'))

print(f"Length of original DataFrame: {len(temps):,}")

print(f"Length of de-duped DataFrame: {len(no_dups):,}")

# Check if datetime index is now unique

no_dups.index.is_unique 
```

```
Length of original DataFrame: 169,513

Length of de-duped DataFrame: 169,492

True 
```

消除重复时间戳的另一种方法是对聚合公共值进行分组。对于示例，如果我们不确定哪个测量值是首选的，我们可以取几个值的平均值。这可能与这个特定的数据无关，并且对于我们注意到的重复数据中有明显异常值的情况可能是错误的。但是让我们看看 API:

```
mean_dups = temps.groupby(temps.index).mean()

print(f"Length of mean-by-duplicate: {len(mean_dups):,}")

mean_dups.index.is_unique 
```

```
Length of mean-by-duplicate: 169,492

True 
```

## 添加时间戳

正如我们已经注意到的，时间序列数据中存在缺口。大多数是在预期的三分钟时间表中的单次缺失测量，但有一次超过一天，有几次超过数小时。我们还注意到了一个问题，其中一个间隙是两分钟而不是三分钟，我们知道这一点，但不会将其视为当前数据集的关键问题。

添加更多日期时间行的典型方法是以所需的频率对数据进行重新采样。例如，如果我们只想知道月份的温度，但作为一个平均值，我们可以执行如下操作:

```
# See Pandas docs, it is easy to confuse M=month with m=minute

no_dups.resample('1M').mean() 
```

```
 index      degrees

————————————————————————

2003-07-31    21.508462

2003-08-31    20.945075

2003-09-30    14.179293

2003-10-31    12.544181

       ...          ...

2004-04-30     7.708277

2004-05-31    14.357831

2004-06-30    15.420425

2004-07-31    20.527493

13 rows × 1 columns 
```

直观上，这样的较低频率重采样与分组非常相似。我们可以用`.groupby()`得到同样的效果。这里我们使用稍微复杂的代码，因为我们希望月份按时间顺序排列，而不是按字母顺序；实现这一点的一个方法是在分组中包含该数字，但随后将其删除。

```
# Groupby both month number and name

by_month = no_dups.groupby(

    [no_dups.index.month, no_dups.index.month_name()])

# The mean temperature over the month

by_month = by_month.mean()

# Discard the month number now that result is sorted

by_month = by_month.droplevel(0)

# Name the index

by_month.index.name = 'month_name'

by_month 
```

```
month_name     degrees

———————————————————————

   January    0.433968

  February   -0.209109

     March    7.848025

     April    7.708277

       ...         ...

 September   14.179293

   October   12.544181

  November    2.332037

  December    0.667080

12 rows × 1 columns 
```

我们在这里做了一些不同的事情，因为平均值是在一个指定的月份上，而不是在一个实际的月份上。在这个例子中没有什么区别，因为我们的数据范围几乎正好是一年。然而，即使在这里，我们也对 2003 年 7 月的一些数据和 2004 年 7 月的一些数据进行了平均。如果这很重要，我们可以将年也包括在分组中以避免这种情况。当然，如果我们正在寻找一年中某个时间的典型温度，这实际上可能更接近我们多年数据的目标。

虽然起点不同，但是 9 月和 10 月在技术之间表现出相同的手段(只有 7 月会有一点不同)。然而，降采样到月数据真的不是我们宣称的任务。相反，我们希望略微增加采样，以填补缺失的三分钟增量。这也一样简单。回想一下，在转换到统一的三分钟频率之前，我们已经开始了 169，513 次观察。

```
filled_temps = no_dups.asfreq('3T')

filled_temps 
```

```
Index                    degrees

—————————————————————————————————

2003-07-25 16:04:00      27.5

2003-07-25 16:07:00      27.3

2003-07-25 16:10:00      27.3

2003-07-25 16:13:00      27.4

...                      ...

2004-07-16 15:19:00      16.9

2004-07-16 15:22:00      16.8

2004-07-16 15:25:00      16.8

2004-07-16 15:28:00      16.4

171349 rows × 1 columns 
```

方法`.asfreq()`有一个可选参数，用于回填或前填。我们没有使用它，因此我们的数据现在包含一定数量的缺失值(标记为 NaN)。 [*第 6 章*](Chapter_6.xhtml#_idTextAnchor008) ，*值插补*讨论填充和插值策略，我们可能会用它们来推测缺失数据的值。

我们可以看到有多少缺失值:

```
sum(filled_temps.degrees.isnull()) 
```

```
1858 
```

对于我们添加了单个缺失时间戳的地方，任何类型的填充或插值可能都足够了。然而，对于几个小时甚至一天的少量较大间隔，线性插值几乎肯定不能很好地弥补缺失的间隔。

还记得时间戳偏移中有些奇怪的变化吗，其中出现了一个两分钟的增量？一个或多个其他间隙修正了时间序列结束时的分钟数，但是一些中间重新采样的测量值偏离了它们的严格测量时间。这里的一个选择是*将*向上采样到一分钟的频率，并将其与更复杂的插值技术相结合。Pandas 提供了丰富的插值集合:最近的、零的、线性的、二次的、三次的、样条的、重心的、多项式的、krogh 的、分段多项式的、pchip 的、akima 的和 from 导数的。

这些高阶插值中的一个可能在几个小时的间隙上执行得相当精确，但是在日长间隙上显然不太好。让我们向上采样到一分钟的频率，然后使用样条插值来填充缺失的时间戳。

```
one_minute_temps = no_dups.asfreq('1T')

one_minute_temps.index.name = 'Timestamp'

one_minute_temps 
```

```
 Timestamp    degrees

———————————————————————————————

2003-07-25 16:04:00       27.5

2003-07-25 16:05:00        NaN

2003-07-25 16:06:00        NaN

2003-07-25 16:07:00       27.3

                ...        ...

2004-07-16 15:25:00       16.8

2004-07-16 15:26:00        NaN

2004-07-16 15:27:00        NaN

2004-07-16 15:28:00       16.4

514045 rows × 1 columns 
```

这种高采样频率在第一次通过时会产生许多行和许多 nan。

```
one_minute_temps.interpolate(method='spline', order=3, 

                             inplace=True)

one_minute_temps.head() 
```

```
 Timestamp       degrees

——————————————————————————————————

2003-07-25 16:04:00     27.500000

2003-07-25 16:05:00     27.082346

2003-07-25 16:06:00     27.079049

2003-07-25 16:07:00     27.300000

2003-07-25 16:08:00     27.072395 
```

这里所有的值都填充了一些估算值，但是观察 2003 年 12 月 11 日失踪的一天半周围的区域特别有趣。

```
(one_minute_temps

     .loc['2003-12-07':'2003-12-12', 'degrees']

     .plot(title="Spline interpolation of missing temps", 

           figsize=(12,3))); 
```

![](img/B17126_07_01.png)

图 7.1:缺失温度的样条插值

相对于更加混乱的原始数据(尽管三分之二的“原始”数据实际上是估算的，但是非常局部)，很容易看出平滑趋势是在哪里被内插/估算的。虽然 2003 年 12 月 11 日前后的长时间间隔可能不准确，但它并非不可信，而且不应过度影响整个数据集的模型。在长间隔之后的几个小时，甚至还有一个更小的几个小时的间隔，这显然与丢失的数据相对接近。

选择最佳的插值技术是一门艺术。很大程度上取决于我们对时间序列数据的预期周期(如果有的话)。事实上，这也取决于数据的顺序到底是一个时间序列，还是其他类型的序列。第五章*[*中的讨论与去趋势数据的数据质量*相关。在缺乏导致特定行为预期的领域知识的情况下，缺失点的简单线性插值限制了潜在的危害，但不一定收获很多好处。当数据*是*时间序列数据时，使用时间敏感回归是有意义的；参见第六章](Chapter_5.xhtml#_idTextAnchor007)[](Chapter_6.xhtml#_idTextAnchor008)*，*值插补*。但是，如果您期望间隙中的图案更复杂但更规则，则使用样条、多项式或分段多项式等插值技术可能会提供更好的值插补。**

 *让我们转向编码在字符串中的数据，即使数字或日期时间希望出现在字符串中。

# 字符串字段

> 语言孕育于罪恶之中，科学是它的救赎。
> 
> 威拉德·范·奥曼·奎因

**概念**:

*   文本的数字抽象
*   嵌入数字的识别
*   字符串距离度量
*   语音规范化
*   分类与小的非重复值计数
*   不常见的值和因子级别
*   将非原子字段解析为不同的数据类型

包含在字符串字段中的数据可以有多种含义。在最坏的情况下，对我们来说，单词可以表达复杂的，微妙的，逻辑上相关的意思。但是数据科学对书籍、文章，甚至对简短的自由形式的注释都不感兴趣。我们只喜欢分类、数字、序数和日期/时间数据。*免责条款法*。

*当然* **自然语言处理** ( **NLP** )是真正的和数据科学、数据分析和机器学习的重要领域。这不能成为这本书的广泛主题，但一个普遍的观点可以。要成为数据，散文文本必须被转换。

字数是数字。n 元语法频率——被视为一个单位的单词或字母序列——可以是参数空间的维度。文本的隐马尔可夫模型中状态转移的转换概率只是向量。大型词汇表可以嵌入较小的向量空间作为合成维度。也许现有的情感分析模型可以用来生成句子或散文文本的其他片段的数字特征。

在我们离题讨论 NLP 可能使用的几种编码之前，让我们先看看文本的简单用法。大量的字符串字段*非常接近于*数据。例如，整数或浮点数可能恰好被表示为字符串。例如，很常见的情况是，遇到的字符串数据显然是用来表示数字的，但只是表面问题。

让我们读入一个非常小的表格数据集，类似于第一章 、*表格格式*中的 [*。*](Chapter_1.xhtml#_idTextAnchor003)

```
df = pd.read_fwf('data/parts2.fwf')

df 
```

```
 Part_No              Description              Maker      Price

———————————————————————————————————————————————————————————————————

0    12345     Wankle rotary engine   Acme Corporation   $ 555.55

1   No.678               Sousaphone      Marching Inc.   $ 333.33

2     2468           Feather Duster        Sweeps Bros    $ 22.22

3    #9922   Area 51 metal fragment     No Such Agency   $9999.99 
```

在特性`Part_No`和`Price`下面，我们可以清楚地看到分别表示整数和浮点数的意图。我们只是在的字符串中添加了一点额外的文本，这两个列都使 Pandas 库无法自动识别这些类型。我们可以清理单个的列，然后再次尝试转换成它们想要的类型。在我们清理的时候，我们可能会施加一个比 Pandas(或其他库)默认推断的稍微窄一点的限制。出于我们的目的，我们假设零件号总是正的，并且不高于 2 ^(16) ，也就是说无符号的 16 位整数。

```
# Regular expression to strip all non-digits

df['Part_No'] = (df.Part_No

                     .str.replace(r'[^0-9]', '')

                     .astype(np.uint16))

# Remove spaces or $ from start of strings

df['Price'] = (df.Price

                   .str.lstrip("$ ")

                   .astype(float))

df.dtypes 
```

```
Part_No         uint16

Description     object

Maker           object

Price          float64

dtype: object 
```

```
df 
```

```
 Part_No              Description             Maker      Price

———————————————————————————————————————————————————————————————————

0      12345     Wankle rotary engine   Acme Corporation    555.55

1        678               Sousaphone      Marching Inc.    333.33

2       2468           Feather Duster        Sweeps Bros     22.22

3       9922   Area 51 metal fragment     No Such Agency   9999.99 
```

清理字符串以允许它们转换成数字在细节上可能很繁琐，但在概念上，它不超过一点点目测和一些试错，假设每个特征完全由“试图出去”的数字组成。在下面的小节中，我们将通过强加字符串的等价，将字符串视为分类，并将字符串字段划分为隐式子字段(每一个都可能有自己的类型)来查看更多的方法。

如果您确定转换成数字是合适的，那么值得记住它们是什么类型的数字。 [*术语表*](Glossary.xhtml#_idTextAnchor012) 中 [NOIR](Glossary.xhtml#_idTextAnchor087) 条目提供了对名义变量、序数变量、区间变量和比率变量的讨论。当然，即使原生数据格式已经是数字格式，这种考虑也是值得的。在上面的例子中，(根据规定)我们可能知道`Part_No:100`比`Part_No:200`更早被添加到目录中，但不知道它们之间相隔多长时间。`Part_No:99`可能是在`Part_No:100`比`Part_No:100`与`Part_No:200`更大的(负)差距上添加的。在这种情况下，变量是序数*的*。特别是，我们不期望`Part_No:100 + Part_No:200`和`Part_No:300`有任何特定的关系(也没有任何意义)。当然，这些数字也可能只是与目录条目相关的随机数字，最好保留为字符串。

与`Part_No`相反，我们假设`Price`条目之间具有比率关系。带`Price:250`的物品价格是带`Price:500`的一半。如果一个买家订购一个`Price:250`和一个`Price:500`，一般会被收取`$750`的费用。当然，这并不意味着可以直接替换带有`Price:750`的商品，这是买家不想要的。

## 模糊匹配

有时我们有一个短字符串字段，用来表示一个 [名义值](Glossary.xhtml#_idTextAnchor086)/分类值。然而，由于数据采集的不确定性，可能会为包含相同标称值的观测值输入不同的字符串。字符串中的字符出现错误的方式有很多种。极其常见的问题是不规范的大写和虚假的间距。对于名义上的特性，简单地将原始字符串小写或大写，并删除所有空格(从填充或内部，取决于特定的期望值)，通常是一个好策略。

虽然空格和大小写的简单规范化会揭示许多预期的等价关系，但我们也可以查看可能相似的字符串之间的编辑距离。 [*第四章*](Chapter_4.xhtml#_idTextAnchor006)*异常检测*中的一个练习让你玩出了这种可能性。简单的输入错误和拼写错误经常被字符串对之间的一小段 Levenshtein 距离捕获。这种比较有两个问题:同样的问题也适用于 Damerau-Levenshtein、Hamming、Jaro-Winkler 或其他与 Levenshtein 一样的编辑距离度量。一个问题是距离是不可传递的。如果 *A* 和 *B* 之间的编辑距离为 5，并且 *B* 和 *C* 之间的编辑距离为 5，那么 *A* 和 *C* 之间的距离可以是 0 到 10 之间的任意值。如果 6 是“足够接近等价”的阈值，那么可能不清楚是将 *B* 视为“类 A”还是“类 C”，或者两者都是，或者都不是。

使用编辑距离的更大问题是它具有二次复杂度。也就是说，正如非传递性所暗示的那样，找到所有相似性的唯一方法是将所有值对与其各自的对编辑距离进行比较。可能有一些捷径，例如，如果我们识别公共前缀的集合，但是通常我们需要接受这种复杂性。对于下面的小例子，这不是禁止性的，但是对于大型数据集，这是禁止性的。

另一个有用的方法是语音规范化。通常，这种方法对于可能以各种方式音译的姓名很有用，尽管具有高错误率的语音识别系统的日益流行可能会带来额外的机会。最有可能的是，语音识别软件会将一个单词误认为听起来有些相似。虽然这种方法也可能捕捉到一类错别字，但不太一致。例如，字符串“GNU”和“GUN”仅相隔一个变调，但它们的发音明显不同。

一种更古老的(1918 年)语音规范化方法被称为 Soundex，它的工作原理是用一个共同的符号代替相似声音的集合。例如，“b”、“f”、“p”和“v”都以相同的方式编码。基于该系统的是 1990 年的[变音](Glossary.xhtml#_idTextAnchor076)。

Metaphone 允许更复杂的规则，例如查看通常具有某种声音的字母簇，这种声音不仅仅是单个字母声音的相加，或者在其他相邻字母的上下文中删除某些字母。这些技术主要依赖于辅音，元音通常会从编码中删除。

双重变音比变音更进了一步，试图解释英语中更多的不规则词汇，这些词汇来自斯拉夫语、日耳曼语、凯尔特语、希腊语、法语、意大利语、西班牙语、汉语和其他来源。这给出了相对复杂的规则集；例如，它针对字母 c 的使用测试了大约 100 种不同的上下文。然而，该算法在任何数据集大小上都保持线性，并且通常在编码单个单词时是连续的。这种技术名称中的“双重”来源于这样一个事实，即它既产生了主要的规范化，很多时候也产生了使用替代规则的次要规范化。这允许更灵活的等价比较。比如 *A* 的二次编码可能和 *B* 的一次编码匹配，这至少是一个关于相似性的暗示。

让我们用一个具体的例子来说明。我们有一个数据集，其中有许多相似的姓氏，这些姓氏来自不同的语言，但可能代表同一个人或同一家族，以转录差异为模。在这个例子中，出于演示的目的，名称被标记为“相似组”，但是在真实数据中，您不太可能有类似的东西。为了使它看起来更像一个典型的数据集，还包含了一个额外的数字列。不管我们是否设法将这些不同的拼法统一起来，这些拼法可能是*相同的名字*，名字形成名义上的变量，因为它们的数量是有限的。

```
names = pd.read_csv('data/names.csv', index_col='Group')

names.head(8) 
```

```
Group      Last_Name   Other_Data

——————————————————————————————————

1        Levenshtein          103

1       Levenschtein          158

1         Levenstein          110

2            Hagelin          136

2             Haslam          105

2           Haugland          190

2            Heislen          181

2             Heslin          106 
```

如果我们使用 Python **Metaphone** 包，我们可以使用函数`doublemetaphone()`，它为每个输入字符串产生一对主/次编码(次编码可以是空的)。

同一个包中的`metaphone()`函数，或者大多数其他规范化库，将产生一个单独的字符串来表示一个输入字符串。库 [Fuzzy](Glossary.xhtml#_idTextAnchor046) 是一个更快的实现，但似乎仅限于 ASCII 输入，这在我们的一些测试名称中不能使用重音字符。我们将这些规范化添加到数据框架中。

```
from metaphone import doublemetaphone

metas = zip(*names.Last_Name.map(doublemetaphone))

names['meta1'], names['meta2'] = metas 
```

让我们来看看相似组 6，它包含了相同名字的许多拼写变化。

```
with show_more_rows():

    print(names.loc[6]) 
```

```
Group  Last_Name  Other_Data meta1   meta2

6          Jeong         191   JNK     ANK

6           Jong         157   JNK     ANK

6          Chŏng         100   XNK        

6          Chung         123   XNK        

6           Jung         118   JNK     ANK

6          Joung         168   JNK     ANK

6          Chong         101   XNK        

6         Cheong         133   XNK        

6         Choung         104   XNK 
```

这个非常常见的韩国姓氏——在韩文![](img/B17126_07_001.png)、国际音标![](img/B17126_07_002.png)——根据不同的风格指南和历史上不同时期的以多种不同的方式音译成英语。您可能会遇到列出的任何一个，但是它们都引用相同的底层名称；或者他们提到韩国人的名字。事情变得复杂了。在韩国，“正”目前是规范的；在朝鲜，“Jong”，是目前官方的音译。

作为一个复杂的例子，美国女权主义小说家埃里卡·琼是俄罗斯/波兰犹太血统，所以你可能会认为她的姓有意第绪语的血统。原来，这实际上是她的第二任丈夫，一位美籍华人精神病学家。中国人的名字和韩国人的名字有很远的关系，但肯定不仅仅是不同的转录。同样，瑞士精神分析学家卡尔·古斯塔夫·荣格的德国名字与韩国名字也没有关系。

我们看到了其中几个的规范化“ANK”，包括发音为![](img/B17126_07_003.png)的德语名称(即在德语、意第绪语、瑞典语、挪威语、荷兰语等中,“J”的发音类似于英语“Y”。).

仍然有一些名称拼写，这种技术不能统一它们，即使考虑二级编码。首字母“J”和首字母“Ch”只是被赋予了不同的表示。然而，我们已经将许多备选拼写简化为一个规范的表示。让我们看另一个例子。前利比亚领导人(穆阿迈尔·卡扎菲)的名字被英语媒体以多种不同的方式转录，以至于拼写变化成了一个幽默的音符。在阿拉伯语中是![](img/B17126_07_004.png)，在国际音标中是![](img/B17126_07_005.png)或![](img/B17126_07_006.png)。我们的双变音位技术在这里做得很好，将几乎所有变体识别为主要或次要规范化。将此作为一个普通的名义值(少数编码为“KTTF”的不会以这种方式统一，“KSF”/“KTSF”也不会，但所有其他的都可以)，对您的目的来说可能是合理的。这可能是一个比许多不同的个人命名为“Jeong”(或一些变体拼写)更好的例子，因为几乎任何英语新闻文章，这可能是我们假设的文档语料库，使用这些拼写中的任何一个指代同一个人。

```
with show_more_rows():

    print(names.loc[5]) 
```

```
 Last_Name  Other_Data  meta1  meta2

Group                                     

5        Gadaffi         197    KTF       

5         Gadafi         189    KTF       

5         Gadafy         181    KTF       

5        Gaddafi         163    KTF       

5        Gaddafy         179    KTF       

5        Gadhafi         112    KTF       

5        Gathafi         187    K0F    KTF

5       Ghadaffi         141    KTF       

5        Ghadafi         152    KTF       

5       Ghaddafi         192    KTF       

5       Ghaddafy         122    KTF       

5       Gheddafi         142    KTF       

5        Kadaffi         139    KTF       

5         Kadafi         188    KTF       

5        Kaddafi         192    KTF       

5        Kadhafi         121    KTF       

5        Kazzafi         193    KSF   KTSF

5       Khadaffy         148    KTF       

5        Khadafy         157    KTF       

5       Khaddafi         134    KTF       

5         Qadafi         136    KTF       

5        Qaddafi         173    KTF       

5        Qadhafi         124    KTF       

5      Qadhdhafi         114   KTTF       

5      Qadhdhāfī         106   KTTF       

5       Qadthafi         186    KTF       

5        Qathafi         130    K0F    KTF

5       Quathafi         145    K0F    KTF

5        Qudhafi         158    KTF 
```

为了完善我们的编码，让我们看看其他几组有相似发音的名字。随意浏览下一个例子；坦白地说，这使得关于 Levenshtein 距离和作者姓氏的笑话成为可能。

```
with show_more_rows():

    print(names.loc[names.index < 5]) 
```

```
 Last_Name  Other_Data   meta1  meta2

Group                                         

1       Levenshtein         103  LFNXTN       

1      Levenschtein         158  LFNXTN       

1        Levenstein         110  LFNSTN       

2           Hagelin         136    HJLN   HKLN

2            Haslam         105    HSLM       

2          Haugland         190   HKLNT       

2           Heislen         181     HLN       

2            Heslin         106    HSLN       

2           Hicklin         151    HKLN       

2          Highland         172   HHLNT       

2          Hoagland         174   HKLNT       

3           Schmidt         107     XMT    SMT

3             Shmit         167     XMT       

3             Smith         160     SM0    XMT

3             Smitt         181     SMT    XMT

3              Smit         192     SMT    XMT

4             Mertz         173    MRTS      

4              Merz         116     MRS      

4            Mertes         178    MRTS      

4             Hertz         188    HRTS 
```

所有类似史密斯的名字都可以统一为“XMT”，尽管我们必须同时考虑主编码和次编码。在我们看来，H 开头的名字并不一定都一样，但我们看到了一些重叠。令人失望的是，“Mertz”和“Merz”并不是这样统一的，尽管在德语或意第绪语中，这位作者的姓可能是“Mertz”的历史性拼写错误。

上述统一名义值的例子集中在人名——特别是姓氏——上，但是这种技术也适用于在分类值的表示中可能出现语音混淆或替换的其他情况。

## 显式类别

从概念上来说，是一个只有少量测量值的变量和一个实际上是分类的变量之间的差异。因子(分类)变量允许我们更准确地表达使用它们的意图，而且还支持一些额外的 API 和性能优化。最常见的情况是，因子与存储为字符串的数据相关联，但这并不是必须的；数据类型本身并不能决定问题。例如，我们可能有一个住宅开发项目中的房屋数据，如下所示:

| 批次号 | 地址 | 英亩 | 出版物编印风格 |
| --- | --- | --- | --- |
| Thirty-two thousand eight hundred and forty-nine | 中路 111 号 | Two | Thirty-seven |
| Thirty-four thousand two hundred and ten | 高街 23 号 | one | Twenty-one |
| Thirty-nine thousand seven hundred and twelve | 洛维大街 550 号 | three | Twenty-two |
| Forty thousand and fifteen | 十字街 230 号 | one | Twenty-one |
| Thirty-two thousand one hundred | 中路 112 号 | one | Fourteen |
| Thirty thousand four hundred and forty-one | 中路 114 号 | Two | Twenty-two |

我们可以用少量的领域知识来对每个特性的性质做出判断。特别地，我们可以假设`Lot #`是唯一地描述一个属性。`Address`大概也差不多。一个字段是整数，另一个是字符串，这一事实并不重要，重要的是该值表示每条记录的独特之处。即使一个地块可能偶尔被细分为多个地址，而其他地块可能没有地址，但通常我们期望这些值具有近似的清晰度。记录中的值可能不完全唯一，但它们趋向于那个方向。这些都不是因子的好候选者。

接下来让我们想想房子的风格和地块大小(以英亩为单位)。房子的风格大概是从开发商现有的数量相对较少的库存平面图中选择的。它被编码为一个整数，但也可能是一个用于相同目的的简称(如“Tudor Revival 4 BR”)。我们可能需要考虑未来的数据，其中房屋是根据定制计划建造的——或者无论如何，计划不是来自开发商的投资组合——但可以用像“定制”这样的名称或像-1 这样的标记号进行编码。最有可能的是，房屋风格最好被描述为一个分类变量。

如果我们只看目前的数据，变量`Acres`可能会误导我们。它是一个比`House Style`有更少不同值的整数。作为领域知识，我们知道新的开发通常被划分为固定的地块大小(1-3 英亩对于住宅来说是不寻常的大，但并不荒谬)。但是，随着时间的推移，批次可能会细分或聚合成与原始分配不匹配的单位。中间路 114 号的所有者可能会将他们 0.35 英亩的土地出售给相邻的中间路 112 号的所有者，留给双方非整数和不常见的地块大小。最有可能的是，事实上，我们并不希望将这个变量编码为分类变量，即使它的初始值可能暗示了这一点。尽管该变量目前只保存整数，但浮点数可能是最合适的。

***

在 [*第四章*](Chapter_4.xhtml#_idTextAnchor006) 、*异常检测*的一个练习中，向您展示了一个包含许多人名的数据集，其中许多可能是更常见的预期姓名的拼写错误。首先使用 Pandas，让我们读入数据，然后丢弃具有不常见名称的行，然后将字符串列`Name`转换为分类变量。

```
humans = pd.read_csv('data/humans-names.csv')

humans 
```

```
 Name       Height      Weight

—————————————————————————————————————————

    0     James   167.089607   64.806216

    1     David   181.648633   78.281527

    2   Barbara   176.272800   87.767722

    3      John   173.270164   81.635672

  ...       ...          ...         ...

24996   Michael   163.952580   68.936137

24997     Marie   164.334317   67.830516

24998    Robert   171.524117   75.861686

24999     James   174.949129   71.620899

25000 rows × 3 columns 
```

出于这个目的，我们不希望看到名字出现少于 10 次的行。我们可以看到，这保留了大部分行，但是从 25，000 行中删除了 417 行。

```
name_counts = humans.Name.value_counts()

uncommon = name_counts[name_counts < 10]

humans = (humans

              .set_index('Name')

              .drop(uncommon.index)

              .reset_index())

humans 
```

```
 Name       Height      Weight

—————————————————————————————————————————

    0     James   167.089607   64.806216

    1     David   181.648633   78.281527

    2   Barbara   176.272800   87.767722

    3      John   173.270164   81.635672

  ...       ...          ...         ...

24579   Michael   163.952580   68.936137

24580     Marie   164.334317   67.830516

24581    Robert   171.524117   75.861686

24582     James   174.949129   71.620899

24583 rows × 3 columns 
```

此时，还剩下 18 个唯一的名称，如下所示。它们作为单独的字符串存储起来有点低效，但是总的来说，所有的 Pandas 操作都表现得非常好。例如，我们可以按名称分组来执行其他操作。此外，像 scikit-learn 这样的库通常乐于将不同字符串的集合视为分类的(对于许多模型；其他的将需要数字编码)。在 Pandas 中，转换为因子除了优化存储大小和使一些选择操作更快之外没什么作用。这些都是有价值的目标，但是对可用的 API 几乎没有影响。我们将在下面看到，R 的 Tidyverse 在某种程度上是根据因子定制的。

```
humans['Name'] = humans.Name.astype('category')

humans.Name.dtype 
```

```
CategoricalDtype(categories=['Barbara', 'David', 'Elizabeth', 'James',

                             'Jennifer', 'Jessica', 'John', 'Jon',

                             'Joseph', 'Linda', 'Marie', 'Mary',

                             'Michael', 'Patricia', 'Richard', 'Robert',

                             'Susan', 'William'],

                 ordered=False) 
```

使用这个数据框架并没有什么真正的改变。特别是，您可以假设`Name`仍然是一个字符串字段，但是过滤器会运行得更快。正如我们在上面看到的，dtype 现在也公开了类别值，但是同样的信息通常也可以通过`Series.unique()`获得，甚至对于字符串列也是如此(尽管需要对字符串的整个列进行线性扫描，但是对于分类列，需要查找单个现有的数据结构)。

```
humans[humans.Name == 'Mary'] 
```

```
 Name       Height      Weight

——————————————————————————————————————

   19   Mary   170.513197   71.145258

   35   Mary   175.783570   73.843096

   54   Mary   166.074242   70.826540

   61   Mary   175.258933   78.888337

  ...    ...          ...         ...

24532   Mary   172.602398   72.602118

24536   Mary   172.159574   70.383305

24547   Mary   173.902497   71.545191

24549   Mary   169.510964   71.460077

1515 rows × 3 columns 
```

让我们用 R 来看同一个数据集，它将称为“因子”,认为它更特殊。尽管如此，在 R as 中，因子变量和它们的底层数据类型(通常是底层字符串，但我们有时会将整数甚至浮点数视为因子)之间很容易来回转换。

```
%%R

humans <- read_csv('data/humans-names.csv')

humans 
```

```
── Column specification ──

cols(

  Name = col_character(),

  Height = col_double(),

  Weight = col_double()

)

# A tibble: 25,000 x 3

   Name      Height Weight

   <chr>      <dbl>  <dbl>

 1 James       167\.   64.8

 2 David       182\.   78.3

 3 Barbara     176\.   87.8

 4 John        173\.   81.6

 5 Michael     172\.   82.8

 6 William     174\.   70.7

 7 Elizabeth   177\.   81.2

 8 Joseph      178\.   78.3

 9 Jessica     172\.   64.5

10 William     170\.   69.2

# ... with 24,990 more rows 
```

在这个数据集的 Tidyverse 版本中，我们将做一些与熊猫稍微不同的事情。首先，我们将使用`mutate_at()`，就像我们在熊猫中使用`.astype()`一样。接下来，我们使用一个自定义的因子变量工具。在这里，所有不常见的名字并没有被丢弃，而是被集中在一起作为一个共同的价值`"UNCOMMON"`。这允许我们保留其他相关的数据列(这显然在 Pandas 中是可能的，但是稍微不太简洁)。

```
%%R

# Make the column Name into a factor variable

humans <- mutate_at(humans, vars(Name), factor) 

# Any values occurring fewer than 100 times will be 

# aggregated under the factor level "UNCOMMON"

humans['Name'] <- fct_lump_min(humans$Name, min = 100, 

                               other_level = "UNCOMMON")

humans 
```

```
# A tibble: 25,000 x 3

   Name      Height Weight

   <fct>      <dbl>  <dbl>

 1 James       167\.   64.8

 2 David       182\.   78.3

 3 Barbara     176\.   87.8

 4 John        173\.   81.6

 5 Michael     172\.   82.8

 6 William     174\.   70.7

 7 Elizabeth   177\.   81.2

 8 Joseph      178\.   78.3

 9 Jessica     172\.   64.5

10 William     170\.   69.2

# ... with 24,990 more rows 
```

唯一可见的变化是列类型发生了变化，但这让我们可以询问因子变量的级别，而相同的调用会为字符列生成`NULL`。

```
%%R

levels(humans$Name) 
```

```
 [1] "Barbara"   "David"     "Elizabeth" "James"     "Jennifer"  "Jessica"  

 [7] "John"      "Jon"       "Joseph"    "Linda"     "Marie"     "Mary"

[13] "Michael"   "Patricia"  "Richard"   "Robert"    "Susan"     "William"  

[19] "UNCOMMON" 
```

同样，在 tibble API 中并没有太多的变化。使用`fct_lump_min()`和类似的函数的能力是因子列特有的，但是访问它们的和以前一样(只是更快)。

```
%%R

humans %>% filter(Name == "UNCOMMON") 
```

```
# A tibble: 417 x 3

   Name     Height Weight

   <fct>     <dbl>  <dbl>

 1 UNCOMMON   172\.   76.5

 2 UNCOMMON   167\.   60.3

 3 UNCOMMON   182\.   85.2

 4 UNCOMMON   176\.   72.3

 5 UNCOMMON   174\.   82.1

 6 UNCOMMON   170\.   66.8

 7 UNCOMMON   171\.   60.0

 8 UNCOMMON   171\.   73.9

 9 UNCOMMON   171\.   80.4

10 UNCOMMON   177\.   73.3

# ... with 407 more rows 
```

现在让我们来看看观察值的分布，因为不常用的名字已经被包括在包罗万象的“`UNCOMMON`”因子级别中。

```
%%R

ggplot(humans, aes(y = Name)) + geom_bar(stat = "count") 
```

![](img/B17126_07_02.png)

图 7.2:名称计数的分布

在下一节中，字符串再次被考虑，但是在的意义上，它们被用在自然语言处理中，作为人类语言的文本，我们可以将它们转换成数字表示。

# 字符串向量

> 摆脱意义。
> 
> 凯西·艾克

**概念**:

*   词汇袋
*   Word2Vec
*   余弦相似性
*   停用词，标记化，词条化

自然语言处理(NLP)是数据科学的一个大的子领域。这个话题本身就值得拥有无数本好书，幸运的是，确实有许多存在。对于这本书，我们只想看一个利基领域；如何将自然语言的字符串编码成数字特征，使机器学习模型可以接受这些数字特征作为输入，并且统计技术可以对这些数字特征进行操作？

按照历史顺序和复杂性，将自然语言文本转换成向量有两种主要方法。在最简单的情况下，我们可以使用一种叫做**单词袋**的技术作为一种技术，这很简单，我们可以用几行代码轻松地创建这种表示。想法是首先为整个[语料库](Glossary.xhtml#_idTextAnchor029)构建一个词汇表；也就是说，只是它包含的所有单词的集合。然后，我们可以用词汇表长度的向量来表示其中的每个文本，每个分量维度表示该单词的数量。显而易见，随着语料库和词汇的增长，这会产生很大的向量。即使它失去了单词的顺序，这种编码在产生捕捉语义差异的有用向量方面也是非常有效的。

举个高度简化的例子，假设你所在的城镇有几家宠物店。每家公司都出版了一个目录，其中提到“狗”和“猫”两个词的次数各不相同。你自己有一只特定种类的宠物，你希望确定哪一只更适合你的宠物护理需求。我们通常在 NLP 中使用的向量可能有数百或数千维，而不是二维。

![](img/B17126_07_03.png)

图 7.3:宠物店向量空间

为了保持词汇至少相对易于管理，我们可以将单词简化成更简单的形式。我们可以丢弃标点符号和*规范化*大小写来得到更少的单词。此外，使用[NLTK](Glossary.xhtml#_idTextAnchor084)(自然语言工具包)包，我们可以删除“停用词”——那些通常很小的连接词、代词和其他一些词，它们对句子的一般语义没有什么影响。显然，这些对于人类交流的清晰性来说经常是必要的，但是没有它们，意义的矢量表示通常做得更好。举个简单的例子，让我们选择一位不幸的政治独裁诗人的一首著名而有力的诗。

```
# William Butler Yeats

second_coming = """

Turning and turning in the widening gyre   

The falcon cannot hear the falconer;

Things fall apart; the centre cannot hold;

Mere anarchy is loosed upon the world,

The blood-dimmed tide is loosed, and everywhere   

The ceremony of innocence is drowned;

The best lack all conviction, while the worst   

Are full of passionate intensity.

Surely some revelation is at hand;

Surely the Second Coming is at hand.   

The Second Coming! Hardly are those words out   

When a vast image out of Spiritus Mundi

Troubles my sight: somewhere in sands of the desert   

A shape with lion body and the head of a man,   

A gaze blank and pitiless as the sun,   

Is moving its slow thighs, while all about it   

Reel shadows of the indignant desert birds.   

The darkness drops again; but now I know   

That twenty centuries of stony sleep

Were vexed to nightmare by a rocking cradle,   

And what rough beast, its hour come round at last,   

Slouches towards Bethlehem to be born?

""" 
```

第一步是为我们的矢量编码确定简化的单词，从而确定词汇表。首先，这首诗本身被简化成一系列更规范的词语。这是一种*标记化*的形式，但是一种非常简化的形式。

```
def simplify_text(text):

    stops = nltk.corpus.stopwords.words('english')

    words = re.findall(r'[a-z]+', text.lower())

    return [w for w in words if w not in stops]

poem = simplify_text(second_coming)

poem[:6] 
```

```
['turning', 'turning', 'widening', 'gyre', 'falcon', 'cannot'] 
```

从这里开始，我们想要一个从词汇表到向量中索引位置的映射。任何特定单词表示在向量中的位置与此目的无关，因为每一个都构成向量的正交轴。例如，如果“`gyre`”被选作向量的第二个、第六个或第二十个元素，这并不重要。

我们的目标是用这样的载体来编码每一个片段。一般来说，片段可能是一个单词，但也可能是一行、一段、一节或我们喜欢的任何其他部分。下面的代码首先创建单词到索引位置的映射，然后生成单词包向量。

```
word2ndx = {w:n for (n, w) in enumerate(set(poem))}

print(f"Vector dimensions={len(word2ndx)}")

def make_vector(words, word2ndx=word2ndx):

    # Generate the vector of zero count per dimension

    vec = np.zeros(len(word2ndx), dtype=np.uint16)

    for word in words:

        # we might ignore unknown word or attempt 

        # to canonicalize it, here we raise exception

        assert word in word2ndx

        n = word2ndx[word]

        vec[n] += 1

    return vec

list(word2ndx.items())[:5] 
```

```
Vector dimensions=84

[('centre', 0), ('loosed', 1), ('blank', 2), ('falconer', 3), ('moving', 4)] 
```

为了说明这种单词袋矢量化技术，我们可以将每个节编码为一个向量。

```
for i, stanza in enumerate(second_coming.split('\n\n')):

    print(f"Stanza {i+1}:")

    print(make_vector(simplify_text(stanza))) 
```

```
Stanza 1:

[1 2 0 1 0 0 0 2 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0

 1 1 0 1 1 0 2 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1

 0 0 0 0 0 1 0 1 0 1]

Stanza 2:

[0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 2 0 1 1 1 1 1 0 1 0 0 0 0 2 0 1 1 0 1

 0 0 1 0 0 1 0 1 1 1 1 2 1 1 1 1 0 1 2 1 0 0 0 0 1 0 1 1 0 0 1 0 1 1 2 1 0

 1 1 1 1 1 0 1 0 1 0] 
```

这些向量代表了两个诗节的“意义”之间的区别。令我惊讶的是——直到写下这一段我才意识到——除了停用词之外，没有单词重复。在每一节中，不同的单词被重复，尽管只有两次，不会超过两次。作为人类读者，我们当然会从每一节中获得不同的“感觉”,并以不同的方式描述其整体意义。

一般来说，比单词袋更强大的矢量化技术是 **Word2Vec** 。这个模型允许你创建任意维度的向量；但更重要的是，Word2Vec 使用了一个双层神经网络，它实际上将每个单词的上下文视为由其周围的单词定义的。这最终产生了具有奇怪意义的向量。一些通常引用的例子是具有不同含义的向量的减法和加法。例如，在一个大型、典型的英语语料库上训练，我们可能会看到:

![](img/B17126_07_007.png)

或者:

![](img/B17126_07_008.png)

根据训练中使用的语料库，第二个可能会被“火鸡”的额外含义所抛弃，因为它是一种不会飞的鸟，尤其在北美被食用。但话说回来，“中国”也是一个表示瓷盘的词，可能有类似的同音异义效应。

建立在 Word2vec 基础上的是一个改进版本，它的最初发明者 Quoc Le 和 Tomas Mikolov 称之为“段落向量”，但在我们这里使用的 [gensim](Glossary.xhtml#_idTextAnchor048) 包中被称为 **Doc2Vec** 。Gensim 是一个非常有用的用于 Python 的 NLP 包，它包含了许多有用的 NLP 建模工具；它在底层库中针对速度进行了很好的优化。同样值得研究的是 [spaCy](Glossary.xhtml#_idTextAnchor127) ，其中有着类似的目的，但是有更多的预建模型。出于多种目的，要么矢量化非常相似；Doc2Vec 主要增加了按作者等属性标记每个文档(例如一个句子、一个段落、一节或整本书)的能力。这种标记允许额外的方法来描述标签(即作者)的整体特征，并将其与其他标签或小说文本进行比较。

对于这次讨论，我们将查看 14，485 条关于航空公司的推文。一个比我们上面使用的单首诗更大的语料库是有用的，但是没有理由我们不能以类似的方式使用它。除了推文本身，这个数据集还有很多东西。

两个这样的字段是(账户的)名称和航空公司。后者有些多余，因为它是基于用户自己附加的 Twitter `@`标签确定的。让我们看几个例子来了解一下。

```
db = sqlite3.connect('data/Airline-Tweets.sqlite')

cur = db.cursor()

sql = """

SELECT name, airline, text 

FROM Tweets 

"""

cur.execute(sql)

tweets = cur.fetchall()

pprint(tweets[5000:5003], width=60) 
```

```
[('Paul_Faust',

  'United',

  '@united Love to report how horrible this flight is to '

  "your team. Let's make it worse...as they get to my "

  'seat...out of all snacks'),

 ('Jennsaint8',

  'Southwest',

  '@SouthwestAir any chance they will change this to '

  'include Northeast airports?  JetBlue has.'),

 ('_stephanieejayy',

  'Delta',

  '@JetBlue do you have any afternoon flights going from '

  'BQN to JFK? I only seem to find early morning flights.')] 
```

使用作者来标记这个语料库肯定是有意义的；然而，由于每位作者写的 tweets 相对较少，使用频率较高的航空公司名称可能会更有趣。这个选择对这本书来说并不太重要，只是说明一下。我们将使用这两个标签来显示 API。

```
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

docs = []

for (author, airline, msg) in tweets:

    td = TaggedDocument(simplify_text(msg), [author, airline])

    docs.append(td)

# Require words occur at least 4x, look 2 words to each side

# The produced vector is 10 dimensional

model = Doc2Vec(docs, vector_size=10, window=2, min_count=4) 
```

让我们看看在最低计数要求下我们的词汇量有多大，并看看几个例句。词汇中单词的顺序也是没有意义的，因为它与单词袋不同。词汇表中有几千个单词，但是我们将表示缩减到任意维度。这里我们选择 10 个维度，这对于这些相当刻板的信息来说可能足够了。具有更多语义变化的更宽的语料库可能会受益于更高的维度(如果没有指定，默认为 100)。

```
print("Number of words:", len(model.wv.vocab))

list(model.wv.vocab)[:7] 
```

```
Number of words: 3359

['jetblue', 'new', 'ceo', 'seeks', 'right', 'balance', 'please'] 
```

*茎*

对*单词*的识别本身就是 NLP 的一个重要领域。“相同”单词的屈折形式通常最好被视为相同的基本形式。这是通过[词干](Glossary.xhtml#_idTextAnchor134)或[词汇化](Glossary.xhtml#_idTextAnchor070)来完成的。词干分析试图通过移除常见词缀来识别组成单词词根的几个字母。词汇化通过使用语法上下文和音位关系走得更远。例如，一个 lemmatizer 可能将“dove”作为动词规范化为“dive”(即跳跃)，而将“dove”作为名词规范化为“dove”(即鸟)。这两种技术都会将示例中的单词“seek”视为与“seeks”、“seeking”等相同。

我们运行这段代码的目的是现在能够将我们可能创建的任何字符串表示为 10 个数字特征。新字符串可能只利用我们词汇表中的术语，但 Gensim 提供了一种机制来构建更大的词汇表和模型，包括初始训练集中没有出现的单词。让我们首先看看现有 tweet 的向量，然后看看一条新消息。

```
msg = tweets[11_001][2]

print(msg)

model.infer_vector(simplify_text(msg)) 
```

```
@AmericanAir thank you for responding rather quickly btw

array([ 0.01165844,  0.00964975, -0.08577796, -0.03201848,  0.00883767,

        0.13692749,  0.06367198,  0.02911634, -0.00109272, -0.16733222],

      dtype=float32) 
```

下面我们创建一个简短的小说信息，并获得其向量。这些维度没有特别的意义，但是我们能够测量它们之间的关系。我们还可以将这些合成数据存储在中间数据集中，用于下游建模技术；后者是这种转换最常见的用法。

```
badservice = model.infer_vector(['bad', 'service'])

badservice 
```

```
array([-0.03352741,  0.0146618 , -0.03105226,  0.036326  ,  0.05287395,

        0.05780041, -0.05203189,  0.07293667, -0.01861257, -0.13574287],

      dtype=float32) 
```

Gensim 库提供了一组丰富的函数来比较这些表示，使用余弦相似度(两个向量之间角度的余弦)和其他技术。举个例子，让我们看看哪个单词最接近我的短信“糟糕的服务”。一个小注意是，我已经冻结了下一个输出；Doc2Vec 底层的神经网络具有状态随机化，因此每次对其进行训练时，都会在底层神经网络中产生不同的向量和不同的连接权重。这里我显示了一次特定运行的结果，但是其他运行的细节会有所不同。

```
model.wv.most_similar(['bad', 'service']) 
```

```
[('terrible', 0.9658449292182922),

 ('clients', 0.9587224125862122),

 ('management', 0.9491853713989258),

 ('greeting', 0.9436992406845093),

 ('msy', 0.9382249116897583),

 ('pathetic', 0.9378621578216553),

 ('dropped', 0.9307988286018372),

 ('keeping', 0.9277007579803467),

 ('lack', 0.924517035484314),

 ('telling', 0.9227219223976135)] 
```

这些词大多是直接否定的；那些看似中性或积极的词可能大多出现在在某种意义上否定其普通含义的上下文中。例如，也许“管理”在推文中通常被负面形容词包围。我们还可以利用标签来获取简单表示与标签相关的文本集合的向量，就像我们在下一个单元格中所做的那样。

我们可以从参数空间中的这些向量对其他航空公司的向量进行测量，或者对表达情感的特定文本进行测量，这将说明这些不同向量的相似性。

```
airlines = ('Delta', 'United', 'JetBlue')

delta, united, jetblue = (model.docvecs[x] for x in airlines)

print(f"Delta:\n{delta}\n")

print(f"United:\n{united}\n")

print(f"JetBlue:\n{jetblue}\n") 
```

```
Delta:

[ 5.578579   2.0885715 -5.8722963 -5.2461944  4.862418   6.6500683

  3.054988   2.5725224  3.1206055 -9.660177 ]

United:

[  0.62689006   2.9862213  -10.10382     -7.578535    -0.44318137

   3.9621575    2.9998243   -0.11659689  -2.9283297   -7.8558965 ]

JetBlue:

[ 0.04514389  0.03341183 -0.02691341  0.01708637  0.02028313 -0.03833938

 -0.0415993  -0.04835104 -0.05358113 -0.03369116] 
```

作为每家航空公司的比较，人们在推特上对这些航空公司的评论有多相似？

```
from scipy.spatial.distance import cosine

print(f"Delta  | United  | {cosine(delta, united):.3f}")

print(f"Delta  | JetBlue | {cosine(delta, jetblue):.3f}")

print(f"United | JetBlue | {cosine(united, jetblue):.3f}") 
```

```
Delta  | United  | 0.239

Delta  | JetBlue | 0.930

United | JetBlue | 0.787 
```

我们可以看到`Delta`和`United`在这个分析中非常相似，但是`Delta`和`JetBlue`在向量空间中尽可能地接近。也就是说，零值将意味着相同的“情感”向量，而值 1 将是最大的不同。这是一个继续从抽象的角度思考向量空间的好时机。

## 分解

> 在考虑了熵之后，剩下的只有噪声。
> 
> 大卫·默茨

**概念**:

*   主成分分析和其他分解
*   白粉
*   降维
*   与 t-SNE 和 UMAP 一起可视化

高维数据集——无论是因为初始数据收集还是因为创建额外的合成特征而具有高维度——可能不太适合建模技术。在这些情况下，使用更少的特征在计算上更容易处理，也更容易预测。特征*选择*超出了本书的大部分范围，但在下面关于*多项式特征*的章节中会简要讨论，这是最显著增加合成特征数量的技术。

然而，一种特殊的类型的“特征选择”是特征集的参数空间的**分解**。这些技术预先假定所有的特征已经以某种方式进行了数字编码，也许是通过下面在*一键编码*章节中讨论的技术。分解在某种意义上创建了合成特征，但它真正做的是创建参数空间的新的[正交基](Glossary.xhtml#_idTextAnchor093)(新轴)。如果保持与先前数据集中相同的维数，那么分解中的转换是信息保留和可逆的*。然而，分解的目的通常是执行*维度缩减*。当对多维数据执行分解时，它将数据的熵集中到初始维度，在剩余维度中留下少得多的信息内容；通常，丢弃较高编号的维度对建模度量没有什么损害，或者实际上改进了它们。*

最常见也是最古老的分解技术是主成分分析(PCA ),它是由卡尔·皮尔逊在 1901 年首先开发的。在本节中，我们将主要关注 PCA，但请记住，对于特定于的数据集和域特征的值分布，其他技术可能会更强大。其他一些技术包括非负矩阵分解(NMF)、潜在狄利克雷分配(LDA)、独立分量分析(ICA)和 t 分布随机邻居嵌入(t-SNE)。最后一个列出的技术，t-SNE，是不可逆的，因此不能准确地描述为分解，但它*是*一种通常对可视化有用的降维技术，我们将看一个例子。方便的是，所有这些分解(以及其他)都是由 scikit-learn 提供的；当然，每本书在其他图书馆也可以找到。

## 旋转和白化

作为初始示例，让我们看一个只有两个特征的数据集，并对其进行分解。当我们执行分解时，我们强调“最重要的合成轴”根据定义，PCA 的结果是方差随着每个连续的 PCA 特征而减小。[白化](Glossary.xhtml#_idTextAnchor145)和[球形化](Glossary.xhtml#_idTextAnchor129)是同义词，意为重新缩放这些合成特征。

通过分解，可以对一些过于强烈的特征进行二次去强调。这取决于所用模型的具体类型，但对于许多模型来说，范围从 0 到 100 的数字特征比范围从 0 到 1 的特征更有效，因为它为计算贡献了更大的数字。通常，让模型选择特征的重要性比用特征工程预先判断它们要好。也就是说，分解(或其他特征工程技术)可能会给合成特征一个大于或小于某个其他特征的数值范围，并因此给出相应的默认权重。最好避免这种情况，如下所述。

```
from src.whiten import data, show

# Only two initial features for illustration, 

# but in general we would have a high dimensionality

show(data, "Parameter space for two features", 

     "Raw Feature 1", "Raw Feature 2") 
```

![](img/B17126_07_04.png)

图 7.4:两个特征的参数空间

这里我们有两个特征，它们显然有很强的相关性。特别是，我们注意到沿着大约 45°的对角线的方差比沿着观察到的轴的方差更大。PCA 将重新定向数据，使这个方差轴(即最大熵)成为主要分量。

```
from sklearn.decomposition import PCA

show(PCA().fit_transform(data), 

     "PCA Components", "Synthetic Axis 1", "Synthetic Axis 2") 
```

![](img/B17126_07_05.png)

图 7.5: PCA 组件

我们在第五章 、*数据质量*中更详细地讨论了缩放。我们可以使用那些标准技术来缩放这些“扁平化”的数据，但是这种问题在 PCA 转换中非常普遍，scikit-learn 在一个参数中自动构建了它。这通常使我们无需在转换后第二次重新缩放数据，并且通常是一种更干净的方法。

```
show(PCA(whiten=True).fit_transform(data), 

     "Whitened Components", "Synthetic Axis 1", "Synthetic Axis 2") 
```

![](img/B17126_07_06.png)

图 7.6:白化组件

“白化”的使用非常类似于声学和频谱分析中“白噪声”和“粉红噪声”的区别。这两种噪声都代表了很大范围的频率值，但是“粉红色”过分强调了视觉光谱的红色端。类似地，非白化 PCA 会过分强调一个特定的轴。

## 降维

虽然对标准正交基的改变本身可能有助于机器学习模型，但分解更常见的用途是减少维数，同时仍保留大部分信息。作为一个例子，让我们使用广泛可用的威斯康星州乳腺癌数据集。这可以从 UCI 机器学习库、Kaggle 获得，或者包含在 scikit-learn 和其他数据科学库中。总之，该数据集包含 30 个肿瘤的数字测量值，目标表征为良性或恶性。

它有 569 个观察值，在目标类别之间相对平衡(212 个恶性，357 个良性)。

```
cancer = load_breast_cancer()

X_raw = StandardScaler().fit_transform(cancer.data)

y = cancer.target 
```

如果我们试图使用典型的机器学习模型进行预测，我们可以用一种简单的方法很好地完成*。为了说明这一点，我们执行训练/测试分割，以避免过度拟合用于训练模型的特定数据。这超出了本讨论的直接范围，但是下面的一行代码执行了这个任务。我们还可以使用主成分分析来降低维数，并且对模型质量的影响是有趣的。对于这个讨论，我们将尝试选择仅仅一个主成分，仅仅两个成分，和四个成分，从最初的 30 个特征中导出。我们在每种情况下都进行了白化，以保持维度的比例(这对于 PCA1 的情况通常是没有意义的)。*

```
X_pca1 = PCA(n_components=1, whiten=True).fit_transform(X_raw)

X_pca2 = PCA(n_components=2, whiten=True).fit_transform(X_raw)

X_pca4 = PCA(n_components=4, whiten=True).fit_transform(X_raw) 
```

使用我们的三个候选特征矩阵，让我们看看相应的 K-邻居模型表现如何。

```
for X in (X_raw, X_pca1, X_pca2, X_pca4):

    X_train, X_test, y_train, y_test = (

        train_test_split(X, y, random_state=1))

    model = KNeighborsClassifier().fit(X_train, y_train)

    accuracy = model.score(X_test, y_test)

    error_rate = 100*(1-accuracy)

    print(f"Features | {X.shape=}\t| {error_rate=:.2f}%") 
```

```
Features | X.shape=(569, 30) | error_rate=4.90%

Features | X.shape=(569, 1)  | error_rate=9.79%

Features | X.shape=(569, 2)  | error_rate=6.99%

Features | X.shape=(569, 4)  | error_rate=4.20% 
```

原始数据上 4.90%的误差率不是太不合理。无论如何，让我们把它当作一个基线。只有一个主成分，错误率跃升到 9.79%；考虑到我们丢弃了多少信息，这是非常好的，并且比我们利用任何单一的原始特征都要好。如果我们保留两个主成分，错误率下降到 6.99%，这是一个合理的中间值。

然而，有趣的是，使用四个主成分，我们实际上获得了比使用完整原始数据稍好的错误率，为 4.20%。本质上，在考虑了数据中的大部分熵之后，剩下的只是随机噪声。

“熵”和“噪音”之间的对比虽然准确，但也是一种戏谑的措辞。在许多上下文中，熵和噪声被视为同义词，尽管“信息内容”实际上更接近熵的含义。但根本的一点是，观察中的一些可变性是由于潜在的自然(或人为)现象，而一些可变性完全是由于对有限总体进行抽样的随机变化。通过分解进行降维有从噪声中挑选出信号的趋势。我要指出的是，这里仍然存在尝试和错误；例如，选择五个或六个组件而不是四个组件再次变得比原始数据差(在这个精确的模型算法上，用这个精确的训练/测试分割，用这些精确的超参数，等等)。

让我们回到 PCA 作为一种转化到底做了什么。它简单地确定每个原始维度的乘数，以线性地导出主成分。例如，在乳腺癌数据集中，每个观察值是 30 个数字的向量。这些数字中的每一个都乘以某个常数，这 30 个乘积加在一起就构成了分量 1。同样，对于组件 2，具有不同的乘数。让我们为`n_components=3`创建一个这些乘数的表格来说明。

```
pca3 = PCA(n_components=3).fit(X_raw)

pd.DataFrame(pca3.components_.T, 

             index=cancer.feature_names, 

             columns=['pca_1', 'pca_2', 'pca_3']) 
```

```
 pca_1         pca_2         pca_3

—————————————————————————————————————————————————————————————————

            mean radius     0.218902     -0.233857     -0.008531

           mean texture     0.103725     -0.059706      0.064550

         mean perimeter     0.227537     -0.215181     -0.009314

              mean area     0.220995     -0.231077      0.028700

                    ...          ...           ...           ...

        worst concavity     0.228768      0.097964     -0.173057

   worst concave points     0.250886     -0.008257     -0.170344

         worst symmetry     0.122905      0.141883     -0.271313

worst fractal dimension     0.131784      0.275339     -0.232791

30 rows × 3 columns 
```

换句话说，我们可以使用拟合的 PCA 对象的`.transform()`方法，但是我们可以等效地仅用在普通的 NumPy 中执行相同的计算。

```
row0_sk = pca3.transform(X_raw)[0]

row0_np = (pca3.components_ * X_raw[0]).sum(axis=1)

print(f"Row 0 as transform: {row0_sk}")

print(f"Row 0 as mul/sum:   {row0_np}") 
```

```
Row 0 as transform: [ 9.19283683  1.94858307 -1.12316599]

Row 0 as mul/sum:   [ 9.19283683  1.94858307 -1.12316599] 
```

## 形象化

对于不同的目的，利用不同的分解可能是有用的。然而，在大多数情况下，主成分分析仍然是您应该尝试的第一项技术。一个特殊的用途是当我们想要生成高维参数空间到我们实际上可以在空间上表示的二维或三维的有用的可视化。**T-分布式随机邻居嵌入** ( **t-SNE** )是一种非线性降维技术，用于将高维数据投影到二维或三维。相似的对象由附近的点建模，而不相似的对象由远处的点建模，这种可能性很高。

作为这种可视化技术的一个例子，让我们看一个由 1，797 个手写数字扫描成的 8×8 灰度像素的集合。这个集合是在 UCI 机器学习库中发布的集合之一，并与 scikit-learn 一起分发。这相当于各种像素值的 64 维参数空间。相对简单的模型，如逻辑回归，可以在该数据集上获得预测[准确性](Glossary.xhtml#_idTextAnchor013)的良好结果；卷积神经网络甚至做得更好。让我们看几个样本扫描并导入底层数据。

```
digits = get_digits() 
```

![](img/B17126_07_07.png)

图 7.7:样本数字

我们可以尝试使用 PCA 来简化这个 64 维的参数空间。这确实会给我们一个二维的可视化，显示在这个投影参数空间中数字的合理区分。例如，肯定有一个区域朝向下图的顶部中心，该区域由数字 0 支配。与此同时，出现数字的区域之间有很强的重叠，并且有些松散的分化。

```
pca_digits = PCA(n_components=2).fit_transform(digits.data)

plot_digits(pca_digits, digits, "PCA") 
```

![](img/B17126_07_08.png)

图 7.8:数字空间的 PCA 分解

PCA 降维的尺度单位，以及其他技术的尺度以下的单位，没有具体的数字意义。它们只是算法的人工产物，但产生的差别数字可以被绘制出来，或用于统计或建模。

相比之下，使用 t-SNE 我们获得了更强的可视化结果。相应地，基于这个投影的建模将提供更多的工作。在这个例子中，t-SNE 加逻辑回归在整个特征空间上并不比逻辑回归表现得更好，但是在使用少得多的基础数据来表示每个观察值的情况下，也不会差太多。例如，中间左侧的 0 数字簇非常强，与其他数字之间有很大的差距。其他一些人就没那么好分开了，但是以我们期望的方式分开了；例如，以某种方式画出的 9 与 3 非常相似。

```
tsne_digits = TSNE(random_state=1).fit_transform(digits.data)

plot_digits(tsne_digits, digits, "t-SNE") 
```

![](img/B17126_07_09.png)

图 7.9: t-SNE 分解

用于降维的一致流形近似和投影是另一种技术，与 t-SNE 有着相似的动机(但数学上非常不同)。UMAP 通常有额外的优势。具体来说，UMAP 大致保持了星团之间的距离*——不仅仅是一个星团内*观测值的接近程度——而 t-SNE 则根本没有试图这么做。在这个特殊的扫描数字的例子中，UMAP 产生了比 t-SNE 还要紧密的聚类。**

事实上，这些簇足够紧密，以至于很难或不可能区分每个簇中许多重叠的数字。

```
from umap import UMAP

umap_digits = UMAP(random_state=1).fit_transform(digits.data)

plot_digits(umap_digits, digits, "UMAP") 
```

![](img/B17126_07_10.png)

图 7.10: UMAP 分解

当然，在使用分解技术生成合成特征时，您并不局限于只使用那些特征。根据您的具体需求，可以利用最上面的几个分解维度，但也可以将这些维度添加到具有一些原始原始要素、一次性编码要素、多项式要素或其他类型的合成数据的相同中间数据集中。数据集的这种特定于任务的构造很可能对您面前的特定目的最有效。显然，需要大量的直觉、一些推理和大量的尝试和错误，才能得到最佳的数据。

让我们看看如何将连续的测量数据转换成有序的数据，这通常可以提高模型的功效。

# 量化和二值化

> 在考虑了熵之后，剩下的只有噪声。
> 
> 大卫·默茨

**概念**:

*   降低粒度
*   平衡箱尺寸
*   设置阈值

有时候，连续数据(甚至是简单的顺序数据)用少量的级别来表示会更有用。在极限情况下，我们可以将一个数值范围减少到只有两个值:真/假或 1/0，但是其他值也可以。在这个极限处，*量化*被称为*二值化*。当所表示的数据比真正有意义的数据更精确时，使用量化变换通常是有用的，无论是从测量精度的角度，还是从我们的数据科学任务的实用性的角度。

作为本节和下一节的一个简单示例，我将使用我在一次会议上提供的关于 scikit-learn 的半天教程中对学生进行的调查结果。我有时在其他培训中使用相同的数据作为执行机器学习的快速数据集。这里介绍的内容删除了一些功能，但保留了对这些部分有用的功能。像所有的数据一样，这个数据集是杂乱的；做了一些清理工作，但是故意避开了一些元素，给你提供了一个真实世界的混乱(但不是太脏而无用)。

```
survey = pd.read_csv('data/ML-survey.csv')

survey.sample(6, random_state=1) 
```

```
 Language   Experience     Age  Post_Secondary     Success

————————————————————————————————————————————————————————————————

 95        C++          1.0      57              12           7

 44     Python          7.0      24              11           5

 56          R          2.0      46               9          10

 97     Python          2.0      23               3           5

 69     Python          5.0      53               4           8

114     Python         25.0      76              23           1 
```

这个数据足够简单。我们收集了一些关于辅导参与者的传记资料，并要求他们用 1-10 分来评价该辅导有多成功。一点点领域知识会告诉你，像这样的评级，回应的分布是高度倾斜的。本质上，9 或 10 是强阳性，任何小于或等于 7 的都是阴性。8 分的回答是适度肯定的。也许有人希望在整个范围内有更多的一致性，但人类心理学和围绕如何回应这种评价的社会压力的历史使这成为现实。这些数据遵循这种熟悉的模式。

```
(survey

    .Success

    .value_counts()

    .sort_index()

    .plot(kind='bar', title="Distribution of Ratings")); 
```

![](img/B17126_07_11.png)

图 7.11:评级分布

给定数据的分布、已知的评级心理和规定的分析目的，我们希望将评级成功简单地视为二进制值。这可以在 Pandas 中非常容易地完成——或者在所有其他数据框库中几乎完全相同——通过简单的比较创建一个布尔数组。

这些布尔数组经常被用作过滤器或掩码，但是它们同样可以直接提供非常好的值。

```
survey.Success >= 8 
```

```
0       True

1       True

2       True

3      False

       ...  

112    False

113     True

114    False

115     True

Name: Success, Length: 116, dtype: bool 
```

如果您在 NumPy 或其他库中使用原始数组，您可能希望使用 scikit-learn 类`Binarizer`。这个实用程序总是期望一个二维矩阵作为输入，但是一个只有一列的矩阵是完全可以接受的。

```
from sklearn.preprocessing import Binarizer

# Set threshold anywhere *between* 7 and 8

binary_rating = Binarizer(threshold=7.5)

# Pass 2-D DataFrame, not Series

success = binary_rating.fit_transform(survey[['Success']])

# Maintaining versions is good practice

survey2 = survey.copy()

survey2['Success'] = success

survey2 
```

```
 Language  Experience  Age  Post_Secondary  Success

——————————————————————————————————————————————————————————

  0      Python        20.0   53              13        1

  1      Python         4.0   33               7        1

  2      Python         1.0   31              10        1

  3      Python        12.0   60              12        0

...         ...         ...  ...             ...      ...

112      Python         4.0   35               4        0

113      Python         3.0   44               6        1

114      Python        25.0   76              23        0

115      Python        25.0   75              12        1

116 rows × 5 columns 
```

二进制值非常适合于`Success`度量。对于其他的专栏，我们希望对它们有所不同。让我们来看看参加高等教育的人数是如何分配的；我们将稍微不同地对待它。

```
(survey2

    .Post_Secondary

    .plot(kind="hist", bins=20,

          title="Distribution of Education")); 
```

![](img/B17126_07_12.png)

图 7.12:教育分布

数据中有两个明显的异常值。一名受访者声称接受了 23 年的大专教育。同一应答者碰巧在上面第 114 行可见，并且该应答者报告为 76 岁。鉴于调查的意图和描述是沿着将博士学位或同等专业学位注明为 10 年的路线，23 年有些可疑；可能这个人还拥有博士、医学博士和法学博士学位，但更有可能的是在交流意图时出现了一些故障，或者输入错误。尽管如此，对于我们的宁滨，我们将只规定，该人将进入最教育的类别。

第二个异常值是-12，这只是一个无意义的值。在任何情况下，其意图都是没有大学教育会被记为零，而不是通过减去直到大学的几年。也许一个三年级学生参加了，并认为这是最好的描述。或者，更有可能是数据输入错误。在这种情况下，我们将简单地将其编码为教育程度最低的类别。出于另一个目的，您可能会考虑在第 6 章 、*值插补*中讨论的技术来处理非法值。我们将只存储与“最低教育程度”、“中等教育程度”和“最高教育程度”相对应的值，而不是保留教育的确切年限，这些值的数字编码为 0、1 和 2。我们的项目文档应该描述这种映射。

为了根据教育程度将数据分成大小大致相等的箱，我们可以使用 scikit-learn 类`KBinsDiscretizer`。和 scikit-learn API 中的其他地方一样，我们首先创建一个参数化的类的实例，然后执行一个`.fit_transform()`来转换数据。

```
from sklearn.preprocessing import KBinsDiscretizer

# Create a binner with 3 balanced bins

edu_bin = KBinsDiscretizer(n_bins=3, 

                           encode='ordinal', 

                           strategy='quantile')

# Bin the Post_Secondary column

level = edu_bin.fit_transform(survey2[['Post_Secondary']])

# In this version, rename the binned field "Education"

survey3 = survey2.copy()

survey3['Education'] = level.astype(np.uint8)

survey3.drop('Post_Secondary', axis=1, inplace=True)

survey3.sample(8, random_state=2) 
```

```
 Language   Experience  Age  Success  Education

—————————————————————————————————————————————————————

24      Python          3.0   28        1          0

89      Python         12.0   46        0          2

28      Python          3.0   31        1          1

56           R          2.0   46        1          2

 2      Python          1.0   31        1          2

53      Python         10.0    3        1          2

45      Python          1.0   31        0          2

79  JavaScript          1.0   32        1          1 
```

我们通常可以看到`Education`值如预期的那样是 0、1 或 2。我们可以更详细地查看选择了哪些临界值，以及每个类别中有多少回答者。

请注意，尽管我将这些描述为类别(并期望一个记录了键的映射)，但它们显然是有序排列的，而不是分类的。

```
print("Education cut-offs:")

print(edu_bin.bin_edges_[0], '\n')

print("Count per bin:")

print(survey3.Education.value_counts()) 
```

```
Education cut-offs:

[-12\.           4.33333333   7\.          23\.        ] 

Count per bin:

2    44

0    39

1    33

Name: Education, dtype: int64 
```

对于教育，我们允许公用事业公司决定平衡垃圾箱的分界点。然而，也许我们更愿意为某个特定的特性划分成固定的数值范围。让我们对`Experience`值尝试这种方法(意在反映多年的编程经验)。`KBinsDiscretizer`可以简单地用不同的参数实例化来实现这一点。同样，我们需要记录数字 0、1、2、3 和 4 用于表示经验范围，而不是原始年份；但是，这里我们在新的数据集版本中保留了相同的列名。

```
# Create a binner with 5 bins of same numeric range

exp = KBinsDiscretizer(n_bins=5, 

                       encode='ordinal', 

                       strategy='uniform')

# Bin the Experience column

exp_level = exp.fit_transform(survey3[['Experience']])

# Retain the Experience name, but new meaning

survey4 = survey3.copy()

survey4['Experience'] = exp_level.astype(np.uint8)

survey4.sample(8, random_state=3) 
```

```
 Language   Experience  Age  Success  Education

—————————————————————————————————————————————————————

 83     MATLAB            1   37        0          2

  5     Python            0   32        1          0

  6     Python            0   34        0          2

 42     MATLAB            0   31        0          2

100     Python            0   47        0          2

 97     Python            0   23        0          0

 40     Python            1   33        1          2

 25          R            0   36        1          0 
```

对宁滨使用“统一”策略的结果产生了强烈不平衡的箱。然而，在许多情况下，这是完全合理的，包括最有可能的这种情况。在我们更广泛的目标中，将经验的数量四舍五入到大约 5 年的倍数可能是一个很好的简化。如果我们适当地改变这种技术，我们可以精确地在 5 年内进行这些削减，但是这个范围大约是通过定期削减数据本身获得的。

```
print("Experience cut-offs:")

print(exp.bin_edges_[0], '\n')

print("Count per bin:")

print(survey4

        .Experience.value_counts()

        .sort_index()) 
```

```
Experience cut-offs:

[ 0\.   5.4 10.8 16.2 21.6 27\. ] 

Count per bin:

0    93

1    14

2     4

3     1

4     4

Name: Experience, dtype: int64 
```

在之前的每个量化中，我们将值编码为序数。然而，另一种方法有时更好。我们可能会认为一个值的不同数值范围是完全不同的，而不是序数或纯粹的数量。教育是按顺序处理的，因为增长是不均衡的。但是经验仅仅是连续的但却是量子化的。在这种情况下，近似地说，恢复原始测量值只是将每个值乘以 5.4。

对于某些测量，不同的值可能反映不同的域或制度。对于这些，我们可以使用一键编码，这将在下一节讨论，但也可以作为参数`KBinsDiscretizer`使用。

尽管我认为这个例子有些异想天开，但让我们规定“年轻的”、“中年的”和“老年的”教程参与者是我们希望区分的完全不同的类型(你的作者将被归入最后一类)。然而，在此之前，我们必须处理一个数据质量问题。一些年龄值看起来很可疑。

```
survey4.Age.describe()[['mean', 'min', 'max']] 
```

```
mean    36.965517

min      3.000000

max     99.000000

Name: Age, dtype: float64 
```

```
survey[survey.Age < 10] 
```

```
 Language  Experience Age Post_Secondary  Success

————————————————————————————————————————————————————

53   Python        10.0   3              9        9

85   Python         3.0   3             10        6 
```

可以想象，教程中有一位 99 岁的老人，但肯定没有 3 岁的参与者。虽然 99 岁可能是不准确的输入，但从数据本身来看，3 岁的孩子是错误的，因为他们的编程经验和大专教育都超过了他们的年龄。我将假设这些是犯了数据输入错误的 30 多岁的与会者，并将他们的年龄都归入 35 岁(与所有与会者的中值或平均年龄相差不远，我也可以合理地使用这个值)。

```
# Create next version and impute for bad data

survey5 = survey4.copy()

survey5.loc[survey5.Age == 3, 'Age'] = 35

# Create a binner with 3 bins to 3 columns

# Note: a sparse array with "onehot"

age_bin = KBinsDiscretizer(n_bins=3, 

                           encode='onehot-dense', 

                           strategy='quantile')

# Bin and split the Age column

age = age_bin.fit_transform(survey5[['Age']])

age = age.astype(np.uint8).T

survey5 = survey5.assign(Young=age[0], 

                         Mid_Age=age[1], 

                         Old=age[2])

survey5.drop('Age', axis=1, inplace=True)

survey5.sample(8, random_state=4) 
```

```
 Language  Experience  Success  Education  Young  Mid_Age  Old

————————————————————————————————————————————————————————————————————

13      Python           0        0          2      0        1    0

 2      Python           0        1          2      0        1    0

25           R           0        1          0      0        1    0

16      Python           0        1          0      0        0    1

19      Python           0        0          1      0        0    1

79  JavaScript           0        1          1      0        1    0

 5      Python           0        1          0      0        1    0

24      Python           0        1          0      1        0    0 
```

已经使用一次性编码为年龄范围创建了合成的列，这是转到下一节讨论一次性编码的好时机。我们将继续使用这个调查数据集，我们已经分阶段对其进行了调整和改造。

# 一键编码

> 如果一个人一旦沉溺于谋杀，他很快就会想到抢劫；从抢劫，他接下来喝酒和安息日，从这到粗鲁和拖延。一旦开始走上这条下坡路，你永远不知道在哪里停下来。许多人把自己的毁灭归咎于某种谋杀或其他他当时可能很少想到的事情。
> 
> 托马斯·德·昆西

**概念**:

*   避免人工排序
*   合成布尔特征

很常见的是，我们使用的特性中包含了大量的类值。对于许多模型或其他统计技术，我们需要将特征编码为数字。一种简单的方法是将值编码为数字序数。例如，在调查数据中，我们*可以*通过映射 Python=1、R=2、JavaScript=3 等等来编码语言特性。虽然这些值是数字，但如果我们不对类别进行人工排序，我们通常会获得更好的质量。不同的编程语言之间没有内在的或明显的顺序。

类值的编码可能不是有意义的字符串，但可能已经使用了一系列小整数。这可能会错误地暗示变量的平凡性。我们应该查阅文档和领域知识，以确定这是否是对特定特性的合理解释。当然，对称地说，字符串有时确实清楚地编码了序数值；例如对某物的评估中的“差”、“好”、“最好”(有意义的顺序不可能是那些字符串的“自然”顺序，例如字母顺序)。

为了用多个值对一个特征进行编码，我们可以将其转换成多个特征，每个特征对应一个类值。该编码名称中的“one-hot”表示这些新特性中只有一个是 1，其他的都是 0(或者是真/假，取决于您的编程语言和库)。调查数据集的“最喜欢的编程语言”列是一次性编码的理想选择。

在 Pandas 中，`get_dummies()`函数将数据帧转换成独热编码。在 scikit-learn 中，类`OneHotEncoder`执行相同的任务，但不限于与熊猫一起工作(任何类似 matrix 的工作)。在这两个 API 中，您有各种选项来提供新特性的命名，是使用密集数组还是稀疏数组进行存储，是否省略一个类别(以减少多重共线性)，以及在 Pandas 中对哪些列进行编码。默认情况下，Pandas 将查找所有字符串或分类列，但是您可以调整它；例如，您可能希望强制对整数列进行一键编码。

为了便于演示，我们显示了一个转置的数据帧，其中包含几行样本和作为行的编码语言特征。我们可以看到大多数样本(这里显示为列)在`Lang_Python`特性中有 1，在其他特性中有 0。

一些样品在不同的特征中具有它们的“一个热点”。

```
survey6 = pd.get_dummies(survey5, prefix="Lang")

survey6.sample(10, random_state=3).T.tail(8) 
```

```
 83  5  6  42  100  97  40  25  115  103

——————————————————————————————————————————————————————————

       Lang_C++    0  0  0   0    0   0   0   0    0    0

Lang_JavaScript    0  0  0   0    0   0   0   0    0    0

    Lang_MATLAB    1  0  0   1    0   0   0   0    0    0

    Lang_Python    0  1  1   0    1   1   1   0    1    0

         Lang_R    0  0  0   0    0   0   0   1    0    1

     Lang_Scala    0  0  0   0    0   0   0   0    0    0

        Lang_VB    0  0  0   0    0   0   0   0    0    0

Lang_Whitespace    0  0  0   0    0   0   0   0    0    0 
```

sci kit-learn API类似于我们看过的其他变形金刚。我们简单地创建一个参数化的实例，然后用它来拟合和/或转换数据。在这个 API 下，元数据(如建议的特性名称)存在于编码对象中，原始编码数据是一个普通的 NumPy 数组。

```
from sklearn.preprocessing import OneHotEncoder

lang = survey5[['Language']]

enc = OneHotEncoder(sparse=False).fit(lang)

one_hot = enc.transform(lang)

print(enc.get_feature_names())

print("\nA few encoded rows:")

print(one_hot[80:90]) 
```

```
['x0_C++' 'x0_JavaScript' 'x0_MATLAB' 'x0_Python' 'x0_R' 'x0_Scala'

 'x0_VB' 'x0_Whitespace']

A few encoded rows:

[[0\. 0\. 0\. 1\. 0\. 0\. 0\. 0.]

 [0\. 0\. 0\. 0\. 0\. 0\. 0\. 1.]

 [0\. 0\. 0\. 1\. 0\. 0\. 0\. 0.]

 [0\. 0\. 1\. 0\. 0\. 0\. 0\. 0.]

 [0\. 0\. 0\. 1\. 0\. 0\. 0\. 0.]

 [0\. 0\. 0\. 1\. 0\. 0\. 0\. 0.]

 [0\. 0\. 0\. 1\. 0\. 0\. 0\. 0.]

 [0\. 0\. 0\. 1\. 0\. 0\. 0\. 0.]

 [0\. 0\. 0\. 0\. 1\. 0\. 0\. 0.]

 [0\. 0\. 0\. 1\. 0\. 0\. 0\. 0.]] 
```

通过我们的转换，我们得到了一个具有更多特性的数据集，但是这些特性更适合我们的下游目的。让我们只看一行，因为数据帧变得太宽，在这个空间中不容易显示。在这个例子中，我们执行的特定编码都给出了小的非负整数，但这可以很容易地与其他连续数字变量结合，也许与那些缩放到与这些小数字相似的数值范围的变量结合。

```
with show_more_rows():

    print(survey6.loc[0]) 
```

```
Experience         3

Success            1

Education          2

Young              0

Mid_Age            0

Old                1

Lang_C++           0

Lang_JavaScript    0

Lang_MATLAB        0

Lang_Python        1

Lang_R             0

Lang_Scala         0

Lang_VB            0

Lang_Whitespace    0

Name: 0, dtype: int64 
```

一键编码是通过合成特征增加维数的一小步。接下来，我们转向具有多项式特性的真正的巨大飞跃。

# 多项式特征

> 归根结底，一幅画不再是一幅画，不管它的执行是多么的自给自足。它是一个符号，投射的假想线越深刻地符合更高维度越好。
> 
> 保罗·克利

**概念**:

*   生成合成要素
*   维度的诅咒
*   特征选择

生成多项式要素可以创建大量新的合成要素。这种转换背后的基本思想很简单:我们添加新的功能，这些功能是多达 N 个现有功能的乘积。在我们将在本节使用的 scikit-learn 版本中，`PolynomialFeatures`对所有参数组合进行乘法运算(直到指定的次数)。当然，手动创建乘法或其他功能组合是非常容易的。在其他 scikit-learn 类的用户所熟悉的一个 API 中，`PolynomialFeatures`完成了对所有组合的识别，并为通用 transformer 对象提供了有用的元数据。

构造多项式特征通常是我们随后需要使用特征选择来筛选特征的主要原因。例如，将 30 个原始特征减少到 15 个，对大多数模型来说不太可能非常重要。但是在下面的例子中减少 496 个合成特征对于模型的能力和所使用的计算资源都变得很重要。如果我们构建大量的合成多项式特征，特征选择的必要性就会变得更强。多项式展开与特征选择的结合产生比原始特征更强的模型是很常见的。

Scikit-learn 提供了关于是否创建正方形(或立方体等)的细节。)的单个特征，这在大多数时候在整个数据管道中并不是非常重要。我总的感觉包括那些条款没有坏处，偶尔也有好处。如果不使用`interactions_only`选项，生成的特征数量为:

![](img/B17126_07_009.png)

例如，对于 30 个原始维度，我们获得 496 个多项式特征，次数为 2；对于 100 个原始特征，我们得到 5151 个。在本节中，我们返回到本章前面使用的威斯康星州乳腺癌数据集。回想一下，它有 30 个数字特征(和一个二元目标)。

```
cancer = load_breast_cancer()

X_raw = MinMaxScaler().fit_transform(cancer.data)

y = cancer.target 
```

## 生成合成要素

创建多项式特征只是另一种变换，很像本书中提到的 scikit-learn 中的所有其他变换。在本节中，我们只关注 2 次多项式，但为了说明合成要素的增长，在下面的循环中创建了几个次数。我们创建了一个转换器字典和另一个结果 X 数组。在生成它们的同时，让我们展示这些合成特征的维度有多高。

```
poly = dict()

X_poly = dict()

print(f"Raw data set shape:  {cancer.data.shape}")

for n in [2, 3, 4, 5]:

    poly[n] = PolynomialFeatures(n)

    X_poly[n] = poly[n].fit_transform(X_raw)

    print(f"Degree {n} polynomial: {X_poly[n].shape}") 
```

```
Raw data set shape:  (569, 30)

Degree 2 polynomial: (569, 496)

Degree 3 polynomial: (569, 5456)

Degree 4 polynomial: (569, 46376)

Degree 5 polynomial: (569, 324632) 
```

数万或数十万个特征对于良好的建模或分析来说实在是太多了。甚至二阶多项式中的 496 个特征在实际应用中也有点不可靠。度 2 可能不会淹没内存(显然这取决于行数；这个例子很小)，但它几乎肯定会导致[维数灾难](Glossary.xhtml#_idTextAnchor032)和模型无效。

让我们看看这些合成特征包含什么，以及它们是如何命名的。由于我们已经将原始特征缩放到区间[0，1]中，乘法组合将保持在该范围内。我们*可以*再次缩放多项式数据以重新规范化，但这在这种情况下并不重要。

当然，我们可以随意命名这些合成特征；但是`PolynomialFeatures`基于原始的特性名称提供了一组方便的建议。

```
names = poly[2].get_feature_names(cancer.feature_names)

row0 = pd.Series(X_poly[2][0], index=names)

row0.sample(8, random_state=6) 
```

```
mean compactness^2                       0.627323

radius error worst perimeter             0.238017

smoothness error worst concavity         0.090577

mean compactness worst concavity         0.450361

perimeter error                          0.369034

area error fractal dimension error       0.050119

radius error concavity error             0.048323

mean fractal dimension symmetry error    0.188707

dtype: float64 
```

我选择了一个特定的随机状态，它获得了一组有代表性的特性名称。特别地，一些特征被命名为原始特征的幂，例如`mean compactness^2`。其他的就是单纯的 raw 特征本身，比如`perimeter error`。大多数合成特征是两个原始特征的乘积，如`smoothness error worst concavity`或`mean compactness worst concavity`。在概念上，代表特征比率的合成特征也可能是有价值的，但是它们不会自动产生。对于由多个单词组成的特性名称，从美学角度来看，使用星号或逗号这样的分隔符比使用空格更可取，但是在任何情况下，后面这些名称所表示的都是乘法。

当然，对于高阶多项式，特征的名称也变得更加复杂。组合多达四种原始特征的不同组合，包括作为可能项的单个原始尺寸的幂。

```
names = poly[4].get_feature_names(cancer.feature_names)

row0 = pd.Series(X_poly[4][0], index=names)

row0.sample(6, random_state=2) 
```

```
mean texture mean symmetry concavity error worst fractal dimension      0.000884

mean texture mean perimeter mean smoothness                             0.007345

mean concave points compactness error worst perimeter^2                 0.114747

fractal dimension error worst radius worst perimeter worst symmetry     0.045447

mean compactness mean fractal dimension worst area worst compactness    0.133861

mean area worst compactness worst concave points^2                      0.187367

dtype: float64 
```

***

r 主要使同样容易生成多项式特征。一个*公式*是一个很好的 R 语法，它使生成所有交互项变得简洁。然而，在公式中包含原始项的幂变得有些麻烦。这是可能的，但是一个支持功能有助于把它做好。使用简单得多的数据集，下面的代码从 tibble 生成 3 次多项式要素。实际的逻辑与更高维的 X 没有什么不同，只是这个小例子的显示更清晰。

```
%%R 

X <- tibble(A = c(0.2, 0.3, 0.4), 

            B = c(0.1, -0.3, 0.5),

            C = c(-0.2, 0.3, 0.1))

formula = ~ .^3 

poly2 <- as.tibble(model.matrix(formula, data=X))

poly2 
```

```
# A tibble: 3 x 8

  '(Intercept)'     A     B     C 'A:B' 'A:C' 'B:C' 'A:B:C'

          <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl>

1             1   0.2   0.1  -0.2  0.02 -0.04 -0.02  -0.004

2             1   0.3  -0.3   0.3 -0.09  0.09 -0.09  -0.027

3             1   0.4   0.5   0.1  0.2   0.04  0.05   0.02 
```

这个例子表示三行数据，每行包含三个原始特征中的每一个的，三个原始特征中的每一个的成对乘积，以及所有三个原始特征的乘积。

## 特征选择

简单地说*拥有*大量的合成特征还没有很大的用处，因为为了利用它们，我们可能必须首先丢弃它们中的大部分。“维数灾难”可以指高维数据的几个相关问题。

本质上，随着参数维数变得太大，模型有效性和统计意义会变得更差。

一个非常粗略的经验法则是，列数不应超过行数的十分之一。这个比率取决于所使用的模型的种类，但是不同的选择强加了比经验法则更严格的要求，经验法则最好被视为下限。此外，即使对于可能有数百万个观察值的数据集，接近几百个维度的最大值也应该是一个目标。

在深度神经网络中，这是一种特殊的机器学习设计，您有时会遇到比经验法则建议的维度更高的输入层。然而，即使在那里，这种网络的初始层几乎总是用来降低维度。实际上，网络*在训练中学习*如何进行特征选择。神经网络的隐藏层通常有数百个神经元，但很少有数千个。通常，即使是有许多层的深层网络，每层的神经元也不到数百个。

这就是*特征选择*的用武之地。我们需要确定我们众多的(大部分是合成的)特征中哪些真正有助于我们的模型，哪些只是增加了噪音。为了进行比较，让我们尝试在各种变换下对乳腺癌数据进行建模。在这种方法中，我们还引入了只选择“最佳”特性。

我们可以使用多种方法来选择最佳特性。其中最简单的是对每个特征本身的预测强度进行单变量建模。这是由`SelectKBest`在 scikit-learn 中执行的操作。在存在大量功能的情况下，这有时是一种合理的方法。然而，一种更强大的技术是基于特定的模型对象(即一个类和一组超参数)递归地消除特征。

在 scikit-learn 中，`RFE`和`RFECV`执行递归特征消除。后一类更精确，也慢得多。类名缩写为“递归特征消除和交叉验证”。平原 RFE 已经重复训练了具有递减特征数量的模型(例如，为 2 次多项式乳腺癌数据训练了 496 个模型)。RFECV 更进一步，在几个不同的训练/测试分割下使用特征重要性，并选择多元顺序。默认情况下，这是五个折叠，因此对于所考虑的每一数量的特征有五个模型(例如，为二次多项式训练的 2，480 个模型)。子采样下的稳健性为评估提供了相当强的信心。

在 R 中，[插入符号](Glossary.xhtml#_idTextAnchor020)包包含一对函数`rfe()`和`rfeControl()`来执行递归特征消除，可选地进行交叉验证。

需要记住的一个限制是，并非所有类型的模型都提供特性重要性的排序。例如，这个概念在我们用来说明降维分解的 K-neighbors 模型中是不相关的。线性模型提供了系数，这些系数充分等同于特征重要性，它们也用于递归特征消除。在我们没有明确特征重要性的那些模型中，仍然可以进行单变量特征选择，并尝试在单变量相关性中最强的各种数量的特征。当然有可能——甚至很有可能——通过这种方式，一个精简的特性集将实现一个更好的指标。在这种情况下，我们只是缺少支持搜索的脚手架。

让我们看一个展示特征重要性的模型类型，并且递归地从我们的 2 次多项式合成数据集中的 496 个特征中消除特征。我们为模型设置了多个超参数，如果使用不同的超参数，具体的特征选择和度量评估会有所不同。最后的几个参数只是控制执行上下文，对模型算法本身并不重要(即使用多个 CPU 内核或在特定的随机状态下初始化)。

```
model = RandomForestClassifier(n_estimators=100, max_depth=5, 

                               n_jobs=4, random_state=2) 
```

接下来的几行代码有很多需要理解的地方。我们创建了一个 RFECV 类的实例，该实例用我们希望重复训练的特定估计器进行了参数化。在这种情况下，它是一个随机森林分类器(具有特定的超参数)，但任何揭示特征重要性的模型都同样适用。然后，我们多次拟合合并的模型，既减少了特征的数量，也排除了交叉验证的折叠。关于每一个拟合和隐式评分操作的数据都存储在 RFECV 实例的属性中，供以后检查使用。

保留的最重要的属性是*支持*，这是一个数组，指示哪些特性包含在最优子集中，哪些不包含。我们可以使用该属性来过滤更大的初始矩阵，以便只包含那些被证明包含比排除更有用的列。在该代码中保存为`X_support`；我们观察它的形状，发现我们减少了特征。

```
rfecv = RFECV(estimator=model, n_jobs=-1)

best_feat = rfecv.fit(X_poly[2], y)

X_support = X_poly[2][:, best_feat.support_]

X_support.shape 
```

```
(569, 337) 
```

这里我们可以比较几个不同的候选特征集的质量。我们先拟合原始数据，然后拟合完整的多项式数据，最后拟合通过特征消除的多项式数据的列子集。每一次，一个新的模型被安装，然后根据测试数据进行评分。请注意，我们使用 RFECV 中的整个数据集来确定最佳 N(在我们的例子中为 337)，但是我们用于评分的训练模型只能访问训练行，以确保这不是简单的过度拟合。

```
for X in (X_raw, X_poly[2], X_support):

    X_train, X_test, y_train, y_test = (

        train_test_split(X, y, random_state=42))

    model.fit(X_train, y_train) 

    accuracy = model.score(X_test, y_test)

    error_rate = 100*(1-accuracy)

    print(f"Features | {X.shape=}\t| {error_rate=:.2f}%") 
```

```
Features | X.shape=(569, 30)  | error_rate=2.80%

Features | X.shape=(569, 496) | error_rate=1.40%

Features | X.shape=(569, 337) | error_rate=0.70% 
```

这些不同方法所达到的错误率很有启发性。即使有原始特征，我们在这里使用的随机森林模型也优于本章前面使用的 K-neighbors。与此更相关的是，通过使用多项式特性，我们看到错误率大大提高；当我们筛选出那些更有预测性的特征时，我们会看到一个更好的错误率。在某些情况下，我们将选择比开始时少一个数量级的特征，以获得更好的度量结果。这里只是适度减少了特征的数量；重要的是精度因此得到提高。

RFECV 选择创建的另一个有用的属性是网格分数。这些是在每个特征被连续消除后获得的指标分数。或者更准确地说，它是每个折叠下的分数的平均值，不包括来自训练的一部分数据。无论如何，我们在这里看到一个典型的模式。对于极少数特征，准确度较低。对于一个适中的数字，它实现了几乎最好的度量。在大量不同的功能计数中，该指标大致处于平稳状态。沿着平台选择这些初始特征中的任意 N 个将提供类似的度量。在特定的选择搜索下，某个特定的数字是最优的，但是通常精确的数字取决于小的细节，例如随机初始化。有时，还存在这样一种模式，其中某个数量范围的特征明显是优选的，而附加特征明显减少。

```
(pd.Series(best_feat.grid_scores_)

     .plot(figsize=(10, 2.5), linewidth=0.75,

           title="#Features vs. Accuracy on 2-Polynomial Data")); 
```

![](img/B17126_07_13.png)

图 7.13:2 多项式数据的特征与精度

在平稳期开始附近选择一些特征可以减少所需的合成数据量，但是您必须判断平稳期是否有明显的开始。

# 练习

下面的练习要求你在离散事件中寻找“类似连续”的结果，然后对称地将连续或频繁的事件作为粗略时间单位的度量。这两种用于创建合成要素的修改通常都非常有用，并且适用于现实数据集。

## 间歇出现

本章讨论了时间戳字段的规律性，但是这个练习要求你在某种程度上逆转这个目标。有些时候，事件的发生本来就不稳定。例如，每当电离辐射达到阈值时，测量辐射的盖革计数器就会产生“咔嗒声”(或其他离散信号)。类似地，我们可以测量每个新芽出现在树林中的时间标记；事件发生的频率在某种程度上与增长率相对应，但单个事件是随机分布的。其他现象——例如，新冠肺炎疫情(目前正在进行中，在撰写本文时),在每个地缘政治区域的特定日期有新的诊断——有类似的离散事件，间接定义了整体模式。

提供了一个人工数据集，其中包含由五种仪器中的任何一种在覆盖 2020 年的一年时间间隔内测量的事件。事件记录只在精确的分钟发生，但这并不排除在同一分钟内发生多个事件。一般来说，每个仪器的典型事件频率低于每分钟一次。这五种乐器被简单地命名为“A”到“E”。你可以自由地想象这个数据描述了上面提到的一个现象，或者你希望的任何其他领域。

该数据集位于:

[https://www.gnosis.cx/cleaning/events.sqlite](https://www.gnosis.cx/cleaning/events.sqlite)

数据集中的记录将类似于以下几条。

| 时间戳 | 工具 |
| --- | --- |
| 2020-07-04 11:28:00 | A |
| 2020-07-04 11:29:00 | B |
| 2020-07-04 11:31:00 | C |
| 2020-07-04 11:34:00 | D |
| 2020-07-04 11:28:00 | A |
| 2020-07-04 11:34:00 | A |

请注意，数据不一定按时间顺序出现。此外，同一时间戳可能包含来自相同或不同仪器的多个事件。例如，在表中，`2020-07-04 11:28:00`测量了仪器 A 的两个事件，`2020-07-04 11:34:00`测量了仪器 A 的一个事件和仪器 d 的另一个事件。总共记录了大约一百万个事件。

每种乐器都表现出与时间顺序相关的不同模式。创建你认为合理描述数字形式的行为所必需的综合特征。然而，*是否认为特征是以十或百为单位编号，而不是以十万为单位。将这些特征放入一个整洁的数据框架中，该框架可能用于进一步的统计分析或机器学习技术。该数据框将包含与您决定利用的合成要素相对应的列。*

 *尝试用一般术语描述每种仪器的行为，使用散文描述，或者可能使用数学函数。尽可能具体地描述你认为有根据的数据，但也要尽可能好地描述你的描述的局限性或不确定性。

## 特征级别

在本练习中，使用与上一练习相同的数据集，该数据集位于:

[https://www.gnosis.cx/cleaning/events.sqlite](https://www.gnosis.cx/cleaning/events.sqlite)

如上所述，五种仪器中的每一种都测量在特定时间戳发生的离散事件。事件以一分钟的精度被识别，时间序列覆盖 2020 日历年。有些分钟有多个事件(来自相同或不同的乐器)，有些分钟没有事件。

本练习的目标是根据事件频率是“低”、“中”还是“高”来描述一年中的每一天。你应该描述每个乐器和一天的量化水平。您应该决定哪种量化策略最适合累积频率和每个乐器的频率。您的策略选择可能取决于每个工具的不同事件分布。

你可以假设所有五种工具在合计时都测量大致相当的东西。例如，如果这些事件是检测到树上的新芽(根据上一个练习中的一个示例),则不同的仪器可能会测量不同的小树林(例如，不是芽对叶对果)。

如果您觉得低/中/高的量化不太适合一个或多个乐器事件分布，请描述您认为适用的问题或限制，并尝试想出一种替代方法来描述乐器行为。

# 结局

> 这个旧世界是一个新世界
> 
> 和一个大胆的世界
> 
> 就我来说
> 
> 妮娜·西蒙

**本章主题:**日期/时间字段；字符串字段；字符串向量；分解；量化；一热编码；多项式特征。

这一章讲述了*发明新功能*的许多方法。这与 [*第六章*](Chapter_6.xhtml#_idTextAnchor008)*值插补*形成对比，后者是关于*发明数据点*。这两种技术都很重要，但是它们在概念上有所不同。经常发生的情况是，我们收集数据的方式，或提供数据的方式，并不代表数据中最有意义的内容，然而更好的表达方式隐藏在我们所拥有的数据中。

在合成特征的创建中呈现了三个一般主题。在一种情况下，我们有时有一个单一的特征，正如所表示的，结合了两个或更多的基本特征，可以很容易地分开并分别表示。类似地，但从另一个方向来看，有时少量直接存在的组件可以更好地组合成单个特征。这两种移动的一个明显例子是日期时间值，它可以是几个组成部分，如年、月、小时或分钟，也可以是单个值。

作为第二个主题，我们看了参数空间，在这个空间中，观察作为向量存在。作为一个抽象的数学实体，初始观测值不需要形成观测向量的标准正交基(维数)。通常，转换参数空间的基础会产生对统计和机器学习更有用的维度。然而，值得记住的是，在这样的转换之后，合成特征很少对它们有任何人类有意义的意义，而仅仅是数字度量。

作为第三个主题，我们研究了从初始特征与其值域或其他初始特征的相互作用中出现的合成特征。直观地说，有一些量是永远无法直接测量的:“热指数”是夏季温度和湿度的一种相互作用；“体重指数”是人的体重和身高的相互作用。有时，互动比我们直接测量的东西更能提供信息。使用多项式特征工程，我们可以探索所有这类交互的空间，但有时会出现许多不可行的特征。在最后一种情况下，特征选择帮助了我们。***