

# 3

# 重新利用数据源

> 所有的语言都是拙劣的翻译。
> 
> 弗朗兹·卡夫卡

有时，数据以需要额外工作才能接收的格式存在。对于公共的和明确面向数据的格式，公共库已经内置了阅读器。例如，数据框库读取大量不同的文件类型。在最坏的情况下，稍微不太常见的格式都有自己更专用的库，这些库在原始格式和您希望使用的通用数据处理库之间提供了一条相对简单的路径。

更大的困难经常出现，因为给定的格式本身不是数据格式，而是为了不同的目的而存在。尽管如此，通常有一些数据以某种方式嵌入或编码在我们想要使用的格式中。例如，网页通常是为人类读者设计的，并由 web 浏览器以“古怪模式”呈现，这种模式处理不太像 HTML 的内容，这是经常需要的。[可移植文档格式(PDF)](Glossary.xhtml#_idTextAnchor097) 文档在考虑人类读者方面是相似的，但也经常包含表格或其他数据，我们希望作为数据科学家来处理这些数据。当然，在这两种情况下，我们更希望数据本身是某种独立的、容易理解的格式；但是现实并不总是如我们所愿。图像格式同样用于向人类展示图片，但我们有时希望以某种数据科学或机器学习的方式来表征或分析图像集合。这里*一方面是*超文本标记语言 ( **HTML** )和 PDF，另一方面是图像，两者之间有一点区别。对于前者，我们希望找到偶然嵌入到文本文档中的表格或数字列表。

对于图像，我们感兴趣的是作为数据的格式本身:像素值的模式是什么，这告诉我们关于图像本身的特征是什么？

还有一些格式确实是作为数据格式设计的，但是它们太不寻常了，以至于不能使用这些格式的通用阅读器。一般来说，自定义文本格式是可以管理的，尤其是如果您有一些关于格式规则的文档。定制的二进制格式通常需要更多的工作，但是如果需要足够紧迫并且其他编码不存在的话，是可以解码的。大多数情况下，这种自定义格式在某种程度上是遗留下来的，一次性转换成更广泛使用的格式是最好的方法。

***

在我们开始本章的章节之前，让我们运行我们的标准设置代码:

```
from src.setup import *

%load_ext rpy2.ipython

%%R 

library(imager)

library(tidyverse)

library(rvest) 
```

# 网页抓取

> 没有错误的重要信件会在邮件中产生错误。
> 
> –匿名

**概念**:

*   HTML 表格
*   非表格数据
*   命令行抓取

大量有趣的数据存在于网页上，不幸的是，我们经常无法访问更多结构化数据格式的相同数据。在最好的情况下，我们感兴趣的数据至少存在于网页的 HTML 表中；然而，即使定义了表格，单元格的内容通常也不仅仅是我们感兴趣的数值或[分类](Glossary.xhtml#_idTextAnchor022)值。例如，给定的单元格可能包含数据点的注释或提供信息来源的脚注。当然，在其他时候，我们感兴趣的数据根本不在 HTML 表中，而是以其他方式组织在网页上。

在本节中，我们将首先使用 R 库 [rvest](Glossary.xhtml#_idTextAnchor117) 提取一些表格数据，然后使用 Python 中的 [BeautifulSoup](Glossary.xhtml#_idTextAnchor015) 处理一些非表格数据。这种转换工具的选择并不是因为一种工具或另一种工具能够独特地完成我们使用它的任务，甚至也不是一种工具一定比另一种工具更好。我只是想提供一些执行类似任务的不同工具。

在 Python 世界里，框架 [Scrapy](Glossary.xhtml#_idTextAnchor121) 也被广泛使用——它比 BeautifulSoup 做得更多更少。Scrapy 实际上可以下拉网页并在其中动态导航，而 BeautifulSoup 只对解析方面感兴趣，它假设你已经使用了一些其他工具或库(比如[请求](Glossary.xhtml#_idTextAnchor111))来实际获取要解析的 HTML 资源。就其功能而言，BeautifulSoup 更加友好，并且能够出色地处理畸形的 HTML。在现实世界中，所谓的“HTML”通常只是松散地符合任何实际的格式标准，因此，例如，web 浏览器在提供结构模糊的标签汤的合理呈现方面相当复杂。

在 2020 年撰写本文时，新冠肺炎疫情仍在持续，全球范围内该疾病的确切情况每天都在发生变化。鉴于这种积极的变化，当前的形势太像一个移动的靶子，无法树立一个好榜样(而且政治和道德负担太重)。让我们看一些过去疾病的数据来说明网络抓取。虽然我们肯定可以找到类似数据的其他来源，其中一些很可能是立即可读的格式，但我们将从维基百科关于 2009 年疫情流感的文章中收集我们的数据。

关于网页的一个重要事实是，它们可以而且经常被维护者修改。有时候[时光倒流机](https://archive.org/web/)可以被用来寻找特定的历史版本。在给定时间点可用的数据在给定的 URL 可能在未来不再可用。或者，即使网页保持相同的底层信息，它也可能改变其格式的细节，这将改变我们用于处理页面的脚本的功能。另一方面，许多变化恰恰代表了我们感兴趣的数据值的更新，而网页的动态特性正是其最大的价值。当从网上抓取数据时，要记住这些权衡。

## HTML 表格

维基百科有很多优点，其中之一就是它的页面版本化。虽然给定主题的默认 URL 具有友好和简单的拼写，甚至经常可以从主题的名称中猜出，但维基百科还在其查询字符串中提供了一个 URL 参数，该参数标识了网页的确切版本，该版本应始终保持按位相同。

这种持久性有一些例外；例如，如果一篇文章被完全删除，它可能变得不可访问。同样，如果模板是重命名的一部分，正如在本书写作期间不幸发生的那样，一个“永久的”链接可能会断开。让我们检查一下我们将在本节中尝试抓取的维基百科页面:

```
# Same string composed over two lines for layout

# XXXX substituted for actual ID because of discussed breakage

url2009 = ("https://en.wikipedia.org/w/index.php?"

           "title=2009_flu_pandemic&oldid=XXXX") 
```

我们感兴趣的页面的部分是一个信息框，位于文章的中间。在我的浏览器中是这样的:

![2009 Flu Infobox](img/B17126_03_01.png)

图 3.1:题为“2009 年疫情流感”的文章中的维基百科信息框

构建 web 抓取的脚本不可避免地涉及大量的试错。从概念上讲，在处理 HTML 之前，可以手动读取底层 HTML，并正确识别感兴趣的元素的位置和类型。在实践中，关注部分过滤或索引的元素并通过重复来细化选择总是更快。

例如，在下面的第一遍中，我通过反复试验确定了“cases by region”表是 web 页面上的第 4 个表，方法是枚举前面的数字并直观地排除它们。由 web 浏览器呈现时，什么元素是表格并不总是显而易见的；在底层 HTML 中，在视觉上呈现在另一个元素之上的元素实际上更早出现，这也不一定。

第一次传递也已经对值名进行了一点清理。通过实验，我确定一些地区名称包含一个 HTML `<br/>`，它在下面的代码中被剥离，单词之间没有留下空格。为了解决这个问题，我用空格替换 HTML 分隔符，然后需要从字符串中重建一个 HTML 对象，并再次选择表:

```
%%R

page <- read_html(url2009) 

table <- page %>% 

    html_nodes("table") %>%

    .[[4]] %>%

    str_replace_all("<br>", " ") %>%

    minimal_html() %>%

    html_node("table") %>%

    html_table(fill = TRUE) 

head(table, 3) 
```

此代码产生了以下内容(在模板更改问题之前):

```
 2009 flu pandemic data  2009 flu pandemic data  2009 flu pandemic data

1                     Area        Confirmed deaths                    <NA>

2        Worldwide (total)                  14,286                    <NA>

3  European Union and EFTA                   2,290                    <NA> 
```

虽然第一遍还是有问题，但是所有的数据基本都是存在的，我们可以清理它，不需要进一步查询来源。由于嵌套表的原因，每一列都错误地推导出相同的标题。更准确的标题放在第一行。

此外，还创建了一个包含脚注的额外列(它在由`head()`显示的行下面的一些行中包含内容)。因为超过一千的数字中有逗号，所以无法推断出整数。让我们把`data.frame`转换成一个 tibble:

```
data <- as_tibble(table, 

        .name_repair = ~ c("Region", "Deaths", "drop")) %>%

    select(-drop) %>%

    slice(2:12) %>%

    mutate(Deaths = as.integer(gsub(",", "", Deaths)),

           Region = as.factor(Region))

data 
```

这可能会给我们一个有用的表格，如:

```
# A tibble: 11 x 2

   Region                                      Deaths

   <fct>                                        <int>

 1 Worldwide (total)                            14286

 2 European Union and EFTA                       2290

 3 Other European countries and Central Asia      457

 4 Mediterranean and Middle East                 1450

 5 Africa                                         116

 6 North America                                 3642

 7 Central America and Caribbean                  237

 8 South America                                 3190

 9 Northeast Asia and South Asia                 2294

10 Southeast Asia                                 393

11 Australia and Pacific                          217 
```

显然，这是一个很小的例子，可以很容易地手工输入。所示的一般技术可能适用于更大的表。更重要的是，它们还可以用来抓取频繁更新的网页上的表格。严格来说，2009 年是历史性的，但其他数据每天甚至每分钟都在更新，每次需要处理数据时，像图中所示的几行代码就可以提取当前数据。

## 非表格数据

对于非表格来源的处理，我们也将使用维基百科。再次，选择一个兴趣广泛且不易删除的话题。同样，在 URL 中会显示一个特定的历史版本，以防在您阅读本文时页面的结构发生变化。以一种稍微有点自我参照的方式，我们将看看在术语/定义布局中列出 HTTP 状态代码的文章。

该页面的一部分在我的浏览器中呈现如下:

![](img/B17126_03_02.png)

图 3.2: HTTP 状态代码，维基百科定义列表

文章中列出了许多截图中没有的其他代码。此外，在整个页面中还有章节划分和其他描述性元素或图像。幸运的是，维基百科在标记的使用上非常有规律和可预测。我们将检查的 URL 是:

```
url_http = ("https://en.wikipedia.org/w/index.php?"

            "title=List_of_HTTP_status_codes&oldid=947767948") 
```

我们需要做的第一件事实际上是检索 HTML 内容。Python 标准库模块`urllib`完全能够完成这项任务。然而，即使是它的[官方文档](https://docs.python.org/3/library/urllib.request.html#module-urllib.request)也推荐在大多数情况下使用第三方包请求。没有什么是你*不能*用`urllib`做的，但是通常 API 更难使用，并且因为历史/遗留原因变得不必要的复杂。对于简单的事情，就像本书中所展示的，差别不大；对于更复杂的任务，养成使用请求的习惯是个好主意。

让我们打开一个页面，检查返回的状态代码:

```
import requests

resp = requests.get(url_http)

resp.status_code 
```

```
200 
```

我们检索到的原始 HTML 不太容易处理。即使除了它被压缩以去除额外的空白之外，一般的结构是一个“标签汤”,各种各样的东西嵌套在不同的地方，其中基本的字符串方法或正则表达式并不能很好地帮助我们识别我们感兴趣的部分。例如，这是中间某处的一小段:

```
pprint(resp.content[43400:44000], width=55) 
```

```
(b'ailed</dt>\n<dd>The server cannot meet the requir'

 b'ements of the Expect request-header field.<sup i'

 b'd="cite_ref-53" class="reference"><a href="#cite'

 b'_note-53">&#91;52&#93;</a></sup></dd>\n<dt><span '

 b'class="anchor" id="418"></span><a href="/wiki/HT'

 b'TP_418" class="mw-redirect" title="HTTP 418">418'

 b' I\'m a teapot</a> (<a class="external mw-magicli'

 b'nk-rfc" rel="nofollow" href="https://tools.ietf.'

 b'org/html/rfc2324">RFC 2324</a>, <a class="extern'

 b'al mw-magiclink-rfc" rel="nofollow" href="https:'

 b'//tools.ietf.org/html/rfc7168">RFC 7168</a>)</dt'

 b'>\n<dd>This code was defined in 1998 as one of th'

 b'e traditional <a href="/') 
```

我们想做的是把汤做得更漂亮。这样做的步骤是首先从原始 HTML 创建一个“soup”对象，然后使用该 soup 的方法挑选出我们对数据集感兴趣的元素。与 R 和 rvest 版本一样——事实上，与您决定使用的任何库一样——在 web 页面中找到正确的数据需要反复试验:

```
from bs4 import BeautifulSoup

soup = BeautifulSoup(resp.content) 
```

首先，在我们的检查中，我们注意到状态代码本身都包含在一个 HTML `<dt>`元素中。下面我们显示由这个标签标识的第一个和最后几个元素。事实上，这样标识的每样东西都是状态代码，但是我只知道通过手工检查所有这些东西(幸运的是，目测不到 100 个项目并不困难；用一百万这样做是不可行的)。

然而，如果我们回头看原始的 web 页面本身，我们会注意到末尾的两个 AWS 定制代码没有被捕获，因为页面格式对于它们是不一致的。在本节中，我们将忽略它们，因为我们已经确定它们无论如何都不是通用的:

```
codes = soup.find_all('dt')

for code in codes[:5] + codes[-5:]:

    print(code.text) 
```

```
100 Continue

101 Switching Protocols

102 Processing (WebDAV; RFC 2518)

103 Early Hints (RFC 8297)

200 OK

524 A Timeout Occurred

525 SSL Handshake Failed

526 Invalid SSL Certificate

527 Railgun Error

530 
```

如果每个`<dt>`都配有对应的`<dd>`就好了。如果是的话，我们可以阅读所有的`<dd>`定义，并把它们和术语压缩在一起。现实世界的 HTML 是混乱的。事实证明——我是在写作时发现这一点的，而不是通过计划这个例子——有时每个`<dt>`后面会有不止一个(有时可能是零个)`<dd>`元素。然后我们的目标将是收集所有跟在`<dt>`后面的`<dd>`元素，直到其他标签出现。

在 BeautifulSoup API 中，元素之间的空白区域是一个纯文本节点，其中包含该区域内的所有字符(包括空白)。在这个任务中使用 API `node.find_next_siblings()`很有诱惑力。我们*可以*成功做到这一点，但是这个方法将获取太多，包括当前元素之后的所有后续`<dt>`元素。相反，我们可以使用`.next_sibling`属性来获取每一个，并在需要时停止:

```
def find_dds_after(node):

    dds = []

    sib = node.next_sibling

    while True:     # Loop until a break

        # Last sibling within page section

        if sib is None:

            break

        # Text nodes have no element name

        elif not sib.name: 

            sib = sib.next_sibling

            continue

        # A definition node

        if sib.name == 'dd':

            dds.append(sib)

            sib = sib.next_sibling

        # Finished <dd> the definition nodes

        else:

            break

    return dds 
```

我上面写的自定义函数很简单，但是专门用于这个目的。也许它可以扩展到其他 HTML 文档中的类似定义列表。BeautifulSoup 提供了许多有用的 API，但它们是构建定制提取器的构建块，而不是预见 HTML 文档中每一种可能的结构。为了理解它，让我们看几个状态代码:

```
for code in codes[23:26]:

    print(code.text)

    for dd in find_dds_after(code):

        print("  ", dd.text[:40], "...") 
```

```
400 Bad Request

   The server cannot or will not process th ...

401 Unauthorized (RFC 7235)

   Similar to 403 Forbidden, but specifical ...

   Note: Some sites incorrectly issue HTTP  ...

402 Payment Required

   Reserved for future use. The original in ... 
```

HTTP 401 响应包含两个独立的定义块。让 us 将该函数应用于所有 HTTP 代码号。返回的是定义块的列表；出于我们的目的，我们将用一个换行符来连接每一个文本。事实上，我们构建了一个数据框架，在下一个单元格中包含我们感兴趣的所有信息:

```
data = []

for code in codes:

    # All codes are 3 character numbers

    number = code.text[:3]

    # Parenthetical is not part of status

    text, note = code.text[4:], ""

    if " (" in text:

        text, note = text.split(" (")

        note = note.rstrip(")")

    # Compose description from list of strings

    description = "\n".join(t.text for t in find_dds_after(code))

    data.append([int(number), text, note, description]) 
```

从 Python 的列表列表中，我们可以创建一个 Pandas 数据框架，用于数据集的进一步工作:

```
(pd.DataFrame(data, 

              columns=["Code", "Text", "Note", "Description"])

    .set_index('Code')

    .sort_index()

    .head(8)) 
```

```
Code                 Text              Note                  Description

————————————————————————————————————————————————————————————————————————

 100             Continue                    The server has received the

                                                   request headers an...

 101  Switching Protocols                    The requester has asked the

                                                   server to switch p...

 102           Processing  WebDAV; RFC 2518         A WebDAV request may

                                               contain many sub-requests

 103           Checkpoint                          Used in the resumable

                                              request proposal to res...

 103          Early Hints          RFC 8297          Used to return some

                                                 response headers before

                                                                   fi...

 200                   OK                          Standard response for

                                             successful HTTP requests...

 201              Created                           The request has been

                                                 fulfilled, resulting in

                                                                    t...

 202             Accepted                           The request has been

                                                accepted for processing,

                                                                     ... 
```

很明显，这本书在某些细节上讲述的两个例子并不适用于你可能希望从中抓取数据的所有网页。组织成表和定义列表当然是 HTML 表示数据的两种常见用法，但也可能使用许多其他约定。元素上特定的领域特定的(或者可能是页面特定的)属性`class`和`id`也是标记不同数据元素的结构角色的一种常见方式。

像 rvest、BeautifulSoup 和 scrapy 这样的库也可以通过元素属性直接识别和提取 HTML。在你得到正确的结果之前，简单地准备好尝试你的网络抓取代码的许多变化。一般你的迭代会是一个缩小的过程；每个阶段*都需要*包括想要的信息，这就变成了一个通过提炼去掉你不想要的部分的过程。

## 命令行抓取

我经常使用的另一个方法是使用命令行网络浏览器`lynx`和`links`。使用系统软件包管理器安装其中一个或两个。这些工具可以将 HTML 内容作为文本转储，如果格式简单，那么相对容易解析。很多时候，只需查找缩进、垂直空格、搜索特定关键字或类似的文本处理模式，就能比试错式解析像 rvest 或 BeautifulSoup 这样的库更快地获得所需的数据。当然，总有一定量的目测和重试命令。对于精通文本处理工具的人来说，这种方法值得考虑。

这两个相似的文本模式网络浏览器都共享一个`-dump`开关，将非交互文本输出到 [STDOUT](Glossary.xhtml#_idTextAnchor133) 。它们都有各种其他开关，可以以各种方式调整文本的呈现。这两个工具的输出是相似的，但是您的脚本的其余部分需要注意细微的差别。这些浏览器都能很好地将 90%的网页转换成易于处理的文本。对于 10%的问题(一个挥舞着手的百分比，而不是一个真实的度量)，通常一个或另一个工具会产生一些合理的解析。在某些情况下，其中一个浏览器可能会产生有用的结果，而另一个则不会。幸运的是，对于一个给定的任务或站点，尝试这两种方法是很容易的。

让我们根据 HTTP 响应代码页的一部分来看看每个工具的输出。很明显，我尝试找到对应的输出的精确行范围。您可以看到在这个友好的 HTML 页面中只存在偶然的格式差异。首先，用`lynx`:

```
%%bash

base='https://en.wikipedia.org/w/index.php?title='

url="$base"'List_of_HTTP_status_codes&oldid=947767948'

lynx -dump $url | sed -n '397,406p' 
```

```
 requester put on the request header fields.^[170][44]^[171][45]

   413 Payload Too Large ([172]RFC 7231)

          The request is larger than the server is willing or able to

          process. Previously called "Request Entity Too Large".^[173][46]

   414 URI Too Long ([174]RFC 7231)

          The [175]URI provided was too long for the server to process.

          Often the result of too much data being encoded as a

          query-string of a GET request, in which case it should be 
```

和页面的相同的部分再次出现，但这次与`links`:

```
%%bash

base='https://en.wikipedia.org/w/index.php?title='

url="$base"'List_of_HTTP_status_codes&oldid=947767948'

links -dump $url | sed -n '377,385p' 
```

```
 requester put on the request header fields.^[44]^[45]

   413 Payload Too Large (RFC 7231)

           The request is larger than the server is willing or able to

           process. Previously called "Request Entity Too Large".^[46]

   414 URI Too Long (RFC 7231)

           The URI provided was too long for the server to process. Often the

           result of too much data being encoded as a query-string of a GET 
```

这里唯一的区别是 definition 元素缩进的一个空格差异和文本中脚注链接格式的一些差异。在这两种情况下，为术语模式及其定义定义一些规则都很容易。大概是这样的:

*   查找以 3 个空格开头，后跟 3 位数的行
*   累加其后的所有非空白行；停在空白行
*   从文本中去掉脚注/链接标记
*   按照与上一个示例相同的方式拆分代码编号和文本

让我们挥手告别 HTML 的 Scylla，当我们擦肩而过，陷入 PDF 的腹背受敌。

# 可移植文档格式

> 这个工作人员欣喜若狂地抓住它，用颤抖的手打开它，迅速看了一眼里面的东西，然后，争先恐后地向门口跑去，最后毫不客气地从房间和房子里冲了出来。
> 
> 埃德加·爱伦·坡

**概念**:

*   识别表格区域
*   提取纯文本

有很多商业工具可以提取隐藏在 PDF 文件中的数据。不幸的是，许多组织——政府、企业和其他组织——以 PDF 格式发布报告，但不提供更易于计算机分析和抽象的数据格式。这很常见，足以推动从这些报告中半自动提取数据的工具的家庭手工业。这本书不推荐使用专有工具，因为它们不能保证随着时间的推移得到维护和改进；当然，这些工具也需要钱，并且阻碍了数据科学家之间的合作，这些数据科学家在项目上一起工作，而不一定居住在同一个“许可区”。

PDF 文件中有两个主要元素可能会引起我们的兴趣。一个明显的例子是数据表，它们通常嵌入在 pdf 中。否则，PDF 通常可以简单地被视为自定义文本格式，我们将在下一节讨论。各种列表、项目符号、标题或简单的段落文本可能包含我们感兴趣的数据。

我推荐两个开源工具来从 pdf 中提取数据。其中之一是命令行工具`pdftotext`，它是 [Xpdf](Glossary.xhtml#_idTextAnchor147) 和衍生的 [Poppler](Glossary.xhtml#_idTextAnchor099) 软件套件的一部分。第二个是一个叫做 **tabula-java** 的 Java 工具。tabula-java 反过来又是 GUI 工具 [Tabula](Glossary.xhtml#_idTextAnchor137) 的底层引擎，也有针对 Ruby ( **tabula-extractor** )、Python ( **tabula-py** )、R ( **tabulizer** )和 Node.js ( **tabula-js** )的语言绑定。Tabula 创建了一个小型网络服务器，允许在浏览器中进行交互操作，比如创建 pdf 列表和选择表格所在的区域。Python 和 R 绑定还允许直接创建数据框或数组，R 绑定包含一个可选的图形小部件用于区域选择。

对于这个讨论，我们不使用任何语言绑定，也不使用 GUI 工具。对于单页数据集的一次性选择，选择工具可能是有用的，但是对于周期性文档更新或相似文档系列的自动化，需要脚本。

此外，尽管各种语言绑定都非常适合脚本编写，但是在这一节中，我们可以通过将自己限制在基本库的命令行工具上，从而在某种程度上更加独立于语言。

作为本节的一个例子，让我们使用一个从本书序言中输出的 PDF。当你读到这篇文章的时候，可能会有一些小的措辞上的变化，印刷书籍或电子书的确切格式肯定会与这个草稿版本有些不同。然而，这很好地说明了以几种不同样式呈现的表格，我们可以尝试将它们提取为数据。特别是，有三个表是我们想要捕获的:

![Preface page 5](img/B17126_03_03.png)

图 3.3:该书前言的第 5 页

在序言草稿的第 5 页上，一个表格由 Pandas 和 tibble 共同呈现，并有相应的微小表示差异。在第 7 页，包含了另一个看起来有些不同的表格:

![Preface page 7](img/B17126_03_04.png)

图 3.4:该书前言的第 7 页

运行 tabula-java 需要一个相当长的命令行，所以我创建了一个小的 Bash 脚本在我的个人系统上包装它:

```
#!/bin/bash

# script: tabula

# Adjust for your personal system path

TPATH='/home/dmertz/git/tabula-java/target'

JAR='tabula-1.0.4-SNAPSHOT-jar-with-dependencies.jar'

java -jar "$TPATH/$JAR" $@ 
```

提取有时会使用`--guess`选项自动识别每页的表格，但是您可以通过指定 tabula-java 应该查找表格的页面部分来获得更好的控制。我们只是在下面的代码单元中输出到 STDOUT，但是输出到文件只是另一个选项开关:

```
%%bash

tabula -g -t -p5 data/Preface-snapshot.pdf 
```

```
[1]:,,Last_Name,First_Name,Favorite_Color,Age

"",Student_No,,,,

"",1,Johnson,Mia,periwinkle,12.0

"",2,Lopez,Liam,blue-green,13.0

"",3,Lee,Isabella,<missing>,11.0

"",4,Fisher,Mason,gray,NaN

"",5,Gupta,Olivia,sepia,NaN

"",6,Robinson,Sophia,blue,12.0 
```

Tabula 做得很好，但并不完美。Pandas 将索引列的名称设置在其他标题之下的方式稍微打乱了它。还有一个虚假的第一列，通常是空字符串，但有一个标题作为输出单元格编号。然而，这些小缺陷非常容易清理，我们在表中有一个非常好的实际数据的 CSV。

然而，请记住，第 5 页实际上有两个表格。Tabula-java 只捕获了第一个，这不是不合理的，但不是我们可能想要的所有数据。略更多的自定义指令(通过适度的试错来确定感兴趣的区域)可以捕捉到第二个:

```
%%bash

tabula -a'%72,13,90,100' -fTSV -p5 data/Preface-snapshot.pdf 
```

```
First     Last      Age

<chr>     <chr>

bl>

Mia       Johnson   12

Liam      Lopez     13

Isabella  Lee       11

Mason     Fisher    NaN

Olivia    Gupta     NaN

Sophia    Robinson  12 
```

为了说明输出选项，我们选择了制表符分隔而不是逗号分隔的输出。还提供了 JSON 输出。此外，通过调整左边距(作为百分比，但作为印刷点也是一个选项)，我们可以消除不必要的行号。像以前一样，摄入是好的，但不是完美的。数据类型标记的 tibble 格式对我们来说是多余的。丢弃带有不必要数据的两行非常简单。

最后，对于此示例，让我们获取第 7 页上没有任何数据框库额外标记的表格。这个可能是你在实际工作中会遇到的更典型的表格。例如，我们使用点数而不是页面百分比来表示表格的位置:

```
%%bash

tabula -p7 -a'120,0,220,500' data/Preface-snapshot.pdf 
```

```
Number,Color,Number,Color

1,beige,6,alabaster

2,eggshell,7,sandcastle

3,seafoam,8,chartreuse

4,mint,9,sepia

5,cream,10,lemon 
```

这里的提取是完美的，尽管表格本身不太理想，因为它重复了两次数字/颜色对。然而，使用数据框库进行修改也同样容易。

tabula-java 工具，顾名思义，只对识别表格有用。相比之下，`pdftotext`创建了一个*尽力而为*的纯文本版本的 PDF。大多数时候这是很好的。由此，标准的文本处理和提取技术通常工作良好，包括那些解析表格的技术。但是，由于输出的是整个文档(或 pages 选择的部分文档),因此我们可以使用其他元素，如项目符号列表、原始散文或文档的其他可识别数据元素:

```
%%bash

# Start with page 7, tool writes to .txt file 

# Use layout mode to preserve horizontal position

pdftotext -f 7 -layout data/Preface-snapshot.pdf

# Remove 25 spaces from start of lines

# Wrap other lines that are too wide

sed -E 's/^ {,25}//' data/Preface-snapshot.txt |

    fmt -s | 

    head -20 
```

```
• Missing data in the Favorite Color field should be substituted with

the string <missing>.

• Student ages should be between 9 and 14, and all other values are

considered missing data.

• Some colors are numerically coded, but should be dealiased. The

mapping is:

   Number     Color      Number    Color

       1      beige           6    alabaster

       2      eggshell        7    sandcastle

       3      seafoam         8    chartreuse

       4      mint            9    sepia

       5      cream          10    lemon

Using the small test data set is a good way to test your code. But try also manually adding more rows with similar, or different, problems in them, and see how well your code produces a reasonable result. 
```

中间的表格部分作为固定宽度的格式易于阅读。顶部的项目符号或底部的段落可能对其他数据提取目的有用。无论如何，在这一点上它是纯文本，很容易处理。

现在让我们来分析图像，主要是它们的元数据和总体统计特征。

# 图像格式

> 正如中国人说的，1001 个单词比一张图片更有价值。
> 
> –约翰·麦卡锡^(图片)

*图片*

麦卡锡引用的这段话当然不是源自古代中国。就像 20 世纪早期的许多美国亲华者一样——不可避免地带有恐华色彩——它起源于一家广告公司。亨利克·易卜生在 1906 年去世前曾说过“千言万语不如一件事给人留下深刻的印象”。这是 1911 年 3 月阿瑟·布里斯班在锡拉丘兹广告人俱乐部的演讲中改编的，名为“使用图片”。胜过千言万语。”后来的重复添加了所谓的来源作为一个“中国谚语”，甚至是一个错误的归属孔子，大概是为了给这个口号增加可信度。

**概念**:

*   OCR 和图像识别(范围之外)
*   颜色模型
*   像素统计
*   通道预处理
*   图像元数据

出于某些目的，光栅图像本身就是我们感兴趣的数据集。“光栅”仅仅意味着像素值的矩形集合。围绕图像识别和图像处理的机器学习领域远远超出了本书的范围。本节中的几项技术可能对准备好数据以开发这些工具的输入有用，但仅此而已。

本书也没有考虑到对图像的*内容*的其他类型的高层次识别。例如，光学字符识别(OCR)工具可能会将包含各种字符串和数字的图像识别为渲染字体，这些值可能是我们关心的数据。

如果你不幸只有打印和扫描形式的数据，我肯定会对你深表同情。使用 OCR 扫描图像可能会产生带有许多误识别的嘈杂结果。检测这些在 [*第 4 章*](Chapter_4.xhtml#_idTextAnchor006) 、*异常检测*中有所论述；本质上，当这些错误发生时，您将得到错误的字符串或错误的数字，并且理想情况下，这些错误将是可识别的。然而，这些技术的细节不在当前范围内。

在本节中，我们只想介绍一些工具，用于以数字数组的形式读入图像，并执行一些可能在下游数据分析或建模中使用的基本处理步骤。在 Python 中，库 [Pillow](Glossary.xhtml#_idTextAnchor098) 是首选工具(后向兼容的继承了 **PIL** ，但不推荐使用)。在 R 中，[成像仪](Glossary.xhtml#_idTextAnchor058)库似乎最广泛地用于本节的通用任务。作为第一项任务，让我们检查并描述一下本书创作中使用的光栅图像:

```
from PIL import Image, ImageOps

for fname in glob('img/*'):

    try:

        with Image.open(fname) as im:

            print(fname, im.format, "%dx%d" % im.size, im.mode)

    except IOError:

        pass 
```

```
img/Flu2009-infobox.png PNG 607x702 RGBA

img/Konfuzius-1770.jpg JPEG 566x800 RGB

img/UMAP.png PNG 2400x2400 RGBA

img/DQM-with-Lenin-Minsk.jpg MPO 3240x4320 RGB

img/HDFCompass.png PNG 958x845 RGBA

img/t-SNE.png PNG 2400x2400 RGBA

img/preface-2.png PNG 945x427 RGBA

img/DQM-with-Lenin-Minsk.jpg_original MPO 3240x4320 RGB

img/PCA.png PNG 2400x2400 RGBA

img/Excel-Pitfalls.png PNG 551x357 RGBA

img/gnosis-traffic.png PNG 1064x1033 RGBA

img/Film_Awards.png PNG 1587x575 RGBA

img/HTTP-status-codes.png PNG 934x686 RGBA

img/preface-1.png PNG 988x798 RGBA 
```

我们看到大多数使用的是 PNG 图像，少量使用 JPEGs。每个都有一定的空间维度，宽度和高度，如果包含 alpha 通道，每个都是 RGB 或 RGBA。其他图像可能是 HSV 格式。使用 Pillow 和 imager 等工具在色彩空间之间进行转换非常容易，但了解给定图像使用的模型非常重要。让我们读入一个，这次使用 R:

```
%%R

library(imager)

confucius <- load.image("img/Konfuzius-1770.jpg")

print(confucius)

plot(confucius) 
```

```
Image. Width: 566 pix Height: 800 pix Depth: 1 Colour channels: 3 
```

![](img/B17126_03_05.png)

图 3.5:孔子

让我们分析像素的轮廓。

## 像素统计

我们可以通过库提供的一些工具来感受数据，数据本质上只是一个值的数组。对于建立在 **CImg** 上的成像仪，其内部表示是四维的。每个平面都是一个 *X* 乘 *Y* 的像素网格(从左到右，从上到下)。但是，该格式可以在深度维度中表示一堆图像，例如动画。几个颜色通道(如果图像不是灰度的)是阵列的最终尺寸。孔子的例子是一个单一的形象，所以第三维是长度为 1。让我们来看一些关于该图像的汇总数据:

```
%%R

grayscale(confucius) %>% 

    hist(main="Luminance values in Confucius drawing") 
```

![](img/B17126_03_06.png)

图 3.6:孔子图中亮度值的直方图

或许我们希望只查看一种颜色通道的分布情况:

```
%%R

B(confucius) %>% 

    hist(main="Blue values in Confucius drawing") 
```

![](img/B17126_03_07.png)

图 3.7:孔子绘画中蓝色值的直方图

前面的直方图只是利用了标准的 R 直方图函数。数据代表图像，这一点没什么特别的。我们可以对数据进行任何我们想要的统计测试或总结，以确保*对我们的目的*有意义；直方图只是一个简单的例子来说明这个概念。我们还可以轻松地将数据转换成一个整洁的数据框架。在撰写本文时，直接转换为 tibble 存在“阻抗误差”,因此下面的单元格使用中间的`data.frame`格式。

当函数被编写为与`data.frame`对象一起工作时，Tibbles 通常*是*而不是*总是*的替代者:

```
%%R

data <- as.data.frame(confucius) %>%

    as_tibble %>%

    # channels 1, 2, 3 (RGB) as factor

    mutate(cc = as.factor(cc))

data 
```

```
# A tibble: 1,358,400 x 4

       x     y cc    value

   <int> <int> <fct> <dbl>

 1     1     1 1     0.518

 2     2     1 1     0.529

 3     3     1 1     0.518

 4     4     1 1     0.510

 5     5     1 1     0.533

 6     6     1 1     0.541

 7     7     1 1     0.533

 8     8     1 1     0.533

 9     9     1 1     0.510

10    10     1 1     0.471

# ... with 1,358,390 more rows 
```

使用 Python 和 PIL/Pillow，处理图像数据非常相似。和在 R 中一样，图像是一个像素值数组，附带一些元数据。只是为了好玩，我们使用一个带有中文字符的变量名来说明 Python 支持这种情况:

```
# Courtesy name: Zhòngní (仲尼)

# "Kǒng Fūzǐ" (孔夫子) was coined by 16th century Jesuits

仲尼 = Image.open('img/Konfuzius-1770.jpg')

data = np.array(仲尼)

print("Image shape:", data.shape)

print("Some values\n", data[:2, :, :]) 
```

```
Image shape: (800, 566, 3)

Some values

 [[[132  91  69]

  [135  94  74]

  [132  91  71]

  ...

  [148  98  73]

  [142  95  69]

  [135  89  63]]

 [[131  90  68]

  [138  97  75]

  [139  98  78]

  ...

  [147 100  74]

  [144  97  71]

  [138  92  66]]] 
```

在 Pillow 格式中，图像存储为 8 位无符号整数，而不是[0.0，1.0]范围内的浮点数。当然，这些之间的转换很容易，其他的规范化也是如此。例如，对于许多神经网络任务，的首选表示是以零为中心的值，标准偏差为 1。用于保持枕形图像的阵列是三维的，因为它不提供在同一物体中堆叠多个图像。

## 频道操作

在进行处理之前，对图像数据进行处理可能是有用的。下面的例子是人为设计的，类似于库教程中使用的例子。接下来几行代码的想法是，我们将根据蓝色通道中的值来遮罩图像，然后使用它来选择性地清除红色值。对于一幅画来说，其结果在视觉上并不吸引人，但人们可以想象它可能对医学成像或假彩色射电天文学图像有用(例如，我还在进行一种转换，这种转换在单色书中以及全色书中都很容易看到)。

下面的`.paste()`方法中使用的约定有点奇怪。规则是:掩码为 255 的地方，照原样复制；当遮罩为 0 时，保留当前值(如果是中间值，则混合)。在彩色版本中的整体效果是，在主要是红色的图像中，绿色在图像最红的边缘占主导地位。在灰度中，大多数情况下只是使边缘变暗:

```
# split the Confucius image into individual bands

source = 仲尼.split()

R, G, B = 0, 1, 2

# select regions where blue is less than 100

mask = source[B].point(lambda i: 255 if i < 100 else 0)

source[R].paste(0, None, mask)

im = Image.merge(仲尼.mode, source)

ImageOps.scale(im, 0.5) 
```

![](img/B17126_03_08.png)

图 3.8:处理后的孔子图像(左)，原始图像(右)

我们提到的另一个例子是颜色空间的转换可能是有用的。例如，与其看红色、绿色和蓝色，不如看色调、饱和度和亮度，它们可能更适合您的建模需求。这是数据的确定性转换，但强调的是不同的方面。它类似于分解，如主成分分析，这将在第 7 章*特征工程*中讨论。这里我们将图像从 RGB 转换为 HSL 表示:

```
%%R

confucius.hsv <- RGBtoHSL(confucius)

data <- as.data.frame(confucius.hsv) %>%

    as_tibble %>%

    # channels 1, 2, 3 (HSV) as factor

    mutate(cc = as.factor(cc))

data 
```

```
# A tibble: 1,358,400 x 4

       x     y cc    value

   <int> <int> <fct> <dbl>

 1     1     1 1      21.0

 2     2     1 1      19.7

 3     3     1 1      19.7

 4     4     1 1      19.7

 5     5     1 1      19.7

 6     6     1 1      19.7

 7     7     1 1      19.7

 8     8     1 1      19.7

 9     9     1 1      19.7

10    10     1 1      20  

# ... with 1,358,390 more rows 
```

在这种转变中，个体价值和空间形状都发生了变化。这种转换是无损的，除了一些小的舍入问题。按渠道汇总可以说明这一点:

```
%%R

data %>% 

    mutate(cc = recode(

        cc, '1'="Hue", '2'="Saturation", '3'="Value")) %>%

    group_by(cc) %>%

    summarize(Mean = mean(value), SD = sd(value)) 
```

```
'summarise()' ungrouping output (override with '.groups' argument)

# A tibble: 3 x 3

  cc           Mean     SD

  <fct>       <dbl>  <dbl>

1 Hue         34.5   59.1  

2 Saturation  0.448  0.219

3 Value       0.521  0.192 
```

现在让我们来看看对数据科学家来说图像最重要的方面。

## [计]元数据

摄影图像可能包含嵌入其中的元数据。具体来说，**可交换图像文件格式** ( **Exif** )规定了如何将此类元数据嵌入 JPEG、TIFF 和 WAV 格式(最后一种是音频格式)。数码相机通常会将这些信息添加到它们创建的图像中，通常包括时间戳和纬度/经度位置等细节。

Exif 映射中的一些数据字段是文本、数字或元组；其他都是二进制数据。而且映射中的*键*来自对人类没有直接意义的 ID 号；这种映射是一种公开的标准，但是一些设备制造商也可能引入他们自己的 id。

二进制字段包含各种类型的数据，以各种方式编码。例如，一些相机可能会附加小的预览图像作为 Exif 元数据，但更简单的字段也会被编码。

下面的函数将利用 Pillow 返回两个字典，一个用于文本数据，另一个用于二进制数据。标签 id 被扩展为人类可读的名称，如果可用的话。Pillow 使用“camel case”作为这些名称，但是其他工具在标签名称中的大写和标点符号有不同的变化。枕头旁边的箱子是我喜欢称之为“双峰驼箱子”的东西——与单峰骆驼箱子相对——两者都不同于 Python 通常的“蛇箱子”(例如`BactrianCase`对`dromedaryCase`对`snake_case`):

```
from PIL.ExifTags import TAGS

def get_exif(img):

    txtdata, bindata = dict(), dict()

    for tag_id in (exifdata := img.getexif()):

        # Lookup tag name from tag_id if available

        tag = TAGS.get(tag_id, tag_id)

        data = exifdata.get(tag_id)

        if isinstance(data, bytes):

            bindata[tag] = data

        else:

            txtdata[tag] = data

    return txtdata, bindata 
```

让我们检查一下孔子的图像是否附有任何元数据:

```
get_exif(仲尼)  # Zhòngní, i.e. Confucius 
```

```
({}, {}) 
```

我们看到这个图像没有任何这样的元数据。让我们来看看作者在明斯克列宁雕像旁的照片:

```
# Could continue using multi-lingual variable names by

# choosing 'Ленин', 'Ульянов' or 'Мінск'

dqm = Image.open('img/DQM-with-Lenin-Minsk.jpg')

ImageOps.scale(dqm, 0.1) 
```

![](img/B17126_03_09.png)

图 3.9:白俄罗斯 PyCon 主题演讲后的作者

这张用数码相机拍摄的图片确实有 Exif 元数据。这些通常与摄影设置有关，这在比较图像时可能对分析很有价值。这个例子也有一个时间戳，尽管在这个例子中没有纬度/经度位置(使用的相机没有 GPS 传感器)。位置数据(如果有的话)显然有许多用途:

```
txtdata, bindata = get_exif(dqm)

txtdata 
```

```
{'CompressedBitsPerPixel': 4.0,

 'DateTimeOriginal': '2015:02:01 13:01:53',

 'DateTimeDigitized': '2015:02:01 13:01:53',

 'ExposureBiasValue': 0.0,

 'MaxApertureValue': 4.2734375,

 'MeteringMode': 5,

 'LightSource': 0,

 'Flash': 16,

 'FocalLength': 10.0,

 'ColorSpace': 1,

 'ExifImageWidth': 3240,

 'ExifInteroperabilityOffset': 10564,

 'FocalLengthIn35mmFilm': 56,

 'SceneCaptureType': 0,

 'ExifImageHeight': 4320,

 'Contrast': 0,

 'Saturation': 0,

 'Sharpness': 0,

 'Make': 'Panasonic',

 'Model': 'DMC-FH4',

 'Orientation': 1,

 'SensingMethod': 2,

 'YCbCrPositioning': 2,

 'ExposureTime': 0.00625,

 'XResolution': 180.0,

 'YResolution': 180.0,

 'FNumber': 4.4,

 'ExposureProgram': 2,

 'CustomRendered': 0,

 'ISOSpeedRatings': 500,

 'ResolutionUnit': 2,

 'ExposureMode': 0,

 34864: 1,

 'WhiteBalance': 0,

 'Software': 'Ver.1.0  ',

 'DateTime': '2015:02:01 13:01:53',

 'DigitalZoomRatio': 0.0,

 'GainControl': 2,

 'ExifOffset': 634} 
```

我们在文本数据中注意到的一个细节是，Pillow 没有取消标签 ID 34864 的别名。我可以找到表明 ID 应该显示“Exif”的外部文档。Photo.SensitivityType”，但是 Pillow 显然不知道这个 ID。字节字符串可能包含您希望使用的数据，但是每个字段的含义是不同的，必须与引用定义进行比较。例如，字段`ExifVersion`被定义为 ASCII 字节，但*不是*像常规文本字段值一样被定义为 UTF-8 编码的字节。我们可以使用以下方式查看:

```
bindata['ExifVersion'].decode('ascii') 
```

```
'0230' 
```

相比之下，名为`ComponentsConfiguration`的标签由四个字节组成，每个字节代表一个颜色代码。`get_exif()`函数产生单独的文本和二进制字典(`txtdata`和`bindata`)。让我们用一个新的特殊函数来解码`bindata`:

```
def components(cc):

    colors = {0: None,

              1: 'Y', 2: 'Cb', 3: 'Cr',

              4: 'R', 5: 'G', 6: 'B'}

    return [colors.get(c, 'reserved') for c in cc]

components(bindata['ComponentsConfiguration']) 
```

```
['Y', 'Cb', 'Cr', None] 
```

其他二进制字段以其他方式编码。规格由日本电子工业发展协会(JEIDA)维护。本节只是为了让您对使用这种元数据有所了解，而绝不是完整的参考。

现在让我们把注意力转向我们有时需要处理的专门的二进制数据格式。

# 二进制序列化数据结构

> 我通常通过让问题吞噬我来解决问题。
> 
> 弗朗兹·卡夫卡

**概念**:

*   首选现有库
*   字节和结构数据类型
*   数据的偏移布局

数据可能以多种二进制格式存在。所有非常流行的东西都发展成了很好的开源库，但是你可能会遇到一些遗留的或者内部的格式，但这并不正确。一个好的一般建议是，除非有持续的和/或对性能敏感的处理异常格式的需求，否则尽量利用现有的解析器。自定义格式可能很棘手，如果一种格式不常见，它很可能也不会被记录下来。

如果现有的工具只提供您不希望在主要数据科学工作中使用的语言，尽管如此，看看是否可以轻松地利用它，只作为一种导出到更容易访问的格式的方法。您可能只需要一个“一劳永逸”的工具，即使它是递归运行的，但与您需要执行的实际数据处理是异步的。

对于本节，假设乐观的情况没有实现，我们除了磁盘上的一些字节和一些可能有缺陷的文档之外什么也没有。编写定制代码更多的是系统工程师的工作，而不是数据科学家的工作；但是我们数据科学家需要博学，我们不应该被写一点点系统代码吓倒。

对于这个相对较短的部分，我们来看一个简单明了的二进制格式。此外，这是一种真实的数据格式，我们实际上不需要定制的解析器。拥有一个真正的经过良好测试的、高性能的、防弹的解析器来比较我们的玩具代码，是确保我们做正确事情的好方法。具体来说，我们将读取以 [NumPy NPY 格式](https://docs.scipy.org/doc/numpy/reference/generated/numpy.lib.format.html#module-numpy.lib.format)存储的数据，记录如下(节略):

*   前 6 个字节是一个神奇的字符串:确切地说是`\x93NUMPY`
*   接下来的 1 个字节是无符号字节:文件格式的主要版本号，例如`\x01`
*   接下来的 1 个字节是无符号字节:文件格式的次版本号，例如`\x00`。
*   接下来的 2 个字节构成了一个小端无符号短整型:头数据 HEADER_LEN 的长度
*   接下来的 HEADER_LEN 字节是一个 ASCII 字符串，包含一个字典的 Python 文字表达式
*   头部之后是数组数据

首先，我们使用 Python 和 NumPy，使用标准阅读器读取一些二进制数据，以了解我们试图重建的对象类型。原来序列化是一个 64 位浮点值的三维数组。本节选择了较小的尺寸，但当然，真实世界的数据通常会大得多:

```
arr = np.load(open('data/binary-3d.npy', 'rb'))

print(arr, '\n', arr.shape, arr.dtype) 
```

```
[[[ 0\.  1\.  2.]

  [ 3\.  4\.  5.]]

 [[ 6\.  7\.  8.]

  [ 9\. 10\. 11.]]] 

 (2, 2, 3) float64 
```

直观地检查字节是更好地了解数据情况的好方法。当然，NumPy 是一个有清晰和正确文档记录的项目，但是对于一些假设的格式，这是一个发现文档与实际字节不匹配的问题的机会。在更详细的解析中可能会出现更微妙的问题；例如，特定位置的字节的含义可能取决于其他位置出现的标志。令人惊讶的是，数据科学在很大程度上是一个目测数据的问题:

```
%%bash

hexdump -Cv data/binary-3d.npy 
```

```
00000000  93 4e 55 4d 50 59 01 00  76 00 7b 27 64 65 73 63  |.NUMPY..v.{'desc|

00000010  72 27 3a 20 27 3c 66 38  27 2c 20 27 66 6f 72 74  |r': '<f8', 'fort|

00000020  72 61 6e 5f 6f 72 64 65  72 27 3a 20 46 61 6c 73  |ran_order': Fals|

00000030  65 2c 20 27 73 68 61 70  65 27 3a 20 28 32 2c 20  |e, 'shape': (2, |

00000040  32 2c 20 33 29 2c 20 7d  20 20 20 20 20 20 20 20  |2, 3), }        |

00000050  20 20 20 20 20 20 20 20  20 20 20 20 20 20 20 20  |                |

00000060  20 20 20 20 20 20 20 20  20 20 20 20 20 20 20 20  |                |

00000070  20 20 20 20 20 20 20 20  20 20 20 20 20 20 20 0a  |               .|

00000080  00 00 00 00 00 00 00 00  00 00 00 00 00 00 f0 3f  |...............?|

00000090  00 00 00 00 00 00 00 40  00 00 00 00 00 00 08 40  |.......@.......@|

000000a0  00 00 00 00 00 00 10 40  00 00 00 00 00 00 14 40  |.......@.......@|

000000b0  00 00 00 00 00 00 18 40  00 00 00 00 00 00 1c 40  |.......@.......@|

000000c0  00 00 00 00 00 00 20 40  00 00 00 00 00 00 22 40  |...... @......"@|

000000d0  00 00 00 00 00 00 24 40  00 00 00 00 00 00 26 40  |......$@......&@|

000000e0 
```

作为第一步，让我们确保文件确实与我们期望的类型相匹配，拥有正确的“神奇字符串”许多类型的文件都由一个特有的和独特的前几个字节来标识。事实上，类 Unix 系统上的公共实用程序`file`正是通过描述许多文件类型的数据库来使用这些知识。对于假设的稀有文件类型(即非 NumPy)，该实用程序可能不知道格式；尽管如此，该文件可能仍然有这样的头:

```
%%bash

file data/binary-3d.npy 
```

```
data/binary-3d.npy: NumPy array, version 1.0, header length 118 
```

这样，让我们为文件打开一个文件句柄，并根据它的规范继续尝试解析它。为此，在 Python 中，我们将简单地以字节模式打开文件，以便不转换为文本，并读取文件的各个片段来验证或处理部分。

对于这种格式，我们将能够严格地按顺序处理它，但在其他情况下，可能需要在文件中寻找特定的字节位置。Python `struct`模块将允许我们从字节串中解析基本的数字类型。`ast`模块将让我们从原始字符串创建 Python 数据结构，而没有`eval()`可能遇到的安全风险:

```
import struct, ast

binfile = open('data/binary-3d.npy', 'rb')

# Check that the magic header is correct

if binfile.read(6) == b'\x93NUMPY':

    vermajor = ord(binfile.read(1))

    verminor = ord(binfile.read(1))

    print(f"Data appears to be NPY format, "

          f"version {vermajor}.{verminor}")

else:

    print("Data in unsupported file format")

    print("*** ABORT PROCESSING ***") 
```

```
Data appears to be NPY format, version 1.0 
```

接下来我们需要确定标题有多长，然后读入它。在 NPY 版本 1 中，标头始终是 ASCII，但在版本 3 中可能是 UTF-8。因为 ASCII 是 UTF-8 的子集，所以即使我们不检查版本，解码也没有害处:

```
# Little-endian short int (tuple 0 element)

header_len = struct.unpack('<H', binfile.read(2))[0]

# Read specified number of bytes

header = binfile.read(header_len)

# Convert header bytes to a dictionary

# Use safer ast.literal_eval()

header_dict = ast.literal_eval(header.decode('utf-8'))

print(f"Read {header_len} bytes "

      f"into dictionary: \n{header_dict}") 
```

```
Read 118 bytes into dictionary: 

{'descr': '<f8', 'fortran_order': False, 'shape': (2, 2, 3)} 
```

虽然存储在头部的这个字典很好地描述了 dtype、值顺序和形状，但是 NumPy 对值类型使用的约定不同于在`struct`模块中使用的约定。我们可以定义一个(部分)映射来为读者获得数据类型的正确拼写。我们只为一些编码为[小端](Glossary.xhtml#_idTextAnchor071)的数据类型定义了这种映射，但是[大端](Glossary.xhtml#_idTextAnchor018)版本只需要一个大于号。

`'fortran_order'`的键表示变化最快或最慢的维度在内存中是否是连续的。大多数系统使用“C 顺序”来代替。

我们这里的目标不是高效率，而是尽量减少代码。因此，我将首先把实际数据读入一个简单的值列表，然后再把它转换成一个 NumPy 数组:

```
# Define spelling of data types and find the struct code

dtype_map = {'<i2': '<i', '<i4': '<l', '<i8': '<q',

             '<f2': '<e', '<f4': '<f', '<f8': '<d'}

dtype = header_dict['descr']

fcode = dtype_map[dtype]

# Determine number of bytes from dtype spec

nbytes = int(dtype[2:])

# List to hold values

values = []

# Python 3.8+ "walrus operator"

while val_bytes := binfile.read(nbytes):

    values.append(struct.unpack(fcode, val_bytes)[0])

print("Values:", values) 
```

```
Values: [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0] 
```

现在让我们将原始值转换成适当形状和数据类型的实际 NumPy 数组。我们还将寻找在内存中使用 Fortran 还是 C 顺序:

```
shape = header_dict['shape']

order = 'F' if header_dict['fortran_order'] else 'C'

newarr = np.array(values, dtype=dtype, order=order)

newarr = newarr.reshape(shape)

print(newarr, '\n', newarr.shape, newarr.dtype)

print("\nMatched standard parser:", (arr == newarr).all()) 
```

```
[[[ 0\.  1\.  2.]

  [ 3\.  4\.  5.]]

 [[ 6\.  7\.  8.]

  [ 9\. 10\. 11.]]] 

 (2, 2, 3) float64

Matched standard parser: True 
```

正如二进制数据可能古怪一样，文本也可能古怪。

# 自定义文本格式

> 需要我们强调这两个序列的相似性吗？是的，因为我们心目中的相似之处并不仅仅是为了消除差异而选择的特征的简单集合。为了获得最微小的真理，以牺牲他人为代价来保留那些共同的特征是不够的。相反，我们希望强调的是这两种行为的主体间性，以及构成这两种行为的三个术语。
> 
> -雅克·拉坎

**概念**:

*   面向行的层次结构
*   启发式识别感兴趣的数据
*   字符编码和 mojibake
*   使用 chardet 猜测(字符检测)

在作为一名数据科学家的生活中，尤其是当你偶尔担任系统管理员或类似角色时，你会遇到不寻常格式的文本数据。日志文件是这类文件的一个常见来源。许多或大多数日志文件*遵循每行记录的惯例；如果是这样，我们有一个简单的方法来分离记录。从那里，可以使用各种规则或试探法来确定该行对应于哪种*类型的*记录。*

 *然而，并不是所有的日志文件都遵循行惯例。此外，随着时间的推移，您同样会遇到由存储嵌套数据的工具生成的其他类型的文件，这些工具选择创建自己的格式，而不是使用一些广泛使用的标准。对于层次结构或其他非表格结构，避免严格的每行记录格式的动机通常是令人信服和明显的。

在许多情况下，创建一次性格式的程序的作者完全没有责任。在 2020 年撰写本文的十年前，用于表示非表格数据的标准格式还不存在，或者至少在不久前还没有被一系列编程语言广泛采用。根据您的具体领域，遗留数据和格式可能会主导您的工作。例如，JSON 于 2013 年首次标准化，命名为 ECMA-404。YAML 创建于 2001 年，但在大约 2010 年之前没有广泛使用。XML 可以追溯到 1996 年，但是从那以后，对于人类可读的格式来说，它就变得笨拙了。因此，许多程序员走了自己的路，留下了您现在需要导入、分析和处理的文件。

## 结构化日志

扫描我自己的系统，我发现了一个很好的例子，一个合理的人类可读的日志文件，它不能以面向行的方式进行解析。Perl 包管理工具`cpan`记录它管理的每个库的安装操作。这种日志的格式因包而异(很像 Perl 风格)。包 *Archive::Zip* 把讨论过的日志留在了我的系统上(用于它的自检)。这个数据文件包含的部分是定义测试类的实际 Perl 代码，散布着无格式的输出消息。每一类都有各种各样的属性，大部分重叠但不完全相同。一个合理的内存数据格式是一个数据帧，在一个类的给定属性名不存在的地方标记缺失值。

显然，我们可以使用 Perl 本身来处理这些类定义。然而，这不太可能是我们希望实际用来处理提取的数据的编程语言。我们将使用 Python 来读取格式，并且只使用我们期望的元素的试探法。值得注意的是，我们*不能*静态解析 Perl，这个任务被证明严格等同于解决 Jeffrey Kegler 在 2008 年为[Perl Review](http://www.jeffreykegler.com/Home/perl-and-undecidability)撰写的几篇文章中提出的[停机问题](https://en.wikipedia.org/wiki/Halting_problem)。尽管如此，我们示例中的输出使用了一个友好的、但没有正式定义的 Perl 语言子集。下面是正在处理的文件的一部分:

```
%%bash

head -25 data/archive-zip.log 
```

```
zipinfo output:

$ZIP = bless( {

  "versionNeededToExtract" => 0,

  "numberOfCentralDirectories" => 1,

  "centralDirectoryOffsetWRTStartingDiskNumber" => 360,

  "fileName" => "",

  "centralDirectorySize" => 76,

  "writeCentralDirectoryOffset" => 0,

  "diskNumber" => 0,

  "eocdOffset" => 0,

  "versionMadeBy" => 0,

  "diskNumberWithStartOfCentralDirectory" => 0,

  "desiredZip64Mode" => 0,

  "zip64" => 0,

  "zipfileComment" => "",

  "members" => [],

  "numberOfCentralDirectoriesOnThisDisk" => 1,

  "writeEOCDOffset" => 0

}, 'Archive::Zip::Archive' );

Found EOCD at 436 (0x1b4)

Found central directory for member #1 at 360

$CDMEMBER1 = bless( {

  "compressedSize" => 300, 
```

除了科学理论之外，我们可以注意到文件中的一些模式对我们来说已经足够了。我们关心的每个*记录*都以美元符号开始，这是 Perl 和其他一些语言中用于变量名的标记。这一行恰好也跟在类构造函数`bless()`后面。我们通过以`);`结尾的一行找到记录的结尾。在同样的最后一行，我们还找到了正在定义的类的名字，但是在这个例子中，我们不希望保留它们都使用的公共前缀`Archive::Zip::`。本例还规定，我们不会尝试处理输出行中包含的任何附加数据。

显然，创建一个我们的启发式规则无法准确捕捉的 Perl 类的有效构造是可能的。然而，我们在这里的目标并不是实现 Perl 语言，而仅仅是解析这个特定文件中包含的 Perl 语言的非常小的子集(并且希望涵盖其他 CPAN 库可能存在的类似日志的家族)。一个小的[状态机](Glossary.xhtml#_idTextAnchor132)被构造成在文件行的循环内分支:

```
def parse_cpan_log(fh):

    "Take a file-like object, produce a DF of classes generated"

    import pandas as pd

    # Python dictionaries are ordered in 3.6+

    classes = {}

    in_class = False

    for n, line in enumerate(fh):

        # Remove surrounding whitespace

        line = line.strip()

        # Is this a new definition?

        if line.startswith('$'):

            new_rec = {}

            in_class = True # One or more variables contain the "state"

        # Is this the end of the definition?

        elif line.endswith(');'):

            # Possibly fragile assumption of parts of line

            _, classname, _ = line.split()

            barename = classname.replace('Archive::Zip::', '')

            # Just removing extra quotes this way

            name = ast.literal_eval(barename)

            # Distinguish entries with same name by line number

            classes[f"{name}_{n}"] = new_rec

            in_class = False

        # We are still finding new key/val pairs

        elif in_class:

            # Split around Perl map operator

            key, val = [s.strip() for s in line.split('=>')]

            # No trailing comma, if it was present

            val = val.rstrip(',')

            # Special null value needs to be translated

            val = "None" if val == "undef" else val

            # Also, just quote variables in vals

            val = f'"{val}"' if val.startswith("$") else val

            # Safe evaluate strings to Python objects

            key = ast.literal_eval(key)

            val = ast.literal_eval(val)

            # Add to record dictionary

            new_rec[key] = val

    return pd.DataFrame(classes).T 
```

所定义的函数比本书中的大多数例子都要长一点，但却是典型的小型文本处理函数。当不同的行可能属于一个解析域或者属于另一个解析域时，通常会使用状态变量。这种模式在这类任务中很常见，即根据某一行的内容寻找开始状态，积累内容，然后根据不同的行属性寻找停止状态。除了状态维护之外，其余的行主要只是一些小的字符串操作。

现在让我们读取并解析数据文件:

```
df = parse_cpan_log(open('data/archive-zip.log'))

df.iloc[:, [4, 11, 26, 35]]  # Show only a few columns 
```

```
 centralDirectorySize   zip64        crc32

——————————————————————————————————————————————————————————————

       Archive_18                     76       0          NaN

 ZipFileMember_53                    NaN       0   2889301810

 ZipFileMember_86                    NaN       0   2889301810

      Archive_113                     72       1          NaN

              ...                    ...     ...          ...

ZipFileMember_466                    NaN       0   3632233996

      Archive_493                     62       1          NaN

ZipFileMember_528                    NaN       1   3632233996

ZipFileMember_561                    NaN       1   3632233996

                    lastModFileDateTime

——————————————————————————————————————————————————————————————

       Archive_18                   NaN

 ZipFileMember_53            1345061049

 ZipFileMember_86            1345061049

      Archive_113                   NaN

              ...                   ...

ZipFileMember_466            1325883762

      Archive_493                   NaN

ZipFileMember_528            1325883770

ZipFileMember_561            1325883770

18 rows × 4 columns 
```

在这种情况下，数据帧可能更好地用作具有分层索引的序列:

```
with show_more_rows(25):

    print(df.unstack()) 
```

```
versionNeededToExtract  Archive_18             0

                        ZipFileMember_53      20

                        ZipFileMember_86      20

                        Archive_113           45

                        ZipFileMember_148     45

                        ZipFileMember_181     20

                        Archive_208           45

                        ZipFileMember_243     45

                        ZipFileMember_276     45

                        Archive_303          813

                        ZipFileMember_338     45

                        ZipFileMember_371     45

                                            ... 

fileAttributeFormat     Archive_208          NaN

                        ZipFileMember_243      3

                        ZipFileMember_276      3

                        Archive_303          NaN

                        ZipFileMember_338      3

                        ZipFileMember_371      3

                        Archive_398          NaN

                        ZipFileMember_433      3

                        ZipFileMember_466      3

                        Archive_493          NaN

                        ZipFileMember_528      3

                        ZipFileMember_561      3

Length: 720, dtype: object 
```

## 字符编码

文本格式的字符编码问题在某种程度上与本书主要解决的数据问题是正交的。然而，能够读取文本文件的内容是处理其中数据的一个重要步骤，所以我们应该考虑可能存在的问题。出现的问题是“遗留编码”的问题，但应该作为基于 Unicode 的标准化文本格式来解决。也就是说，您需要处理几十年前的文件并不罕见，这些文件要么是在 Unicode 之前创建的，要么是在组织和软件(如操作系统)将其文本格式完全标准化为 Unicode 之前创建的。我们将着眼于出现的问题和解决它们的启发式工具。

美国信息交换标准码(ASCII)创建于 20 世纪 60 年代，作为编码文本数据的标准。然而，当时在美国，只考虑对英文文本中使用的字符进行编码。这包括大写和小写字符、一些基本的标点符号、数字和一些其他特殊或控制字符(如换行符和结束符)。为了容纳这个符号集合，128 个位置就足够了，所以 ASCII 标准只定义了 8 位字节的值，其中*高阶位*为零。任何高位设置为 1 的字节都不是 ASCII 字符。

ISO-8859 字符编码以“兼容”的方式扩展了 ASCII 标准。这些被开发来覆盖(大约)音位字母表中的字符，主要是那些起源于欧洲的字符。许多字母语言以罗马字母为基础，但添加了各种英语中不使用的音调符号。

其他字母大小适中，但在字母形式上与英语无关，如西里尔语、希腊语和希伯来语。构成 ISO-8859 系列的所有编码都保留 ASCII 的低位值，但使用每个字节的高位来编码额外的字符。问题是 128 个附加值(在一个总共有 256 个值的字节中)不足以容纳所有这些不同的额外字符，因此该系列的特定成员(例如，阿拉伯语的 ISO-8859-6)以不兼容的方式使用高位比特值。这使得英语文本可以在这个家族的所有编码中表示，但是每个兄弟都是互不兼容的。

对于 CJK 语言(中文、日文和韩文)，所需的字符数远远大于 256，因此任何单字节编码都不适合表示这些语言。为这些语言创建的大多数编码对每个字符使用 2 个字节，但有些是可变长度的。然而，产生了大量不兼容的编码，不仅针对不同的语言，而且针对特定的语言。例如，EUC-JP、SHIFT_JIS 和 ISO-2022-JP 都是用于以互不兼容的方式表示日语文本的编码。Abugida 书写系统，如梵文、泰卢固语或 Geʽez 语，代表音节，因此比字母系统有更大的字符集；然而，大多数不使用字母大小写，因此所需的代码点大约减半。

让历史更加混乱的是，不仅 ISO-8859 家族之外的其他字母语言编码也存在(包括 ISO-8859 成员涵盖的一些编码)，而且微软在 20 世纪 80 年代狂热地推行其“拥抱-扩展-消灭”策略，试图扼杀开放标准。特别是，windows-12NN 字符编码与相应的 ISO-8859 编码故意“几乎但不完全”相同。例如，windows-1252 使用与 ISO-8859-1 相同的大部分代码点，但也有足够的不同，不会完全兼容。

试图使用错误的编码解码字节序列的结果有时很有趣，但通常令人沮丧，这被称为 mojibake(在日语中的意思是“字符转换”，或者更全面地说，“损坏的文本”)。根据用于书写和阅读的编码对，文本表面上可能与真实文本相似，或者可能显示了明显放错位置的不可用字符和/或标点符号的标记。

Unicode 是所有人类语言中所有字符的代码点规范。它可能以多种方式被*编码*为字节。但是，如果使用默认的和流行的 UTF-8 以外的格式，文件的开头将总是有一个“幻数”，并且前几个字节将明确地编码字节长度和编码的字节序。UTF-8 文件既不要求也不鼓励使用字节顺序标记(BOM ),但是存在一个不与任何代码点混淆的标记。UTF-8 本身是一种可变长度编码；所有 ASCII 字符仍然作为单个字节进行编码，但对于其他字符，使用高位的特殊值会触发读取额外字节的预期，以确定编码的是哪个 Unicode 字符。对于数据科学家来说，知道所有现代编程语言和工具都无缝地处理 Unicode 文件就足够了。

接下来的几篇短文是为各种语言编写的关于字符编码的维基百科文章的片段:

```
for fname in glob('data/character-encoding-*.txt'):

    bname = os.path.basename(fname)

    try:

        open(fname).read()

        print("Read 'successfully':", bname, "\n")

    except Exception as err:

        print("Error in", bname)

        print(err, "\n") 
```

```
Error in character-encoding-nb.txt

'utf-8' codec can't decode byte 0xc4 in position 171: invalid continuation byte 

Error in character-encoding-el.txt

'utf-8' codec can't decode byte 0xcc in position 0: invalid continuation byte 

Error in character-encoding-ru.txt

'utf-8' codec can't decode byte 0xbd in position 0: invalid start byte 

Error in character-encoding-zh.txt

'utf-8' codec can't decode byte 0xd7 in position 0: invalid continuation byte 
```

试图读取这些文件中的文本时出现了错误。如果我们有幸知道所使用的编码，就很容易解决这个问题。然而，文件本身并不记录它们的编码。此外，根据您使用的显示字体，一些字符可能会在屏幕上显示为方框或问号，这使得识别问题更加困难:

```
zh_file = 'data/character-encoding-zh.txt'

print(open(zh_file, encoding='GB18030').read()) 
```

```
![](img/B17126_03_001.png) 
```

即使我们从文件名中得到编码代表中文文本的提示，如果我们在尝试中使用错误的编码，我们也会失败或者得到 [mojibake](Glossary.xhtml#_idTextAnchor077) :

```
try:

    # Wrong Chinese encoding

    open(zh_file, encoding='GB2312').read()

except Exception as err:

    print("Error in", os.path.basename(zh_file))

    print(err) 
```

```
Error in character-encoding-zh.txt

'gb2312' codec can't decode byte 0xd5 in position 12: illegal multibyte sequence 
```

请注意，我们没有立即看到错误。如果我们的只读取了 11 个字节，它将是“有效的”(但是是错误的字符)。同样，上面的`character-encoding-nb.txt`文件在整个 170 字节的时间内都不会遇到问题。我们可以在这些文件中看到一个错误的猜测。例如:

```
ru_file = 'data/character-encoding-ru.txt'

print(open(ru_file, encoding='iso-8859-10').read()) 
```

```
![](img/B17126_03_002.png) 
```

这里我们读到了*一些*的东西，但即使不一定知道任何有争议的语言，这显然是胡言乱语。作为英语的读者，我们至少可以识别出这些主要的发音符号所源自的基本字母。它们以一种不遵循任何真正合理的语音规则的方式混杂在一起，例如元音和辅音大致交替，或者有意义的大写模式。这里包括一个简短的英语短语“字符集”

在这个特殊的例子中，文本确实在 ISO-8859 系列中，但是我们在其中选择了错误的兄弟。这给了我们一种 mojibake。正如文件名所暗示的，这恰好是在俄语中，并且使用了 ISO-8859 族的西里尔成员。读者可能不知道西里尔字母，但如果你顺便看到任何标志或文字，这些文字看起来不会*明显错误*:

```
print(open(ru_file, encoding='iso-8859-5').read()) 
```

```
![](img/B17126_03_003.png) 
```

类似地，如果你见过用希腊语写的东西，这个版本可能看起来不会有明显的错误:

```
el_file = 'data/character-encoding-el.txt'

print(open(el_file, encoding='iso-8859-7').read()) 
```

```
![](img/B17126_03_004.png) 
```

仅仅是在你不熟悉的语言中没有明显的错误是一个弱标准。如果可能的话，让母语读者，或者至少是稍微精通这些语言的读者会有所帮助。如果这是不可能的——如果您正在处理具有许多编码的许多文件，这通常是不可能的——自动化工具可以进行合理的启发式猜测。这并不能保证正确，但有一定的启发性。

Python [chardet](Glossary.xhtml#_idTextAnchor023) 模块的工作方式类似于所有现代网络浏览器中的代码。HTML 页面可以在它们的头中声明它们的编码，但是由于各种原因，这种声明经常是错误的。当数据明显与声明的编码不匹配时，浏览器会做一些处理，并尝试做出更好的猜测。这种探测的一般思想有三个方面。检测器将扫描多个候选编码以达到最佳猜测:

*   在候选编码下，是否有任何字节值或序列是无效的？
*   在候选编码下，字符频率是否与通常使用该编码的语言中遇到的字符频率相似？
*   在候选编码下，有向图的频率与通常遇到的相似吗？

我们不需要担心概率排名的具体细节，只需要使用 API。同一算法的实现可以在多种编程语言中使用。让我们看看`chardet`对一些文本文件的猜测:

```
import chardet

for fname in glob('data/character-encoding-*.txt'):

    # Read the file in binary mode

    bname = os.path.basename(fname)

    raw =  open(fname, 'rb').read()

    print(f"{bname} (best guess):")

    guess = chardet.detect(raw)

    print(f"    encoding: {guess['encoding']}")

    print(f"  confidence: {guess['confidence']}")

    print(f"    language: {guess['language']}")

    print() 
```

```
character-encoding-nb.txt (best guess):

    encoding: ISO-8859-9

  confidence: 0.6275904603111617

    language: Turkish

character-encoding-el.txt (best guess):

    encoding: ISO-8859-7

  confidence: 0.9900553828371981

    language: Greek

character-encoding-ru.txt (best guess):

    encoding: ISO-8859-5

  confidence: 0.9621526092949461

    language: Russian

character-encoding-zh.txt (best guess):

    encoding: GB2312

  confidence: 0.99

    language: Chinese 
```

这些猜测只是部分正确。语言代码`nb`实际上是挪威语博克马尔语，而不是土耳其语。这个猜测的概率明显低于其他猜测。此外，它实际上是用 ISO-8859-10 编码的。然而，在这个特定的文本中，ISO-8859-9 和 ISO-8859-10 之间的所有字符都是相同的，所以这方面并没有错。较大的文本将通过字母和数字频率更可靠地猜测博克马尔语和土耳其语；如果这在大多数情况下都是正确的，那也没什么区别，因为作为数据科学家，我们关心的是让*数据*正确:

```
print(open('data/character-encoding-nb.txt', 

           encoding='iso-8859-9').read()) 
```

```
Tegnsett eller tegnkoding er det som i datamaskiner 

definerer hvilket lesbart symbol som representeres av et gitt 

heltall. Foruten Unicode finnes de nordiske bokstavene ÄÅÆÖØ 

og äåæöø (i den rekkefølgen) i følgende tegnsett: ISO-8859-1, 

ISO-8859-4, ISO-8859-9, ISO-8859-10, ISO-8859-14, ISO-8859-15 

og ISO-8859-16. 
```

关于`zh`文本的猜测也是错误的。我们已经尝试将该文件作为 GB2312 来读取，但这样做显然会失败。这就是领域知识变得相关的地方。严格来说，GB18030 是 GB2312 的超集。原则上，Python `chardet`模块知道 GB18030，所以这个问题本身并不是一个缺失的特性。尽管如此，在这种情况下，不幸的是，`chardet`猜测一个不可能的编码，其中一个或多个编码字符不存在于子集编码中。

编码推理中的错误是说明性的，即使在这些特殊情况下不太严重。添加多于 2-3 个句子的文本会使猜测更可靠，并且大多数文本文档会更长。然而，非文本数据的文本格式通常只有很短的文本片段，通常只是分类特征中的单个单词。

特定的字符串“blue”、“mavi”、“blu”、“blau”和“sininen”在英语、土耳其语、挪威语、德语和芬兰语中都是似是而非的单词。a 环字符在土耳其语或英语中并不存在，但除此之外，区别严格来说是在词汇上，而不是字母或有向图似真。

例如，包含个人姓名的 CSV 文件中，每个姓名只有 5-10 个字母的簇，而不是完整的段落。字母和数字的数量很少，即使不常见的孤立出现，也很难确定。如果您对这个问题有一些领域知识或指导，您可以编写更多的定制代码来根据特定语言的单词列表(包括常用名称)验证候选编码；即使在那里，你也必须允许一定比例的拼写错误和生僻字不匹配。

# 练习

我们在这里提出两个练习。其中一个处理定制的二进制格式，另一个处理 web 抓取。练习中并没有涉及本章的每个主题，但这两个主题是实用数据科学的重要领域。

## 增强 NPY 解析器

我们从 NPY 中读取的二进制数据是我们能选择的最简单的格式。在本练习中，您希望使用自己的代码处理一个稍微复杂一些的二进制文件。编写一个将文件读入 NumPy 数组的自定义函数，并针对使用`numpy.save()`或`numpy.savez()`序列化的几个数组对其进行测试。

您的函数的测试用例位于以下 URL:

[https://www.gnosis.cx/cleaning/students.npy](https://www.gnosis.cx/cleaning/students.npy)

[https://www.gnosis.cx/cleaning/students.npz](https://www.gnosis.cx/cleaning/students.npz)

我们之前没有看到 NPZ 格式，但它是一个或多个 NPY 文件的 zip 存档，允许压缩和存储多个阵列。理想情况下，您的函数将处理这两种格式，并根据前几个字节中的神奇字符串来确定您正在读取的文件类型。作为第一步，只尝试解析 NPY 版本，然后从那里进行增强。

使用官方的阅读器，我们可以看到这个数组增加了前面的例子没有的东西。具体来说，它存储了一个将几种数据类型组合成数组中每个值的`recarray`，如下面的输出所示。我们在本章前面描述的规则实际上仍然足够了，但是你必须仔细考虑它们。

我们希望在您的阅读器中匹配的数据将与使用官方阅读器完全相同:

```
students = np.load(open('data/students.npy', 'rb'))

print(students)

print("\nDtype:", students.dtype) 
```

```
[[('Mia', 12, 1.3) ('Liam', 13, 0.6) ('Isabélla', 11, 2.1)]

 [('Mason', 12, 1.6) ('Olivia', 11, 2.3) ('Sophia', 12, 0.7)]]

Dtype: [('first', '<U8'), ('age', '<i2'), ('distance', '>f4')] 
```

当您继续处理 NPZ 格式时，您可以再次与官方阅读器进行比较。如上所述，它可能有几个数组，尽管在本例中只存储了一个:

```
arrs = np.load(open('data/students.npz', 'rb'))

print(arrs)

arrs.files 
```

```
<numpy.lib.npyio.NpzFile object at 0x7f5e12d8d070>

['arr_0'] 
```

NPZ 文件中`arr_0`的内容与 NPY 中的单个数组相同。但是，在成功解析这个 NPZ 文件之后，尝试创建一个或多个实际存储多个数组的其他文件，并使用自定义代码解析这些文件。为可能需要返回一个或多个数组的函数确定最佳 API。对于这部分任务，Python 标准库模块`zipfile`会对你很有帮助。

没有理由一定要用 Python 来执行这个练习。其他编程语言完全能够读取二进制数据，所涉及的一般步骤与本章中的*二进制序列化数据结构*部分非常相似。例如，您可以将 NPY 文件中的数据读入 R 数组。

## 抓取网络流量

作者的 web 域名 gnosis.cx 已经运营了二十多年，并保留了最初创作时使用的大部分“ [Web 0.5](Glossary.xhtml#_idTextAnchor144) ”技术和视觉风格。和大多数其他网站一样，网站主机提供的一件事是网站流量报告(使用几乎和域名本身一样古老的样式)。

您可以在以下网址找到最新的报告:

[https://www.gnosis.cx/stats/](https://www.gnosis.cx/stats/)

撰写本报告时，当前报告的快照也被复制到:

[https://www.gnosis.cx/cleaning/stats/](https://gnosis.cx/clearning/stats/)

在编写报告时，报告页面的图像如下:

![Traffic report for gnosis.cx](img/B17126_03_10.png)

图 3.10:gnosis . CX 的流量报告

显示的周表是从 2010 年 2 月开始的。实际的网站比这早十年，但是服务器和日志数据库被修改，丢失了旧的数据。中间还有一个相当大的故障，大约 5 年，流量显示为零。在快照之前的 6 周内，流量的大幅下降反映了对 DNS 和 SSL 使用 CDN 代理的变化(因此对实际的 web 主机隐藏了流量)。

在本练习中，您的目标是编写一个工具来动态收集各种表格中的可用数据，这些表格列出了按不同时间增量和重复周期(一周中的哪一天、一年中的哪一个月等等)划分的流量。作为本练习的一部分，让您的脚本生成比屏幕图片中显示的图表不那么糟糕的图表(线图中无意义的错误透视会冒犯良好的敏感性，并且在 2013 年初左右负面流量的明显负面峰值只是令人费解)。

刮一个类似这些报道的网站是普遍需要的。具有定期且不经常改变的结构但每天更新内容的模式通常反映了数据采集需求。您将在本练习中编写的脚本可以运行在 cronjob 上或类似的机制下，以维护此类滚动报告的本地副本和修订。

# 结局

> 他们入侵六边形，出示并不总是虚假的证书，不高兴地翻阅一本书，并谴责整个书架:他们卫生、禁欲的狂热导致了数百万本书的无谓毁灭。
> 
> 豪尔赫·路易斯·博尔赫斯(巴别塔图书馆)

**本章主题**:网页抓取；可移植文档格式；图像格式；二进制格式；自定义文本格式。

这一章考虑了一些数据源，在你的第一印象中，这些数据源可能不是数据本身。在网页和 PDF 文档中，目的通常是呈现人类可读的内容，只包含次要关注的可分析数据。在理想的情况下，无论是谁生成了那些结构化程度较低的文档，都将提供相同数据的结构化版本；然而，那种理想的情况只是偶尔实现。一些写得很好的自由软件库让我们可以合理地从这些资源中提取有意义的数据，尽管总是以某种特定于特定文档的方式，或者至少特定文档的系列或修订版。

图像是机器学习中非常常见的兴趣。例如，对图像中描绘的内容得出各种结论或表征是深度神经网络的一个关键应用。虽然这些实际的机器学习技术超出了本书的范围，但本章向您介绍了获取图像的数组/张量表示的基本 API，并执行一些基本的校正或归一化，这将有助于后面的机器学习模型。

还有一些格式，虽然直接作为记录和交流数据的手段，但并没有被广泛使用，并且您可能没有直接读取它们的工具。对于二进制和文本定制格式，我们给出的具体例子是库支持存在的(对于本章研究的文本格式不那么支持)，但是给出的创建定制摄取工具的一般推理和方法类似于当您遇到过时的、内部的或者仅仅是特殊的格式时需要使用的那些。

下一章开始了这本书的下一个传奇。这些早期章节特别关注您需要处理的数据格式。接下来的两章着眼于数据元素本身特有的问题，而不仅仅是它们的表示。我们从寻找数据中的异常开始。*