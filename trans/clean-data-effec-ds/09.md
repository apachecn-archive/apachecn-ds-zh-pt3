

# 六、数值估计

> 我是另一个人的替代品
> 
> 我看起来很高，但是我的鞋跟很高
> 
> 你看到的简单事物都是复杂的
> 
> 我看起来很年轻，但我只是过时了，是的
> 
> 皮特·汤森

由于各种原因，数据可能以各种方式丢失或不可信。这些方法在 [*第四章*](Chapter_4.xhtml#_idTextAnchor006)*异常检测*和 [*第五章*](Chapter_5.xhtml#_idTextAnchor007)*数据质量*中特别讨论。有时候，处理坏数据的最佳选择是简单地丢弃它。但是，很多时候，以某种方式估计值会更有用，以便保留观测值中的其余特征。从本章的角度来看，让我们假设所有被识别为不可信的数据值——即使最初存在*错误的*值——都已经被明确标记为缺失。

输入数据时，务必要记录好您创建(输入)的值与原始数据集获得的值之间的差异。这个记录*可能*采取对每个数据项的显式注释的形式，这取决于您的数据格式支持什么。保存记录最常用的方法是在以各种方式清理数据时维护数据的版本，并维护(和版本控制)可重复执行修改的显式脚本。

一般来说，数据由许多记录或观察值组成。我们最终用于机器学习和许多统计目的的表格形式是这样清楚的。一行是一个“观察”，至少不严格地说，每一列代表我们希望每个观察都具有的一个特性。即使是最初存储在层次结构或非表格结构中的数据，在我们对其进行大多数分析之前，也需要转换为面向记录的表示。初始表单仍将以某种类似记录的方式进行分区:可能是单独的文件，或者是嵌套数据的单独顶层键，或者是基于某种特定任务目的的单独分区。

估计值与丢弃记录的决策不必是全有或全无。可能的情况是，我们已经决定，一些记录是可能的或值得保存的，而其他的则不是。在我们的决定中通常有几个考虑因素，无论是通过记录还是对问题的一般考虑。这些考虑中的主要重点假设数据集的机器学习使用；本质上不是“机器学习”的可视化或分析很少担心插补，但肯定有时会。需要考虑的一些问题包括:

*   你有很多数据吗？如果您的数据有限，保存每一条可能的记录可能特别重要。机器学习模型*比照*，他们处理的数据越多，就越开心。如果在剔除那些缺失数据后，你还剩下几百万甚至几万条记录，你可能就不用那么担心插补了。^(效力)如果你只有几百张唱片，每一张都觉得珍贵；当然，在记录较少的情况下，有缺陷的估计也会产生不成比例的影响。
*   您是否知道或怀疑缺失数据以有偏差的方式出现？如果丢失的记录可能涉及与整个数据集相比具有不同特征或模式的观测值，那么对它们进行补救就显得尤为重要。也许一个传感器位置或一个时间帧与丢失的数据密切相关。很可能需要该位置或时间来很好地捕捉所建模的领域的某些方面。
*   根据偏差问题，您可能决定丢弃那些属于“随机缺陷”子集的缺失数据的记录，但是那些具有系统缺失数据的记录是至关重要的，因为它们解决了问题的参数空间的不同区域。
*   你的唱片有很多还是很少的特色？一个记录有五个特征，其中两个缺失，对于好的模型来说不太可能保留多少有用的权重。50 或 1000 个要素中有一个要素缺失的记录更有可能值得补救。
*   缺失值的作用是什么？如果监督学习训练集中缺少目标特征，即对于分类或回归问题，插补不太可能对您有多大帮助。估计输入要素可能更有用。然而，即使在这种情况下，输入特征在问题或领域中的作用也可能不同；从“业务目的”的角度来看，一个特定的特性可能是至关重要的，不管它实际上是否是最具预测性的特性。将一个特征归结为中心任务的重要性通常是不明智的。

*效力*

Alon Halevy、Peter Norvig 和 Fernando Periera 写了一篇著名且引人注目的文章，讲述了大型数据集如何解决我们的许多问题，文章名为[数据的不合理有效性](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf)。

本章的前两节讨论单值插补。这直接符合我们一直认为的归罪。最后一节着眼于过采样和欠采样，这是整个数据集的修改。从组织和概念上来说，值得在估计专题下讨论这些问题。抽样的目标是产生一个我们认为更类似于我们试图建模的现实的数据集——这正是插补的目的。

# 典型值插补

> 还有另一个营销策略
> 
> 典型的女孩得到典型的男孩
> 
> –阿丽·阿普、帕洛玛·麦克拉迪、泰莎·波利特和薇薇·艾伯丁

**概念**:

*   确定要估计的值
*   数据集中的趋势
*   平均值、中值、几何平均值和多模态数据
*   基于人口的集中趋势
*   表示趋势的相邻数据

我们能做的最简单的事情就是假设缺失值与同一特性的总体趋势相似。在某些情况下，在缺乏关于特定记录的特定信息的情况下，领域知识可以告诉我们什么是合理的缺省。然而，如果没有这种背景，现有的数据可以为估计提供指导。

## 典型表格数据

让我们看看可从 UCI 机器学习资料库获得的[皮肤病学数据集](https://archive.ics.uci.edu/ml/datasets/Dermatology)。该数据包含 366 名患者的 34 次测量，其中每一位患者都被诊断为具有六种皮肤状况中的一种。大多数特征是观察到的一个特征的严重性的顺序编码测量。

我们得到的数据有些原始。`dermatology.data`文件是一个没有头文件的 CSV 文件。`dermatology.names`文件包含的内容比它的名字所暗示的要多一些。除了提供特性名称之外，它还以散文的形式对数据集进行了额外的阐述，例如值编码、未知值出现的位置以及其他一些内容。这本书的知识库中的`dermatology.py`文件包含了一些适度的数据信息。

```
from src.setup import *

from src.dermatology import *

df.iloc[:, [0, 1, 2, 3, -2, -1]].sample(6) 
```

```
 erythema    scaling    definite borders    itching    Age

—————————————————————————————————————————————————————————————————

247           2          2                   2          0     62

127           2          2                   2          2     44

230           3          2                   0          1     30

162           3          2                   2          2     22

159           3          2                   2          1     47

296           2          1                   1          3     19

                    TARGET

———————————————————————————

247              psoriasis

127          lichen planus

230    seboreic dermatitis

162          lichen planus

159    seboreic dermatitis

296      cronic dermatitis 
```

快速查看样本行并不会发现明显缺失的数据。我们可以进一步调查，以确定可能丢失的数据。根据所提供的描述，我们知道观察到的严重程度被编码为 0、1、2 或 3(特征“家族史”被编码为 0 或 1)。有没有超出这个编码的东西？

```
clean, suspicious = [], {}

for col in df.columns:

    values = df[col].unique()

    if set(values) <= {0, 1, 2, 3}:

        clean.append(col)

    else:

        suspicious[col] = values 
```

大多数字段限于预期的编码值。

```
print("No problem detected:")

pprint(clean[:8])

print(f"... {len(clean)-8} other fields") 
```

```
No problem detected:

['erythema',

 'scaling',

 'definite borders',

 'itching',

 'koebner phenomenon',

 'polygonal papules',

 'follicular papules',

 'oral mucosal involvement']

... 25 other fields 
```

一个很少有其他字段不属于编码集。然而，其中一个是`TARGET`，它只包含被诊断的几种情况的合理名称和拼写。年龄在很大程度上也包含合理的人类年龄，除了一个值`'?'`也存在于此。这是该数据集对缺失数据进行编码的方式。^(失踪)

*失踪*

尤其是 Pandas 库，默认情况下会将各种字符串值识别为“缺失”您可以按列手动配置在`pandas.read_csv()`和其他推断数据类型的函数中哪些值被认为是缺失的。在撰写本文时，以及在熊猫 1.0 中，这些缺省值正是这些字符串:`''`、`'#N/A'`、`'#N/A N/A'`、`'#NA'`、`'-1.#IND'`、`'-1.#QNAN'`、`'-NaN'`、`'-nan'`、`'1.#IND'`、`'1.#QNAN'`、`'<NA>'`、`'N/A'`、`'NA'`、`'NULL'`、`'NaN'`、`'n/a'`、`'nan'`和`'null'`。

其他库可能会也可能不会执行类似的推断/猜测，那些执行的库可能会使用不同的默认字符串集合。对于直接对浮点值进行编码的数据格式，通常使用 NaN(“非数字”)值来标识丢失的数据，NaN 是 IEEE-754 浮点数规范的一部分。对于这种编码的正确性，哲学态度各不相同，但你肯定会经常看到。在其他时候，会出现“特殊”值，例如-1(希望度量值必须为正)或 99999(希望度量值比这个值低几个数量级)。

```
# Notice age has some expected ages and also a '?'

print("Suspicious:")

pprint(suspicious) 
```

```
Suspicious:

{'Age': array(['55', '8', '26', '40', '45', '41', '18', '57', '22', '30', '20',

               '21', '10', '65', '38', '23', '17', '51', '42', '44', '33', '43',

               '50', '34', '?', '15', '46', '62', '35', '48', '12', '52', '60',

               '32', '19', '29', '25', '36', '13', '27', '31', '28', '64', '39',

               '47', '16', '0', '7', '70', '37', '61', '67', '56', '53', '24',

               '58', '49', '63', '68', '9', '75'], dtype=object),

 'TARGET': array(['seboreic dermatitis', 'psoriasis', 'lichen planus',

                  'cronic dermatitis', 'pityriasis rosea',

                  'pityriasis rubra pilaris'], dtype=object)} 
```

在识别出这个数据集使用的用于缺失数据的有些不寻常的值后，我们应该经常使用更标准的方法对它进行重新编码。特别是，将字符串值 ages 转换为浮点数，并用 NaN 表示缺失的数据，这是一种非常常见的样式，Pandas 以一些方便而有用的方式处理这种样式。为了在 Pandas 中实现这一点，我们首先用一个已知的“缺失”值代替`'?'`，然后将该列转换为浮点型。我们可以看到有几行有调整过的值。

```
# Assign missing ages marked with '?' as None

df.loc[df.Age == '?', 'Age'] = None  # or NaN

# Convert string/None ages to floating-point

df['Age'] = df.Age.astype(float)

# Display those rows with missing ages

df.loc[df.Age.isnull()].iloc[:, -4:] 
```

```
 inflammatory     band-like    Age                 TARGET

    monoluclear inflitrate    infiltrate

———————————————————————————————————————————————————————————————————————

33                       0             0    NaN              psoriasis

34                       0             0    NaN       pityriasis rosea

35                       0             0    NaN    seboreic dermatitis

36                       0             3    NaN          lichen planus

262                      3             0    NaN      cronic dermatitis

263                      2             0    NaN      cronic dermatitis

264                      3             0    NaN      cronic dermatitis

265                      3             0    NaN      cronic dermatitis 
```

问题出现了，对于这个数据集，我们可以估计出什么样的“典型值”。358 排有具体年龄，都在人类寿命的合理范围内。八行缺少值。有许多熟悉的方法来识别数据收集的“中心趋势”。突出的是众数、中位数、平均数、几何平均数，以及不太常见的调和平均数。具体到熊猫，只有前三个是内置方法。对于几何平均值或调和平均值，您通常会使用`scipy.stats.gmean`或`scipy.stats.hmean`(作为自定义函数，这两个函数都不难构建)。在不同的编程语言或工具中，这些细节会有所不同，但概念是相同的。

当数据覆盖几个数量级时，几何平均数是有用的。通常，这些数据与您使用对数刻度轴绘制的数据类型相同。关于指数增长的测量值通常用几何平均数适当地“平均”。当你比较行动的*速率*时，调和平均值是有用的。例如，如果您有一个测量某些对象速度的功能，典型值最好作为调和平均值来测量。

请记住，这几个平均值通常在数值上彼此接近，由于插补一开始就是一种猜测，它们之间的选择*可能*是为了争取一个*假精度*。^(精密)

*精度*

当数字数据以暗示比合理精度更好的方式呈现时，假精度(也称为超精度、假精度、错位精度和假精度)出现；由于精度是精度的极限，这通常会导致对精度的过度自信，称为*精度偏差*。

对于或多或少呈线性分布的数据集合，包括正态分布，一个更普通的平均值可能是合适的。我们可以尝试用病人的典型年龄作为一个很好的代表。我们在这个数据集中遇到了多模态分布，这在小数据中很常见。此外，年龄在 0 到 80 岁之间，只有 358 个数据点，数据通常是“不规则的”模式可能不是一个好方法(但如果一个值明显占主导地位，可能是)。

```
df.Age.mode() 
```

```
0    40.0

1    50.0

dtype: float64 
```

我们可以使用快速绘图来更好地了解年龄的分布，也许还可以了解什么值可能是典型值。轴标签和刻度被省略了，因为我们在研究中只想要一个分布的总体感觉。

```
(df.Age

   .value_counts()

   .sort_index()

   .plot(kind="bar", yticks=[], xticks=[], 

         title="Age distribution of patients "

               f"({df.Age.min():.0f} to {df.Age.max():.0f})")

); 
```

![](img/B17126_06_01.png)

图 6.1:患者的年龄分布(0 至 75 岁)

在这种情况下，没有什么特别突出的可能候选人。有几个峰值仅略少于两种模式，并且对于噪声数据没有普遍的模式。

最有可能的平均值或中位数更有代表性。在这里，这些值彼此相当接近，尽管两者在两种模式下都有明显的不同。

```
df.Age.mean(), df.Age.median() 
```

```
(36.29608938547486, 35.0) 
```

然而，我们也可以尝试使用领域知识来对要估计的值做出更明智的选择。例如，描述该数据集的元数据表明，它是由几名土耳其研究人员开发的，并于 1998 年发表。患者保密协议禁止披露更精确的细节，但我们可能会参考历史人口统计数据，例如从 [Statista](https://www.statista.com/) 获得的基于世界贸易组织数据集的表格。

1998 年土耳其的平均年龄似乎约为 24 岁。

| 年 | 年龄中值 | 年 | 年龄中值 |
| --- | --- | --- | --- |
| One thousand nine hundred and fifty | Nineteen point seven | One thousand nine hundred and ninety | Twenty-one point seven |
| One thousand nine hundred and fifty-five | Nineteen point four | One thousand nine hundred and ninety-five | Twenty-three |
| One thousand nine hundred and sixty | Nineteen point six | Two thousand | Twenty-four point five |
| One thousand nine hundred and sixty-five | Eighteen point four | Two thousand and five | Twenty-six point four |
| One thousand nine hundred and seventy | Eighteen point five | Two thousand and ten | Twenty-eight point two |
| One thousand nine hundred and seventy-five | Nineteen point one | Two thousand and fifteen | Twenty-nine point eight |
| One thousand nine hundred and eighty | Nineteen point five | Two thousand and twenty | Thirty-one point six |
| One thousand nine hundred and eighty-five | Twenty point five |  |  |

(资料来源:WTO，2018；预计 2020 年)

当然，如果我们的领域知识比人口信息更深入，我们也可能知道年龄与皮肤状况的一般关系。作为一个非专家，我倾向于假设这种情况通常会随着年龄的增长而增加，但是好的估计应该有一个基础，而不仅仅是一个模糊的预感。出于本书的目的，让我们将未知值估计为数据本身内的中位年龄*。*

```
df.loc[df.Age.isnull(), 'Age'] = df.Age.median()

df.Age.value_counts().head() 
```

```
35.0    22

50.0    17

40.0    17

36.0    16

27.0    16

Name: Age, dtype: int64 
```

估计的 35 岁的人变得有点多，但并不明显。对于这些估计的观察结果来说，年龄相关的趋势应该是中等的。

## 位置插补

从某种意义上来说，趋势和地点显然是相关的。例如，在时间序列中，在某一特定分钟进行的测量是同一仪器在下一分钟进行的测量的“局部”。也就是说，假设一个大致分钟尺度的测量频率；在我工作多年的领域——分子动力学——时间步长大约是飞秒(10 ^(-15) )，一分钟远远超出了任何可实现模拟的范围。相反，在地质学或宇宙学中，当对纪元进行排序时，分钟是小得不可测量的。无论如何，线性或序列局部性将在下一节趋势插补中讨论。

然而，一般来说，局部性并不特别指序列。例如，在维空间中——无论是直接表示物理空间，还是涉及参数或相空间——局域性可能只是空间中的“接近度”。基于附近的其他值来估计值通常是填充我们实际上没有的数据的合理方式。在某些情况下，基于地点的插补比假设全局默认值更有可能代表基础数据。

例如，另一个可从 UCI 机器学习库中获得的数据集是一个由光学字符识别应用程序可以识别的手写数字的集合。这些特定的扫描图像包括抗锯齿，因此黑色墨水中的实际笔画通常被不同暗度的灰色像素包围。虽然暗和亮之间确实存在相邻的边界，但通常在黑和白像素之间存在中间灰度。在摄影图像中，图像区域之间的中间色更为常见。

我修改了 UCI 数字图像的子集，随机去掉了一些像素。在此表示中，丢失的灰度值由-1 表示。实际扫描的像素值介于 0(白色)和 16(黑色)之间。让我们简单看一下数据集。我们可以看到是 50 个 8 × 8 图像的样本。8 × 8 数组中的每个位置都是一个小整数。

```
print("Array shape:", digits.shape) 
```

```
Array shape: (50, 8, 8) 
```

每个数字数组中有几个-1 值。我们可以通过将带有阴影的像素与值一起可视化来了解缺失的数据。显示了几个样本，每个缺失的像素内部都包含一个“`x`”。

```
show_digits(digits) 
```

![](img/B17126_06_02.png)

图 6.2:可视化一些数字

如果我们愿意的话，我们可以应用复杂的技术来进行边缘检测、卷积过滤等等，这可能会找到更好的像素。然而，在本演示中，我们将简单地假设每个缺失的像素是其相邻像素的平均值。当然，是否像水平和垂直邻居一样对对角线进行加权是一个额外的决定。潜在地，水平和垂直，或者向上和向下的不同权重也是如此，等等。

```
# Coded for clarity, not for best vectorized speed

# Function definition only; used in later cell

def fill_missing(digit):

    digit = digit.copy()

    missing = np.where(digit == -1)

    for y, x in zip(*missing): # Pull off x/y position of pixel

        # Do not want negative indices in slice

        x_start = max(0, x-1)

        y_start = max(0, y-1)

        # No harm in index larger than size

        x_end = x+2

        y_end = y+2

        # What if another -1 is in region? Remove all the -1s

        region = digit[y_start:y_end, x_start:x_end].flatten()

        region = region[region >=0]

        total = np.sum(region) 

        avg = total // region.size

        digit[y, x] = avg

    return digit 
```

函数`fill_missing()`只是基于相邻的数字创建一个新的数字。我们可以通过遍历原始数据集的样本来轻松构建新的数据集。

```
new = np.empty_like(digits)

for n in range(new.shape[0]):

    new[n] = fill_missing(digits[n])

show_digits(new) 
```

![](img/B17126_06_03.png)

图 6.3:输入缺失值的数字

正如本书各处一样，我的意图是促进对提高数据质量的最佳方式的思考，利用实际数据中有缺陷的资源。我在样本代码中执行的特定邻接平均法是*通常是*一种合理的方法——显然在示例中表现得非常好——但是你必须清楚地表达你的插补目标；同样，考虑您的特定方法会如何影响您稍后执行的建模或分析。也许不同的估计方法会更适合您选择的模型。

让我们转向数据趋势，包括时间序列和其他种类的线性趋势。

# 趋势估计

> 排队时间越长，站错队的可能性就越大。
> 
> –匿名

**概念**:

*   趋势的类型(回归)
*   充满
*   线性的
*   时间敏感的
*   非本地
*   与另一个变量相关
*   研究一个更大的例子:按类聚合时间戳
*   判断上下文是否足以进行插补
*   相当于中心值插补的静态趋势
*   时间序列以外的趋势
*   多项式拟合趋势插补

数据科学家用于插补的最明显、也可能是最广泛使用的趋势是时间序列数据。如果我们按照相对有规律的时间表进行观测——每飞秒、每秒、每分钟、每年、每个世纪，或者其他时间——我们有理由猜测，一个丢失的观测结果与它附近的时间戳观测结果相似。趋势估计的一个非常常见的用途是在金融模型中；例如，证券的市场交易可能具有不规则的事件间隔(或者缺失数据，或者交易不如分笔成交点频率常见)。然而，许多其他领域也出现了同样的问题。

有几种通用的趋势插补方法。这些方法包括向前填充、向后填充、局部回归、时间敏感回归、非局部回归和相关插补。在我在这一节讨论的所有插补中，有一点需要注意的是，它们不能处理比缺失数据中的缺口周期更短的高频信号。

例如，如果某个东西可以在随机的 10 赫兹频率上波动，那么一秒钟间隔的观测值对于估计就没有什么价值。显然，在某种程度上，这取决于重叠信号的强度，但这是一个需要记住的问题。

## 趋势的类型

**向前/向后填充**:假设一个缺失值与序列中该值之前/之后的值相同。Pandas `Series.fillna()`方法可以执行这种插补，R tidyverse 中的`tidyr`包的`fill()`函数也可以。

**局部回归**:假设有一个连续函数连接与缺失观测值相邻的观测值。大多数时候我们简单地假设一个线性函数；例如，我们取相邻观测值的平均值来填充缺失值。然而，在概念上，我们可以基于作为非线性函数样本的相邻点来估计值。

时间敏感回归(Time-sensitive regression):即使我们只查看与缺失值相邻的值，如果那些相邻的值代表日期时间，我们可能会利用观察的实际时间间隔。如果所有观测在时间上间隔均匀，这是没有意义的。这里的一般直觉是，价值在较长的时间内可能比在较短的时间内变化更大。

**非局部回归**:在一个序列中，一个回归可以是全局的，也可以是比相邻元素范围更广的窗口回归。同样，线性回归是常见的，也是最简单的方法，但其他函数形式的回归也是可能的。全局或窗口回归可能对潜在趋势的随机局部波动不太敏感。当然，缺失的数据本身可能就是这种波动，所以这种方法——以及大多数趋势插补的其他方法——相当于对可变性进行较小程度的平滑。

**相关性插补**:可能是一列(特征)中缺失值的数据与一列或多列其他数据显著相关。如果是这种情况，可能下游的模型应该识别互相关，例如通过分解和维数减少。但是作为最初的插补步骤，假设基于相关性的值通常是有用的。

从某种技术角度来说，我们可以注意到插补通常会减少异方差，因为几乎每种插补都遵循一个趋势，而不是该趋势的可变性。对于几乎所有的数据科学目的来说，这是可取的，或者至少是可以接受的，但是我们应该避免对估计数据进行多种统计归纳(通常使用原始数据来代替)。

***

让我们先看一个非常简单的时间序列例子，来说明其中的几种方法。我们简单地构建了一个具有日期级分辨率的小熊猫序列，但是观测日期的间隔不均匀。第一个观察值与后来的观察值有显著不同，主要是为了强调隐含的整体斜率不同于元素之间的局部差异。

```
date_series 
```

```
2001-01-01   -10.0

2001-01-05     1.0

2001-01-10     2.0

2001-02-01     NaN

2001-02-05     4.0

dtype: float64 
```

向前或向后填充很简单。

```
date_series.ffill()  # or .bfill() 
```

```
2001-01-01   -10.0

2001-01-05     1.0

2001-01-10     2.0

2001-02-01     2.0

2001-02-05     4.0

dtype: float64 
```

局部回归，或者说白了叫**平均化**，也很容易。

```
date_series.interpolate('linear') 
```

```
2001-01-01   -10.0

2001-01-05     1.0

2001-01-10     2.0

2001-02-01     3.0

2001-02-05     4.0

dtype: float64 
```

在 Pandas(和其他工具)中，我们可以基于时间增量对趋势进行加权。这仍然是局部操作(在相邻值的意义上)，但是它是基于`2001-02-01`到`2001-02-05`比到`2001-01-10`更接近的加权平均。也就是说，不使用不相邻的极值-10。

```
date_series.interpolate('time') 
```

```
2001-01-01   -10.000000

2001-01-05     1.000000

2001-01-10     2.000000

2001-02-01     3.692308

2001-02-05     4.000000

dtype: float64 
```

假设这个数列是单调递增的，我们可以通过从起点到终点画一条直线来进行简单的回归。这不是最小二乘线性回归，但它强调了均匀插值和基于时间的插值之间的差距。2 月 1 日 0.5 的估计值可能看起来不合适，但如果我们想象一下全球趋势，它是有意义的。OLS(普通最小二乘法)值也将显著低于时间内插值，因为一个初始值比序列中的其他值低得多。

```
plot_filled_trend(date_series) 
```

![](img/B17126_06_04.png)

图 6.4:基于线性趋势的全球插补

我们还可以寻找特征之间的相关性来估计缺失值。例如，在本章前面使用的皮肤病学数据中，一些观察到的特征与偶尔缺失的`Age`特征明显相关。在这种情况下，所有的医学观察都是有序的，但是类似的方法也适用于连续的特征。特别是，特征`follicular horn plug`与患者年龄强烈(且单调)负相关。我们可以简单地根据其他特征的顺序值来指定每个缺失的年龄。让我们计算每一个`follicular horn plug`度的平均年龄。

```
from src.dermatology import derm

feat = 'follicular horn plug'

age_groups = derm.groupby(feat).Age.mean()

age_groups 
```

```
follicular horn plug

0    37.696429

1    20.400000

2    10.625000

3     9.750000

Name: Age, dtype: float64 
```

几行中等密度的熊猫代码可以根据序数特征将它们分组的平均年龄分配给每个缺失的`Age`。碰巧在这个特定的数据集中，所有缺失的年龄都在零度为“`follicular horn plug`”的患者中，但是其他数据可能会不同(或者可能在收集或整理方法中有什么东西导致了这种相关性)。

```
# The row labels for rows with missing Age

missing = derm.loc[derm.Age.isnull()].index  

# Assign Age based on mapping the feature

derm.loc[missing, 'Age'] = derm[feat].map(age_groups)

# Look at filled data for a few features

derm.loc[missing, [feat, 'scaling', 'itching', 'Age']].head(3) 
```

```
 follicular horn plug   scaling   itching         Age

——————————————————————————————————————————————————————————

33                      0         2         0   37.696429

34                      0         1         0   37.696429

35                      0         2         2   37.696429 
```

熊猫计算平均年龄的精确度没有意义，但明确降低它也没有特别的好处。

## 更大的粗略时间序列

美国宾夕法尼亚州的费城提供了一个名为[opendatahilly](https://www.opendataphilly.org/)的极好资源，它是“费城地区的开放数据目录”。除了作为该城市的官方开放数据存储库，它还包括来自该地区许多组织的数据集。”我们在本节中使用的数据集很有价值，质量也很好，但它也包含足够的细微差别，因此需要一些清理步骤来使其符合我们的目的。

我们将在本节中讨论的特定真实世界数据集涉及每处房产的估定市值。我通过在 HTTPS 接口上传递一个 SQL 查询并返回一个 JSON 结果来获得这些数据。特定的查询是:

```
SELECT parcel_number, year, market_value FROM assessments 
```

“地块”只是一个税收/监管词，指普通契约下的财产。我应该清楚地注意到，OpenDataPhilly 实际上在这个返回的结果中有完整的信息(在撰写本文时)，但是我人为地设计了一个随机缺少值的版本。完整的数据在文件`philly_house.json`中，缺少值的版本是`philly_missing.json`，两者都在本书的存储库中。对于缺失的数据，大约 5%的市场价值已被 NaN 替代。

### 理解数据

我相信服务限制了少于完整数据集的结果；与费城人口相比，包含的包裹相对较少。这个问题对于本节来说并不重要，但是如果我们有其他目的的话，这个问题可能与我们的研究相关。让我们看看数据集，在插补之前做一些基本的取证工作。即使以相当好的初始形式提供了“干净的数据”,也需要很多步骤。

```
parcl = pd.read_json('data/philly_missing.json')

parcl.sample(7, random_state=4) # Random state highlights details 
```

```
 parcel_number   year   market_value

———————————————————————————————————————————

 1862     123018500.0   2014        96100.0

 3921     888301242.0   2015        15000.0

  617             NaN   2018            0.0

 1068     311033500.0   2018        16500.0

11505     888301508.0   2015        15000.0

 3843     252327300.0   2014            NaN

10717     314204200.0   2016        41800.0 
```

数据集的一般概念是每个地块在几年中的每一年都有市场价值。我们可以在示例中看到，一些`parcel_number`值缺失，一些`market_value`值缺失。后者在我得到的数据中；每一行都有年份，但市值为零。缺失的市值是我人为构建的。

让我们了解一下这些东西的分布情况。

```
nparcel = len(parcl[parcl.parcel_number.isnull()])

nmarket = len(parcl[parcl.market_value.isnull()])

print(f"All rows:  {len(parcl):>8,}")

print(f"No parcel: {nparcel:>8,}")

print(f"No market: {nmarket:>8,}") 
```

```
All rows:    18,290

No parcel:    1,140

No market:      965 
```

在这个例子中，我不知道为什么返回的结果没有包裹号，但是在分析价格趋势的既定目标下，我们不能使用这些结果。丢失的包裹号是我获得数据时的特征，而不是我的修改。让我们抛弃它们，因为它们对我们的分析没有帮助。我们还想知道，排除这一因素后，一处房产在五年内的典型价格变化是多少。也许我们想知道千元组的标准差。我们在下一个单元格中计算这个。

请注意，通常(但*不总是*)每个包裹都有五个不同的年份。因此，下面部分显示的值计数的总和加起来略多于被筛选行总数的五分之一。

```
parcl = parcl[parcl.parcel_number.notnull()]

print(f"Remaining rows: {len(parcl):,}")

stds = parcl.groupby('parcel_number')['market_value'].std()

(stds // 1000 * 1000).value_counts().head() 
```

```
Remaining rows: 17,150

0.0       2360

7000.0     114

6000.0     109

2000.0     103

3000.0      83

Name: market_value, dtype: int64 
```

显然，到目前为止，最常见的标准差似乎是零美元范围。因为我们要进行舍入，所以这可能是一个实际的零，也可能只是一个小于 1000 美元的金额。我们应该仔细看看。

```
stds[stds == 0].count() 
```

```
2309 
```

那些市值变化很小的地块中的大部分实际上在五年内变化为零(至少根据评估)。此外，零变化情况约占所有数据的三分之二。当然，其中一些零变化的宗地可能部分没有变化，因为它们缺少数据。Pandas 通常会忽略聚合的缺失数据。例如，对于具有四个相同市场价值和一个缺失市场价值的地块，不清楚最佳补救措施是什么。观察其中的几个可以让我们的直觉有所了解。

首先，让我们进一步清理我们的数据框架。现在所有的 NaN 值都被删除了，我们希望所有的宗地号都是整数。我们也可以受益于年份是真实的年份，而不仅仅是整数。

```
parcl['parcel_number'] = parcl.parcel_number.astype(np.uint32)

parcl['year'] = pd.to_datetime(parcl.year, format="%Y")

parcl.head() 
```

```
 parcel_number         year   market_value

——————————————————————————————————————————————

0       213302600   2016-01-01       196800.0

1       213302600   2015-01-01       196800.0

2       213302600   2014-01-01       196800.0

3       213308200   2018-01-01       198000.0

4       213308200   2017-01-01       198000.0 
```

一些稍微有点纠结的熊猫代码可以告诉我们，零钞包裹有多长时间有缺失数据，缺失数据的包裹有多少。当然，除了下面的特定流畅代码之外，还有其他方法可以得出这个答案，但是这种风格是许多库中数据框操作的典型风格，因此值得理解。

```
(parcl

     # Look at those parcels with zero STD among years

     # We calculated those standard deviations as 'stds'

     # The '.index' for non-deviation to find parcels

     .loc[parcl.parcel_number.isin(stds[stds == 0].index)]

     # Group by which parcel we are looking at

     .groupby('parcel_number')

     # We care about market values for parcel

     .market_value

     # Aggregation is count of different market values

     .agg('count')

     # Summarize rather than show individual parcels

     .value_counts()

) 
```

```
5    1767

4     473

3      66

2       3

Name: market_value, dtype: int64 
```

### 删除不可用的数据

如果存在少于四个年的观测值，则该宗地不可用于下游分析。这是针对这个问题的一个特定领域的判断。很明显，这不是什么万能法则，只是简单的任务驱动。我们可以用更多的熊猫代码移除那些有问题的包裹。下面的代码与上一个示例非常相似，但是它使用了描述性的临时名称，而不是流畅的样式。这两种风格本身都不好，但是你肯定会在其他数据科学家或开发人员的代码中遇到这两种风格。

这段代码中需要注意的一个微妙之处是，Pandas `.groupby()`操作忽略了聚合的缺失数据，即使只是为了计数。因此，如果一个组有三个数值和两个 nan(也就是说，五行符合类别),不仅`.mean()`会给出三个非缺失值的平均值，而且`.count()`会给出答案 3，而不是 5。方法`.size()`将包括 NaNs。

```
# Parcels that have no change between years (bool array)?

nochange = parcl.parcel_number.isin(stds[stds == 0].index)

# Parcel data grouped by parcel

by_parcel = parcl[nochange].groupby('parcel_number')

# Aggregate on number of market values and compare with 4

few_vals = by_parcel.market_value.count() < 4

# The parcel numbers that have fewer than 4 market values 

few_index = few_vals[few_vals == True].index

# What are the actual row numbers we wish to drop?

drop_rows = parcl[parcl.parcel_number.isin(few_index)].index

# New name and DataFrame holds the non-dropped rows

parcl2 = parcl.drop(drop_rows)

# We trim from 17,150 rows to 16,817

parcl2 
```

```
 parcel_number         year   market_value

——————————————————————————————————————————————————

    0       213302600   2016-01-01       196800.0

    1       213302600   2015-01-01       196800.0

    2       213302600   2014-01-01       196800.0

    3       213308200   2018-01-01       198000.0

  ...             ...          ...            ...

18286       661010710   2016-01-01       215000.0

18287       661010710   2015-01-01       215000.0

18288       661010710   2014-01-01       215000.0

18289       661010720   2018-01-01       215000.0

16817 rows × 3 columns 
```

让我们转向实际趋势估计。根据规定，当除了一年之外的所有年份都显示一个共同的市场价值时，其余年份(缺少一个价值)应该被估计为相同的价值。在某种意义上，这是“零趋势”，但它也是与上述相关插补相同的动作。将宗地号视为分类变量(它是“本体上的”，尽管有许多类)，我们估计的是一个典型的值，也正是该类的平均值、中值、最小值、最大值和众数。

### 输入一致性

这里的方法不是唯一可能的方法。例如，如果我们决定费城的住房价值在 2014 年至 2018 年间普遍上涨，那么即使不知道特定地块的特定年份，我们也可以估计出这一趋势。然而，只有当缺失的年份是第一年或最后一年时，这种替代方法才容易理解。如果某个地块的 2014、2015、2017 和 2018 年的所有值都相同，那么线性全球趋势实际上不会告诉我们该地块在 2016 年的情况。

```
# Aggregate group to find parcels w/ exactly four years

# The 'by_parcel' group already assumes no change

four_vals = by_parcel.market_value.count() == 4

# The parcel numbers that have 4 market values

four_index = four_vals[four_vals == True].index

# Row numbers of parcels to impute on

impute_rows = parcl2[parcl2.parcel_number.isin(four_index)].index

# Group parcels only for parcels with 4 market values

by_four = parcl2.loc[impute_rows].groupby('parcel_number')

# Impute the mean (or identically median, etc) to rows

new_vals = by_four.market_value.mean()

# A mapping of SOME parcel numbers to value

new_vals 
```

```
parcel_number

42204300     30800.0

42205300     33900.0

42206800     30800.0

42207200     30800.0

              ...   

888301511    15000.0

888301512    15000.0

888301814    15000.0

888301815    15000.0

Name: market_value, Length: 473, dtype: float64 
```

上面的代码省略了一个细节。我们寻找一个地块有四个非缺失值的地方，假设这可能意味着某个市场价值有一个 NaN 匹配该地块。然而，从技术上讲，这不一定是真的。如果一个包裹总共只有四行，这表明丢失了一整行，而不仅仅是与该行相关的市场价值。下一个代码块填充这些公共的组值，但是我们添加了几行代码来显示它只是将相同的值重新分配给四个现有的行。

为了允许检测和显示我们希望记录的异常情况，下一个代码是一个显式循环。熊猫通常更习惯的做法是——或通常的数据帧——向量化操作以提高速度。我们可以在熊猫身上用另一个`.groupby()`伴随着一个略带魔法的`.transform(lambda x: x.fillna(x.mean()))`来做到这一点。

对于少于 20，000 行的数据，速度差异并不重要，但是对于数百万行的数据，速度差异就很重要了。

```
# We keep a history of changes in different DFs

parcl3 = parcl2.copy()

# Loop through the new filled values by parcel

for n, (index, val) in enumerate(new_vals.items()):

    # Assignment will fill multiple rows, most redundantly

    parcl3.loc[parcl3.parcel_number == index, 'market_value'] = val

    # Did we start with only four rows in total?

    if len(parcl3.loc[parcl3.parcel_number == index]) == 4:

        print(f"Parcel #{index} has only 4 rows total (all ${val:,.0f})") 
```

```
Parcel #352055600 has only 4 rows total (all $85,100)

Parcel #541286700 has only 4 rows total (all $116,600)

Parcel #621431100 has only 4 rows total (all $93,800) 
```

我们在这部分做的清理已经比较详细了。我们应该检查我们的工作。我们希望`parcl3`包含与`parcl2`相同的行数，因为缺失值插补不应改变这一点。我们还知道，最后一位代码对 473 个包裹进行操作。然而，其中三个地方开始时只有四行。因此，如果一切顺利，两个版本之间应该修改了 470 行，在所有情况下都用一个值替换一个 NaN。

```
assert len(parcl2) == len(parcl3) == 16_817

(parcl3[parcl3.market_value.notnull() &

       (parcl2.market_value != parcl3.market_value)]

     .sort_values('parcel_number')) 
```

```
 parcel_number         year   market_value

——————————————————————————————————————————————————

1733         42204300   2018-01-01        30800.0

3718         42205300   2017-01-01        33900.0

1306         42206800   2014-01-01        30800.0

1346         42207200   2014-01-01        30800.0

...               ...          ...            ...

11517       888301511   2018-01-01        15000.0

11525       888301512   2015-01-01        15000.0

 7802       888301814   2016-01-01        15000.0

14156       888301815   2015-01-01        15000.0

470 rows × 3 columns 
```

### 插入文字

在真正实现趋势估计之前，这一部分已经走了很长一段路。然而，最初理解数据集总是必不可少的，并且在我们能够执行趋势插补本身之前，经常需要进行其他清理。插补在成为可能之前需要适度的清洁度。幸运的是，在 Pandas 和其他类似的数据框架工具中，实际趋势插补非常紧凑。

在我们这里选择的方法中，填充值需要两步。线性(局部)插值作为一种方法在这里感觉是合理的。由于只有五个时间步长，并且数据集中的大多数市场价值实际上根本没有变化，因此任何类型的全局回归都不支持该示例。

默认的熊猫`.interpolate()`给了我们*几乎*想要的东西；但是，它不会处理丢失的第一个元素*。因为它以向前的方式操作，所以该方法默认为对尾部元素进行向前填充。为了确保第一个元素也被估计，我们需要进行反向填充。*

这里有一个技巧需要注意。如果我们简单地对整个数据框进行插值，则可能会根据之前的宗地填充一些值。具体来说，如果与一个地块相关联的第一年是 NaN，我们将获得前一个地块的最后一个值和下一个地块的第一个值之间的无意义趋势。因此，我们需要以小组为基础的方式运作。

我们将要做的简单预览可以在一个小系列中看到，首先只有插值，然后添加回填。

```
s = pd.Series([None, 1, 2, None, 3, 4, None])

s.interpolate() 
```

```
0    NaN

1    1.0

2    2.0

3    2.5

4    3.0

5    4.0

6    4.0

dtype: float64 
```

```
s.interpolate().bfill() 
```

```
0    1.0

1    1.0

2    2.0

3    2.5

4    3.0

5    4.0

6    4.0

dtype: float64 
```

让我们把这些碎片拼在一起。我们首先确保按照包裹编号和年份正确排序，然后进行插值，再进行回填。

```
# Sort data to keep parcels together & years in order

parcl4 = parcl3.sort_values(['parcel_number', 'year'])

# Interpolate per group

parcl4['market_value'] = (

    parcl4

    .groupby('parcel_number')

    .market_value

    .transform(pd.DataFrame.interpolate))

# Back fill per group

parcl4['market_value'] = (

    parcl4

    .groupby('parcel_number')

    .market_value

    .transform(pd.DataFrame.bfill)) 
```

既然我们已经(可能)完成了清理和趋势插补，我们应该对我们的数据框架进行健全性检查。

```
print(f"Total rows after operations: {len(parcl4):,}")

# Overlooked missing data

parcl4.loc[parcl4.market_value.isnull()] 
```

```
Total rows after operations: 16,817

        parcel_number         year   market_value

——————————————————————————————————————————————————

16461       571291500   2018-01-01            NaN 
```

最后一次检查显示，有一个地块只有一年的数据，因此没有趋势插值。很可能我们也想从我们的分析中丢弃这一行。在我们离开这一部分之前，我们可以向自己保证这个不寻常的行不是我们过滤和计算的产物，而是存在于原始数据本身中。

```
# As read from disk (other than missing parcels)

parcl.loc[parcl.parcel_number == 571291500] 
```

```
 parcel_number         year   market_value

——————————————————————————————————————————————————

16461       571291500   2018-01-01            NaN 
```

## 非时间趋势

这本书试图尽可能多地使用真实世界的数据。真实数据集的奇怪事故、模式和怪异角落值得一试。合成数据——除了用于狭义解释 API 的非常短的例子之外——有遗漏一些混乱的风险。尽管如此，对于这一部分，我发明了一个异想天开的虚构数据集，我认为它有一个有趣的结构。我先向我的读者中的固态物理学家或量子化学家道歉，他们可能会注意到，即使是漫画书里的金属也不能以我声称的方式表现。

莱克斯·卢瑟实验室做了很多实验，包括用激光照射各种形式的氪石，以此来打败超人，统治世界。特别是，他们注意到许多种类的氪星石在暴露于不同波长的激光时，会获得一个宽的视觉波段。由于氪星石供应不足，他们无法测试所有激光波长下所有元素类型的行为。此外，他们使用的千瓦级激光器都有特定的频率，但他们可能希望使用不同于试验中使用的激光器来开发武器。

数据框包含实验室进行的观察。单位是以方向*坎德拉*而不是总*流明*来测量的，因为激光聚焦在一个方向上。

```
krypt = pd.read_fwf('data/excited-kryptonite.fwf')

krypt 
```

```
 Laser_type_kw    Subtype    Wavelength_nm    Kryptonite_type

————————————————————————————————————————————————————————————————————

  0      Helium–neon        NaN            632.8              Green

  1      Helium–neon        NaN            543.5              Green

  2      Helium–neon        NaN            593.9              Green

  3      Helium–neon        NaN            611.8              Green

...              ...        ...              ...                ...

 95          Excimer        ArF            193.0               Gold

 96          Excimer        KrF            248.0               Gold

 97          Excimer       XeCL            308.0               Gold

 98          Excimer        XeF            353.0               Gold

       candela_per_m2

——————————————————————

  0           415.837

  1               NaN

  2           407.308

  3           401.305

...               ...

 95           611.611

 96               NaN

 97           608.125

 98               NaN

99 rows × 5 columns 
```

一次可视化将会清楚地表明，至少在测试的激光波长范围内，每种测试的氪星石——绿色、红色和金色——似乎都有不同的或多或少的对数线性响应曲线。外来金属在未经测试的波长下仍有可能具有令人惊讶的特性。然而，在第一遍中，我们基本上有一个回归问题。

```
plot_kryptonite() 
```

![](img/B17126_06_05.png)

图 6.5:氪石类型对波长的亮度响应

对于这一部分，我们不一定对完全回归感兴趣，而只是对缺失的观测值进行估计。在表格和图表中，您可以看到测试套件中的一些激光器没有针对某些类型`kryptonite`的可用数据。例如，1520 纳米的氦氖激光器仅针对金和红氪进行测试，9400 纳米的 CO [2] 激光器仅针对绿和红氪进行测试。

```
(krypt[

    (krypt.Wavelength_nm > 1500) & 

    (krypt.Wavelength_nm < 10000)]

.sort_values('Wavelength_nm')) 
```

```
 Laser_type_kw    Subtype    Wavelength_nm    Kryptonite_type

———————————————————————————————————————————————————————————————————

  5      Helium–neon        NaN           1520.0              Green

 38      Helium–neon        NaN           1520.0                Red

 71      Helium–neon        NaN           1520.0               Gold

  6      Helium–neon        NaN           3391.3              Green

...              ...        ...              ...                ...

 72      Helium–neon        NaN           3391.3               Gold

 28              CO2        NaN           9400.0              Green

 61              CO2        NaN           9400.0                Red

 94              CO2        NaN           9400.0               Gold

       candela_per_m2

——————————————————————

  5               NaN

 38           497.592

 71           616.262

  6           444.054

...               ...

 72           624.755

 28           514.181

 61           334.444

 94               NaN

9 rows × 5 columns 
```

虽然两个测量值可以直接相互计算，但可见光范围内的电磁频率占据了更线性的数值范围，而波长则跨越了几个数量级。为了我们的目的，用激光频率工作可能更友好。

```
λ = krypt.Wavelength_nm / 10**9   # Wavelength in meters

c = 299_792_458                   # Speed of light in m/s

krypt['Frequency_hz'] = c/λ

# Plot frequency vs luminance

plot_kryptonite(df=krypt, logx=False,

                independent='Frequency_hz') 
```

![](img/B17126_06_06.png)

图 6.6:不同类型氪石的亮度响应频率

从视觉上看，在使用频率的线性-线性图中，红色氪石的响应曲线明显弯曲，绿色氪石或许也是如此。很明显，数据是有噪声的，并且不紧密匹配任何平滑曲线；这是因为元素的物理性质还是实验装置的限制，我们目前还不知道。有了这个动机，我们可能会进行一次多项式拟合。

```
# Only perform the polyfit on the non-missing data

kr_vals = (krypt[krypt.candela_per_m2.notnull()]

           .sort_values('Frequency_hz'))

# Do a fit for each kryptonite color

for color in ('Red', 'Green', 'Gold'):

    # Limit to the color being fit

    kcolor = kr_vals.loc[kr_vals.Kryptonite_type == color]

    x = kcolor["Frequency_hz"]

    y = kcolor["candela_per_m2"]

    coef2, coef1, offset = np.polyfit(x, y, deg=2)

    # Print out the discovered coefficients

    print(f"{color:>5s} (hz → nit): "

          f"{coef2:.1e}*x^2 + {coef1:.1e}*x + {offset:.1e}")

    # Use coefficients to fill missing values

    kmissing = krypt.loc[krypt.candela_per_m2.isnull() & 

                         (krypt.Kryptonite_type == color)]

    x = kmissing.Frequency_hz

    krypt.loc[x.index, 'candela_per_m2'] = (

                            coef2*x**2 + coef1*x + offset) 
```

```
 Red (hz → nit): -2.6e-28*x^2 + 5.5e-13*x + 3.5e+02

Green (hz → nit): 1.4e-28*x^2 + -2.7e-13*x + 5.0e+02

 Gold (hz → nit): -4.1e-30*x^2 + 2.8e-15*x + 6.2e+02 
```

用基于多项式拟合的缺失数据再次标绘,没有任何新点出现明显的错位。当然，他们是否正确是需要更多领域知识的事情。至少我们的回归如我们预期的那样。

```
plot_kryptonite(df=krypt, logx=False,

                independent='Frequency_hz') 
```

![](img/B17126_06_07.png)

图 6.7:输入缺失数据时的亮度响应

通过插补，我们已经“填充”了所有明显缺失的值，这使得许多统计测试和机器学习算法有可能做到这一点。现在让我们转向一个更具全球性的采样问题。

# 抽样

**概念**:

*   分类变量和离散化连续变量
*   平衡目标类值
*   无替换取样
*   补替抽样法
*   复制过采样
*   模糊统计过采样

采样是为了以某种方式重新平衡数据集而对数据集进行的修改。不平衡可以反映出*或者*所使用的数据收集技术*或者*你所测量的现象的潜在模式。当一个变量是分类的，并且有一个明显的类别分布的显式计数时，这种不平衡会特别明显。一种特殊的采样是*时间序列重采样*，这将在 [*第 7 章*](Chapter_7.xhtml#_idTextAnchor009) 、*特征工程*中讨论。

在连续变量的分布仅仅是不均匀的情况下，不平衡也是相关的。这是很常见的，因为许多量——在某种意义上，可能是人们可以测量的大多数量——是不均匀分布的，例如正态分布或贝塔分布。为此，我们排除了极端的“长尾”分布，如幂律分布或指数分布。也就是说，仅在相对较窄的范围内具有峰值的连续值与跨越多个数量级的值相比，存在不同的问题。通常，将长尾分布转换为更线性的分布是有用的，例如，通过取原始值的对数或将值离散化为分位数。

一个简单的大致正态分布的例子是人类的身高。深入到细节中，实际数据可能是基于性别的双峰型，并且可能还有基于国籍、年龄等的附加二阶模式。对于这个简单的例子，不平衡本身就足以说明问题。很明显，人类的身高各不相同，但即使是最矮的新生儿和最高的成年人之间，也只有不到 5 倍的差异。仅在成年人中(排除一些极少数、极矮的人)，几乎都在 1.5x 以内，换句话说，身高本质上是一个线性量；但它不是均匀分布的。

本节中的例子将转向使用 R Tidyverse 而不是 Python。Python 数据框库——熊猫和其他——使显示的所有内容都同样简单；这种转变是在假设更多的读者更熟悉 Python 的基础上做出的，这是为了鼓励读者思考概念，而不是狭隘地思考库。

```
%load_ext rpy2.ipython 
```

我们可以读入一个包含 25，000(模拟)人的身体测量数据的数据集。就我们这里的目的而言，我们只想看看身高是如何分布的。

```
%%R -o humans

library('tidyverse')

humans <- read_csv('data/height-weight.csv')

humans 
```

```
── Column specification ───

cols(

  Height = col_double(),

  Weight = col_double()

)

# A tibble: 25,000 x 2

   Height Weight

    <dbl>  <dbl>

 1   167\.   51.3

 2   182\.   61.9

 3   176\.   69.4

 4   173\.   64.6

 5   172\.   65.5

 6   174\.   55.9

 7   177\.   64.2

 8   178\.   61.9

 9   172\.   51.0

10   170\.   54.7

# ... with 24,990 more rows 
```

将高度分成规则的数字增量，我们肯定会看到一个模糊的高斯分布，至少因为中等高度比更短或更高的范围出现得更频繁。

即便如此，这个样本中的所有人——以及几乎所有成年人——都在 153 厘米到 191 厘米的狭窄范围内。

```
humans.hist(figsize=(10,3), bins=12); 
```

![](img/B17126_06_08.png)

图 6.8:显示身高和体重分布的直方图

```
%%R

table(cut(humans$Height, breaks = 5)) 
```

```
(153,161] (161,168] (168,176] (176,183] (183,191] 

      145      4251     14050      6229       325 
```

如果身高是我们试图从其他特征(例如，营养、国籍、性别、年龄、收入等)预测的目标，那么对于许多种机器学习模型来说，罕见的类别(“非常矮”、“非常高”)几乎或绝对不会从其他特征中预测出来。有太多的人在其他方面与少数非常矮的人(样本中约 0.5%)相似，因此默认预测即使不只是“平均”，也只是“有点矮”

然而，注意，如果独立变量的参数空间的区域不平衡，则存在类似的问题。例如，如果印度尼西亚或荷兰在假设的训练集中只有几个样本(但其他国家有很多)，我们将很难利用这些国家的居民(在撰写本文时)分别拥有最短和最高平均身高的事实。此外，如果少量样本包括特别矮的荷兰人或特别高的印度尼西亚人，则类值的存在可能会使预测与我们希望的方向完全相反。

## 欠采样

> 细节决定成败。
> 
> -新堡

让我们看一看使用实际分类值而不是人为离散化范围的数据集。UCI 机器学习 1997 [汽车评估数据集](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation)在这里很有用。原始数据集使用各种分类词作为序数值，例如主干是“小”、“中”或“大”，或者维护价格是“低”、“中”、“高”或“高”。在本书中，这些被转换成连续整数。然而，我们将重点关注的总体评分是作为描述性词语留下的，尽管它也是以明显的隐含顺序排列的。

```
%%R

cars <- read_csv('data/cars.csv', 

                 col_types = cols("i", "i", "i", "i", "i", "i", "f"))

cars 
```

```
# A tibble: 1,728 x 7

    price_buy  price_maintain  doors  passengers  trunk  safety  rating      

        <int>           <int>  <int>       <int>  <int>   <int>  <fct>       

 1          1               0      3           6      0       0  Unacceptable

 2          2               2      3           6      2       1  Acceptable  

 3          2               2      5           2      1       1  Unacceptable

 4          0               1      3           2      2       1  Unacceptable

 5          2               1      5           2      0       1  Unacceptable

 6          3               1      2           6      2       1  Acceptable  

 7          0               2      4           4      0       0  Unacceptable

 8          1               2      2           4      2       0  Unacceptable

 9          1               0      4           4      0       1  Acceptable  

10          1               3      3           2      0       0  Unacceptable

# ... with 1,718 more rows 
```

想象一下，我们正试图根据一辆汽车的其他记录特征来预测它的“可接受性”。显而易见，在前十行中，大量是不可接受的。让我们来看看评级的总体等级分布。

```
%%R

table(cars$rating) 
```

```
Unacceptable   Acceptable    Very Good         Good 

        1210          384           65           69 
```

这些车的评估者在发现很少几辆车是好的或非常好的时候可能会相当挑剔。在任何情况下，这显示了评级特征中的强烈不平衡，这可能是我们在分类模型中使用的目标。我们希望以一种可能产生更高质量模型的方式清理我们的训练数据。请记住，不同的特定建模技术或多或少都有可能通过采样技术得到改进。例如，线性模型在很大程度上对类别不平衡不敏感，而 K-最近邻模型往往对这些问题高度敏感。但是，即使在这种概括中，不同数据集和不同领域的不同取样，也将在不同程度上有效。下游型号的选择非常重要。

如果三个条件成立，欠采样就不成问题:

*   我们在数据集中有很多行；
*   即使是不常见的类别也有合理数量的样本；
*   样本很好地覆盖了参数空间。

如果我们足够幸运地拥有所有这些条件，简单地选择最小类的样本大小就足够了。然而，如果我们不能达到这些条件——特别是，如果最小的班级太小——允许一定程度的不平衡通常并不可怕。50:1 不平衡很可能是个问题；2:1 很可能不重要。对于我们的汽车评估，让我们尝试从每个类别中找到 100 个样本，但满足于尽可能多的样本。这个数据集中不常见的类的样本少于 100 个，这并没有给我们太多的余地。

```
%%R

unacc <- sample(which(cars$rating == "Unacceptable"), 100)

acc <- sample(which(cars$rating == "Acceptable"), 100)

good <- sample(which(cars$rating == "Good"), 69)

vgood <- sample(which(cars$rating == "Very Good"), 65)

samples <- slice(cars, c(vgood, good, acc, unacc))

samples 
```

```
# A tibble: 334 x 7

    price_buy  price_maintain  doors  passengers  trunk  safety     rating   

        <int>           <int>  <int>       <int>  <int>   <int>      <fct>    

 1          0               1      2           6      2       2  Very Good

 2          0               0      4           4      2       2  Very Good

 3          1               0      3           6      1       2  Very Good

 4          0               0      5           6      1       2  Very Good

 5          1               0      3           4      2       2  Very Good

 6          1               1      3           6      1       2  Very Good

 7          1               0      5           4      1       2  Very Good

 8          1               0      4           4      1       2  Very Good

 9          0               0      3           6      2       2  Very Good

10          1               1      4           6      2       2  Very Good

# ... with 324 more rows 
```

这里我们手动选择每个类可用的行数，并且不使用更高级别的库，像[DMwR(R 数据挖掘)](Glossary.xhtml#_idTextAnchor039)、 [caret(分类和回归训练)](Glossary.xhtml#_idTextAnchor020)或 [ROSE(随机过采样示例)](Glossary.xhtml#_idTextAnchor115)，这将使采样更加简洁。这些软件包都包括各种更复杂的采样技术，其中一些我们很快就会用到。在 Python 世界中，包[不平衡学习](Glossary.xhtml#_idTextAnchor059)是的首选，它包含了提到的 R 包中的大多数技术。^(套餐)

*套餐*

虽然 R 和 Python 中可用的工具有很多重叠，但是这两种语言和社区在文化和关注点上还是有一些差异。一方面，R 无疑更侧重于统计，该领域可用的库的广度更深；相应地，其他领域的库在 R 中较浅。

除了技术焦点之外，编程语言社区中还有一个显著的哲学差异。Python 倾向于围绕具有许多贡献者的公共库进行联合，或者至少是覆盖相似领域的库之间的公共 API。r 倾向于开发许多包，每个包的贡献者相对较少，功能上只有部分重叠，对包之间共享 API 的坚持较少。NumPy、Pandas、scikit-learn 以及更狭义的不平衡学习是“标准”API。相比之下，在 R 中，data.table、data.frame 和 tibble 竞争着各种 API 和优势；更确切地说，DMwR、caret 和 ROSE 也在竞争。

让我们看一下我们获得的分布，以确保我们做了预期的事情。

```
%%R

samples %>%

  group_by(rating) %>%

  count() 
```

```
# A tibble: 4 x 2

# Groups:   rating [4]

  rating           n

  <fct>        <int>

1 Unacceptable   100

2 Acceptable     100

3 Very Good       65

4 Good            69 
```

只有 60 多个样本来自不常见的类，这可能太稀疏了。在很大程度上，无论我们使用什么技术，一个样本很少的类都不能覆盖特征的参数空间。我们从较大的类中选择的 100 个样本并不大，但是我们可以合理地希望，由于潜在总体要大得多，并且我们的采样是无偏的，这些样本不太可能完全错过参数区域。

虽然采样并不完美，但通过将欠采样和过采样结合起来，我们至少可以避免目标失衡，这种失衡可能会使我们的模型产生偏差。让我们通过允许替换(因此从低计数类复制)从每个类中取 150 个样本。

```
%%R

# Find indices for each class (dups OK)

indices <- unlist(

  lapply(

    # For each level of the rating factor,

    levels(cars$rating), 

    # sample with replacement 150 indices

    function(rating) {

      pred <- which(cars$rating == rating)

      sample(pred, 150, replace = TRUE) }))

# Check that we have drawn evenly

slice(cars, indices) %>%

  group_by(rating) %>%

  count() 
```

```
# A tibble: 4 x 2

# Groups:   rating [4]

  rating           n

  <fct>        <int>

1 Unacceptable   150

2 Acceptable     150

3 Very Good      150

4 Good           150 
```

## 过采样

> 上帝住在细节里。
> 
> –路德维希·密斯·凡·德罗(参见居斯塔夫·福楼拜)

当数据充足时，欠采样是为机器学习模型产生更平衡的训练数据的快速方法。大多数情况下，数据集不能很好地覆盖您的参数空间，以至于您可以简单地丢弃纯欠采样的训练数据。即使你有相当多的观察，即使是普通的类也会聚集在高维空间的一个原型区域周围。如果需要尽可能敏感地评估参数空间，丢弃数据是有风险的。当然，这也可能仅仅是这样的情况，以你所拥有的模型类型和计算资源的数量，你根本不能在一个完整的数据集上训练一个模型；如果是这样，欠采样有一个独立的吸引力，这样做的类敏感性完全是一件好事。

我们已经了解了如何执行最简单的过采样。例如，在汽车评估数据集中，我们可以简单地对最常见的类进行替换采样。正是这种技术会在最常见的类中产生一些噪声，因为一些样本会被重复，而另一些会被省略。^(取样)

*采样*

对每个类进行重采样的最直接的方法是不区分最常见的类和其他类。这意味着，如果最常见的类有 100 个项目，使用替换进行重采样将在重采样版本中忽略其中大约 36 个项目，并复制其他项目。相比之下，从一个只有 10 个初始项目的类中重新采样到 100 个项目将几乎肯定地表示每个项目至少一次。

在概念上，我们可以使用额外的代码来做一些更“公平”的事情我们将创建原始数据的副本。然后，我们将只从其他类中抽取`max_class_size-current_class_size`个项目。然后我们会把未接触过的原件和新样品结合起来。这至少可以确保每个原始数据在结果数据中至少出现一次。虽然这种方法可能是一种改进，但它仍然比下面讨论的 SMOTE 等方法没有那么细微。

另一种方法是简单地根据需要多次复制不常用的类，使它们与更常用的类大致相当。例如:

```
# Read the raw data and count most common rating

cars = pd.read_csv('data/cars.csv')

cars2 = cars.copy()  # Modify a copy of DataFrame

most_common = max(cars2.rating.value_counts())

for rating in cars2.rating.unique():

    # A DataFrame of only one rating class

    rating_class = cars2[cars2.rating == rating]

    # Duplicate one less than overshooting most_common

    num_dups = (most_common // len(rating_class)) - 1

    for _ in range(num_dups):

        cars2 = pd.concat([cars2, rating_class])

cars2.rating.value_counts() 
```

```
Unacceptable    1210

Good            1173

Very Good       1170

Acceptable      1152

Name: rating, dtype: int64 
```

这种方法使每个不常见的类尽可能接近多个类的频率，而不会在复制中不一致。也就是说，如果我们想要*确切的*1210 个`Acceptable`样本，我们将会比其他样本多复制一次一些样本。允许非常轻微的不平衡是一个更好的方法。

比天真的过采样更有趣的是一种叫做**合成少数过采样技术** ( **SMOTE** )，以及一种与之密切相关的叫做**不平衡数据自适应合成采样方法** ( **ADASYN** )。在 R 中，有许多选择来执行 SMOTE 和类似的技术。库包括 **smotefamily** 、DMwR 和 ROSE，用于相关但略有不同的技术。然而，对于接下来的几个代码示例，我们将使用 Python 的不平衡学习，仅仅是因为所需的库中选择较少。

虽然 SMOTE 系列中的几种技术之间存在一些技术差异，但它们大体上都是相似的。他们所做的是使用 K-最近邻技术生成新的数据点。在少数样本中，他们查看特征的参数空间中的几个最近邻居，然后在参数空间的该区域内创建新的合成样本，该合成样本不等同于*任何*现有观察。在非正式的意义上，我们可以称之为“模糊”过采样。当然，分配给这个合成点的类或目标与已经存在的少数类观察的集群的类或目标是相同的。底线是，这种特征值模糊的过采样通常比原始过采样产生更有用的合成样本。

正如上面所讨论的，汽车评级等级明显不平衡。

```
cars.rating.value_counts() 
```

```
Unacceptable    1210

Acceptable       384

Good              69

Very Good         65

Name: rating, dtype: int64 
```

在不平衡学习中有几种类似的过采样技术。阅读该库的文档以了解更多细节。它们都构建在相同的 scikit-learn API 之上，并且它们可能包含在 scikit-learn 管道中，或者与该库进行互操作。您不需要使用 scikit-learn 来使用不平衡学习，除非在幕后使用该库的 K 近邻。

类似于在模块名`sklearn`下导入的包名 scikit-learn，我们使用的安装包名为 unbalanced-learn，但是导入为`imblearn`。

```
# Only define the feature and target matrices, use in next cell

from imblearn.over_sampling import SMOTE

# Divide data frame into X features and y target

X = cars.drop('rating', axis=1)

y = cars['rating']

# Create the resampled features/target

X_res, y_res = SMOTE(k_neighbors=4).fit_resample(X, y) 
```

让我们将特征和目标组合回一个类似于原始的数据帧。

```
synth_cars = X_res.copy()

synth_cars['rating'] = y_res

synth_cars.sample(8, random_state=2) 
```

```
 price_buy  price_maintain  doors  passengers  trunk  safety

——————————————————————————————————————————————————————————————————

 748          2               2      5           6      0       0

  72          0               3      2           6      0       1

2213          3               0      2           4      0       2

1686          2               3      5           2      0       0

3578          0               0      4           6      1       1

3097          0               0      2           4      0       2

4818          0               1      4           4      1       2

 434          2               3      5           6      2       0

             rating

————————————————————

 748   Unacceptable

  72   Unacceptable

2213     Acceptable

1686   Unacceptable

3578           Good

3097           Good

4818      Very Good

 434   Unacceptable 
```

如我们所愿，目标的类是完全平衡的。我们可以改变采样策略，不要求精确的平衡，但在这种情况下，精确是合理的。

```
synth_cars.rating.value_counts() 
```

```
Good            1210

Very Good       1210

Unacceptable    1210

Acceptable      1210

Name: rating, dtype: int64 
```

这里有一个小点值得注意。与执行 SMOTE 的几个 R 库不同，不平衡学习保留了特性的数据类型。特别是，功能的顺序整数保持为整数。这可能是也可能不是你想要的。语义上，从“低”到“非常高”的`price_buy`的评估可以被合理地编码为 0-3 范围内的连续值。然而，门的数量在语义上是一个整数。尽管如此，如果“门越多越好”对你这个消费者来说，这种字面意义有点不可思议的合成排可能没有坏处。

比直接解释给定的特征值更重要的是该值对模型的有用程度。对于许多类型的模型，连续变量提供了更有用的聚类，并且您很可能更喜欢对浮点输入进行训练。让我们将数据类型转换为浮点数，并再次执行重采样，注意一些新的非整数特征值。

```
cars.iloc[:, :6] = cars.iloc[:, :6].astype(float)

cars.head() 
```

```
 price_buy  price_maintain  doors  passengers  trunk  safety

———————————————————————————————————————————————————————————————

0        1.0             0.0    3.0         6.0    0.0    0.0

1        2.0             2.0    3.0         6.0    2.0    1.0

2        2.0             2.0    5.0         2.0    1.0    1.0

3        0.0             1.0    3.0         2.0    2.0    1.0

4        2.0             1.0    5.0         2.0    0.0    1.0

          rating

—————————————————

0   Unacceptable

1     Acceptable

2   Unacceptable

3   Unacceptable

4   Unacceptable 
```

```
# Divide data frame into X features and y target

X = cars.drop('rating', axis=1)

y = cars['rating']

# Create the resampled features/target

X_, y_ = SMOTE().fit_resample(X, y)

pd.concat([X_, y_], axis=1).sample(6, random_state=4) 
```

```
 price_buy  price_maintain     doors  passengers   trunk  safety

—————————————————————————————————————————————————————————————————————

4304       1.0        0.158397  2.158397         6.0  2.0000     2.0

 337       3.0        0.000000  3.000000         4.0  0.0000     1.0

2360       2.0        2.000000  3.247795         4.0  2.0000     2.0

3352       0.0        1.000000  2.123895         4.0  2.0000     1.0

2064       0.0        3.000000  4.000000         6.0  1.8577     2.0

4058       1.0        0.000000  3.075283         6.0  2.0000     2.0

             rating

————————————————————

4304      Very Good

 337   Unacceptable

2360     Acceptable

3352           Good

2064     Acceptable

4058      Very Good 
```

# 练习

在本章的练习中，你将首先评估估计趋势的质量。在第二个练习中，您需要考虑如何处理可能在多个要素中不平衡的数据，而不仅仅是在单个要素中。

## 交替趋势插补

在本章的氪石示例中，我们使用输入激光频率的二阶多项式拟合来估计每平方米坎德拉的缺失值。在某种意义上，仅仅使用局部插值，甚至前向填充或后向填充肯定会更简单。大多数数据框库为我们提供了现成的局部插补

该数据集位于:

[https://www.gnosis.cx/cleaning/excited-kryptonite.fwf](https://www.gnosis.cx/cleaning/excited-kryptonite.fwf)

你应该量化不同估计方法之间的差异。用均方根偏差(RMSD)来表示样本之间的差异是一种很好的方法，在本练习中使用这种方法。显然，我们不知道丢失值的正确答案是什么，所以我们只是试图评估各种方法之间的差异。

对于每种颜色/类型的氪石以及所有颜色的集合体，您应该测量和比较许多差异:

*   所有点(原始点和估计点)和二阶多项式拟合函数本身之间的 RMSD。
*   原点和*点之间的 RMSD:*
    *   对它们进行线性回归；
    *   二阶多项式拟合；
    *   三阶多项式拟合。
    *   一些你认为相关的其他回归(可能来自机器学习库)。
*   仅*的 RMSD 估计了 1、2 和 3 次多项式拟合之间的*点，以及仅基于邻居的局部插值。
*   仅在不同程度的多项式拟合和简单的正向填充之间，*的 RMSD 估计出*点。

描述一下你觉得打败那个讨厌的超人的最佳策略。![](img/B17126_06_001.png)

## 平衡多种功能

人类身高/体重数据显示数值`Height`不平衡。`Weight`显示了类似的分布。此数据集的一个版本附有一个奇特的目标，可从以下网址获得:

[https://www.gnosis.cx/cleaning/height-weight-color.csv](https://www.gnosis.cx/cleaning/height-weight-color.csv)

该数据添加了一个名为`Favorite`的列，它大致相等，只是从集合{ `red`、`green`、`blue` }中随机生成的。

```
humcol = pd.read_csv('data/height-weight-color.csv')

humcol.sample(6, random_state=1) 
```

```
 Height      Weight   Favorite

——————————————————————————————————————————

21492   176.958650   72.604585        red

 9488   169.000221   79.559843       blue

16933   171.104306   71.125528        red

12604   174.481084   79.496237       blue

 8222   171.275578   77.094118      green

 9110   164.631065   70.557834      green 
```

在本练习中，您希望探索根据身高和体重预测最喜欢的颜色的模型。我们在本章前面看到了高度的分布。体重在数值范围之间也有类似程度的不平衡。

```
pd.cut(humcol.Weight, 5).value_counts().sort_index() 
```

```
(44.692, 55.402]      125

(55.402, 66.06]      3708

(66.06, 76.717]     14074

(76.717, 87.375]     6700

(87.375, 98.033]      393

Name: Weight, dtype: int64 
```

根据我们规定的假设，身高和体重可能预示着最喜欢的颜色。然而，我们也假设体重指数(身体质量指数)可能是预测性的。这是根据身高体重确定性地推导出来的，但不是根据 [*第七章*](Chapter_7.xhtml#_idTextAnchor009)*特征工程*中讨论的多项式特征推导出来的。具体来说，身体质量指数公式是:

![](img/B17126_04_007.png)

您的任务是创建一个包含合成样本的新数据集，其中单独的身高、单独的体重和身体质量指数分别由相对相等数量的观测值表示。出于这个目的，假设身高、体重和身体质量指数每个都分为*五个*等级，你可以非正式地称之为，例如，“非常矮”、“矮”、“平均”、“高”、“非常高”，或者其他特征的类似名称。

解决这个问题的一个简单方法是简单地复制现有的行，以增加它们的量化类的表示。您可以先尝试这种方法。然而，你也应该尝试使用 SMOTE、ADASYN 或 ROSE 等技术来生成新的合成样本，这些样本能够代表他们的身高、体重或身体质量指数等级。当您生成这些合成样本时，您将需要分配一个合适的喜爱的颜色(当您仅仅复制行时，这很简单；但是，当您根据几个不同的平衡需求创建新的合成行时，这将更加微妙)。

假设类不平衡大约为 100:1，但只在每个要素的五个类之间进行平衡，每个要素的每个平衡操作将使数据集大小增加大约 4 倍。考虑是否需要将这些乘法运算链接起来，以产生大约 4×4×4 或 64 倍于原始大小的数据集。您应该能够找到一种通过特性独立平衡的方法，从而将扩展限制在原始大小的大约 4+4+4 或 12 倍。

比如说，多达 300，000 行的过采样并不是不合理的。然而，如果您开始时有超过 25，000 个观察值，乘法运算可能是这样的。假设您的初始过采样确实产生了大约 300，000 行的数据，以保持粗略的类别平衡的方式将这些 300，000 行的大部分合成样本欠采样到仅 100，000 行(这里不需要精确的平衡；目标是每个类的行数差异小于 25%)。

作为这个练习的最后一个要素，如果你能做到的话，试着建立一个身高、体重、身体质量指数和目标最喜欢的颜色之间关系的真实模型。具体的建模或机器学习技术超出了本书的范围，但它们通常是本书希望有所帮助的目的。

*   你的模型能做出多好的预测？
*   你的模型做出了什么样的预测？哪些人更喜欢哪种颜色，有多喜欢？
*   哪个特征最具预测性？

作为一个提示，我将指出一个相对强烈的模式是嵌入在最喜爱的颜色分配。如果我们有关于最喜欢的颜色的实际调查数据，正如你所料，这里面也有很多噪音和随机性。但是，在物理测量和这种偏好之间，也有一些很可能实际上并不存在的模式。

# 结局

> 永远不要回答问你的问题。回答你希望被问到的问题。
> 
> 罗伯特·麦克纳马拉

**本章涉及的主题**:集中趋势；相关的趋势；趋势估计；局部性；欠采样；过采样。

在这一章中，我们看了两个相关但略有不同的主要概念。一方面，在数据缺失时估计单个值通常是有用的。当我们这样做时，我们可以使用数据中的各种模式和/或我们所知道的数据所来自的基础领域的事实。有时我们根据给定变量的典型值估计值，有时根据参数空间的特定区域调节典型值。其他时候，我们发现数据中的趋势可以用某种方式排序，并根据这些趋势进行估计。

在第二个方面，抽样也有一种插补。在过采样的情况下，我们直接估计全新的合成样本，要么简单地重复现有的样本，要么使用聚合技术来推断不常见类别的典型样本。然而，即使在欠采样的情况下，也有一种插补在进行。对数据集进行欠采样不会改变任何单个值，但绝对会改变剩余数据的*分布*。毕竟，这就是全部要点:我们希望在原始数据集没有遵循的分类或范围变量内创建相对平衡。

对于数据科学和数据分析，您的负担始终是获取以原始形式呈现给您的原始材料，并赋予其适合建模和分析目的的形式。

在下一章，我们将继续关注*特征工程*和新合成特征的创造。