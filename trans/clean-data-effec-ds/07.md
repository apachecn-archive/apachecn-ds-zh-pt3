<title>Chapter_5</title> <link href="../Styles/epub.css" rel="stylesheet" type="text/css"> <link href="../Styles/syntax-highlighting.css" rel="stylesheet" type="text/css">

# 5

# 数据质量

> 所有的数据都是脏的，有些数据是有用的。
> 
> –参见乔治·博克斯

欢迎来到本书的中间点。就像摇滚*“概念专辑”*通过其个别歌曲讲述总体故事的松散方式一样，这本书在一定程度上意味着遵循数据科学家从获取原始数据到将合适的数据输入机器学习模型或数据分析的过程。到目前为止，我们已经了解了如何将数据输入程序或分析系统(例如笔记本)，并且在 [*第 4 章*](Chapter_4.xhtml#_idTextAnchor006) 、*异常检测*中，我们涉及了在单个数据点级别识别明显“变质”的数据。在这一章之后的章节中，我们将着眼于对前面章节分阶段交付的混乱且有标记的数据的修复。

然而，现在是时候去寻找你的数据可能有问题的地方了，不是在个别细节上，而是在它的整体“形状”和特征上。在某些情况下，这些问题与所使用的一般收集技术有关，特别是与收集过程中可能引入的系统偏差有关。在其他情况下，问题不是数据收集者的错，而仅仅是单位和比例的错，纠正可能相当机械和例行公事。在这一点上，我们逐渐放松到积极的干预，不只是像我们迄今为止所做的那样简单地检测污垢，而且着手清洁它。其中一种清理可能涉及处理数据周期性经常产生的固有偏差(通常是在一段时间内，但不是唯一的)。

在本章的最后一节，我们来看看执行验证的想法，它是特定于领域的，并利用实用的规则，而不仅仅是简单的数字。当然，每个领域可能都有自己的规则，本章中的例子是为了启发思考，而不是为您的特定任务提供蓝图。事实上，这本书的所有内容都旨在为思考数据科学问题提供灵感，而不仅仅是直接复制到你面前的任务的方法，这一点很难经常被提及。

***

在我们开始本章的章节之前，让我们运行我们的标准设置代码。

```
from src.setup import *

%load_ext rpy2.ipython

%%R

library(gridExtra)

library(tidyverse) 
```

# 缺失数据

> 没有证据不代表不存在。
> 
> 马丁·里斯

**概念**:

*   缺失数据的各个方面
*   参数空间中记录的分布
*   缺失数据中的偏差

丢失数据的故事构成了本书的三部曲。前一章， [*第四章*](Chapter_4.xhtml#_idTextAnchor006) ，*异常检测*，以缺失数据一节开始。在这种情况下，我们关心的是识别“缺失”，这可以由不同数据格式的不同数据集以不同方式标记。下一章 [*第 6 章*](Chapter_6.xhtml#_idTextAnchor008)*值插补*，主要是关于我们可以做些什么来用合理的猜测填补缺失值。

这一章介于前一章和下一章之间。我们已经采取了机械或统计测试来识别一些缺失的数据(或者不可靠到最好假装缺失)。但是我们还没有决定是保留还是删除那些丢失的数据点所属的观测值。对于这一部分，我们需要评估缺失数据对整个数据集的重要性。

当我们有一个缺失数据的记录时，我们基本上有两种处理方法。一方面，我们可以丢弃那个特定的记录。另一方面，我们可以为缺失值估算一些值，这将在第六章 的 [*中讨论。实际上，从某种意义上来说，还有第三种选择:我们可能会认为，由于数据集中缺失数据的数量和分布，这些数据根本无法用于手头的目的。虽然作为数据科学家，我们永远不想宣布一项任务没有希望，但作为负责任的研究人员，我们需要考虑特定数据根本无法支持任何结论的可能性。缺失数据并不是导致我们得出这一结论的唯一*因素，但它确实是一个常见的致命缺陷。**](Chapter_6.xhtml#_idTextAnchor008)

如果我们希望丢弃记录——但如果我们希望估算值，在很大程度上也是如此——我们需要考虑剩下的是否是数据参数空间的公平表示。样本偏差不仅存在于数据集的整体构成中，而且更微妙地存在于缺失值的分布中。请记住，此处的“缺失”可能是由第 4 章 中的 [*处理造成的，其中一些值可能被标记为缺失，因为我们确定它们是不可靠的，即使它们本身在原始数据中并不存在。*](Chapter_4.xhtml#_idTextAnchor006)

例如，我创建了一个假想的数据集，里面有姓名、年龄、性别以及最喜欢的颜色和花。年龄、性别和姓名是根据美国社会安全局报告的流行姓名随时间的实际分布来建模的。在这幅插图中，我给人们分配了最喜欢的颜色和花。

```
df = pd.read_parquet('data/usa_names.parq')

df 
```

```
 Age  Gender       Name  Favorite_Color   Favorite_Flower

———————————————————————————————————————————————————————————————

0     48       F       Lisa          Yellow             Daisy

1     62       F      Karen           Green              Rose

2     26       M    Michael          Purple              None

3     73       F   Patricia             Red            Orchid

...  ...     ...        ...             ...               ...

6338  11       M      Jacob             Red              Lily

6339  20       M      Jacob           Green              Rose

6340  72       M     Robert            Blue              Lily

6341  64       F      Debra          Purple              Rose

6342 rows × 5 columns 
```

一般来说，这是一个看起来很普通的数据集，有一个相当大的记录集合。我们可以在数据框摘要中注意到，至少有一些数据丢失了。这是值得更仔细调查的。

```
with show_more_rows():

    print(df.describe(include='all')) 
```

```
 Age   Gender      Name  Favorite_Color  Favorite_Flower

count   6342.000000     6342      6342            5599             5574

unique          NaN        2        69               6                5

top             NaN        F   Michael          Yellow           Orchid

freq            NaN     3190       535             965             1356

mean      42.458846      NaN       NaN             NaN              NaN

std       27.312662      NaN       NaN             NaN              NaN

min        2.000000      NaN       NaN             NaN              NaN

25%       19.000000      NaN       NaN             NaN              NaN

50%       39.000000      NaN       NaN             NaN              NaN

75%       63.000000      NaN       NaN             NaN              NaN

max      101.000000      NaN       NaN             NaN              NaN 
```

使用 Pandas 的`.describe()`方法或其他工具的类似汇总，我们可以看到`Age`、`Gender`和`Name`具有所有 6，342 条记录的值。然而，`Favorite_Color`和`Favorite_Flower`各有大约 750 条记录缺少。就其本身而言，在 10-15%的行中丢失数据很可能不是一个大问题。这种说法假设错过本身并没有偏见。即使我们需要完全丢弃这些记录，那也只是相对较大的数据集中相对较小的一部分。同样，输入值*可能*不会引入太多的偏差，并且可以在那些记录中利用其他特征。在以下章节和第 6 章 、*值插补*的 [*中，关于欠采样和过采样，我们讨论了导致类别失衡的排斥危险。*](Chapter_6.xhtml#_idTextAnchor008)

虽然均匀随机缺失的数据可以相对容易地解决，但有偏差缺失的数据可能会带来更严重的问题。为了弄清楚我们在这个数据集中属于哪一类，让我们将这些缺失的花偏好与人们的年龄进行比较。观察每一个年龄高达 101 岁的个体是很难想象的；为此，我们将把人们分成 10 岁年龄组。下图使用了一个名为 [Seaborn](Glossary.xhtml#_idTextAnchor122) 的统计绘图库，它构建在 Matplotlib 之上。

```
df['Age Group'] = df.Age//10 * 10

fig, ax = plt.subplots(figsize=(12, 4.5))

sns.countplot(x="Age Group", hue="Favorite_Flower", 

              ax=ax, palette='gray', data=df)

ax.set_title("Distribution of flower preference by age"); 
```

![](img/B17126_05_01.png)

图 5.1:不同年龄的花偏好分布

在这个可视化中，有几个模式跳了出来。看来，老年人往往对兰花有强烈的偏好，而年轻人对玫瑰有适度的偏好。这也许是值得分析的数据的一个特性。更值得注意的是，在 20-30 岁年龄组中，几乎没有关于最喜欢的花的数据。人们可能会想到几种解释，但真正的答案取决于问题和领域知识。例如，可能对应于这些年龄的数据不是在某个时间段收集的。或者可能该年龄组的人报告了不同的喜爱的花，但是其名称在一些先前不准确的数据验证/清理步骤中丢失了。

如果我们查看缺少颜色偏好的记录，我们会看到与年龄相关的类似模式。然而，在 30-40 岁年龄段，可用值的频率反而下降了。

```
fig, ax = plt.subplots(figsize=(12, 4.5))

sns.countplot(x="Age Group", hue="Favorite_Color", 

              ax=ax, palette='gray', data=df)

ax.set_title("Distribution of color preference by age"); 
```

![](img/B17126_05_02.png)

图 5.2:颜色偏好的年龄分布

如果我们删除所有有缺失数据的记录，我们将会发现几乎没有 20-40 岁年龄段的人。这种有偏见的数据不可获得性很可能会削弱总体分析。记录的数量将保持相当大，但是如前所述，参数空间将有一个空的(或者至少不太密集的)区域。显然，这些陈述既取决于我们数据分析的目的，也取决于我们对潜在领域的假设。如果年龄不是问题的一个重要方面，我们的方法可能没多大关系。但是如果我们认为年龄是一个重要的独立变量，放弃这个数据可能不是一个可行的方法。

像许多其他部分一样，这一部分展示了通常应该对数据集执行的各种探索。它没有提供一个简单的答案来弥补缺失数据中的偏差。这一决定在很大程度上取决于数据的使用目的，也取决于可能澄清数据丢失原因的背景领域知识。补救不可避免地是针对每个问题的决定。

让我们来看看偏差可能与其他要素相关联而不仅仅是在数据集中全局发生的方式。

# 偏向趋势

> 产生怪物的不是理性的沉睡，而是警惕和失眠的理性。
> 
> -吉尔·德雷泽

**概念**:

*   收藏偏差与潜在领域的趋势
*   视角是偏见的来源
*   收集方法的工件
*   可视化以识别偏差
*   按组的差异
*   外部识别基本利率
*   本福特定律

有时，您可能能够检测到数据中的样本偏差，并且需要对该偏差的显著性做出领域区域判断。至少有两种样本偏差是你应该注意的。一方面，观察值的分布可能与基础域中的分布不匹配。很可能，您需要参考其他数据源——或者简单地使用您自己的领域知识——来检测样本中的这种偏差。另一方面，数据本身可能揭示了多个变量之间存在的趋势偏差。在后一种情况下，重要的是要考虑检测到的“趋势”是否可能是您在数据中检测到的一种现象，或者是一个收集或管理工件。

## 理解偏差

偏见在统计学和人类科学中都是一个重要的术语，它的含义有很强的相关性，但是 T2 在不同的领域有不同的价值。从最中性的统计学意义上来说，偏倚就是这样一个事实，通常是真实的，即一个数据集不能准确地代表其潜在的可能的观察总体。这种赤裸裸的陈述隐藏了更多的细微差别，甚至超出了对人类和充满政治意味的事物的观察。通常情况下，无论是我们这些分析数据的数据科学家，还是最初收集原始数据的人或工具，都无法明确界定哪些数据属于底层人群。事实上，就数据收集技术而言，人口通常在某种程度上是循环定义的。

一个老笑话说，有人晚上在路灯下寻找他们丢失的钥匙。当被问及为什么不去别处看看时，他们回答说，这是因为他们看的地方能见度更好。这是一个儿童笑话，讲得并不特别吸引人，但它也为*大多数*数据集的*大多数*数据收集奠定了模式。

观察者观察他们能*看到的*(打个比喻，大多数可能是仪器中的电压，或者电线上的比特，而不是实际的人眼)，而不是他们看不到的。生存偏差是一个认知错误的术语，它假设我们现有的观察数据代表了潜在人群。

人们很容易意识不到数据中存在的偏见，当*确实*涉及人类或社会主体，并且人类观察者带来心理和社会偏见时，这可能更容易。但最终，即使有我们设置的仪器的帮助，也是人类在观察其他一切事物。例如，动物行为学(动物行为的研究)的历史很大程度上是科学家们看到动物身上存在的行为——或者他们认为应该存在的行为——在他们周围的人类身上的历史，这是他们通过隐喻和失明强加的。如果你对当地图书馆的书籍进行调查，以确定人类文学或音乐的范围，你会发现使用当地语言和演奏当地音乐风格的作家和音乐家占主导地位。即使在看起来最明显*与*人类无关的领域，我们的有利位置也可能造成视角偏差。例如，如果我们对宇宙中存在的恒星类型以及不同类型的普遍性进行分类，我们总是在我们的宇宙视界内观察这些恒星，这不仅表达了空间和时间的相互作用，而且可能无法统一描述整个宇宙。宇宙学家当然知道这一点，但他们知道这是他们观测的固有偏差。

在本节的大部分内容中，我们将查看一个版本的合成美国姓名/年龄数据，以检测这两种模式。与上一节一样，该数据大致准确地代表了基于社会保障管理局数据的不同年龄组中不同姓名的出现频率。我们可以看到，在实际的域名中，随着时间的推移，各种名称的流行程度确实发生了变化。如上一节所述，将人们聚集成粗略的年龄组以便可视化是有用的。

在整本书中，我试图在我选择或创建的数据集中避免社会偏见。对于姓名表行中想象的人，我添加了像最喜欢的颜色或花这样的特征，而不是像眼睛颜色、最喜欢的食物或音乐偏好这样更明显的种族或文化特征。尽管我使用的那些虚构的特征并不完全独立于文化，也许我在社会世界中的地位导致我选择了与其他地方的人不同的因素值。

此外，通过选择每年美国最受欢迎的前五名名字，我强加了一种多数偏向:所有的名字大致都是盎格鲁人的名字，例如，没有一个是典型的非裔美国人、拉丁美洲人、中国人或波兰人的名字，尽管这些名字在每年前五名的排序方法之外都很常见。

```
names = pd.read_parquet('data/usa_names_states.parq')

names['Age Group'] = names.Age//10 * 10

names 
```

```
 Age   Birth_Month         Name  Gender            Home  Age Group

———————————————————————————————————————————————————————————————————————

0     17          June      Matthew       M          Hawaii         10

1      5     September         Emma       F   West Virginia          0

2      4       January         Liam       M          Alaska          0

3     96         March      William       M        Arkansas         90

...  ...           ...          ...     ...             ...        ...

6338  29        August      Jessica       F   Massachusetts         20

6339  51         April      Michael       M         Wyoming         50

6340  29           May  Christopher       M  North Carolina         20

6341  62      November        James       M           Texas         60

6342 rows × 6 columns 
```

字段`Birth_Month`和`Home`被添加到这个数据集中，让我们规定我们怀疑它们可能指示观察中的一些偏差。在我们看这个之前，让我们看一下或多或少的预期趋势。请注意，这个数据集是根据每个出生年份最流行的男性和女性名字人工构建的。某个特定的名字可能不在某一年或某个特定十年的前五名(按性别)中，但尽管如此，美国一定数量的人可能被赋予了这个名字(并且可能出现在非合成数据中)。

```
fig, ax = plt.subplots(figsize=(12, 4.5))

somenames = ['Michael', 'James', 'Mary', 'Ashley']

popular = names[names.Name.isin(somenames)]

sns.countplot(x="Age Group", hue="Name", 

              ax=ax, palette='gray', data=popular)

ax.set_title("Distribution of name frequency by age"); 
```

![](img/B17126_05_03.png)

图 5.3:按年龄划分的姓名频率分布

我们可以从这个数据中看到趋势。Mary 是数据集中老年人的常用名字，但不再出现在年轻人最常用的名字中。阿什利在 20-40 岁的年轻人中很受欢迎，但我们没有看到这个年龄组以外的人。詹姆斯似乎在大多数年龄段都被使用，尽管它在 10-40 岁的人群中跌出了前五名，但在 10 岁以下的儿童中又出现了。迈克尔，同样，似乎特别代表从 10-60 岁。

在数据生成中使用的 top-5 阈值无疑在可视化中创建了一些工件，但是一些名称变得流行而另一些名称变得衰落的一般模式正是我们在最少的领域知识下所预期的现象。此外，如果我们对美国流行的婴儿名字只多了解一点点，名字的具体分布就会显得似是而非；对于显示的 4 个名字和剩余的 65 个名字，如果您下载它，您可以在数据集中调查。

## 检测偏差

让我们对出生月份进行类似的分析，就像我们对命名频率所做的那样。最低限度的领域知识会告诉你，虽然出生月份有小的年度周期，但不应该有年龄的总趋势。即使一些世界历史事件极大地影响了某一年某一月的出生，当我们对几十年的年龄进行汇总时，这也不会产生什么总体趋势。

```
fig, ax = plt.subplots(figsize=(12, 4.5))

months = ['January', 'February', 'March', 'April']

popular = names[names.Birth_Month.isin(months)]

sns.countplot(x="Age Group", hue="Birth_Month", 

              ax=ax, palette='gray', data=popular)

ax.set_title("Distribution of birth month frequency by age"); 
```

![](img/B17126_05_04.png)

图 5.4:按年龄分列的出生月份频率分布

与我们排除偏向趋势的希望相反，我们发现——出于未知的原因——一月份出生的婴儿在最年轻的人群中明显偏低，而在最年长的人群中明显偏高。总的来说，这与年轻人越来越多的年龄趋势相重叠，但这种模式仍然很明显。我们没有考虑过 4 月以后的月份，但当然我们也可以以类似的方式来考虑。

仅仅因为采样问题，数据集中就会出现一定量的随机波动。在数据集中，4 月是 50 多岁的人比 40 多岁的人更常见的出生月份，这一事实很可能没有意义，因为一旦我们按年龄和出生月份进行交叉，数据点就相对较少(大约 50 个)。区分真正的数据偏差和随机性可能需要额外的分析(尽管从结构上看，即使在这个简单的可视化中，一月模式也非常突出)。

我们可以有许多方法来分析它，但是寻找一个变量相对于另一个变量的显著差异可能是一个很好的提示。例如，我们认为我们在一月出生月份的模式中看到了一种奇怪的现象，但是在每个年龄的分布中是否存在普遍的不规则性呢？我们*可以*尝试使用确切的年龄来分析这一点，但这可能会使区分过于精细，从而没有良好的子样本大小。对于这个测试，十岁是一个合适的分辨率。像往常一样，在做这样的判断时要考虑你的主题。

由于人口数量随着年龄的增长而减少，我们需要找到不受原始数据过度影响的统计数据。特别是，我们可以计算每个年龄组和出生月份的记录数量，并查看这些计数是否明显不同。(计数的)方差或标准差将随着年龄组的增加而增加。然而，我们可以简单地通过除以所有月份的年龄组中的原始计数来使其正常化。

一点点熊猫的魔力让我们得到了这个。我们希望按年龄组对数据进行分组，查看出生月份，并计算每个`Age` ⨯ `Birth_Month`内的记录数量。我们希望用表格的方式而不是用层次索引来看这个问题。该操作按照月份在数据中出现的顺序来排列月份，但是按年表排序更友好。

```
by_month = (names

    .groupby('Age Group')

    .Birth_Month

    .value_counts()

    .unstack())

by_month = by_month[month_names]

by_month 
```

```
Birth_Month  January  February  March  April  May  June  July  August

  Age Group

——————————————————————————————————————————————————————————————————————

          0       20        67     59     76   66    77    71      65

         10       37        72     71     78   70    73    82      81

         20       52        60     76     72   65    65    71      66

         30       54        56     66     64   73    58    87      82

        ...      ...       ...    ...    ...  ...   ...   ...     ...

         70       57        43     39     33   39    36    45      34

         80       57        39     28     21   31    37    23      28

         90       55        17     31     24   21    23    30      29

        100       10         7      4      2    6     2     4       6

Birth_Month  September  October  November  December

  Age Group

————————————————————————————————————————————————————

          0         67       67        56        63

         10         83       79        70        79

         20         68       75        76        71

         30         66       65        57        58

        ...        ...      ...       ...       ...

         70         38       30        37        37

         80         27       31        34        37

         90         33       25        28        20

        100          5        5         7         7

11 rows × 12 columns 
```

这个数据表格太多了，不能马上得出结论，所以，如前所述，让我们看看不同年龄组的标准化方差。

```
with show_more_rows():

    print(by_month.var(axis=1) / by_month.sum(axis=1)) 
```

```
Age Group

0          0.289808

10         0.172563

20         0.061524

30         0.138908

40         0.077120

50         0.059772

60         0.169321

70         0.104118

80         0.227215

90         0.284632

100        0.079604

dtype: float64 
```

超过 100 岁的组显示出较低的标准化方差，但它是一个小的子集。在其他年龄组中，中年人显示出明显低于老年人或年轻人的跨月标准化方差。对于 10 岁以下的人和 80 岁以上的人来说，这种差异相当惊人。在这一点上，我们可以合理地得出结论:在收集出生月份时，样本偏差发生了某种变化*；具体来说，根据被抽样人员的年龄组，实际上存在不同的偏差。不管这种偏见*是否关系到*手头的目的，*事实应该清楚地记录在*你的分析或模型的任何工作产品中。原则上，将在第 6 章* 、*值插补*中讨论的一些采样技术可能与此相关。

## 与基线的比较

当然，这个合成数据集的设置是一个赠品。除了介绍出生月份之外，我还在居住和/或出生的州或地区的意义上添加了`Home`。虽然没有文档化的元数据明确阐明该列的含义，但让我们将它视为当前居住的州。如果我们选择将其解释为出生地，我们可能需要找到不同年龄的人出生时的人口历史数据；显然这是可能的，但是当前的假设简化了我们的任务。

让我们看看美国各州目前的人口。这将提供一个外部基线，相对于该基线，我们可以在所考虑的数据集中寻找样本偏差。

```
states = pd.read_fwf('data/state-population.fwf')

states 
```

```
 State   Population_2019   Population_2010   House_Seats

———————————————————————————————————————————————————————————————————————

  0        California          39512223          37254523          53.0

  1             Texas          28995881          25145561          36.0

  2           Florida          21477737          18801310          27.0

  3          New York          19453561          19378102          27.0

...               ...               ...               ...           ...

 52              Guam            165718            159358           0.5

 53   U.S. Virgin Isl            104914            106405           0.5

 54    American Samoa             55641             55519           0.5

 55    N. Mariana Isl             55194             53883           0.5

 56 rows × 4 columns 
```

正如大多数读者所知，美国各州和地区的人口数量差异很大。在这个特定的数据集中，众议院中各州的代表是以整数给出的，但是为了表明一些具有无投票权代表的实体的特殊地位，使用了特殊值`0.5`(这与本节没有密切关系，只是作为一个注释)。

让我们看看数据集中人们的家庭状态的分布。对索引进行排序的步骤用于确保状态按字母顺序排列，而不是按计数或其他方式。

```
(names

    .Home

    .value_counts()

    .sort_index()

    .plot(kind='bar', figsize=(12, 3), 

          title="Distribution of sample by home state")

); 
```

![](img/B17126_05_05.png)

图 5.5:按家庭所在州的样本分布

从每个州的居民身上抽取的样本数量明显不同。然而，所代表的最大的州，加利福尼亚，只有最小州的 3 倍样本数。相比之下，对潜在人口的类似观点强调了不同的分布。

```
(states

    .sort_values('State')

    [['State', 'Population_2019']]

    .set_index('State')

    .plot(kind='bar', figsize=(12, 3),

          title="2019 Population of U.S. states and territories")

); 
```

![](img/B17126_05_06.png)

图 5.6: 2019 年美国及领地人口

虽然加州为该数据集提供了最多的样本，但相对于各州的基线人口，加州人同时也是*最缺乏代表性的*。一般来说，较小的国家往往代表性过高。我们可以，或许应该，把这看作是基于各州大小的选择偏差。和以前一样，除非我们有准确的文档或元数据来描述收集和管理程序，否则我们无法确定不平衡的原因。但是，这种总体与相对样本频率的反比关系存在一种强烈的趋势。

这里要注意的是，有时抽样方法故意引入类似的不平衡。如果实际样本是精确平衡的，每个州收集一些固定的 *N* ，这将相当清楚地指向这样一个故意的分类抽样，而不是基于潜在比率的抽样。我们实际拥有的模式没有那么明显。我们可能会形成一个假设，即采样率是基于一些其他潜在的特征，而不是直接出现在这些数据中。

例如，也许在每个州的每个*县*进行固定数量的观察，较大的州往往有更多的县(这是*而不是*的的实际潜在推导，但以这种方式思考应该在你的头脑中)。理解数据完整性问题类似于一个实验和假设的科学过程，或者更像是一个谋杀之谜。开发一个合理的理论来解释为什么数据是脏的总是补救的好的第一步(或者甚至忽略与实际问题无关的问题)。

## 本福特定律

在许多被观察到的数字中，有一个关于*数字*的分布的奇怪事实，叫做**本福德定律**。对于大范围的真实世界数据集，我们看到前导 *1* 数字比前导 *2* 数字更常见，而前导*2*数字又比前导 *3* 数字更常见，依此类推。如果你看到这种模式，它可能并不反映有害的偏见；事实上，对于许多种类的观察，如果你*没能*看到它，那可能本身就反映了偏见(甚至是欺诈)。

如果一个分布精确地遵循本福特定律，那么它将特别具有如下分布的数字:

![](img/B17126_05_001.png)

然而，这种分布通常只是真实世界数据的近似值。

当数据按照幂律或比例因子分布时，理解哪些前导数字将以“有偏”的方式分布就变得相对直观了。然而，许多本质上没有明显标度的观测数据仍然遵循本福特定律(至少是近似的)。让我们挑一个例子来检查；我搜集并整理了美国人口最多的城市的人口和地区的格式。

```
cities = pd.read_fwf('data/us-cities.fwf')

cities 
```

```
 NAME    POP2019    AREA_KM2

0   New York City    8336817       780.9

1     Los Angeles    3979576      1213.9

2         Chicago    2693976       588.7

3         Houston    2320268      1651.1

...           ...        ...         ...

313     Vacaville     100670        75.1

314       Clinton     100471        72.8

315          Bend     100421        85.7

316    Woodbridge     100145        60.3

317 rows × 3 columns 
```

让我们先来数一数人口的最高位数。

```
pop_digits =  cities.POP2019.astype(str).str[0].value_counts()

with show_more_rows():

    print(pop_digits) 
```

```
1    206

2     53

3     20

4     10

6      9

5      8

8      5

7      3

9      3

Name: POP2019, dtype: int64 
```

现在我们问同样的问题面积平方公里。

```
area_digits =  cities.AREA_KM2.astype(str).str[0].value_counts()

with show_more_rows():

    print(area_digits) 
```

```
1    118

2     47

3     31

4     23

9     21

8     21

7     20

6     20

5     16

Name: AREA_KM2, dtype: int64 
```

两个数据集合都不完全符合本福德定律的理想分布，但都显示出一般的 T2 模式，即倾向于以大致升序排列的前导数字。

让我们转而评估分类变量的不均匀分布的重要性。

# 阶级不平衡

> 从真实的和具体的，从真实的前提开始似乎是正确的，因此开始[...]与人口。然而，仔细研究后发现这是错误的。例如，如果我不考虑组成人口的阶级，人口就是一个抽象概念。
> 
> 卡尔·马克思

**概念**:

*   预测罕见事件
*   功能与目标之间的不平衡
*   域与数据完整性不平衡
*   不平衡来源的司法分析
*   刺激因果关系的方向

您收到的数据将有不平衡的类，如果它有分类数据的话。分类变量可能具有的几个不同值也称为有时称为*因子水平*(“因子”与“[特征](Glossary.xhtml#_idTextAnchor044)”或“[变量](Glossary.xhtml#_idTextAnchor143)”同义，如*前言*和*词汇表*中所述)。此外，正如将在 [*第 6 章*](Chapter_6.xhtml#_idTextAnchor008) 、*值插补*中关于*抽样*一节中讨论的，将连续变量分成增量通常也可以有效地形成综合类别。原则上，任何变量都可能有一个分类方面，这取决于手头的目的。当这些因素水平以显著不同的频率出现时，它可能显示选择偏差或其他类型的偏差；然而，它通常只是代表了数据的固有性质，并且是观察的重要部分。

因为许多类型的机器学习模型难以预测罕见事件，所以出现了一个问题。具体的类再平衡的讨论被推迟到第六章和关于欠采样和过采样的讨论，但是在这里我们至少要考虑识别类的不平衡。此外，虽然许多机器学习技术对类别不平衡高度敏感，但其他技术或多或少对此漠不关心。记录特定模型的特征，以及它们与其他模型的对比，超出了本书的范围。

然而，具体而言，当类不平衡造成困难时与当类不平衡是数据预测值的中心时之间的主要区别恰恰是目标和特征之间的区别。或者等价地，因变量和自变量之间的差异。当我们想到可能给模型带来困难的罕见事件时，我们通常指的是罕见的目标值，只有偶尔我们才会关注罕见的特性。当我们希望使用采样来重新平衡类时，几乎总是与目标类值相关。

我们将使用一个简单的例子。我的 web 服务器上两周的 Apache 服务器日志作为示例数据提供。这种日志文件中编码了许多特性，但是每个请求中的一个特定值是返回的 HTTP 状态代码。如果我们试图对我的 web 服务器的行为进行建模，很可能我们希望将这个状态代码作为一个目标，这个目标可以由其他(独立)变量来预测。当然，日志文件本身并没有强加任何这样的目的；它只包含每个请求(包括响应)的许多特性的数据。

从对我的 web 服务器的实际请求返回的状态代码非常不平衡，这通常是一件好事。我希望大多数请求得到 *200 OK* 响应(或者至少*一些* 2xx 代码)。如果没有，要么是用户使用的 URL 有问题，要么是 web 服务器本身有问题。也许这些 URL 是以不正确的形式发布的，比如来自其他网页的链接；或者可能是故意使用了错误的请求来入侵我的服务器。我从来没有真正的*想要*一个 2xx 之外的状态码，但不可避免地出现了一些。让我们看看它们的分布情况:

```
%%bash

zcat data/gnosis/*.log.gz | 

    cut -d' ' -f9 | 

    sort | 

    uniq -c 
```

```
 10280 200

      2 206

    398 301

   1680 304

    181 403

    901 404

      9 500 
```

200 状态在这里占主导地位。下一个最高出现的是 *304 未修改*，这实际上也很好。它只是表明客户端上的缓存副本保持最新。那些 4xx 和 5xx(可能还有 301)状态代码通常是不希望的事件，我可能想对导致它们的模式进行建模。让我们提醒自己 Apache `access.log`文件中有什么(名称因安装而异，确切的字段也是如此)。

```
%%bash

zcat data/gnosis/20200330.log.gz | head -1 | fmt -w50 
```

```
162.158.238.207 - - [30/Mar/2020:00:00:00 -0400]

"GET /TPiP/024.code HTTP/1.1" 200 75 
```

这条线上有各种各样的数据，但值得注意的是，很容易认为几乎所有的数据都是分类的。IP 地址是一个*点状的四元组*，第一个(通常是第二个)四元组往往与地址来源的组织或地区相关联。IPv4 地址的分配比我们在这里详述的要复杂得多，但是来自特定的`/8`或`/16`源的请求可能会得到非 200 响应。同样，日期——尽管不幸的是没有以 [ISO 8601](Glossary.xhtml#_idTextAnchor062) 格式编码——可以被认为是月、小时、分钟等等的分类字段。

让我们展示一点 Pandas 代码来读取这些记录，并将它们整理到一个数据帧中。完成的特定操作不是本节的主要目的，但是熟悉其中的一些方法是值得的。

然而，要注意的一件事是，我已经决定我并不真正关心这种模式，例如，我的 web 服务器在某一天变得不稳定。在这个特定的数据中没有发生这种情况，但如果发生了，我会认为这是一次性的，不需要进行分析。小时和分钟这两个独立的循环元素可能会检测到重复出现的问题(这将在本章后面的章节中详细讨论)。例如，我的 web 服务器可能在凌晨 3 点左右给出许多 404 响应，这将是一个值得识别的模式/问题。

```
def apache_log_to_df(fname):

    # Read one log file.  Treat is as a space separated file

    # There is no explicit header, so we assign columns

    cols = ['ip_address', 'ident', 'userid', 'timestamp', 

            'tz', 'request', 'status', 'size']

    df = pd.read_csv(fname, sep=' ', header=None, names=cols)

    # The first pass gets something workable, but refine it

    # Datetime has superfluous '[', but fmt matches that

    fmt = "[%d/%b/%Y:%H:%M:%S"

    df['timestamp'] = pd.to_datetime(df.timestamp, format=fmt)

    # Convert timezone to an integer

    # Not general, I know these logs use integral timezone

    # E.g. India Standard Time (GMT+5:30) would break this

    df['tz'] = df.tz.str[:3].astype(int)

    # Break up the quoted request into sub-components

    df[['method', 'resource', 'protocol']] = (

                df.request.str.split(' ', expand=True))

    # Break the IP address into each quad

    df[['quad1', 'quad2', 'quad3', 'quad4']] = (

                df.ip_address.str.split('.', expand=True))

    # Pandas lets us pull components from datetime

    df['hour'] = df.timestamp.dt.hour

    df['minute'] = df.timestamp.dt.minute

    # Split resource into the path/directory vs. actual page

    df[['path', 'page']] = (

                df.resource.str.rsplit('/', n=1, expand=True))

    # Only care about some fields for current purposes

    cols = ['hour', 'minute', 

            'quad1', 'quad2', 'quad3', 'quad4', 

            'method', 'path', 'page', 'status']

    return df[cols] 
```

这个函数允许我们简单地通过映射文件名集合和连接数据帧，将所有的每日日志文件读入一个 Pandas 数据帧。结果数据框中除了或许还有`page`之外的一切都可以合理地认为是一个分类变量。

```
reqs = pd.concat(map(apache_log_to_df, 

                 glob('data/gnosis/*.log.gz')))

# Each file has index from 0, so dups occur in raw version

reqs = reqs.reset_index().drop('index', axis=1)

# The /16 subnetwork is too random for this purpose

reqs.drop(['quad3', 'quad4'], axis=1, inplace=True)

reqs 
```

```
 hour  minute  quad1  quad2  method                           path

    0     0       0    162    158     GET    /download/pywikipedia/cache

    1     0       3    172     68     GET                          /TPiP

    2     0       7    162    158     GET   download/pywikipedia/archive

    3     0       7    162    158     GET                     /juvenilia

  ...   ...     ...    ...    ...     ...                            ...

13447    23      52    162    158     GET          /download/gnosis/util

13448    23      52    172     69     GET                               

13449    23      52    162    158     GET               /publish/resumes

13450    23      56    162    158     GET    /download/pywikipedia/cache

                                 page   status

    0   DuMont%20Television%20Network      200

    1                        053.code      200

    2                        ?C=N;O=A      200

    3  History%20of%20Mathematics.pdf      200

  ...                             ...      ...

13447                     hashcash.py      200

13448                     favicon.ico      304

13449                                     200

13450          Joan%20of%20Lancaster      200

13451 rows × 8 columns 
```

在我的 web 服务器中，内容所在的目录相对较少，但是在这些目录中有相对较多的不同具体页面。事实上，path `/download/pywikipedia/cache`实际上是一个机器人，它执行一些维基百科页面的格式清理，我已经忘记我在 15 年前就离开运行了。考虑到它可能指向任何维基百科页面，实际上我的服务器将回复无限空间的可能页面。还有少量长路径组件，因为 URL 参数有时会传递给一些资源。让我们来看看这个数据集中其他要素的分布情况，看看哪些地方出现了类别不平衡。

```
fig, axes = plt.subplots(3, 2, figsize=(12, 9))

# Which factors should we analyze for class balance?

factors = ['hour', 'minute', 'quad1', 'quad2', 'method', 'status']

# Loop through the axis subplots and the factors

for col, ax in zip(factors, axes.flatten()):

    # Minute is categorical but too many so quantize

    if col == 'minute':

        data = (reqs[col] // 5 * 5).value_counts()

    else:

        data = reqs[col].value_counts()

    data.plot(kind='bar', ax=ax)

    ax.set_title(f"{col} distibution")

# Matplotlib trick to improve spacing of subplots

fig.tight_layout() 
```

![](img/B17126_05_07.png)

图 5.7:不同特性的分布

在这些图中，我们看到一些高度不平衡的阶级和一些基本平衡的阶级。时间显示出轻微的不均衡，但是在大西洋夏令时的 21:00–24:00 左右有相当多的请求。我不清楚为什么我的托管服务器在那个时区，但这是美国太平洋时间下午 6 点左右，所以加州和不列颠哥伦比亚的用户可能会在下班后阅读我的页面。一小时内 5 分钟增量的分布通常是均匀的，尽管一些增量的轻微升高可能不仅仅是随机波动。

IP 地址开头四个字节的不平衡似乎很惊人，最初可能暗示着一个重要的偏差或错误。然而，在稍微深入调查后，我们可以确定使用在线“whois”数据库(在撰写本文时)的`162.158.0.0/16`和`172.69.0.0/16`都被分配给了我用来代理流量的 CDN(内容交付网络)。所以这些特性的不平衡仅仅提供了一个线索，几乎所有的请求都是通过一个已知的实体代理的。特别是，这意味着我们不太可能在任何类型的预测模型中有效地使用这些特征。最多，我们可能会执行特征工程——正如将在第 7 章 、*特征工程*中讨论的那样——来创建一个衍生特征，比如`is_proxied`。

剩下的类不平衡存在于 HTTP 方法和返回的状态代码中。在这两种情况下，`GET`和`200`主导各自的特征一点也不奇怪。这是我所期望的，甚至是希望的，在我的 web 服务器和网站的行为中。因此，没有任何迹象表明数据收集中存在偏见；因为所有的*请求都被记录，所以这不是一个示例，而是一个完整的域。*

作为一个旁注，人口是具体划定的，不一定用来描述任何超出这些线。这些都是在 2020 年 3 月 29 日到 2020 年 4 月 11 日之间对端口`80`或端口`443`对网络域`gnosis.cx`的请求；如果不进一步分析或推理这些数据在整个网络中有多典型，我们就无法得出关于其他网络领域或其他日期的结论。

作为数据科学家，我们不一定受时间因果关系的约束。例如，很明显，以文字和顺序的方式，请求的 IP 地址，可能是`userid`，可能是请求的时间，肯定是请求的 URL，包括方法和路径，将*导致*返回特定的状态代码和字节数。在许多情况下(可能都是在我简单的静态网站上)，大小就是底层 HTML 页面的大小。但是在概念上，服务器可能会根据日期和时间或者请求者的地址做一些不同的事情。在任何情况下，在服务器决定适当的状态代码和响应大小并记录所有这些之前，关于请求的某些*事实*存在几毫秒。

然而，对于一个分析，我们可能想要做出准确地逆转因果关系的预测。在我们预测一天中的时间的努力中，也许我们想把反应的大小当作一个独立变量。例如，大文件可能总是在晚上 7 点左右被请求，而不是在其他时间。我们的*模型*可能试图从结果中预测原因——这在数据科学中是完全合法的，只要我们意识到这一点。事实上，我们可能只寻找相关性，完全忽略了一个特定任务中多重特征的潜在隐藏原因。数据科学不同于其他科学；希望这些努力是互补的。

***

在这一节中，我们仅仅关注于对阶级不平衡的认识和有限程度的分析。对于我们希望使用这些数据的实际任务来说，这意味着什么是另一回事。要记住的一个重要区别是自变量和因变量之间的区别。通常，因变量的不平衡会比自变量的不平衡更严重地扭曲分类模型。因此，举例来说，如果我们希望根据请求的其他特征来预测请求可能产生的状态代码，我们可能会使用将在第 6 章 、*值插补*、*中讨论的采样技术来综合平衡数据集*。

另一方面，阶级不平衡在自变量中也不是完全不相关的，至少对各种模型来说不是。这在很大程度上取决于模型的类型。例如，如果我们使用决策树家族中的某些东西，如果我们希望检测 HEAD 与 500 个状态代码密切相关的(假设)事实，那么 HEAD 请求很少会有什么不同。然而，如果我们使用 K-最近邻算法族，参数空间中的实际距离可能是重要的。就对独立变量中类别不平衡的敏感性而言，神经网络处于中间位置。如果我们将 HTTP 方法编码成一个序数值或者使用一键编码，我们可能会天真地低估了 T2 强大而罕见的特性。一键编码在 [*第 7 章*](Chapter_7.xhtml#_idTextAnchor009) 、*特征工程*中讨论。对于一个独立变量，我们通常不希望*过采样*一个稀有因子水平；但是我们可能希望人为地让它超重。

我们还应该考虑数据的数值范围，这可能反映了非常不同的基本单位。

# 标准化和缩放

> 用千分尺测量。用粉笔做标记。用斧头砍。
> 
> –精确度规则

**概念**:

*   变量中数值范围的影响
*   单变量和多变量效应
*   各种定标器的数字形式
*   因子和样本加权

数据标准化背后的想法是简单地将数据集中使用的所有特征纳入一个可比较的数值范围。当截然不同的单位用于不同的特征时，也就是说，用于参数空间的维度时，一些机器学习模型将不成比例地利用那些仅仅具有较大数值范围的特征。当一个要素具有尚未移除的异常值时，或者当一个要素呈正态分布而另一个要素呈指数分布时，会出现不同比例数值范围的特殊情况。

这本书通常避免展示机器学习的例子或代码。作为一名数据科学家，有许多很棒的库可以解决你 20%的工作，在你完成了这本书教你的 80%的工作后，你还会做这些工作。然而，为了强调标准化的动机，我们将在一些过于整洁的数据上创建一个非常简单的机器学习模型，以展示缩放的巨大优势。对于这个例子，使用了 scikit-learn 中的少量代码。然而，值得注意的是， scikit-learn 中的 scaler 类非常有用，即使您不希望使用该库进行建模。使用 scikit-learn 当然是合理的——甚至可能是 Python 中的最佳实践——即使您只使用它执行过规范化。

这里的合成数据集有两个特征和一个目标；都是连续变量。

```
unscaled = make_unscaled_features()

unscaled 
```

```
 Feature_1     Feature_2       Target

——————————————————————————————————————————

  0    0.112999   19247.756104   11.407035

  1    0.204178   23432.270613   20.000000

  2    0.173678   19179.445753   17.336683

  3    0.161411   17579.625264   16.633166

...         ...            ...         ...

196    0.137692   20934.654450   13.316583

197    0.184393   18855.241195   18.241206

198    0.177846   19760.314890   17.839196

199    0.145229   20497.722353   14.371859

200 rows × 3 columns 
```

一眼就可以看出，`Target`的值在 15 的数量级，而`Feature_1`在 0.1 的数量级，`Feature_2`在 20，000 的数量级。本发明的示例没有为这些测量指定任何特定的单位，但是您可以测量其单位在这些范围内产生数值的许多量。作为第一个问题，我们可能会问是否有任何特征与目标具有单变量相关性。机器学习模型会发现不止这些，但这是一个有用的第一个问题。

```
unscaled.corr() 
```

```
 Feature_1   Feature_2      Target

—————————————————————————————————————————————

Feature_1    1.000000   -0.272963    0.992514

Feature_2   -0.272963    1.000000   -0.269406

   Target    0.992514   -0.269406    1.000000 
```

我们看到`Feature_1`与`Target`有很强的正相关性，`Feature_2`与有适度的负相关性。因此，从表面上看，一个模特应该有足够的时间去工作。事实上，我们可以从相关矩阵中看出，不管有没有归一化，线性模型都会做得非常好；但那是另一本书的主题。这一点可以通过在每个特征上绘制`Target`直观地表现出来。

```
plot_univariate_trends(unscaled) 
```

![](img/B17126_05_08.png)

图 5.8:作为目标函数的特征 1 和特征 2

`Feature_1`具有视觉上明显的相关性；对于人眼来说，最多显示非常弱的一个。

## 应用机器学习模型

正如承诺的那样，让我们针对这些数据应用一个机器学习模型，尝试根据这些特征预测目标。在 ML 中，我们习惯上分别使用名称`X`和`y`来表示特性和目标。这遵循了高中代数中常见的模式，命名一个自变量`x`和一个因变量`y`。因为我们通常有多个特征，所以使用大写的`X`。虽然我们不能深入讨论动机，但机器学习的良好实践是始终保留一部分训练数据用于测试，这样就不会过度拟合模型。这是通过功能`train_test_split()`完成的。

```
from sklearn.model_selection import train_test_split

X = unscaled.drop('Target', axis=1)

y = unscaled['Target']

X_train, X_test, y_train, y_test = (

    train_test_split(X, y, random_state=1)) 
```

对于本例，我们使用 K-neighbors 回归器来尝试对数据进行建模。对于许多类型的问题，这是一种非常有效的算法，但它也是一种直接查看参数空间中的距离的算法，因此对缩放非常敏感。如果我们天真地将这个模型应用于我们的原始数据，R 平方得分非常低(其他指标也会同样糟糕)。

```
from sklearn.neighbors import KNeighborsRegressor

knn = KNeighborsRegressor()

knn.fit(X_train, y_train).score(X_test, y_test) 
```

```
0.027756186064182953 
```

“完美”的 R 平方得分是 1.0。很差的分数是 0.0(负分有时也是可能的，某种意义上更差)。但是对于任何低于 0.25 左右的，我们基本上拒绝这个模型。

在这种情况下，通过使用最小-最大缩放器，我们获得了好得多的指标分数。我们在这里使用的缩放器简单地获取原始特征的最小值，并通过减法将所有值向零移动该量，然后将所有值除以移动后的最大值。其效果是为每个特征产生一个始终为`[0, 1]`的范围。这个合成特征本身没有任何物理意义，而最初的测量可能有。

但是通过应用这个定标器，保证所有特征占据相同的数值范围(特定值在其范围内不同地分布)。让我们在再次拟合模型之前将这种最小-最大缩放应用于我们的特征。

```
from sklearn.preprocessing import MinMaxScaler

X_new = MinMaxScaler().fit_transform(X)

X_train, X_test, y_train, y_test = (

    train_test_split(X_new, y, random_state=1))

knn2 = KNeighborsRegressor()

knn2.fit(X_train, y_train).score(X_test, y_test) 
```

```
0.9743878175626131 
```

请注意，在上面的代码中，我没有考虑缩放目标。这样做对模型没有害处，但是也没有好处，因为目标不是特征的参数空间的一部分。此外，如果我们缩放目标，我们将不得不记住相应地取消缩放，以在期望的单位中获得有意义的数字。

## 缩放技术

我们上面使用的缩放技术利用了 scikit-learn 的`MinMaxScaler`。scikit-learn 中的所有定标器都使用相同的 API，并以有效和正确的方式实现。在 Python 中使用这些当然有很好的理由，即使 scikit-learn 不是您整个建模管道的一部分。然而，使用较低级别的矢量化操作“手动”进行同样的缩放并不困难。例如，这在 NumPy 中很简单；这里我们展示一个 R 中的例子，只关注算法。scikit-learn API 的一个很好的细节是它知道逐列规范化。对比中，我们只做一个栏目。

```
%%R -i X,X_new

# Import the data frame/array from Python

py_raw_data <- X$Feature_1  # only feature 1

py_scaled <- X_new[,1]      # scaled column 1

# Utility function to scale as [0, 1]

normalize <- function(x) {

    floor <- min(x)  # Only find min once

    return ((x - floor) / (max(x) - floor))

}

# Scale the raw data

r_scaled <- normalize(py_raw_data)

# Near equality of elements from normalize() and MinMaxScaler

all.equal(py_scaled, r_scaled) 
```

```
[1] TRUE 
```

注意，即使是像这样的简单操作，不同库和语言的不同实现也不会以相同的顺序执行相同的操作。这允许一些浮点舍入差异悄悄进入。比较浮点值的严格相等几乎总是错误的；测量具有有限的精度，并且操作经常引入 1-ULP(最后一位单位)误差。另一方面，这些微小的数字差异对实际模型没有实际影响，只对等式检查有影响。

```
%%R

print("A few 'equalities':")

print(py_scaled[1:5])

print(r_scaled[1:5])

print("Exactly equal?")

print((py_scaled == r_scaled)[1:10])

print("Mean absolute difference:")

print(mean(abs(py_scaled - r_scaled))) 
```

```
[1] "A few 'equalities':"

[1] 0.1776148 1.0000000 0.7249096 0.6142706 0.8920478

[1] 0.1776148 1.0000000 0.7249096 0.6142706 0.8920478

[1] "Exactly equal?"

[1] TRUE FALSE FALSE FALSE FALSE TRUE FALSE TRUE TRUE TRUE

[1] "Mean absolute difference:"

[1] 6.130513e-17 
```

另一种非常常见的缩放技术在 scikit-learn 中被称为`StandardScaler`。它将要素的平均值设置为 0，标准差设置为 1。当变量(非常粗略地)呈正态分布时，这种缩放尤为重要。该名称暗示这种方法通常是选择的默认缩放器(尽管在选择该名称时，它可能源自“标准偏差”)。让我们实现它来说明这个简单的转换。这里我们显示来自`Feature_2`的值，在原始数据中大约是 20，000。

```
from sklearn.preprocessing import StandardScaler

X_new2 = StandardScaler().fit_transform(X)

# Second column for example (both were scaled)

plt.hist(X_new2[:, 1], bins=30)

plt.title("Value distribution after StandardScaler"); 
```

![](img/B17126_05_09.png)

图 5.9:standard scaler 变换后的 Feature_2 值分布

`StandardScaler`比`MinMaxScaler`使用更多的数字运算，因为它涉及标准偏差，这给计算带来更多的机会引入数字误差。scikit-learn 中的代码比我们给出的简单版本更好地执行了一些技巧来最小化这种错误，尽管大小不太可能真正重要。让我们手动重现 StandardScaler 的基本操作。

```
%%R -i X,X_new2

# Import the data frame/array from Python

py_raw_data <- X$Feature_2  # Only feature 2

py_scaled <- X_new2[, 2]    # scaled column 2

r_scaled = (py_raw_data - mean(py_raw_data)) / 

            sd(py_raw_data)

all.equal(py_scaled, r_scaled) 
```

```
[1] "Mean relative difference: 0.002503133" 
```

在这个计算中，我们没有通过`all.equal()`测试。r 仅通过一个布尔`FALSE`来描述故障。我们可以通过设置公差参数进行稍微宽松一点的比较。让我们也验证缩放数据的特征。

```
%%R

print("Mean from R scaling:")

print(mean(r_scaled))

print("Standard deviation:")

print(sd(r_scaled))

print("Almost equal with tolerance 0.005")

all.equal(py_scaled, r_scaled, tolerance = 0.005) 
```

```
[1] "Mean from R scaling:"

[1] 6.591949e-17

[1] "Standard deviation:"

[1] 1

[1] "Almost equal with tolerance 0.005"

[1] TRUE 
```

通过基本的乘法和减法运算，有许多变化可用于缩放。例如，我们可以使用四分位间距(IQR)进行归一化，而不是基于标准差进行归一化。例如， scikit-learn 类`RobustScaler`就是这样做的。在某种程度上，IQR——或通常基于分位数的方法——对异常值更稳健。然而，IQR 距离标度正常化的程度是有限的，更严格的分位数方法可能更激进。

让我们在我们呈现的样本数据集中复制`Feature_1`，但是仅使一个值(200 个中的)成为*极端*异常值。回想一下`Feature_1`的值大约为 0.1。我们将在变量中引入一个值 100。可以说，这是一个足够极端的异常值，我们应该已经使用第 4 章*异常检测*中讨论的技术消除了它，但出于某种原因，我们没有这样做。

```
X['Feature_3'] = X.Feature_1

X.loc[0, 'Feature_3'] = 100 
```

当我们试图利用`RobustScaler`时，转换后的数据仍有一个数据点处于极值值。事实上，这个极端值比我们选择的界外值 100 更糟糕；此外，离群值甚至比在`StandardScaler`变换下更远。`RobustScaler`实际上只在包含适度数量的适度异常值(可能逃过异常检测的那种)的集合下才有效。

```
from sklearn.preprocessing import RobustScaler

X_new3 = RobustScaler().fit_transform(X)

# Third column for example (all were scaled)

plt.hist(X_new3[:, 2], bins=30)

plt.title("Value distribution after RobustScaler"); 
```

![](img/B17126_05_10.png)

图 5.10:鲁棒定标器后的 Feature_1 值分布

我们可以使用的一个更强的方法是严格地缩放值，使它们只落在分位数的*内*。从本质上讲，这是在每个分位数范围内分别对数据进行缩放，从而在总体上实现合理的分布，并对值施加严格的限制。

```
from sklearn.preprocessing import QuantileTransformer

# Ten quantiles is also called "decile"

deciles = QuantileTransformer(n_quantiles=10)

X_new4 = deciles.fit_transform(X)

# Third column for example (all were scaled)

plt.hist(X_new4[:, 2], bins=30)

plt.title("Value distribution after QuantileTransformer"); 
```

![](img/B17126_05_11.png)

图 QuantileTransformer 后的 Feature_1 值分布

显然，这种转换后的数据并不完全一致——如果没有超出有序顺序的*和一些*可变性，它将没有什么价值——但它是有界限的，并且合理地均匀分布在范围`[0, 1]`内。单个异常值点仍然是主分布中的次要异常值，但在数值上并不是很远。

原则上，即使 scikit-learn 中的特定转换器以列方式操作，我们也可能希望对每个列或特性应用不同的缩放技术。只要特定变换在大致相同的标度上(即，至少对于大多数数据，通常最大值和最小值之间的距离大约为一或二)的变换值之间产生数值范围，所有利用参数空间中的距离作为其算法的一部分的机器学习技术都将得到满足。这种算法的例子包括线性模型、支持向量机和 K-最近邻。如前所述，决策树家族中的算法根本不关心维度中的特定距离，神经网络可以通过允许我们可以非正式地称之为“**缩放层**来执行一种缩放，至少*可以*充当每个输入特征的乘数(一个经过训练的网络“决定”使用神经元和层的确切目的对于我们的意图或理解来说总是有些不透明)。

## 因子和样本加权

有时候你会希望赋予某个特性比*跨特性公平缩放*所允许的更重要的意义。这个问题与第 6 章 、*值插补*中通过 [*抽样解决的问题略有不同。在后一章中，我将讨论欠采样或过采样来产生少数目标类的更多见证。这当然是平衡特性中的类而不是目标中的类的一种可能的方法，但通常不是最好的方法。*](Chapter_6.xhtml#_idTextAnchor008)

至少，对两个不同的不平衡类进行过采样有可能导致合成样本数量激增。

对于不平衡的要素类，可以使用另一种方法。我们可以简单地*加重*少数民族类，而不是过度采样。许多机器学习模型包含一个显式的[超参数](Glossary.xhtml#_idTextAnchor056)，称为，类似于`sample_weight`(sci kit-learn 拼写)。然而，与样本权重分开，这些相同的模型类有时也会有类似于`class_weight`的东西作为单独的超参数。这里的区别正是我们一直在做的:样本权重允许您超重(或欠重)特定的输入数据行，而类权重允许您超重/欠重特定的目标类值。

为了在这个问题上增加更多的细微差别，我们不局限于仅仅为了解决阶级不平衡而增加/减少体重。事实上，我们可以将它应用于任何我们喜欢的理由。例如，我们可能知道数据集中的某些测量值比其他测量值更可靠，并希望对它们进行加权。或者，我们可能知道，出于特定任务的原因，正确预测具有某种特征的样本更重要，即使我们不希望完全丢弃那些缺乏该特征的样本。

让我们回到 Apache 日志文件的例子来说明所有这些问题。回想一下，处理后的数据如下所示:

```
reqs.sample(8, random_state=72).drop('page', axis=1) 
```

```
 **hour    minute    quad1    quad2    method**

——————————————————————————————————————————————————

 **3347**     0         4      172       69       GET

 **2729**     9        43      172       69       GET

 **8102**     4        16      172       69       GET

 **9347**     0        48      162      158       GET

 **6323**    21        30      162      158       GET

 **2352**     0        35      162      158       GET

**12728**     9         0      162      158       GET

**12235**    19         3      172       69       GET

 **path    status**

————————————————————————————————————————————————————

 **3347**              /publish/programming       200

 **2729  **                             /TPiP       200

 **8102  **                    /member/images       404

 **9347  **                   /publish/images       304

 **6323  **       /download/pywikipedia/cache       200

 **2352  **  /download/gnosis/xml/pickle/test       200

**12728  **                   /download/relax       200

**12235  **                            /dede2       404 
```

我们注意到,`method`和`status`都高度不平衡，就像我们期望它们在一个工作的网络服务器中那样。方法数据特别有这个不平衡，我们在上面的*图 5.7* 中看到了。我们设想的假设任务是根据数据集的其他特性预测状态代码(例如，不实际发出 HTTP 请求，该请求可能会根据当前时间而改变)。

```
reqs.method.value_counts() 
```

```
GET     13294

HEAD      109

POST       48

Name: method, dtype: int64 
```

换句话说，`GET`请求比`HEAD`请求常见 122 倍，比`POST`请求常见 277 倍。我们可能会担心这限制了我们对该方法的稀有类值进行预测的能力。通常我们的模型会简单地为我们指出这一点，但有时他们不会。此外，尽管这是一个频繁出现的路径，但我们已经决定需要我们的模型对路径`/TPiP`更加敏感，因此也将人为地将其增加 5 倍。请注意，在这个规定中，超重与特性的底层分布没有任何关系，而是我们建模的底层目的的一个领域需求。

同样，我们特别关注预测 404 状态代码(即增强该标签的[召回](Glossary.xhtml#_idTextAnchor107))，但不一定对目标的整体平衡感兴趣。相反，我们将所有其他结果的权重设为 1，但将 404 的权重设为 10，这是我们在执行建模之前确定的任务目的。让我们用代码来完成所有这些，在本例中使用了 scikit-learn 中的随机森林模型。如果某一行同时与超额分配的`path`和代表不足的`method`匹配，则`method`的较大乘数将优先。

```
# The row index positions for rows to overweight

tpip_rows = reqs[reqs.path == '/TPiP'].index

head_rows = reqs[reqs.method == 'HEAD'].index

post_rows = reqs[reqs.method == 'POST'].index

# Configure the weights in a copy of data frame

reqs_weighted = reqs.copy()

reqs_weighted['weight'] = 1  # Default weight of one

reqs_weighted.loc[tpip_rows, 'weight'] = 5

reqs_weighted.loc[head_rows, 'weight'] = 122

reqs_weighted.loc[post_rows, 'weight'] = 277

# Do not use column page in the model

reqs_weighted.drop('page', axis=1, inplace=True)

# View the configured weights

reqs_weighted.sample(4, random_state=72) 
```

```
 hour  minute  quad1  quad2  method                  path  status

———————————————————————————————————————————————————————————————————————

3347     0       4    172     69     GET  /publish/programming     200

2729     9      43    172     69     GET                 /TPiP     200

8102     4      16    172     69     GET        /member/images     404

9347     0      48    162    158     GET       /publish/images     304

      weight

—————————————

3347       1

2729       5

8102       1

9347       1 
```

这些样本权重以每行为基础存储；换句话说，我们有 13451 个。在这个例子中，大多数只是权重 1，但在概念上，它们可能都是不同的数。配置我们希望用于目标的权重是不同的。我们*可以*利用样本权重本身来选择带有特定目标标签的行；然而，这种方法是不必要的笨重，通常不是我们的首选方法。相反，我们只是希望创建一个从标签到重量的小映射。

```
target_weight = {code:1 for code in reqs.status.unique()}

target_weight[404] = 10

target_weight 
```

```
{200: 1, 304: 1, 403: 1, 404: 10, 301: 1, 500: 1, 206: 1} 
```

在这里，我们将创建、拟合、训练和评分一个 scikit-learn 模型。如果您使用其他库，API 会有所不同，但概念将保持不变。执行训练/测试分割只需要一行代码，这在真实代码中是很好的实践。作为一个次要的 API 细节，我们需要为这个模型类型编码我们的字符串分类值，所以我们将使用`OrdinalEncoder`。

```
from sklearn.ensemble import RandomForestClassifier

from sklearn.preprocessing import OrdinalEncoder

# Create the model object with target weights

rfc = RandomForestClassifier(class_weight=target_weight,

                             random_state=0)

# Select and encode the features and target

X = reqs_weighted[['hour', 'minute', 

                   'quad1', 'quad2',

                   'method', 'path']]

# Encode strings as ordinal integers

X = OrdinalEncoder().fit_transform(X)

y = reqs_weighted['status']

weight = reqs_weighted.weight

# Perform the train/test split, including weights

X_train, X_test, y_train, y_test, weights_train, _ = (

     train_test_split(X, y, weight, random_state=1))

# Fit the model on the training data and score it

rfc.fit(X_train, y_train, sample_weight=weights_train)

rfc.score(X_test, y_test) 
```

```
0.8183169788878977 
```

与回归示例中使用的 R 平方一样，1.0 代表完美的[精度](Glossary.xhtml#_idTextAnchor013)。但是精度不能低于 0.0。

没有更多的背景和分析，我不能说这个模型对于预期的目的是好是坏。很有可能一些其他的模型等级和/或一些更好调整的权重将更好地服务于假设的目的。尝试这些的步骤很简单，并且与所示的代码基本相同。

我们现在转向一个困难但重要的概念。很多时候，我们希望从数据中去除预期趋势，以揭示这些趋势的例外。

# 周期性和自相关性

> 我自相矛盾吗？
> 
> 很好，那我就自相矛盾，
> 
> (我是大的，我包含众多。)
> 
> 沃尔特·惠特曼

**概念**:

*   去除顺序数据趋势
*   检测到的循环与先验领域知识
*   预期与独特的可变性
*   多重周期性
*   自相关

有时，您希望数据中包含周期性行为。在这种情况下——特别是当连续数据中存在多个重叠周期时——周期模式与的偏差可能比原始值更能提供信息。当然，最常见的是，我们看到这与时间序列数据联系在一起。在某种程度上，这个关注点属于第七章[](Chapter_7.xhtml#_idTextAnchor009)*，*特征工程*的范围，实际上我们回到了一些相同的关注点，甚至是我们在这里讨论的相同的数据集。*

 *作为第一步，我们希望能够识别和分析数据中的周期性或周期性。一旦我们有了一些领域知识，其中的一些直觉上是显而易见的，但是其他的潜伏在数据本身中，而不一定在我们最初的直觉中。在本节中，我将利用我的朋友、偶尔的合著者 Brad Huntting 多年前收集的数据集。在过去的一段时间里，Brad 收集了他在科罗拉多州(美国)的房子内外的温度，通常每 3 分钟一次。这里展示的数据涵盖了少于一年的几天。

房子里的房间由恒温器控制；户外自然呈现出季节变化。此外，数据本身是不完善的。当我们在 [*第 7 章*](Chapter_7.xhtml#_idTextAnchor009)*特征工程*中返回这些数据时，我们将查看数据收集中的缺口、记录错误和其他问题。就本节而言，在加载数据集的代码中进行了少量的数据清理和值插补。参见第 6 章 、*值插补*，了解插补的更多一般讨论，以及不同的示例。

首先，让我们使用 Python 函数加载 Pandas 数据帧来读入数据。然而，在加载步骤之后，我们将在 R 及其 Tidyverse 中执行分析和可视化。其他库和语言中也有非常相似的功能，包括 Pandas。这里重要的是底层概念，而不是所使用的特定 API 和语言。Brad 使用了一个网络域名“glarp ”,所以我们用这个发明的词作为一些变量的名称，来指代他的房屋温度数据。

```
thermo = read_glarp()

start, end = thermo.timestamp.min(), thermo.timestamp.max()

print("Start:", start)

print("  End:", end)

# Fencepost counting includes ends

print(" Days:", 1 + (end.date() - start.date()).days) 
```

```
Start: 2003-07-25 16:04:00

  End: 2004-07-16 15:28:00

 Days: 358 
```

让我们看几行数据集，感受一下它的本质。我们可以看到，在记录间隔期间，每 3 分钟就有一行。对于这个部分，时间间隔完全是规则的，为 3 分钟，不存在丢失的值。此外，原始数据中一些明显的记录错误在这里用估算值进行了清理。

```
%%R -i thermo

glarp <- as.tibble(thermo)

glarp 
```

```
# A tibble: 171,349 x 5

   timestamp           basement   lab livingroom outside

   <dttm>                 <dbl> <dbl>      <dbl>   <dbl>

 1 2003-07-25 16:04:00     24    25.2       29.8    27.5

 2 2003-07-25 16:07:00     24    25.2       29.8    27.3

 3 2003-07-25 16:10:00     24    25.2       29.8    27.3

 4 2003-07-25 16:13:00     24.1  25.2       29.8    27.4

 5 2003-07-25 16:16:00     24.1  25.2       29.8    27.8

 6 2003-07-25 16:19:00     24.1  25.2       29.8    27.5

 7 2003-07-25 16:22:00     24.1  25.2       29.8    27.6

 8 2003-07-25 16:25:00     24.1  25.2       29.8    27.6

 9 2003-07-25 16:28:00     24.1  25.2       29.8    27.7

10 2003-07-25 16:31:00     24.1  25.2       29.8    27.6

# ... with 171,339 more rows 
```

我们可以将这些数据视为消除周期性的第一步，目标是关注个体测量与*预期*的差异。这些操作就是也叫**去趋势**数据。让我们先看看室外温度，用 [ggplot2](Glossary.xhtml#_idTextAnchor050) 绘制出它们的模式。

```
%%R

ggplot(glarp, aes(x=timestamp, y=outside)) +

  geom_line() + clean_theme +

  ggtitle("Outside temperature over recording interval") 
```

![](img/B17126_05_12.png)

图 5.12:记录间隔期间的外部温度

很容易猜测，北半球七月份的气温比一月份的气温高，这是一个普遍的模式，在全球趋势中有很大的波动。尽管只有 1 年的数据可用，但根据非常基本的领域知识，我们知道其他年份会有类似的年度周期。相比之下，正如我们也能预料到的，室内温度的变化范围较窄，也不太明显。

```
%%R

ggplot(glarp, aes(x=timestamp, y=basement)) +

  geom_line() + clean_theme +

  ggtitle("Basement temperature over recording interval") 
```

![](img/B17126_05_13.png)

图 5.13:记录间隔期间的基底温度

总的来说，地下室的室内温度相对较窄，大约在 14°C 到 23°C 之间。有些点超出了这个范围，有些夏天的高温表明房子有供暖系统，但没有空调，有些冬天的低温则急剧上升，可能反映了窗户打开的时间。然而，室外的最低温度达到了零下 20 度，而室内的最低温度一般都在 10 度以上。2003 年 9 月和 10 月左右似乎也发生了一些奇怪的事情；也许这反映了那个时期供暖系统的一些变化。

## 领域知识趋势

作为第一项任务，让我们想想室外温度大概不会受到室内供暖系统的影响。我们希望将意外温暖或意外寒冷的测量值作为下游模型的输入。例如，10°C 的温度可能是令人惊讶的寒冷的夏季温度，也可能是令人惊讶的温暖的冬季温度，但就其本身而言，它只是全球范围内的典型温度，在没有额外背景的情况下，它不会携带太多关于观测的信息。

考虑到每年的温度会不断重复，将这种年度模式建模为正弦波的一部分可能是有意义的。然而，在形状上，从大约 2003 年最热的一天到大约 2004 年最热的一天这段时间，它确实像一条抛物线。因为我们仅仅是去趋势化一个年尺度的模式，而不是*模拟*的行为，让我们用一个二阶多项式来拟合数据，这将说明*测量中存在的大部分*变化。

```
%%R

# Model the data as a second order polynomial

year.model <- lm(outside ~ poly(timestamp, 2), data = glarp)

# Display the regression and the data

ggplot(glarp, aes(x=timestamp)) + clean_theme +

  geom_line(aes(y = outside), color = "gray") +

  geom_line(aes(y = predict(year.model)), 

            color = "darkred", size = 2) +

  ggtitle("Outside temperature versus polynomial fit") 
```

![](img/B17126_05_14.png)

图 5.14:用多项式曲线拟合外部温度数据

我们可以在图中看到我们的年度去趋势化占了数据变化的大部分，因此我们可以简单地从潜在点中减去趋势，作为第一步，获得测量结果的意外程度。一个名为`outside`的新表将保存这个更窄的焦点的数据。

```
%%R

outside <- glarp[, c("timestamp", "outside")] %>%

    add_column(no_seasonal = glarp$outside - predict(year.model))

outside 
```

```
# A tibble: 171,349 x 3

   timestamp           outside no_seasonal

   <dttm>                <dbl>       <dbl>

 1 2003-07-25 16:04:00    27.5        1.99

 2 2003-07-25 16:07:00    27.3        1.79

 3 2003-07-25 16:10:00    27.3        1.79

 4 2003-07-25 16:13:00    27.4        1.89

 5 2003-07-25 16:16:00    27.8        2.29

 6 2003-07-25 16:19:00    27.5        1.99

 7 2003-07-25 16:22:00    27.6        2.10

 8 2003-07-25 16:25:00    27.6        2.10

 9 2003-07-25 16:28:00    27.7        2.20

10 2003-07-25 16:31:00    27.6        2.07

# ... with 171,339 more rows 
```

可视化季节性去趋势温度，我们看到一个从大约-20°C 到+20°C 的剩余范围。这比原始温度的范围小一些，但只是小一些。可变性有所降低，但幅度不大。

然而，一旦我们进行了这种去除，就没有明显的整体年度趋势，合成值以 0 为中心。

```
%%R

ggplot(outside, aes(x=timestamp)) +

  geom_line(aes(y = no_seasonal)) + clean_theme +

  ggtitle("Outside temperature with removed seasonal expectation") 
```

![](img/B17126_05_15.png)

图 5.15:减去季节预期的室外温度

我们对室外温度循环的第二个明显的认识是，白天比晚上更暖和。假设有 358 天的数据，多项式显然不适合，但三角模型可能更适合。这里我们不计算傅立叶分析，而是简单地寻找预期的每日周期。由于我们每天每 3 分钟进行一次观测，我们希望将这 3360 个间隔转换成 2 *π* 弧度，以便回归到模型中。该模型将简单地由拟合的正弦和余弦项组成，它们可以在指定的周期上附加地构建任何正弦状曲线。

```
%%R

# Make one day add up to 2*pi radians

x <- 1:nrow(outside) * 2*pi / (24*60/3)

# Model the data as a first order trigonometric regression

day_model <- lm(no_seasonal ~ sin(x) + cos(x), 

                data = outside)

print(day_model)

# Create a new tibble the holds the regression 

# and its removal from the annually detrended data

outside2 <- add_column(outside, 

                day_model = predict(day_model),

                no_daily = outside$no_seasonal - day_model)

outside2 
```

```
Call:

lm(formula = no_seasonal ~ sin(x) + cos(x), data = outside)

Coefficients:

(Intercept)       sin(x)       cos(x)  

  0.0002343   -0.5914551    3.6214463  

# A tibble: 171,349 x 5

   timestamp           outside no_seasonal day_model no_daily

   <dttm>                <dbl>       <dbl>     <dbl>    <dbl>

 1 2003-07-25 16:04:00    27.5        1.99      3.61    -1.62

 2 2003-07-25 16:07:00    27.3        1.79      3.60    -1.81

 3 2003-07-25 16:10:00    27.3        1.79      3.60    -1.80

 4 2003-07-25 16:13:00    27.4        1.89      3.59    -1.69

 5 2003-07-25 16:16:00    27.8        2.29      3.58    -1.28

 6 2003-07-25 16:19:00    27.5        1.99      3.56    -1.57

 7 2003-07-25 16:22:00    27.6        2.10      3.55    -1.46

 8 2003-07-25 16:25:00    27.6        2.10      3.54    -1.44

 9 2003-07-25 16:28:00    27.7        2.20      3.53    -1.33

10 2003-07-25 16:31:00    27.6        2.07      3.51    -1.44

# ... with 171,339 more rows 
```

仅从数据框的前几行很难判断，但每日去趋势通常比单独的季节性去趋势更接近于零。回归主要由余弦因子组成，但被较小的负正弦因子偏移了一点。截距非常接近于零，正如我们从季节性趋势中所预期的那样。如果我们将这三条线形象化，我们可以得到一些意义；为了更好地显示它，只显示了 2003 年 8 月初的一周。其他时间段也有类似的模式；由于去趋势化，所有都将集中在零。

```
%%R

week <- outside2[5000:8360,]

p1 <- ggplot(week, aes(x = timestamp)) +

  no_xlabel + ylim(-8, +8) + 

  geom_line(aes(y = no_seasonal))

p2 <- ggplot(week, aes(x = timestamp)) +

  no_xlabel + ylim(-8, +8) + 

  geom_line(aes(y = day_model), color = "lightblue", size = 3)

p3 <- ggplot(week, aes(x = timestamp)) +

   clean_theme + ylim(-8, +8) +

  geom_line(aes(y = no_daily), color = "darkred")

grid.arrange(p1, p2, p3,

            top = "Annual de-trended; daily regression; daily de-trended") 
```

![](img/B17126_05_16.png)

图 5.16:年度去趋势数据；日常回归；每日去趋势

更粗、更平滑的线是温度的每日模型。在这本书的电子版中，它将显示为浅蓝色。顶部是更广泛变化的季节性去趋势数据。在底部，每日去趋势数据大多幅度较低(如果你的阅读格式允许，用红色表示)。第三个支线剧情简单来说就是从上面一个减去中间一个支线剧情。

8 月 7 日左右是一些奇怪的低值。这些看起来足够尖锐，足以表明数据问题，但也许是一场雷暴在一个下午带来了八月的气温低得多。在绘制的日期范围内，我们可以注意到的一点是，即使是每日去趋势数据也显示了一个弱的日周期，尽管有更多的噪音。这表明一年中其他周的温度波动比这一周小；事实上，一些周将显示反周期模式，去趋势数据是回归线的近似倒数。值得注意的是，即使在这个图上，看起来 8 月 8 日是反周期的，而 8 月 5 日和 6 日有一个剩余的信号匹配回归的符号，其他日子有一个不太清晰的对应关系。所谓反周期，我们并不是指，例如，一个夜晚比它周围的日子更温暖，而是指波动比预期的要小，因此去趋势产生了一个反转的模式。

也就是说，虽然我们还没有去除更复杂的循环趋势的每一个可能的元素，但是双重去趋势数据中大多数值的范围大约是 8°C，而原始数据的范围大约是 50°C。我们的目标不是完全消除潜在的可变性，而是强调更极端的星等测量，这已经做到了。

## 发现的周期

关于室外温度可能会怎样，我们有很好的先验信念。夏天比冬天暖和，夜晚比白天冷。然而，对于室内温度却没有类似的明显假设。我们之前看过布拉德地下室的温度图。有趣的是，这些数据是嘈杂的，但我们特别注意到，在大约两个夏季月的时间里，地下室的温度昼夜都保持在 21°C 以上。由此，我们推断布拉德的房子有供暖系统，但没有制冷系统，因此室内温度大致跟随较高的室外温度。我们希望在这里只分析供暖系统及其人工维持的温度，而不是季节趋势。让我们将数据限制在非夏季(这里根据数据中的模式而不是正式的季节日期来命名)。

```
%%R

not_summer <- filter(glarp, 

                     timestamp >= as.Date("2003-08-15"), 

                     timestamp <= as.Date("2004-06-15")) 

# Plot only the non-summer days

ggplot(not_summer, aes(x=timestamp, y=basement)) +

  geom_line() + clean_theme +

  ggtitle("Basement temperature over non-summer days") 
```

![](img/B17126_05_17.png)

图 5.17:非夏季的地下室温度

在稍微变窄的时间段内，几乎每天的测量温度都在 18-20°C 上下，因此很可能在几乎所有这些非夏季的日子里，供暖系统每天都有一部分时间在运行。我们想要分析的问题——也可能是趋势的问题——是室内温度数据中是否存在循环模式，在原始数据中明显存在相当大的噪声。

一种叫做**自相关**的技术很好地帮助了进行这种分析。自相关是一种识别重复模式的数学技术，例如存在与噪声混合的周期性信号或非周期性变化。在熊猫中，系列方法`.autocorr()`寻找这个。在 R 中，相关函数被称为`acf()`。其他库或编程语言也有类似的功能。让我们看看我们的发现。注意，如果我们的领域知识告诉我们，在主题中只有某些周期性“有意义”,我们不希望盲目地寻找自相关。

虽然我们的数据框已经包含了一个`timeseries`列，但是在这里简单地从我们将要使用的`basement`列中创建一个更容易。对应于数据点的实际日期与操作无关；只对它们的时间间隔感兴趣。特别是，我们可以强加一个与一天中的观察次数相匹配的频率，以得到一个直观地用天数标记的图。`acf()`函数会自动生成一个图，并返回一个带有大量值的对象，您可以使用这些值。就本节而言，图表就足够了。

```
%%R

per_day <- 24*60/3

basement.temps <- ts(not_summer$basement, frequency = per_day)

auto <- acf(basement.temps, lag.max = 10*per_day) 
```

![](img/B17126_05_18.png)

图 5.18:不同增量下相似性的密度分布

顾名思义，这显示了单个数据序列在每个可能的偏移处与其自身的相关性。一般来说，零增量与自身 100%相关。除此之外的一切都告诉我们一些关于这个特定数据中的周期性的具体信息。在每个整数天都有很强的峰值。我们把分析限制在 10 天以后。这些尖峰让我们看到地下室的恒温器有一个设置，可以在每天的不同时间将温度调节到不同的水平，但在一天和接下来的 10 天中，温度调节的方式基本相同。

该数据中的尖峰是倾斜的，而不是尖锐的(它们至少是连续的，而不是阶梯状的)。任何给定的 3 分钟间隔都倾向于具有与它附近的温度相似的温度，随着测量在更远的地方发生，温度下降得相当快，但不是瞬间的。当然，这是我们在一个有自动调温器控制的供暖系统的房子里所期望的。其他系统可能有所不同；例如，如果一盏灯在计时器上准确地亮 3 分钟然后熄灭，在某个时间表上，相邻测量之间的亮度级测量将会突然不同，而不是逐渐不同。

然而，自相关模式提供了比日周期更多的信息。我们还在大约半天的时间间隔内看到了较低的相关性。这也很容易通过思考领域和产生它的技术来理解。为了节省能源，Brad 把他的定时闹钟设定在早上他醒来的时候，然后在他在办公室的时候调到较低的音量，然后在傍晚他回家的时候再调到较高的音量。我碰巧知道这是一个自动设置，但同样的效果可能会发生，例如，如果只是在这些时间手动上下调节恒温器的人类模式(信号可能不如机械计时器强，但很可能存在)。

超过每日周期，在 7 天的自相关中也有稍微更高的峰值。这表明一周中的每一天都与恒温器的温度设置相关。例如，最有可能的是，由于计时器的设置或者人类的习惯和舒适度，工作日和周末设置了不同的温度。这种二级模式不如一般的 24 小时周期强，但与半天周期差不多强；更仔细地检查自相关峰值可以准确地揭示 Brad 通常在办公室和回家的持续时间。次要峰值相对于 24 小时峰值的偏移可能不正好在 12 小时，而是在小于完整的 24 小时的某个增量处。

在本节中，我们不做这些操作，但是考虑使用自相关作为去趋势回归，就像我们使用三角回归一样。这将有效地具有 12 小时和 24 小时以及 7 天的独立周期。很明显，显示的原始数据有很多额外的噪声，但通过减去这些已知的模式，噪声可能会减少。一些非常不典型的值会在这些去趋势数据中更加突出，从而潜在地具有更强的分析意义。

有时，我们需要执行的数据验证仅仅是高度特定于所讨论的领域。为此，我们倾向于需要更多的定制方法和代码。

# 定制验证

> 存在解释；它们一直存在；对于人类的每个问题，总有一个众所周知的解决方案——简洁、合理、错误。
> 
> 门肯

**概念**:

*   利用异常检测之外的领域知识
*   示例:评估重复数据
*   验证作为进一步调查的健全性检查

很多时候,领域知识决定了可能是真实的数据与更可能反映某种记录或核对错误的数据的形状。尽管数据的一般统计没有显示异常、偏差、不平衡或其他一般性问题，但我们对该领域或特定问题有了更多的了解，这表明了我们对“干净”数据的期望。

举例来说，我们可能期望某些种类的观察与其他观察相比，应该以大致特定的频率出现；也许这将由第三个分类变量的类值进一步指定。例如，作为背景领域知识，我们知道在美国，家庭规模平均略小于 2 个孩子。如果我们的数据包含了抽样家庭中所有个人的信息，我们可以用它作为数据形状的指导方针。事实上，如果我们有每个州每个家庭的孩子的辅助数据，我们可能会在验证我们的数据时进一步细化这个参考预期。

显然，我们并不期望每个家庭都有 1.9 个孩子。鉴于人类是以整数为单位的，事实上我们在任何特定的家庭中都不可能有这样的小数。然而，如果我们发现在我们抽样的家庭中，平均每个家庭有 0.5 个孩子，或者每个有孩子的家庭有 4 个孩子，我们将有一个强烈的迹象表明某种样本偏差正在发生。也许儿童在单个家庭的家庭数据中被少报或多报了。也许选择哪些家庭作为样本会使数据偏向有孩子的家庭，或者没有孩子的家庭。这个场景很大程度上类似于本章前面提到的与基线比较的问题。这只是给前面的例子增加了一点小小的麻烦，因为我们只确定了我们希望根据若干次观察(即一个家庭)中的共享地址特征来验证我们对儿童(即 18 岁以下)数量的预期的家庭。

## 排序规则验证

让我们来看一个完全不同的例子，它实际上不能用基准期望的公式表示。在本节中，我们考虑从日本(DDBJ)DNA 数据库下载的核糖体 RNA (rRNA)基因组数据，特别是 FASTA 格式数据集的 [16S rRNA(原核生物)。对于这个例子，你不需要知道任何关于基因组学或细胞生物学的知识；我们只关注使用的数据格式和这种格式的记录集合。](ftp://ftp.ddbj.nig.ac.jp/ddbj_database/16S/)

这个数据集中的每一个序列都包含有问题的生物体的描述和记录的序列的性质。FASTA 格式广泛用于基因组学，是一种简单的文本格式。面向行格式的多个条目可以简单地连接在同一个文件或文本中。例如，一个序列条目可能如下所示:

```
FASTA

>AB000001_1|Sphingomonas sp.|16S ribosomal RNA

agctgctaatattagagccctatatatagagggggccctatactagagatatatctatca

gctaatattagagccctatatatagagggggccctatactagagatatatctatcaggct

attagagccctatatatagagggggccctatactagagatataagtcgacgatattagca

agccctatatatagagggggccctatactagagatatatctatcaggtgcacgatcgatc

cagctagctagc 
```

与该数据集一起发表的描述表明，包含的每个序列至少是 300 个碱基对，平均长度是 1，104 个碱基对。截至本文撰写时，包含了 998，911 个序列。请注意，在 DNA 或 RNA 中，每个核碱基都唯一地决定了双螺旋结构中的另一个碱基，因此格式不需要同时标注两个碱基。存在各种高质量的工具来处理基因组数据；这些细节超出了本书的范围。然而，作为一个例子，让我们使用 [SeqKit](Glossary.xhtml#_idTextAnchor123) 来识别重复的序列。在这个数据集中，没有具有相同名称或 ID 的序列对，但是相当多的序列包含相同的碱基对。这本身并不是错误，因为它反映了不同的观察结果。然而，它可能是对我们的分析没有用处的冗余数据。

```
%%bash

cd data/prokaryotes

zcat 16S.fasta.gz | 

  seqkit rmdup --by-seq --ignore-case \

               -o clean.fasta.gz \

               -d duplicated.fasta.gz \

               -D duplicated.detail.txt 
```

```
[INFO] 159688 duplicated records removed 
```

大约 15%的序列是重复的。一般来说，这些是属于同一有机体的多个 id。我们可以在对`seqkit`产生的复制报告的快速检查中看到这一点。作为一个练习，您可能会考虑如何用通用编程语言编写一个类似的重复检测函数；这并不特别困难，但是 SeqKit 肯定比您自己开发的快速实现更优化，测试也更好。

```
%%bash

cut -c-60 data/prokaryotes/duplicated.detail.txt | head 
```

```
1384  JN175331_1|Lactobacillus, MN464257_1|Lactobacillus, MN4

1383  MN438326_1|Lactobacillus, MN438327_1|Lactobacillus, MN4

1330  AB100791_1|Lactococcus, AB100792_1|Lactococcus, AB10079

1004  CP014153_1|Bordetella, CP014153_2|Bordetella, CP014153_

934   MN439952_1|Lactobacillus, MN439953_1|Lactobacillus, MN43

912   CP003166_2|Staphylococcus, CP003166_3|Staphylococcus, CP

908   CP010838_1|Bordetella, CP010838_2|Bordetella, CP010838_3

793   MN434189_1|Enterococcus, MN434190_1|Enterococcus, MN4341

683   CP007266_3|Salmonella, CP007266_5|Salmonella, CP007266_6

609   MN440886_1|Leuconostoc, MN440887_1|Leuconostoc, MN440888 
```

生物之间 rRNA 的水平转移是可能的，但是数据中的这种情况也可能代表被检测生物的错误分类。我们可以编写一些代码来确定这种同一序列的多个 id 的事件是否有时被标记为不同的细菌(或者古细菌)。

```
def matched_rna(dupfile):

    """Count of distinct organisms per sequence match

    Return a mapping from line number in the duplicates

    to Counters of occurrences of species names

    """

    counts = dict()

    for line in open(dupfile):

        line = line.rstrip()

        _, match_line = line.split('\t')

        matches = match_line.split(', ')

        first_id = matches[0].split('|')[0]

        names = [match.split('|')[1] for match in matches]

        count = Counter(names)

        counts[first_id] = count

    return counts 
```

事实证明，对具有明显相同 rRNA 序列的多种生物进行编目是相当常见的事情。但是我们的分析/验证可能揭示这些重复记录可能发生的情况。复制报告中的许多行只显示了一个物种的许多观察结果。很大一部分人表现出了其他的东西。让我们看几个例子。

```
dupfile = 'data/prokaryotes/duplicated.detail.txt'

counts = matched_rna(dupfile) 
```

在一些例子中，不同的观察具有不同的特异性水平，但本质上不是不同的生物体。

```
print(counts['CP004752_1'])

print(counts['AB729796_1']) 
```

```
Counter({'Mannheimia': 246, 'Pasteurellaceae': 1})

Counter({'Microbacterium': 62, 'Microbacteriaceae': 17}) 
```

曼海姆氏菌属是巴氏杆菌科的一个属，微杆菌属是微细菌科的一个属。然而，这些“差异”是否需要在清理中进行补救是非常具体的问题。例如，我们可能希望使用更一般的家族，以便将匹配序列分组在一起。另一方面，这个问题可能需要尽可能多的鉴定生物体的特异性。您必须决定如何在您的领域本体中处理或处理不同层次的特殊性。

在另一个记录中也出现了类似的问题，但似乎是一个额外的、简单的数据错误。

```
counts['AB851397_1'] 
```

```
Counter({'Proteobacteria': 1, 'proteobacterium': 2, 'Phyllobacteriaceae': 8}) 
```

叶杆菌科是变形菌门的一个大家族，所以无论哪种方式，我们都是在进行非特定的分类。但是“proteobacterium”似乎是林奈氏家族的一种非标准拼写方式，既有单数形式，也没有大写字母。

查看另一个记录，我们可能会将分类判断为一个观察误差，但如果没有更深入的领域知识，显然很难确定。

```
counts['CP020753_6'] 
```

```
Counter({'Shigella': 11, 'Escherichia': 153}) 
```

志贺氏菌和埃希氏菌都属于肠杆菌科。相同的序列在这里被表征为属于不同的属。这是否表明对潜在生物的错误识别或这些生物之间 rRNA 的水平转移，仅从该数据还不清楚。然而，在您的数据科学任务中，这是您需要做出的决定，可能需要咨询领域专家。

相对于这个数据集，我们可以查看的另一个记录非常奇怪。它显示了许多副本，但这并不是真正令人惊讶的方面。

```
counts['FJ537133_1'] 
```

```
Counter({'Aster': 1,

         "'Elaeis": 1,

         "'Tilia": 1,

         "'Prunus": 2,

         "'Brassica": 3,

         'Papaya': 1,

         "'Phalaris": 1,

         "'Eucalyptus": 1,

         "'Melochia": 1,

         'Chinaberry': 1,

         "'Catharanthus": 4,

         "'Sonchus": 1,

         "'Sesamum": 1,

         'Periwinkle': 1,

         'Candidatus': 1}) 
```

在这种情况下，我们有许多开花植物的属——即真核生物——与记录原核生物 rRNA 目录的数据集混合。还有一个拼写不一致的地方是，许多列出的属在其名称的开头有一个虚假的单引号字符。这些不同的植物，主要是树木，共享 rRNA 是否合理是一个领域知识的问题，但这些数据似乎根本不属于我们对原核生物 rRNA 的假设性分析。

对 rRNA 序列数据集中重复序列的检查指出了集合中的许多可能的问题。它还暗示了可能潜伏在系列其他地方的问题。例如，即使相同的序列没有被不同水平的分支系统发育所命名，这些不同的水平也可能与其他序列的分类相混淆。例如，这可能需要将数据标准化到一个共同的物种级别(这是一个非常大的项目，但可能是一项任务所必需的)。无论哪种方式，这种粗略的验证表明需要过滤数据集，以仅处理定义明确的生物属或科的集合。

## 转录验证

在本节中，我们讨论了上面的,记录的集合(即序列)可能在注释或聚合方面存在问题。也许记录彼此不一致，或者以某种方式提供了相互矛盾的信息。我们确定的例子指出了移除或补救的可能途径。在这一部分的第二部分中，我们想要查看各个记录中可能存在的可识别的错误。

这个假设仅仅是作为一个数据例子提出的，本质上并不是由 RNA 测序技术的深入知识所驱动的。这通常是与领域专家一起工作的数据科学家的观点。例如，我不知道在数据集中有多少测量使用了 RNA-Seq 和旧的基于杂交的微阵列。

但是为了这个目的，让我们假设测序技术中一个相对常见的错误导致 RNA 碱基对的短片段的不准确重复，这些短片段在实际测量的 rRNA 中不存在。另一方面，我们也确实知道微卫星和小卫星*确实也在 rRNA 中出现*(尽管端粒不存在)，所以仅仅重复序列的存在并不能证明发生了数据收集错误；这仅仅是提示性的。

这个例子的目的仅仅是提出这样一个想法，即像我们在下面*所做的那样定制的东西可能*与您特定领域的数据验证相关。我们要寻找的是相对较长的子序列在特定序列中重复出现的所有地方。这是一个错误还是一个有趣的现象，这是 T2 专家的事情。默认情况下，在下面的代码中，我们寻找 45 个碱基对的重复子序列，但是提供了一个配置选项来改变长度。如果每个核苷酸都是简单随机选择的，那么长度为 45 的每个特定模式出现的概率约为 10^(–27)，重复——即使考虑到“[生日悖论](https://en.wikipedia.org/wiki/Birthday_problem)”——基本上也不会发生。但是遗传过程并不像那样随机。

作为第一步，让我们创建一个短函数来迭代 FASTA 文件，为包含的每个序列及其元数据生成一个更具描述性的`namedtuple`。许多库会做类似的事情，也许比所示的代码更快更健壮，但是 FASTA 格式足够简单，这样的函数编写起来很简单。

```
Sequence = namedtuple("FASTA", "recno ID name locus bp")

def get_sequence(fname):

    fasta = gzip.open(fname)

    pat = re.compile(r'n+')  # One or more 'n's

    sequence = []

    recno = 0

    for line in fasta:

        line = line.decode('ASCII').strip()

        if line.startswith('>'):

            # Modify base pairs to contain single '-' 

            # rather than strings of 'n's 

            bp = "".join(sequence)

            bp = re.sub(pat, '-', bp)  # Replace pat with a dash

            if recno > 0:

                yield Sequence(recno, ID, name, locus, bp)

            ID, name, locus = line[1:].split('|')

            sequence = []

            recno += 1

        else:

            sequence.append(line) 
```

`get_sequence()`函数允许我们对包含在一个 gzipped 文件中的所有序列进行惰性迭代。考虑到总数据是 1.1 [GiB](Glossary.xhtml#_idTextAnchor049) ，不一次全部读取是一个优势。除了假设这样的文件被 gzipped 之外，它还假设文件头是以 DDBJ 的方式格式化的，而不是根据不同的约定或缺少文件头。正如我所说的，其他工具更加健壮。让我们试着只读取一条记录来看看这个函数是如何工作的:

```
fname = 'data/prokaryotes/16S.fasta.gz'

prokaryotes = get_sequence(fname)

rec = next(prokaryotes)

print(rec.recno, rec.ID, rec.name, rec.locus)

print(fill(rec.bp, width=60)) 
```

```
1 AB000106_1 Sphingomonas sp. 16S ribosomal RNA

ggaatctgcccttgggttcggaataacgtctggaaacggacgctaataccggatgatgac

gtaagtccaaagatttatcgcccagggatgagcccgcgtaggattagctagttggtgagg

taaaggctcaccaaggcgacgatccttagctggtctgagaggatgatcagccacactggg

actgagacacggcccagactcctacgggaggcagcagtagggaatattggacaatgggcg

aaagcctgatccagcaatgccgcgtgagtgatgaaggccttagggttgtaaagctctttt

acccgggatgataatgacagtaccgggagaataagccccggctaactccgtgccagcagc

cgcggtaatacggagggggctagcgttgttcggaattactgggcgtaaagcgcacgtagg

cggcgatttaagtcagaggtgaaagcccggggctcaaccccggaatagcctttgagactg

gattgcttgaatccgggagaggtgagtggaattccgagtgtagaggtgaaattcgtagat

attcggaagaacaccagtggcgaaggcggatcactggaccggcattgacgctgaggtgcg

aaagcgtggggagcaaacaggattagataccctggtagtccacgccgtaaacgatgataa

ctagctgctggggctcatggagtttcagtggcgcagctaacgcattaagttatccgcctg

gggagtacggtcgcaagattaaaactcaaaggaattgacgggggcctgcacaagcggtgg

agcatgtggtttaattcgaagcaacgcgcagaaccttaccaacgtttgacatccctagta

tggttaccagagatggtttccttcagttcggctggctaggtgacaggtgctgcatggctg

tcgtcagctcgtgtcgtgagatgttgggttaagtcccgcaacgagcgcaaccctcgcctt

tagttgccatcattcagttgggtactctaaaggaaccgccggtgataagccggaggaagg

tggggatgacgtcaagtcctcatggcccttacgcgttgggctacacacgtgctacaatgg

cgactacagtgggcagctatctcgcgagagtgcgctaatctccaaaagtcgtctcagttc

ggatcgttctctgcaactcgagagcgtgaaggcggaatcgctagtaatcgcggatcagca

tgccgcggtgaatacgtccccaggtcttgtacacaccgcccgtcacaccatgggagttgg

tttcacccgaaggcgctgcgctaactcgcaagagaggcaggcgaccacggtgggatcagc

gactgggtgagtcgtacaggtgc 
```

为了检查我们关心的每个序列/记录的子序列重复，另一个简短的函数可以帮助我们。这个 Python 代码再次使用了计数器，就像前面的`matched_rna()`函数一样。它只是查看给定长度的每个子序列，许多子序列因此重叠，并且只返回那些大于 1 的计数。

```
def find_dup_subseq(bp, minlen=45):

    count = Counter()

    for i in range(len(bp)-minlen):

        count[bp[i:i+minlen]] += 1

    return {seq: n for seq, n in count.items() if n > 1} 
```

综合起来，让我们只查看前 2800 条记录，看看是否有我们正在解决的潜在问题。鉴于整个数据集包含近 100 万个序列，这样的重复会更多。仅通过反复试验来选择初始范围，以准确找到两个示例。重复的子序列相对较少，但也不至于在一百万个序列中出现无数次。

```
for seq in islice(get_sequence(fname), 2800):

    dup = find_dup_subseq(seq.bp)

    if dup:

        print(seq.recno, seq.ID, seq.name)

        pprint(dup) 
```

```
2180 AB051695_1 Pseudomonas sp. LAB-16

{'gtcgagctagagtatggtagagggtggtggaatttcctgtgtagc': 2,

 'tcgagctagagtatggtagagggtggtggaatttcctgtgtagcg': 2}

2534 AB062283_1 Acinetobacter sp. ST-550

{'aaaggcctaccaaggcgacgatctgtagcgggtctgagaggatga': 2,

 'aaggcctaccaaggcgacgatctgtagcgggtctgagaggatgat': 2,

 'accaaggcgacgatctgtagcgggtctgagaggatgatccgccac': 2,

 'aggcctaccaaggcgacgatctgtagcgggtctgagaggatgatc': 2,

 'ccaaggcgacgatctgtagcgggtctgagaggatgatccgccaca': 2,

 'cctaccaaggcgacgatctgtagcgggtctgagaggatgatccgc': 2,

 'ctaccaaggcgacgatctgtagcgggtctgagaggatgatccgcc': 2,

 'gcctaccaaggcgacgatctgtagcgggtctgagaggatgatccg': 2,

 'ggcctaccaaggcgacgatctgtagcgggtctgagaggatgatcc': 2,

 'ggggtaaaggcctaccaaggcgacgatctgtagcgggtctgagag': 2,

 'gggtaaaggcctaccaaggcgacgatctgtagcgggtctgagagg': 2,

 'ggtaaaggcctaccaaggcgacgatctgtagcgggtctgagagga': 2,

 'ggtggggtaaaggcctaccaaggcgacgatctgtagcgggtctga': 2,

 'gtaaaggcctaccaaggcgacgatctgtagcgggtctgagaggat': 2,

 'gtggggtaaaggcctaccaaggcgacgatctgtagcgggtctgag': 2,

 'taaaggcctaccaaggcgacgatctgtagcgggtctgagaggatg': 2,

 'taccaaggcgacgatctgtagcgggtctgagaggatgatccgcca': 2,

 'tggggtaaaggcctaccaaggcgacgatctgtagcgggtctgaga': 2,

 'tggtggggtaaaggcctaccaaggcgacgatctgtagcgggtctg': 2,

 'ttggtggggtaaaggcctaccaaggcgacgatctgtagcgggtct': 2} 
```

和以前一样，这个验证只指向询问特定领域和特定问题的方向，并不决定正确的行动。子序列重复可能表明测序过程中的错误，但它们也可能揭示一些与潜在结构域和基因组机制相关的东西。然而，碰撞不太可能是偶然发生的。

# 练习

对于本章的练习，我们首先要求您使用您所学的技术执行典型的多步数据清理。在第二个练习中，您尝试使用本书提到的分析工具(或您选择的其他工具)来描述所提供数据集中的样本偏差。

## 数据表征

对于这个练习，您将需要执行一套相当完整的数据清理步骤。重点是本章讨论的技术，但也需要其他章节讨论的概念。这些任务中的一些需要在后面的章节中讨论的技能，所以如果需要的话，可以简单地跳过来完成这些任务。

在这里，我们回到“布拉德的房子”的温度数据，但在其原始形式。原始数据由四个文件组成，对应于存在的四个温度计。这些文件可以在以下位置找到:

[https://www.gnosis.cx/cleaning/outside.gz](https://www.gnosis.cx/cleaning/outside.gz)

[https://www.gnosis.cx/cleaning/basement.gz](https://www.gnosis.cx/cleaning/basement.gz)

[https://www.gnosis.cx/cleaning/livingroom.gz](https://www.gnosis.cx/cleaning/livingroom.gz)

[https://www.gnosis.cx/cleaning/lab.gz](https://www.gnosis.cx/cleaning/lab.gz)

这些数据文件的格式是简单但自定义的文本格式。您可能希望参考第 1 章[*、*表格格式*，以及第 3 章*](Chapter_1.xhtml#_idTextAnchor003) 、*重新利用数据源*，以获得解析格式的灵感。让我们看几行:

```
%%bash

zcat data/glarp/lab.gz | head -5 
```

```
2003 07 26 19 28 25.200000

2003 07 26 19 31 25.200000

2003 07 26 19 34 25.300000

2003 07 26 19 37 25.300000

2003 07 26 19 40 25.400000 
```

如您所见，空格分隔的字段表示日期时间的组成部分，后面是温度读数。所有文件的格式本身是一致的。但是每个文件中记录的具体时间戳并不一致。所有四个数据文件都在`2004-07-16T15:28:00`结束，其中三个在`2003-07-25T16:04:00`开始。每个文件中都缺少各种不同的时间戳。作为比较，我们可以回忆一下，我们使用执行一些清理的实用函数读取的完整数据帧有 171，346 行。相比之下，几个数据文件的行数为:

```
%%bash

for f in data/glarp/*.gz; do 

    echo -n "$f: "

    zcat $f | wc -l 

done 
```

```
data/glarp/basement.gz: 169516

data/glarp/lab.gz: 168965

data/glarp/livingroom.gz: 169516

data/glarp/outside.gz: 169513 
```

这个练习中的所有任务都与您决定使用的特定编程语言和库无关。总体目标是将 685，000 个数据点中的每一个描述为我们在下面提出的几个概念类别中的一个。

**任务 1** :将所有四个数据文件读入一个公共数据帧。此外，我们希望每个记录都由适当的本地时间戳来标识，而不是由单独的部分来标识。大家不妨向前参考一下 [*第七章*](Chapter_7.xhtml#_idTextAnchor009) 、*特征工程*，其中讨论了日期/时间字段。

**任务 2** :用标记符填充所有缺失的数据点，表明它们明显缺失。这将有两个稍微不同的方面。有一些隐含的时间戳不存在于任何数据文件中。我们的目标是在整个数据持续时间内有 3 分钟的增量。在第二个方面，一些时间戳在一些数据文件中被表示，但在其他文件中不被表示。大家不妨参考一下本章的*缺失数据*一节和 [*第四章*](Chapter_4.xhtml#_idTextAnchor006)*异常检测*中的同名；同样，第 7 章 中 [*日期/时间字段的讨论也可能是相关的。*](Chapter_7.xhtml#_idTextAnchor009)

**任务三**:从数据中去除所有有规律的趋势和周期。不同的仪器之间的相关技术可能有所不同。正如我们在本章的讨论中提到的，三个测量系列是室内温度的测量，至少部分是由恒温器调节的，一个是室外温度的测量。有问题的房子是否在房间之间的恒温器或加热系统上有差异，留给读者根据数据来尝试确定(尽管至少，任何房子的热量循环总是不完美和不均匀的)。

注:作为去除趋势的一个步骤，临时估算缺失数据可能是有用的，如第 6 章 、*值估算*中所述。

**任务 4** :根据以下类别描述每个数据点(时间戳和位置)的特征:

*   落入一般预期范围内的“常规”数据点。
*   一个“有趣”的数据点,可能表明与趋势的相关偏差。
*   “数据错误”反映了相对于预期的不太可能的值，更有可能是记录或转录错误。考虑到一个给定的值可能不太可能是基于其与附近值的差值，而不仅仅是因为绝对大小。 [*第四章*](Chapter_4.xhtml#_idTextAnchor006) 很可能与此有关。
*   缺失的数据点。

**任务 5** :描述你在特征数据点分布中发现的任何模式。是否有时间趋势或时间间隔来显示以某种方式表征的大多数或所有数据？这是否会因我们观察四种工具中的哪一种而有所不同？

## 过度抽样调查

民意调查公司经常在他们的数据收集中故意使用过采样(过度选择)。这是一个与本章某个主题中讨论的超重或机械过采样有些不同的问题，机械过采样将在 [*第 6 章*](Chapter_6.xhtml#_idTextAnchor008) 、*值插补*中讨论。相反，这里的思想是，已知特定的类或值范围在基础总体中是不常见的，因此对于该部分总体，整个参数空间可能被稀疏填充。或者，过采样类在总体中可能是常见的，但也代表一个亚总体，分析目的需要对其进行特别高的识别。

在数据收集中使用过采样本身并不局限于民意调查公司调查的人类受试者。有时候，对于完全不相关的主题领域来说，这同样是有意义的，例如，在回旋加速器中产生的不寻常的粒子或在研究的森林中的不寻常的植物。负责任的数据收集者，如皮尤研究中心(Pew Research Center)收集了本次研究中使用的数据，他们将始终明确记录他们的过采样方法和对基础人群分布的预期。事实上，您可以在以下网址阅读我们利用的 2010 年意见调查的所有详细信息:

[https://www . pewssocial trends . org/2010/02/24/millennials-confidential-connected-open-to-change/](https://www.pewsocialtrends.org/2010/02/24/millennials-confident-connected-open-to-change/)

但是，为了完成这个练习，我们建议您跳过最初查阅该文档的步骤。对于这里的工作，假设您在没有足够的附带文档和元数据的情况下收到了这些数据(需要明确的是:Pew 在这里非常细致)。在杂乱数据的现实世界中，这种情况太常见了。原始数据本身没有引入偏差或过采样的系统性改变，可从以下网址获得:

[https://www.gnosis.cx/cleaning/pew-survey.csv](https://www.gnosis.cx/cleaning/pew-survey.csv)

**任务 1** :读入数据，判断哪些年龄被故意过采样或欠采样，以及到什么程度。我们可能会在以后的合成采样或加权中利用这种加权，但对于现在的,只需将名为`sampling_multiplier`的新列添加到与您的信念相匹配的数据集的每个观察值中。

为此，将 1x 视为“中性”术语。因此，举例来说，如果你认为 40 岁的受试者被过度选择了 5 倍，那就把乘数定为 5.0。同样，如果你认为 50 岁的人被系统地低估了两倍，那么将乘数定为 0.5。请记住，2010 年美国的人类并不是按年龄均匀分布的。

此外，由于样本量约为 2000 人，有 75 个不同的可能年龄，我们预计亚组规模的不一致性完全来自随机性。仅仅是中性选择率的随机变化仍应编码为 1.0。

任务 2 :一些分类字段似乎编码了相关但不同的二进制值。例如，这个关于技术的问题可能不适合数据科学目标:

```
pew = pd.read_csv('data/pew-survey.csv')

list(pew.q23a.unique()) 
```

```
['New technology makes people closer to their friends and family',

 'New technology makes people more isolated',

 '(VOL) Both equally',

 "(VOL) Don't know/Refused",

 '(VOL) Neither equally'] 
```

由于给定的被调查者可能相互相信或者都不相信前两个描述，因此将每个描述编码为单独的布尔值是有意义的。如何处理拒绝回答是你在重新编码时要做的额外决定。确定哪些分类值最好编码为多个布尔值，并相应地修改数据集。解释并证明你对每个领域的决定。

**任务 3** :确定除了年龄之外，是否有任何其他人口统计字段被过采样。虽然这些列的名称很难理解，但您可以有把握地假设，带有定性答案的字段表示意见的程度，是被调查的因变量，而不是人口统计的自变量。例如:

```
list(pew.q1.unique()) 
```

```
['Very happy', 'Pretty happy', 'Not too happy', "(VOL) Don't know/Refused"] 
```

您可能需要参考外部数据源来对该任务做出判断。例如，您应该能够找到美国时区(2010 年)的粗略人口分布，以便与数据集分布进行比较。

```
list(pew.timezone.unique()) 
```

```
['Eastern', 'Central', 'Mountain', 'Pacific'] 
```

**任务 4** :有些字段，比如*任务 3* 中出现的`q1`，是清晰的普通编码。虽然不可能直接指定(`Very happy:Pretty happy`)与(`Pretty happy:Not too happy`)的相对比率，但这三个值的排序是显而易见的，将它们依次称为 1、2 和 3 是合理且有用的。当然，你还必须以某种方式对拒绝回答进行编码。重新编码所有相关字段，以利用您所拥有的直观领域知识。

# 结局

> 质量从来不是偶然。它总是智慧努力的结果。
> 
> 约翰·罗斯金

**本章涉及的主题**:缺失数据(重新访问)；偏见；阶层失衡；正常化；缩放；超重；周期性；定制验证。

在这一章中，我们集中讨论了数据中的偏倚问题。数据集很少，如果有的话，完全代表一个群体；相反，他们从人群中扭曲和选择，以形成某种特定的画面。有时，这种偏见是有意的，并且作为填充参数空间的一种方式是有充分根据的。其他时候，它只是反映了潜在现实中数量或类别的分布。在这种情况下，这既是我们数据的固有优点，也是我们分析中的一个陷阱。但在其他时候，数据收集、整理、转录或汇总的元素可能会引入更微妙的偏差，可能需要以某种方式对我们的数据分析和建模进行补救。检测偏差是解决问题的第一步。

数据的周期性与偏差有关，但也是一个类似的问题。通常，一个特定的数据序列——当数据以某种方式排序时，通常作为一个时间序列——有“信号”和“变化”的成分，可以有效地分离。在某种意义上，信号是一种偏差，因为它提供了一种预期，即在时间 *T* 处，测量值更有可能接近于 *M* 。识别信号通常是数据分析的一个重要方面——它们通常不是先验的——但识别信号的偏差也提供了一个有趣信息的额外渠道。

关于异常检测的前一章提供了关于识别数据的提示，这些数据在统计上一般不太可能出现在值的集合中。但是很多时候，我们想看更特定领域的问题。我们通常能够利用我们对干净数据中的模式的预期，这些模式可能会被我们实际拥有的数据所违反。这些模式可能只由自定义代码来表示，这些代码在算法上表达了这些期望，但是不能用一般的统计测试来表达。

在下一章，我们将转向重要而微妙的数据输入问题。*