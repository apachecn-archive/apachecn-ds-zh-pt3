<title>Chapter_2</title> <link href="../Styles/epub.css" rel="stylesheet" type="text/css"> <link href="../Styles/syntax-highlighting.css" rel="stylesheet" type="text/css">

# 2

# 分层格式

> 没有神，没有主人。
> 
> 路易·奥古斯特·布兰科尼

当我们利用机器学习模型时，实际上当我们执行一般统计分析时，我们几乎总是假设我们的数据是表格形式的。观察和特征；行和列。然而，有许多非常流行的存储数据的方式，类似于树而不是犁过的田。数据对象属于其他数据对象，而其他数据对象又属于其他数据对象，对分支的深度或名称没有具体限制。无论是为了理解的经济性，还是在数据库系统的情况下，为了访问的效率，分层数据格式通常对于一大类数据更有意义。

有许多域只是更自然地映射到层次结构，而不是表。是的，关系代数——支撑 SQL 和关系数据库的概念结构——在某种程度上能够表示每一种可能的结构。但是对于自然分层的数据来说感觉很尴尬。例如，文件系统有嵌套的路径，最终在它们的叶子上指向实际的文件。一路上的目录可能有无限多的子目录，每一层的名称都表达了一些有意义的东西，直到我们到达文件，文件本身可能有层次结构、表格或其他数据排列。

同样，如果我们制作一个连接网页的图表——或者实际上是任何一种网络的图表，无论是社交网络、电子通讯网络、生态互动网络还是其他网络——它更像是一个层次结构，而不是一张表格。是的，并不是所有或大多数的图都是有向无环图(Dag ),但更少的是行和列。

或者想象一下，你有一本描述许多生物有机体的“生命之书”，按照林奈分类法组织起来——领域、王国、门、纲、目、科、属、种(例如，有时可能还有亚种、亚科或族)。这种层次结构不仅是重要的数据，而且不同物种的叶子属性也大不相同。关于原核生物牙齿的信息是不相关的。牙齿只是脊索动物门生物的属性，而且大多只属于脊椎动物亚门。相应地，关于菌丝的属性只与真菌、卵菌门或放线菌有关(跨越领域、界和门，但仍然只在层次的一小部分内)。

不管是好是坏，当我们使用分层数据源进行数据科学时，这通常意味着我们构建了底层数据的表格抽象。根据我们的目的，叶属性和分支结构中的一个或两个可能是相关的。无论哪种情况，我们都希望将它们编码为变量的列和记录的行。这与数据科学之外的许多目的有些不同；对于其他目的，通常只是“深入”到相关的叶子或远处的分支，并呈现或修改该级别的少量信息。数据科学更多的是对许多不同的数据点进行归纳，涉及许多不同的对象。

***

在我们开始本章的章节之前，让我们运行我们的标准设置代码:

```
from src.setup import *

%load_ext rpy2.ipython

%%R

library(tidyverse) 
```

现在让我们深入研究 JavaScript 对象符号，这是本章的第一个层次格式。

# JSON

> 她是无菌青少年
> 
> 她痴迷于清洁
> 
> 她每天刷牙十次
> 
> 擦掉擦掉擦掉擦掉
> 
> 特别行政区的方式...
> 
> –聚苯乙烯

**概念**:

*   JSON 是一种语法，而不是语义
*   REST 查询和响应
*   命令行工具`jq`
*   安全的 JSON 阅读器
*   NaNs，Infinities 和溢出
*   聚合 JSON 记录
*   使用大型深度嵌套的 JSON
*   提取 JSON 数据的表格摘要
*   使用 JSON 模式验证结构

[JavaScript Object Notation(JSON)](Glossary.xhtml#_idTextAnchor064)是一种广泛使用的数据交换格式。顾名思义，它是一种源自 JavaScript 的格式，但它是严格的语言中立的。JSON 目前是由互联网工程任务组(IETF) RFC 8259 指定的。虽然它可以用于许多目的，但它作为计算机服务相互通信的一种方式尤其流行。因此，JSON 数据的很大一部分由瞬态消息组成，这些消息不一定存在于永久存储中，比如磁盘上的文件或数据库中的值。当然，有时这些消息会被记录下来或以某种方式存储起来，对于数据科学的目的来说是很有价值的。

许多编程语言都支持 JSON，包括标准库、内置库或这些语言广泛可用的库。在语法上，JSON 非常类似于 JavaScript 中的原生数据结构，但既不是其超集也不是子集，在很大程度上也类似于 Python 中的原生数据结构。关于 JSON，需要理解的一件重要事情是，它指定了语法，而不是语义。每种语言都必须决定如何处理符合 JSON 语法的文本。

JSON 中定义了四种值类型和三种文字值。空白在整个 JSON 中都被忽略。

*   `false`、`true`、`null`为文字值。
*   一个*对象*是一个语法结构，用大括号`{`和`}`括起来，用字符串表示键，用冒号与任何语法类型的值分开。多个键/值对用逗号分隔。
*   一个*数组*是一个语法结构，用方括号`[`和`]`括起来，所有的语法值用逗号分隔。
*   一个*数字*可选地以一个减号开始，随后是一个数字序列，可选地在小数部分之后跟随一个小数部分，可选地跟随一个指数。这与 Python、R、JavaScript、Julia、C 等语言中数字的拼写基本相同。，但限制性稍强。
*   一个*字符串*是一个用双引号括起来的语法结构(代码点 U+0022)，它可以包含几乎任何字符。例如，Unicode 码位可以表示为`\u0022`，一些特殊字符必须用反斜杠进行转义。

例如，下面的片段利用了所有的四种值类型。该示例包含一个带有字符串键的对象，该字符串键的值是一个数组，该数组包含一个文本值和两个数字:

```
{"key": [true, false, null, 15, 55.66]} 
```

## JSON 是什么样子的

JSON 经常被用来在计算机系统之间交互地交流信息。在我的本地机器上，我运行着一个小型的演示 web 服务。在 book repository 中，目录`node-server/`包含启动它的所有代码。它碰巧是用 JavaScript/Node 编写的，但也可以用任何编程语言编写。关于它的关键点是它提供了一个[表述性状态转移(RESTful)](Glossary.xhtml#_idTextAnchor112) 接口，在这个接口中，客户端可以发送 JSON 消息，并将接收其他 JSON 消息。以下输出中显示的简短文档是这种用途的典型代表:

```
# A response to an HTTP request

response = requests.get('http://localhost:3001/users') 

# Show status code, content-type, and JSON body

print(response.status_code, response.headers['Content-Type'])

response.text

200 application/json; charset=utf-8 
```

```
'{"1":{"name":"Guido van Rossum","password":"unladenswallow","details":{"profession":"ex-BDFL"}},"2":{"name":"Brendan Eich","password":"nontransitiveequality","details":{"profession":"Mozillan"}},"3":{"name":"Ken Thompson","password":"p/q2-q4!","details":{"profession":"Unix Creator"}}}' 
```

空白在 JSON 中并不重要，但是它绝对可以让人们更容易阅读。以为例，本书的`setup`模块中的一个小函数可以做到这一点:

```
pprint_json(response.text) 
```

```
{

  "1": {

    "name": "Guido van Rossum",

    "password": "unladenswallow",

    "details": {

      "profession": "ex-BDFL"

    }

  },

  "2": {

    "name": "Brendan Eich",

    "password": "nontransitiveequality",

    "details": {

      "profession": "Mozillan"

    }

  },

  "3": {

    "name": "Ken Thompson",

    "password": "p/q2-q4!",

    "details": {

      "profession": "Unix Creator"

    }

  }

} 
```

一个名为 [jq](Glossary.xhtml#_idTextAnchor063) 的命令行工具对于处理 JSON 数据非常有用，无论是流数据还是磁盘数据。经常使用 JSON 的数据科学家或开发人员应该考虑学习稍微晦涩但紧凑的查询语言`jq`提供的；然而，这超出了本书的范围。在撰写本文时，该工具的[主页](https://stedolan.github.io/jq/)包含一个非常好的简介:

> jq 就像是 JSON 数据的`sed`——你可以用它来分割、过滤、映射和转换结构化数据，就像`sed`、`awk`、`grep`和朋友们让你玩文本一样容易。

`jq`可以完成的一个非常简单的任务是美化打印(缩进、换行、着色和等等):

```
with open('data/3001.json', 'w') as fh:

    fh.write(response.text)

!jq . data/3001.json 
```

```
{

  "1": {

    "name": "Guido van Rossum",

    "password": "unladenswallow",

    "details": {

      "profession": "ex-BDFL"

    }

  },

  "2": {

    "name": "Brendan Eich",

    "password": "nontransitiveequality",

    "details": {

      "profession": "Mozillan"

    }

  },

  "3": {

    "name": "Ken Thompson",

    "password": "p/q2-q4!",

    "details": {

      "profession": "Unix Creator"

    }

  }

} 
```

尽管它与 Python 和 JavaScript(以及其他语言)中数据结构的原生拼写非常相似，但您必须使用读取/解析函数将 JSON 转换为原生数据。有时，JavaScript、Python 或其他语言中的函数`eval()`会成功地将字符串转换成本地数据。然而，这是一个*非常糟糕的主意*；一方面，它有时会失败(即使在 JavaScript 内部)。另一方面更重要:尝试这样做可能会执行 JSON(或伪 JSON)中包含的恶意代码。几乎所有的编程语言都有 JSON 阅读器/解析器作为其标准库的一部分，或者广泛可用。

例如，在 JavaScript 中，使用 [Node.js](Glossary.xhtml#_idTextAnchor085) 运行时，我们可以编写:

```
%%bash

js="

const fs = require('fs');

let raw = fs.readFileSync('data/3001.json');

let users = JSON.parse(raw);

console.log(users);

"

echo $js | node 
```

```
{ '1':

   { name: 'Guido van Rossum',

     password: 'unladenswallow',

     details: { profession: 'ex-BDFL' } },

  '2':

   { name: 'Brendan Eich',

     password: 'nontransitiveequality',

     details: { profession: 'Mozillan' } },

  '3':

   { name: 'Ken Thompson',

     password: 'p/q2-q4!',

     details: { profession: 'Unix Creator' } } } 
```

在 Python 中，等效项是:

```
with open('data/3001.json') as fh:

    # Could also call 'json.load(fh)' to read file

    raw = fh.read()

    users = json.loads(raw)

users 
```

```
{'1': {'name': 'Guido van Rossum',

  'password': 'unladenswallow',

  'details': {'profession': 'ex-BDFL'}},

 '2': {'name': 'Brendan Eich',

  'password': 'nontransitiveequality',

  'details': {'profession': 'Mozillan'}},

 '3': {'name': 'Ken Thompson',

  'password': 'p/q2-q4!',

  'details': {'profession': 'Unix Creator'}}} 
```

在 R 中，我们没有与标准数据结构直接等价的字典或 hashmap 结构。因此表示为一个*命名的列表*(通常是嵌套的)。为了便于说明，这里我们只显示该列表的第三个元素:

```
%%R

library(rjson)

result <- fromJSON(file = "data/3001.json")

result[3] 
```

```
$'3'

$'3'$name

[1] "Ken Thompson"

$'3'$password

[1] "p/q2-q4!"

$'3'$details

$'3'$details$profession

[1] "Unix Creator" 
```

其他编程语言会有不同的拼写，但是库或标准函数可以在原生数据和 JSON 之间转换。

## NaN 处理和数据类型

JSON 语法的半形式化描述在它的直接信息下面有一个隐藏的 T2 目的。读者可能会注意到《T4》遗漏了一些东西。特别是，有一个名为“number”的单一语法类型，但是整数、浮点、小数、复数、分数/有理数或所表示数字的位长之间没有区别。如何解释数值的决定完全由库或个人用户决定。

这可能不那么明显，但也有一些重要的浮点“数字”完全丢失。IEEE-754 浮点数包括特殊值**非数字** ( **南**)和无穷大/-无穷大。说得迂腐一点，二进制标准将许多不同的位模式表示为“NaN”，尽管+无穷大和-无穷大各有一个(负零是另一个奇数，但不太重要)。JSON 不能表示这些值，即使许多或大多数编程语言都有拼写这些值的方法；在编程语言中，NaN 通常只有一种拼写，如`NaN`，而不是所有位模式都有数百万种拼写。

在 Python 中，标准库和其他常见的 JSON 解析器做出了一个启发性的假设，即包含小数点或指数的数字表示浮点，不包含小数点或指数的数字表示整数。在一些极端情况下，这些假设可能会失效。像`1e309`这样完全符合 Python 的无限大小整数的数字被视为浮点数，并因此失败(然而，它们*可以用数百个尾随零拼写，没有小数点被解释为整数)。更常见的可能是，通过将 JSON 数字视为浮点数，它们的精度被限制在本机浮点类型。在 64 位中，这相当于 17 个十进制数字；在 32 位中，它只有 9 位数。读者通常会失去这种潜在的精确性。*

一个简单的例子显示了一些溢出或截断问题。在这里，Python 和 R 是相同的；其他语言的行为可能不同(但大多数是相似的):

```
# An interpreted float, an overflow, and a truncation

json_str = "[1e308, 1e309, 1.2345678901234567890]"

json.loads(json_str) 
```

```
[1e+308, inf, 1.2345678901234567] 
```

```
%%R -i json_str

options(digits = 22)

fromJSON(json_str) 
```

```
[1] 1.000000000000000010979e+308                          Inf

[3]  1.234567890123456690432e+00 
```

您可能很容易被原谅的一种倾向是，认为这个问题只不过是浮点舍入的本质所固有的。毕竟，值 10 ^(308) 也只是近似值，正如我们在 R 输出的长表示中看到的。然而，Python 至少提供了一个自然替代，它更接近地匹配`decimal`模块中的 JSON 数字语法。不幸的是，在标准库中生成类型为`Decimal`的值很麻烦(但却是可能的)。幸运的是，第三方模块`simplejson`让这变得简单，如下所示:

```
simplejson.loads(json_str, use_decimal=True) 
```

```
[Decimal('1E+308'), Decimal('1E+309'), Decimal('1.2345678901234567890')] 
```

其他语言，如 JavaScript 和 R，缺少标准的十进制或无限精度数据类型，在表示一些语法上有效的 JSON 数字时会失去精度。

这个故事中的一个小问题是，像 Python 这样的语言中的默认“JSON”库在默认情况下实际上并不*读写 JSON。它们读取 JSON 的超集，但这可能包括额外的文字`NaN`、`Infinity`和`-Infinity`。JSON5 提案包括这些扩展和其他一些扩展，但目前还不是官方标准。例如，Python 标准库不支持文字`nan`、`Nan inf`、`+Infinity`或其他看起来合理的拼写；至少在撰写本文时没有。其他语言和库到底支持什么文字由他们自己决定，并且可能会改变。让我们尝试一些特殊的值:*

```
specials = "[NaN, Infinity, -Infinity]"

vals = json.loads(specials)

vals 
```

```
[nan, inf, -inf] 
```

R 中的几个库以不同于 Python 库的方式表示这些特殊的 IEEE-754 值。我在这些例子中使用了 [rjson](Glossary.xhtml#_idTextAnchor114) ，但是`RJSONIO`和`jsonlite`使用了类似的约定。欠规范的 R 解决方案是将其特殊值拼写为带有特殊建议值的字符串，如下面的第三行输出所示:

```
%%R -i vals

vals = c(NaN, Inf, -Inf)

print(vals)

print("R version of 'enhanced JSON':")

rjson_str = toJSON(vals)  # function from rjson library

print(rjson_str) 
```

```
[1]  NaN  Inf -Inf

[1] "R version of 'enhanced JSON':"

[1] "[\"NaN\",\"Inf\",\"-Inf\"]" 
```

这种技术在往返时会失败，甚至在`rjson`本身内的也会失败，除非你编写自定义代码来解释字符串。我们只是将内容作为字符串而不是特殊的数值来读回:

```
%%R

print("Read back in 'enhanced JSON':")

fromJSON(rjson_str) 
```

```
[1] "Read back in 'enhanced JSON':"

[1] "NaN"  "Inf"  "-Inf" 
```

***

使用 JavaScript reader，我们可以看到严格的 JSON 兼容行为:

```
%%bash

js="JSON.parse('[NaN, Infinity, -Infinity]');"

echo $js | node | cat 
```

```
undefined:1

[NaN, Infinity, -Infinity]

 ^

SyntaxError: Unexpected token N in JSON at position 1

    at JSON.parse (<anonymous>)

    at [stdin]:1:6

    at Script.runInThisContext (vm.js:122:20)

    at Object.runInThisContext (vm.js:329:38)

    at Object.<anonymous> ([stdin]-wrapper:6:22)

    at Module._compile (internal/modules/cjs/loader.js:778:30)

    at evalScript (internal/bootstrap/node.js:590:27)

    at Socket.<anonymous> (internal/bootstrap/node.js:323:15)

    at Socket.emit (events.js:203:15)

    at endReadableNT (_stream_readable.js:1145:12) 
```

我们还可以使用一个稍微命名错误，并且繁琐的参数(`parse_constant`)在 Python 标准库中强制执行严格模式。这仅捕获以如下方式拼写的特殊浮点数的特定值:

```
json.loads("[NaN, Infinity, -Infinity]", parse_constant=lambda _: "INVALID") 
```

```
['INVALID', 'INVALID', 'INVALID'] 
```

换句话说，不只是这些特定拼写之外的任何假设文字会被传递给`parse_constant`函数:

```
try:

    json.loads("[nan, +Inf, Foobar]", parse_constant=lambda _: "INVALID")

except Exception as err:

    print_err(err) 
```

```
JSONDecodeError

Expecting value: line 1 column 2 (char 1) 
```

工具`jq`有一个奇怪的“半严格”行为。在几种拼法下，无穷大是可以被识别的，但是不把当作IEEE-754 实际值“无穷大”这些选择本身没有对错，但是不兼容是需要警惕的危险:

```
%%bash

echo "[NaN, inf, -Infinity]" | jq . 
```

```
[

  null,

  1.7976931348623157e+308,

  -1.7976931348623157e+308

] 
```

## JSON 行

在下一小节中，我们看看 JSON 文档的大小和结构。然而，正如我们在前一小节中看到的稍微有点奇怪的例子，JSON 经常被用来编码小的数据束。我们经常遇到“小束数据”的一个地方是在日志文件中，例如在本书的 [*第 7 章*](Chapter_7.xhtml#_idTextAnchor009) 、*特征工程*、 [*第 3 章*](Chapter_3.xhtml#_idTextAnchor005) 、*重新利用数据源*和其他地方讨论的。日志文件中的条目通常是相似的，通常每行一个；然而，通常需要不同的条目来保存不同的字段。在解析日志文件时，这往往需要大量的条件逻辑。

JSON 流是一种非常有用且被广泛使用的减轻这种负担的方法。由于空白在 JSON 中被忽略，所以每个文档都可以包含在一行中(换行符编码为`\n`)，任何结构和字段名都可以用 JSON 语法结构表示。这并没有删除所有的条件逻辑，因为特定条目的处理仍然经常依赖于其中的数据，但至少它消除了解析步骤本身的顾虑。

准确地说，称为**换行符分隔的 JSON** ( **ndjson** )或 **JSON Lines** 的语法是聚合(小)JSON 文档的几种方法之一。换行符是使用最广泛的样式，并且最容易使用命令行文本处理工具，这些工具通常面向行。但是，您可能偶尔会遇到其他几种风格:

*   **记录分隔符分隔的**:Unicode 字符信息分隔符二(U+001E)用作分隔符(RFC 7464)，也就是说，换行符可以出现在 JSON 文档条目内部。
*   **串接的 JSON** :没有使用分隔符，每个 JSON 条目都是一个对象或数组。这允许流解析器识别将终止顶层结构的匹配的`}`或`]`。每个 JSON 行流自动也是一个连接的 JSON 流。
*   **带长度前缀的 JSON** :每个条目由一个整数组成，该整数表示条目剩余部分的字节数，后跟一个 JSON 对象或数组(原则上，字符串也可以)。这比简单的连接有优势，因为阅读器不需要在每个字符上测试一个结构是否完整。

让我们考虑一个 JSON Lines 的例子，它基于当前版本的维基百科关于 [JSON 流](https://en.wikipedia.org/wiki/JSON_streaming)的文章。这些行比这些页边空白的宽度要大一些，所以一个小的 Bash 管道将为一个演示长度进行格式化。如图所示，每行显示一个前导整数(不是该行的一部分)，随后显示的没有前导数字的行是磁盘上同一行的一部分(许多文本编辑器使用类似的方法):

```
%%bash

cat -n data/jsonlines.log | fmt -w55 | tr -d " " 
```

```
1      {"ts":"2020-06-18T10:44:13",

"logged_in":{"username":"foo"},

"connection":{"addr":"1.2.3.4","port":5678}}

2      {"ts":"2020-06-18T10:44:15",

"registered":{"username":"bar","email":"bar@example.com"},

"connection":{"addr":"2.3.4.5","port":6789}}

3      {"ts":"2020-06-18T10:44:16",

"logged_out":{"username":"foo"},

"connection":{"addr":"1.2.3.4","port":5678}}

4      {"ts":"2020-06-18T10:47:22",

"registered":{"username":"baz","email":"baz@example.net"},

"connection":{"addr":"3.4.5.6","port":7890}} 
```

这三个 JSON 文档每行一个，包含的字段略有不同。所有共享字段`"ts"`和`"connection"`来标记它们何时发生，以及来自什么客户端地址。然而，不同种类的事件需要不同的附加字段。这可以允许命令行处理。

例如，使用通用的文本处理工具，我们可以列出(作为一个 JSON 文档)所有新注册用户的用户名和电子邮件:

```
%%bash

# Extract registrations

grep "registered" data/jsonlines.log |

    sed 's/^.*registered"://' |

    sed 's/}.*/}/' 
```

```
{"username":"bar","email":"bar@example.com"}

{"username":"baz","email":"baz@example.net"} 
```

你可能已经注意到上面的命令行可能出错了(因为我们没有选择最好的工具)。如果一个注册对象包含嵌套对象(即更多的右括号，`}`)，我们将不会匹配我们实际想要的`"registered"`事件。就此而言，如果某个`"username"`字段是字符串`"registered"`，我们也会出错。为了正确地做到这一点，我们需要实际解析 JSON。这里再次说明，从命令行来看，`jq`是一个有用的工具:

```
%%bash

jq '.registered | select(.username != null)' data/jsonlines.log 
```

```
{

  "username": "bar",

  "email": "bar@example.com"

}

{

  "username": "baz",

  "email": "baz@example.net"

} 
```

最有可能的是，在对数据集进行初步探索之后，这些命令行工具非常有用，我们希望用通用编程语言来执行这些类型的任务。名为`jsonlines`的第三方 Python 模块是存在的，但是简单地使用标准库就足够了，如下所示:

```
with open('data/jsonlines.log') as log:

    for line in log:

        record = json.loads(line)

        if 'registered' in record:

            user = record['registered']

            if 'username' in user:

                print(user) 
```

```
{'username': 'bar', 'email': 'bar@example.com'}

{'username': 'baz', 'email': 'baz@example.net'} 
```

当然，在一个更加充实的版本中，我们会做一些不仅仅是打印出注册人信息的事情。如果使用 JSON 流的另一种变体而不是 JSON 行，代码会稍微困难一些，但是手动编程仍然是合理的。

## 杰奥森

GeoJSON 是一种格式,用于对 IETF RFC 7946 中描述的各种地理数据结构进行编码。这本书不能解决地理信息系统特有的众多编程和数据问题。有各种专门的编程工具、书籍和其他学习材料可以用来探索这个领域。出于我们的目的，我们只需要理解 GeoJSON 文件是一个 JSON 文件，它通常包含大量数据，并且是适度嵌套的。与其他一些使用 JSON 的格式相比，GeoJSON 中可用的层次结构并不是无限深的，而是简单地由嵌套的几个层次上的各种可选键组成。

我们将在本小节中使用的特定数据是由 Eric Celeste 根据美国人口普查局发布的数据生成的，描述了美国的各个县。公共领域数据最初由人口普查局作为 shapefiles ( `.shp`)提供。这里讨论的 GeoJSON 和下一节讨论的 **Keyhole 标记语言** ( **KML** )都是原始数据的机械变换(数据应该是等价的)。对于这里的例子，我使用了最低分辨率的形状定义，尽管如此，这仍然是相当可观的数据。

请注意，我们从 2010 年人口普查中读取的 JSON 文件编码为 ISO-8859-1。在过去的那些日子里，我们年轻而天真，当时流行的 JSON 标准还没有规定编码为 UTF-8。参见 [*第三章*](Chapter_3.xhtml#_idTextAnchor005) 、*重用数据源*，讨论如何确定和使用不同的字符编码；事实上，我自己不得不利用这些技术来确定如何读取这些数据而不引发异常。让我们稍微探讨一下这些概念:

```
with open('data/gz_2010_us_050_00_20m.json', encoding='ISO-8859-1') as fh:

    counties = json.load(fh)

counties.keys() 
```

```
dict_keys(['type', 'features']) 
```

在顶层，JSON 对象有一个名为`"type"`的键和另一个名为`"features"`的键。前者只是一个描述性的字符串，后者是 2010 年美国 3221 个县的大部分数据所在的位置，我们可以从下面的输出中推断出来:

```
counties['type'], type(counties['features']), len(counties['features']) 
```

```
('FeatureCollection', list, 3221) 
```

让我们来看看其中的一个特征。我们可以看到它在关键字`"properties"`下有一些元数据。主要数据是特定县边界的地理位置，在关键字`"geometry"`下。较高分辨率的数据文件包含相同的元数据和数据结构；不同之处在于，形状是由具有更多边的多边形定义的，因此更准确地描述了所讨论的县的形状。我们所用的足够支持这些例子了。用 Python 的术语来说，实际的形状是列表的列表的列表:

```
counties['features'][999] 
```

```
{'type': 'Feature',

 'properties': {'GEO_ID': '0500000US19153',

  'STATE': '19',

  'COUNTY': '153',

  'NAME': 'Polk',

  'LSAD': 'County',

  'CENSUSAREA': 573.795},

 'geometry': {'type': 'Polygon',

  'coordinates': [[[-93.328614, 41.507824],

    [-93.328486, 41.49134],

    [-93.328407, 41.490921],

    [-93.41226, 41.505549],

    [-93.790612, 41.511916],

    [-93.814282, 41.600448],

    [-93.815527, 41.863419],

    [-93.698032, 41.86337],

    [-93.347933, 41.863104],

    [-93.348681, 41.600999],

    [-93.328614, 41.507824]]]}} 
```

每个叶列表只是一个经度/纬度位置，这些列表是一个多边形，但一个县可能有不连续的区域，需要多个多边形来定义。

正如我提到的，有太多的 GIS 和地理空间数据处理工具。其中包括一个更专业的 Python 模块，名为`geojson`；在更广阔的 Python GIS 空间中， **Cartopy** 是一个维护良好的包，具有许多功能，构建在 **PROJ** 、NumPy 和 **Shapely** 之上。在其他功能中，这些类型的 GIS 工具允许将经度/纬度坐标可视化到许多地图投影上，可选地呈现地理和政治特征，并基于哈弗线距离而不是不准确的笛卡尔距离进行计算。为了只关注 JSON 数据，向我的读者中的制图师道歉，让我们做一个简单的渲染来可视化美国各县。

下面的代码简单地创建了一个 [Matplotlib](Glossary.xhtml#_idTextAnchor073) 图形和轴，循环遍历 GeoJSON 数据中的每个要素，向下钻取到坐标，并将各县映射为面片。可视化帮助我们理解我们正在处理的数据的“形状”。Matplotlib API 的细节在这里并不重要。相关的方面是我们*下降*到从 JSON 读取的嵌套数据的方式。例如:

```
polk = counties['features'][999]['geometry']['coordinates'][0] 
```

这将加载描述爱荷华州波尔克县边界的列表列表:

```
fig, ax = plt.subplots(figsize=(8, 5))

patches, colors, ncolor = [], [], 8

for n, county in enumerate(counties['features']):

    # Only use first polygon if multiple discontiguous regions

    poly = np.array(county['geometry']['coordinates'][0])

    poly = poly.reshape(-1, 2)

    polygon = Polygon(poly)

    patches.append(polygon)

    colors.append(n % ncolor)

p = PatchCollection(patches, cmap=cm.get_cmap('Greys', ncolor))

p.set_array(np.array(colors))

ax.add_collection(p)

ax.set_ylim(24, 50)

ax.set_ylabel("Latitude")

ax.set_xlim(-126, -67)

ax.set_xlabel("Longitude")

ax.set_title("Counties of the United States"); 
```

![](img/B17126_02_01.png)

图 2.1:美国各县的地图

这当然不是美国最好的地图，但是等高线作为一个绘图可以帮助我们理解数据集。

## 整洁的地理

作为利用此数据的示例，除了可视化之外，我们希望创建一个包含以下各列的表格数据框:

*   州名
*   国家名称
*   面积(土地平方公里)
*   最北纬度
*   最南端的纬度
*   最西经
*   最东端经度

这一小节将比大部分演示更多的代码。在处理分层数据时，很难避免一些混乱。在不同的级别测试不同的数据属性几乎总是需要循环或递归、临时容器、数据的查找和记忆以及许多步骤，这些步骤通常可以在更高的级别上用整齐的数据框架的方法来处理。

首先，我们可以注意到美国人口普查局提供了以平方英里为单位的土地面积测量；我们可以用转换常数 2.59 来表示![](img/B17126_02_001.png)。一种不太直接的转换是从联邦信息处理标准(FIPS)代码中确定州名。在网上查看政府数据来源，我们可以找到一个用制表符分隔的描述，我们可以使用:

```
fips = pd.read_csv('data/FIPS.tsv', sep='\t')

fips 
```

```
 Name     Postal Code     FIPS

─────────────────────────────────────────────────────────

0                       Alabama              AL        1

1                        Alaska              AK        2

2                       Arizona              AZ        4

3                      Arkansas              AR        5

...                         ...             ...      ...

51                         Guam              GU       66

52     Northern Mariana Islands              MP       69

53                  Puerto Rico              PR       72

54               Virgin Islands              VI       78

55 rows × 3 columns 
```

我们希望将这个数据帧转换成一个序列，该序列以 FIPS 为关键字并映射到州名。一旦我们抓取了 JSON 层次结构的数据和级别，我们就可以进行机械转换了:

```
fips_map = fips.set_index('FIPS').Name

fips_map 
```

```
FIPS

1                      Alabama

2                       Alaska

4                      Arizona

5                     Arkansas

                ...           

66                        Guam

69    Northern Mariana Islands

72                 Puerto Rico

78              Virgin Islands

Name: Name, Length: 55, dtype: object 
```

幸运的是，对于手头的任务，我们知道我们需要下降到固定的深度来找到感兴趣的数据项。在其他情况下，我们可能希望使用递归方法，嵌套函数调用对应于不确定深度的嵌套键。我们可以简单地遍历各个县，就像我们创建可视化一样，并且作为第一步，将数据收集到普通列表中。

为了简化处理，让我们首先定义一个名为`extremes()`的函数，它将收集多边形并返回基本方向的极值:

```
def extremes(coords):

    lat, lon = [], []

    # Expect a list of lists of lists

    for region in coords:

        for point in region:

            lat.append(point[1])

            lon.append(point[0])

    # We are assuming western hemisphere here

    north = max(lat)

    south = min(lat)

    east = max(lon)

    west = min(lon)

    return north, south, east, west 
```

接下来，我们想要一个函数从 GeoJSON 字典中产生一个数据帧。

```
def county_summary(features):

    geo_id = []

    state, county_name, area = [], [], []

    north, south, east, west = [], [], [], []

    for county in features:

        props = county['properties']

        polys = county['geometry']['coordinates']

        geo_id.append(props['GEO_ID'])

        # District of Columbia not US state (default to None)

        state_name = fips_map.get(int(props['STATE']), None)

        state.append(state_name)

        county_name.append(props['NAME'])

        area.append(props['CENSUSAREA'] * 2.59)

        n, s, e, w = extremes(polys)

        north.append(n)

        south.append(s)

        east.append(e)

        west.append(w)

    df = pd.DataFrame({

            'geo_id': geo_id,

            'state': state,

            'county': county_name,

            'area': area,

            'northmost': north,

            'southmost': south,

            'eastmost': east,

            'westmost': west

        })

    return df.set_index('geo_id') 
```

尽管代码相当简单，但它包含了足够多的内容，我们希望在单元测试中提供健全性检查:

```
def test_counties(df):

    assert (df.northmost > df.southmost).all()

    assert (df.westmost < df.eastmost).all()

    assert (df.area > 0).all() 
```

我们可以使用刚刚编写的函数将 JSON 层次结构转换成一个整洁的数据框架，并检查我们的边界假设:

```
census_counties = county_summary(counties['features'])

# Sanity checks (if no assertion violated, we are happy)

test_counties(census_counties)

census_counties 
```

```
 `state        county        area  northmost`

`geo_id`

0500000US01001   Alabama       Autauga  1539.58924    32.7074

0500000US01009   Alabama        Blount  1669.96984    34.2593

0500000US01017   Alabama      Chambers  1545.01529    33.1081

0500000US01021   Alabama       Chilton  1794.49186    33.0719

           ...       ...           ...         ...        ...

0500000US51021  Virginia         Bland   926.50775    37.2935

0500000US51027  Virginia      Buchanan  1302.15617    37.5378

0500000US51037  Virginia     Charlotte  1230.95189    37.2488

0500000US51041  Virginia  Chesterfield  1096.33923    37.5626

 `southmost  eastmost  westmost`

`geo_id`

0500000US01001    32.3408  -86.4112  -86.9176

0500000US01009    33.7653  -86.3035  -86.9634

0500000US01017    32.7285  -85.1234  -85.5932

0500000US01021    32.6617   -86.375  -87.0192

           ...        ...       ...       ...

0500000US51021    36.9524  -80.8546  -81.4622

0500000US51027    37.0417  -81.7384  -82.3059

0500000US51037    36.6979  -78.4433  -78.9046

0500000US51041    37.2227  -77.2442  -77.8551

3221 rows × 7 columns 
```

在这一点上，什么样的分析或建模是由您的任务决定的。但是一般来说，获得整洁的数据类似于在层次结构中爬行，从不同的层次中提取相关的信息。

## JSON 模式

当我们在前面的小节中整理一些 GeoJSON 数据时，我们对嵌套在对象中的哪些键将在哪些级别上遇到做了一些假设。如果违反了这些假设，在处理函数中就会出现各种异常，或者出现其他错误。当然，可以通过条件分支、异常处理、使用 Python 的`dict.get()`等方法以及其他类似的技术来检查这些情况。然而，充斥着大量此类错误处理结构的代码可能会使其底层处理逻辑变得模糊不清。

实施关于 JSON 文档的假设的一种方法是在将文档传递给数据提取函数之前，使用 JSON 模式来验证文档。[Configure Unify Execute(CUE)](https://cuelang.org/)是一种很有前途的新的验证方法，但我不在本书中讨论它。JSON 模式本身是一个遵循特定规范的 JSON 文档。最简单的是，它需要为被验证的 JSON 指定一个类型。在其中，它可以指示对象中可能出现的键，哪些是必需的，数组的基数，以及许多其他元素，包括递归结构。“验证，然后处理”的方法通常是有用的；这里验证仅仅描述了 JSON 文档的*结构*。并不打算对其包含*良好数据*做出任何声明，例如在 [*第 4 章*](Chapter_4.xhtml#_idTextAnchor006) 、*异常检测*、 [*第 5 章*](Chapter_5.xhtml#_idTextAnchor007) 、*数据质量*中讨论。

下面的例子使用了 Python 第三方模块 **jsonschema** ，但是将它的 API 封装在一个略有不同的函数`not_valid()`中，这个函数是从本书的`setup.py`模块中导入的。如果一切正常，这个函数将返回`False`，但是如果遇到问题，将返回一个描述性的错误消息。例如，我们可以使用官方 GeoJSON 方案验证美国县数据:

```
response = requests.get('https://geojson.org/schema/GeoJSON.json')

geojson_schema = json.loads(response.text)

if msg := not_valid(counties, geojson_schema):

    print(msg)

else:

    print("Everything is Happy!") 
```

```
Everything is Happy! 
```

正如所希望的那样，美国人口普查局的数据是有效的。GeoJSON 模式非常大，因此我在下面提供了一个我自己开发的较小的模式作为示例。上面查询的小型“用户数据库”web 服务器发送的用户记录应该遵循某种格式，但是在开发过程中，这种格式可能只在开发人员之间的电子邮件线程和电话交谈中非正式地指定。在运行脚本来处理这些用户记录之前，识别将违反嵌入我们代码逻辑中的假设的用户或潜在用户文档是很有用的。

让我们看看可以从示例模式中获得什么信息:

```
user_schema = json.loads("""

{

  "$schema": "http://json-schema.org/draft-07/schema#",

  "$id": "http://kdm.training/user-schema.json",

  "title": "User",

  "description": "A User of Our Computer System",

  "type" : "object",

  "required": ["name", "password"],

  "properties" : {

     "name" : {"type" : "string"},

     "password": {

         "description": "Use special characters and mixed case",

         "type": "string"},

     "lucky_numbers": {

         "description": "Up to 6 favorite numbers 1-100",

         "type": "array",

         "items": {

           "type": "number",

           "minimum": 1,

           "maximum": 100

         },

         "uniqueItems": true,

         "minItems": 0,

         "maxItems": 6

    }

  }

}

""") 
```

这个简单的“用户”模式没有使用 JSON 模式中的所有功能，但是它是一个很好的表示。一些元数据包含在关键字`"$schema"`、`"$id"`、`"title"`和`"description"`中。所有这些在 JSON 模式规范中都是可选的，但是如果使用的话，它们的名称是标准的。唯一严格要求的键是`"type"`，它必须是四种 JSON 数据类型之一。在一个对象中，键可能是必需的或可选的；然而，JSON 模式没有机制来*排除*其他没有描述的键。模式仅仅说明了*如果*一个键存在，那么它必须具有某种类型的值。

键`"name"`和`"password"`是必需的，都是字符串。对`"password"`的可选描述表明，在许多计算机系统强制执行的意义上，人们希望密码是“好的”,但是 JSON Schema 本身并没有检查这种编程规则的机制。关键`"lucky_numbers"`描述的还挺多；它不仅必须有一个数组作为值，而且该数组必须由 1 到 100 之间的数字组成，并且不超过 6 个。让我们看一个通过验证的文档:

```
david = json.loads("""

{

  "name": "David Mertz",

  "password": "badpassword",

  "details": {

    "profession": "Data Scientist",

    "employer": "KDM"

  },

  "lucky_numbers": [12, 42, 55, 87]

}

""")

if msg := not_valid(david, user_schema):

    print(msg) 
```

模式中没有提到顶级键`"details"`，因此可以包含任何内容(当然，任何有效的 JSON)。该文档验证成功，因此我们可能希望对其进行下游处理。让我们考虑几个作为用户失败的文档:

```
barbara_feldon = json.loads("""

{

  "name": 99, 

  "password": "1ibydieZ!S@8"

}

""")

if msg := not_valid(barbara_feldon, user_schema):

    print(msg) 
```

```
99 is not of type 'string'

Failed validating 'type' in schema['properties']['name']:

    {'type': 'string'}

On instance['name']:

    99 
```

故障诊断将有望提供与补救相关的信息。下面的 JSON 以稍微不同的方式失败，对问题有更详细的描述:

```
intruder = json.loads("""

{

  "password": "P4cC!^*8chWz8", 

  "profession": "Hacker"

}

""")

if msg := not_valid(intruder, user_schema):

    print(msg) 
```

```
'name' is a required property

Failed validating 'required' in schema:

    {'$id': 'http://kdm.training/user-schema.json',

     '$schema': 'http://json-schema.org/draft-07/schema#',

     'description': 'A User of Our Computer System',

     'properties': {'lucky_numbers': {'description': 'Up to 6 favorite '

                                                     'numbers 1-100',

                                      'items': {'maximum': 100,

                                                'minimum': 1,

                                                'type': 'number'},

                                      'maxItems': 6,

                                      'minItems': 0,

                                      'type': 'array',

                                      'uniqueItems': True},

                    'name': {'type': 'string'},

                    'password': {'description': 'Use special characters '

                                                'and mixed case',

                                 'type': 'string'}},

     'required': ['name', 'password'],

     'title': 'User',

     'type': 'object'}

On instance:

    {'password': 'P4cC!^*8chWz8', 'profession': 'Hacker'} 
```

让我们再看几条失败消息:

```
the_count = json.loads("""

{

  "name": "Count von Count",

  "password": "fourbananas",

  "lucky_numbers": ["one", "two", "three"]

}

""")

if msg := not_valid(the_count, user_schema):

    print(msg, "\n--------------------") 
```

```
'one' is not of type 'number'

Failed validating 'type' in schema['properties']['lucky_numbers']['items']:

    {'maximum': 100, 'minimum': 1, 'type': 'number'}

On instance['lucky_numbers'][0]:

    'one' 

-------------------- 
```

我们在嵌套数组的数据类型上失败。它的基数也被检查:

```
george = json.loads("""

{

  "name": "Georg Cantor",

  "password": "omega_aleph",

  "lucky_numbers": [1, 2, 3, 4, 5, 6, 7, 8]

}

""")

if msg := not_valid(george, user_schema):

    print(msg) 
```

```
[1, 2, 3, 4, 5, 6, 7, 8] is too long

Failed validating 'maxItems' in schema['properties']['lucky_numbers']:

    {'description': 'Up to 6 favorite numbers 1-100',

     'items': {'maximum': 100, 'minimum': 1, 'type': 'number'},

     'maxItems': 6,

     'minItems': 0,

     'type': 'array',

     'uniqueItems': True}

On instance['lucky_numbers']:

    [1, 2, 3, 4, 5, 6, 7, 8] 
```

在最后一个例子中，我们看到可以在数组中验证唯一性。这提供了一种区分集合和序列的方法，即使 JSON 本身并不区分这些数据类型:

```
revolution_9 = json.loads("""

{

  "name": "Yoko Ono",

  "password": "grapefruit",

  "lucky_numbers": [9, 9, 9]

}

""")

if msg := not_valid(revolution_9, user_schema):

    print(msg) 
```

```
[9, 9, 9] has non-unique elements

Failed validating 'uniqueItems' in schema['properties']['lucky_numbers']:

    {'description': 'Up to 6 favorite numbers 1-100',

     'items': {'maximum': 100, 'minimum': 1, 'type': 'number'},

     'maxItems': 6,

     'minItems': 0,

     'type': 'array',

     'uniqueItems': True}

On instance['lucky_numbers']:

    [9, 9, 9] 
```

是时候转向另一个更可怕的等级话题了。

# 可扩展标记语言

> XML 就像暴力——如果它不能解决你的问题，你就没有充分利用它。
> 
> –匿名

**概念**:

*   定义可扩展标记语言
*   方言和图式
*   属性和元素
*   处理深而粗糙的巢

伴随这一部分的几乎是强制性的题词，并延伸了路德维希·冯·罗豪的现实政治概念，当然，这在本质上是可悲的，尽管可能意味着讽刺。我认为暴力永远是不可接受的，XML 也是如此。这两种情况在我们的世界里都太常见了。这个纠正性的解释只是部分地解决了这个问题:“XML 就像暴力:只在特定的情况下有用，在其他地方完全不可接受。”

[可扩展标记语言(XML)](Glossary.xhtml#_idTextAnchor146) 是一种复杂的格式，表面上看起来可能很简单。大量比本书篇幅更长的书籍仅仅讨论了一两种与 XML 相关的工具或技术。特别是，与其说 XML 是一种格式，不如说它是一种包含多种方言的元格式。从语法上讲，XML 是一种相对简单的格式，它用尖括号标记(小于和大于符号)定义元素，允许在标记中包含属性，并为特殊实体和指令提供了一些其他的语法形式。下面显示的用户记录提供了一个简单的例子。作为粗略的近似，XML 是 HTML 的概括；或者更准确地说，HTML 是 XML 的一种方言(然而，说得迂腐一点，HTML 的最新版本在某些技术细节上并不完全是 XML 方言)。

XML 方言通常由一个模式来定义，该模式精确地指定了哪些标签和属性是允许的，以及它们相互嵌套的方式。模式还可以定义特定字段的数据类型解释。数百种这样的方言被广泛使用；例如，所有现代的文字处理器和出版系统都使用 XML 方言来定义它们的文档(用压缩层包装底层的 XML)。许多其他非文档格式也使用 XML，包括科学数据格式。

几种不同的模式语言可以用来定义特定的 XML 方言。所有这些都超出了本书的范围。但是，作为一般过程，如果有模式可用，在进一步处理 XML 文档之前对其进行验证几乎总是一个好主意。这与上一节中关于使用 JSON 模式的讨论非常相似，但是将使用不同的工具和库。定义 XML 模式最常用的方法可能是**文档类型定义** ( **DTD** )。更现代的选择是 XML Schema 和 RELAX NG。注意，虽然 XML Schema 和 RELAX NG 允许数据类型的声明和验证，但是我不知道有什么广泛使用的工具或库在将 XML 转换为原生数据结构时使用这些类型声明。例如，验证可以向您保证给定的数据值“看起来像一个整数”，但是当您希望以这种方式使用它时，您仍然需要在代码中对它进行强制转换。

## 用户记录

举个小例子，我将把 JSON 小节中讨论的一个用户记录公式化为 XML 文档。这里我不创建或指定模式，但是原则上有一个定义有效文档的所有约束的模式是可能的。和 JSON 一样，空白(通常)并不重要，但是可以帮助提高可读性:

```
<?xml version="1.0" encoding="utf-8" ?>

<users>

  <user>

    <name>David Mertz</name>

    <password>badpassword</password>

    <details>

      <profession employer="KDM" duration="26" units="months">

      Data Scientist</profession>

      <telephone>+1 323 863 5571</telephone>

    </details>

    <lucky-numbers>

      <item>12</item>

      <item>42</item>

      <item>55</item>

      <item>87</item>

    </lucky-numbers>

  </user>

  <user> ... </user>

</users> 
```

在 XML 中，我们对于是将给定的数据放在元素体中还是放在属性中有些不确定。该示例显示了这两种情况。

对于这一节，我将使用 Python 标准 XML 库`ElementTree`。甚至在 Python 标准库中也存在其他 API，各种其他编程语言都有各种各样的库和 API 可用于处理 XML。`ElementTree`在感觉像 Python 和感觉像 XML 之间做出合理的妥协。然而，如果你想以一种更加 Pythonic 化的风格处理 XML 树，那么 **lxml** 库附带了一个名为`lxml.objectify`的 API。

反过来，`lxml.objectify` API 是基于我的同事 Uche Ogbuji 在 Amara Bindery 上更早的工作，以及我在`gnosis.xml.objectify`上更早的工作。这两个老项目目前都没有被维护，但是`xml.objectify`非常相似，使用起来非常直观。总的来说，`lxml`是一个快速的和经过良好测试的 XML 库，构建在 **libxml2** 和 **libxslt** 之上，它提供了`objectify`接口和一个增强的更快版本的`ElementTree`。

XML 文档中的两种数据表示方式是您需要记住的。作为数据科学家，对于我们来说，数据是存在于 XML 属性中还是作为元素(标记)的主体没有根本的区别。澄清一下，*标签*是尖括号内的实际单词(例如`<item>`)，而*元素*是出现在开始标签和相应结束标签(例如`<item>55</item>`)之间的所有内容。元素和属性对我们同样有用。然而，在大多数 API 中，对它们的访问是不同的。让我们在一个代码示例中展示这两者:

```
import xml.etree.ElementTree as ET

tree = ET.parse('data/users.xml')

# Let us first find the attributes and text of a profession

prof = tree.find('user').find('details').find('profession')

print("Body (title):", prof.text.strip())

print("Attributes:  ", prof.attrib) 
```

```
Body (title): Data Scientist

Attributes:   {'employer': 'KDM', 'duration': '26', 'units': 'months'} 
```

在属性中，我们有一个非常规则的原生 Python 字典，可以从中提取字段值。注意，所有的键和值都只是字符串。例如，如果我们希望将'【T0 ]'视为一个整数，我们可以在代码中对它进行强制转换。此外，我们经常希望循环遍历文档层次结构中同一级别的元素，以相似的方式对待它们。

正如我们在 JSON 中看到的，元素可能是不规则的，包含不同的子元素，即使它们共享相同的父标签:

```
items = tree.find('user').find('lucky-numbers').findall('item')

lucky_numbers = [int(item.text) for item in items]

lucky_numbers 
```

```
[12, 42, 55, 87] 
```

嵌套或递归遍历是遍历 XML 文档的一种常用方法，例如在层次结构的不同级别调用`.findall()`。XML 文档可能非常大，对于这些文档，`ElementTree`和其他库中提供了一种增量方法。在下一节中，作为一个更加具体的例子，我们将处理与 JSON 一节中相同的地理数据。

## 锁眼标记语言

KML 是一种 XML 格式，通常在功能上等同于 shapefiles 或 GeoJSON。与其他格式一样，更专业的 GIS 工具将比我们在本小节中展示的做得更多。我们需要变一点小魔术，在定义这个文档中的标签的 KML 命名空间中寻找标签。我们可以看到，在我们获得文件的真正“数据”之前，一些模式和名称空间信息是在文件的顶部定义的(`"Placemark"`元素):

```
<?xml version="1.0" encoding="utf-8" ?>

<kml >

<Document>

  <Folder>

    <name>gz_2010_us_050_00_20m</name>

    <Schema name="gz_2010_us_050_00_20m" id="gz_2010_us_050_00_20m">

      <SimpleField name="Name" type="string"></SimpleField>

      <SimpleField name="Description" type="string"></SimpleField>

      <SimpleField name="GEO_ID" type="string"></SimpleField>

      <SimpleField name="STATE" type="string"></SimpleField>

      <SimpleField name="COUNTY" type="string"></SimpleField>

      <SimpleField name="NAME" type="string"></SimpleField>

      <SimpleField name="LSAD" type="string"></SimpleField>

      <SimpleField name="CENSUSAREA" type="float"></SimpleField>

    </Schema>

    <Placemark>

      <name>Autauga</name>

      <Style>

        <LineStyle><color>ff0000ff</color></LineStyle>  

        <PolyStyle><fill>0</fill></PolyStyle>

      </Style>

      <ExtendedData>

        <SchemaData schemaUrl="#gz_2010_us_050_00_20m">

          <SimpleData name="Name">Autauga</SimpleData>

          <SimpleData name="GEO_ID">0500000US01001</SimpleData>

          <SimpleData name="STATE">01</SimpleData>

... more content, eventual closing tags ... 
```

一个 XML 文件可以包含许多名称空间，不同的标记就位于其中。所以`ElementTree`允许我们定义一个将短名称映射到名称空间 URL 的字典，以允许更方便的访问。我们向下钻取几个层次，其中只有一个父节点，以找到包含我们真正关心的`"Placemark"`元素的`"Folder"`。这些在 GeoJSON 中被称为`features"`:

```
ns = {'kml': "http://www.opengis.net/kml/2.2"}

document = ET.parse('data/gz_2010_us_050_00_20m.kml')

root = document.getroot()

kml_doc = root.find('kml:Document', ns)

folder = kml_doc.find('kml:Folder', ns)

# Make sure we have the same number of counties as with GeoJSON

placemarks = folder.findall('kml:Placemark', ns)

print("Count of placemarks:", len(placemarks))

# Show one Placemark element object

placemarks[0] 
```

```
Count of placemarks: 3221

<Element '{http://www.opengis.net/kml/2.2}Placemark' at 0x7fe220289680> 
```

提取多少有些晦涩的嵌套数据比理想情况下要多做一点工作。让我们看看我们希望从第一个 county 子节点中得到什么:

```
# The name of the county is comparatively straightforward

print("County name:", placemarks[0].find('kml:name', ns).text)

# Other county info is only distinguished by attribute

sdata = (placemarks[0].find('kml:ExtendedData', ns)

                      .find('kml:SchemaData', ns)

                      .findall('kml:SimpleData', ns))

# We are going to want GEO_ID, STATE and CENSUSAREA

for record in sdata:

    print(record.attrib, record.text) 
```

```
County name: Autauga

{'name': 'Name'} Autauga

{'name': 'GEO_ID'} 0500000US01001

{'name': 'STATE'} 01

{'name': 'COUNTY'} 001

{'name': 'LSAD'} County

{'name': 'CENSUSAREA'} 594.436000000000035 
```

县的实际名称被冗余编码在两个地方。我们下面的函数`kml_county_summary()`，应该检查数据完整性(即一致的值)。现在，我们需要深入到层次中稍有不同的部分来定位多边形:

```
coords = (placemarks[0].find('kml:Polygon', ns)

                       .find('kml:outerBoundaryIs', ns)

                       .find('kml:LinearRing', ns)

                       .find('kml:coordinates', ns))

pprint(coords.text) 
```

```
('-86.497916734108713,32.346347937379285,123.940341341309249 '

 '-86.719045580223096,32.404719907202413,124.507383406162262 '

 '-86.816062031841554,32.342711234558017,124.433184524998069 '

 '-86.891734835750142,32.50487314981855,125.151479452848434 '

 '-86.918751525796665,32.666059588245083,125.785741473548114 '

 '-86.714541775531544,32.66362459160964,125.451970156282187 '

 '-86.715371359148733,32.707584324141543,125.614226697944105 '

 '-86.414261392701192,32.709278995622782,125.144079957157373 '

 '-86.41231357529395,32.411845326016262,124.046804890967906 '

 '-86.497916734108713,32.346347937379285,123.940341341309249') 
```

如果我们查阅 KML 文档，我们可以确定在 KML 的一个`"LinearRing"`元素中，坐标(多边形)采用由空格分隔的`lon,lat[,alt]`结构的形式。对于我们寻找最北端、最南端、最东端和最西端的任务，正如我们在 GeoJSON 案例中所做的那样，我们对高度不感兴趣。然而，我们*确实*需要解析结构化的原始文本来获得实际的边界。我们将通过函数`kml_extremes()`来实现。由于大部分实际逻辑与上一节中的 GeoJSON 版本相同，`kml_extremes()`在调用之前的`extremes()`函数之前，只需稍微调整一下数据格式:

```
def kml_extremes(coordinates):

    "Pass in a KML coordinates ElementTree object"

    text_points = coordinates.text.split()

    points = [p.split(',') for p in text_points]

    points = [[float(p[0]), float(p[1])] for p in points]

    # We pass a list-of-list-of-lists here

    return extremes([points])

kml_extremes(coords) 
```

```
(32.70927899562278, 32.34271123455802, -86.41231357529395, -86.91875152579667) 
```

接下来，我们需要一个函数来从 KML 数据中产生一个数据帧。这与 GeoJSON 类似，但挖掘数据略有不同(通常更麻烦):

```
def kml_county_summary(placemarks, ns=ns):

    geo_id = []

    state, county_name, area = [], [], []

    north, south, east, west = [], [], [], []

    for placemark in placemarks:

        # Get county name here and below to assure consistency

        name = placemark.find('kml:name', ns).text

        # Other county info is distinguished by XML attribute

        sdata = (placemark.find('kml:ExtendedData', ns)

                          .find('kml:SchemaData', ns)

                          .findall('kml:SimpleData', ns))

        # We want Name, GEO_ID, STATE and CENSUSAREA

        for record in sdata:

            rectype = record.attrib['name']  # XML attrib

            if rectype == 'Name':  # String 'Name' (county)

                # If name is recorded differently, problem!

                assert record.text == name

                county_name.append(name)

            elif rectype == 'GEO_ID':

                geo_id.append(record.text)

            elif rectype == 'CENSUSAREA':

                # Convert to km^2 from mi^2

                area.append(float(record.text) * 2.59)

            elif rectype == 'STATE':

                # District of Columbia is not a US state

                state_name = fips_map.get(int(record.text), None)

                state.append(state_name)

        # We are going to "cheat" here a little bit.  

        # Sometimes a placemark has a top level <MultiGeometry>

        # with several Polygons; we will skip that calculation 

        try:

            coordinates = (placemark

                    .find('kml:Polygon', ns)

                    .find('kml:outerBoundaryIs', ns)

                    .find('kml:LinearRing', ns)

                    .find('kml:coordinates', ns))                

            n, s, e, w = kml_extremes(coordinates)

        except AttributeError:

            n, s, e, w = None, None, None, None

        north.append(n); south.append(s); 

        east.append(e); west.append(w)

    df = pd.DataFrame({

            'geo_id': geo_id, 'state': state, 

            'county': county_name, 'area': area,

            'northmost': north, 'southmost': south,

            'eastmost': east, 'westmost': west

        })

    return df.set_index('geo_id') 
```

我们可以将 KML 等级转换成一个整洁的数据框架。使用 XML 通常是很挑剔的；造成这种情况的主要原因通常不是物理格式本身，而是 XML 方言的创造者倾向于将元素嵌套得特别深，并利用非常复杂的模式。这个 KML 的例子在某种程度上就是如此。

```
kml_counties = kml_county_summary(placemarks)

kml_counties 
```

```
 `state        county        area  northmost`

`geo_id`

0500000US01001   Alabama       Autauga  1539.58924  32.709279

0500000US01009   Alabama        Blount  1669.96984  34.261131

0500000US01017   Alabama      Chambers  1545.01529  33.109960

0500000US01021   Alabama       Chilton  1794.49186  33.073731

...                  ...           ...         ...        ...

0500000US51021  Virginia         Bland   926.50775  37.295189

0500000US51027  Virginia      Buchanan  1302.15617  37.539502

0500000US51037  Virginia     Charlotte  1230.95189  37.250505

0500000US51041  Virginia  Chesterfield  1096.33923  37.564372

 `southmost    eastmost    westmost`

`geo_id`

0500000US01001  32.342711  -86.412314  -86.918752

0500000US01009  33.767154  -86.304677  -86.964531

0500000US01017  32.730429  -85.124537  -85.594308

0500000US01021  32.663625  -86.376119  -87.020318

           ...        ...         ...         ...

0500000US51021  36.954152  -80.855694  -81.463294

0500000US51027  37.043415  -81.739470  -82.306981

0500000US51037  36.699679  -78.444320  -78.905600

0500000US51041  37.224467  -77.245139  -77.856138

3221 rows × 7 columns 
```

现在让我们向构成配置格式的百花齐放迈进一大步。

# 配置文件

> 标准的奇妙之处在于有如此多的标准可供选择。
> 
> –格蕾丝·默里·霍普^(属性)

*属性*

这句话的出处并不确定，尽管人们普遍认为是海军上将霍普说的。它有时也被认为是安德鲁塔南鲍姆，帕特里夏塞博尔德，或肯·奥尔森。第一个人确实在他的*计算机网络* (1981)中使用了它，但也许不是作为一个原始的评论。

**概念**:

*   过多的略有不同的格式
*   名称空间可以模拟层级
*   INI 和 TOML
*   亚姆

小数据通常存在于配置文件中。可能其中最流行的，至少对于编程项目来说，是现在的[YAML 不是标记语言](Glossary.xhtml#_idTextAnchor148)；以前是另一种标记语言(YAML)。非正式的 INI 格式也很常见，尤其是在 Windows 世界中(但主要是在较老的软件中)。 **Tom 显而易见的极简语言** ( **TOML** )与 INI 非常相似，但是包含了一些增强和更严格的定义。有时 JSON 或 XML 也用于同样的目的，尽管两者都明显不太容易编辑。最大的困难来自于许多软件项目，这些项目由于各种原因(很少有好的原因)采用了它们自己的定制配置格式。

这些配置格式通常具有一定程度的层次结构。根据格式的不同，这个层次可能是固定的，也可能是无限深的。然而，大多数格式允许无限制的嵌套，因此抓取它们类似于我们在 JSON 和 XML 中看到的技术。

## INI 和平面自定义格式

无限制深度的例外似乎是 env ( `.env`)文件和 INI 文件，前者也是一种非正式约定，而不是标准。Env 文件(通常)实际上根本没有层次结构，只是以一种简单的方式将值分配给名称。有时这等同于在 shell 配置中定义环境变量，但是通常不需要引用包含空白的值，并且字符转义规则可以不同。INI 文件通常允许在用方括号(`[`和`]`)标记的部分和在单行上用名称和等号标记的赋值之间有一个层次。让我们看一个简单的 INI 例子，它在维基百科关于 [INI 文件](https://en.wikipedia.org/wiki/INI_file)的文章中给出:

```
; last modified 1 April 2001 by John Doe

[owner]

name=John Doe

organization=Acme Widgets Inc.

[database]

; use IP address in case network name resolution is not working

server=192.0.2.62     

port=143

file="payroll.dat" 
```

有时，INI 文件通过在概念上给它们的节名命名空间来模拟更深层次的结构。因此，这样的文件可能包含部分`[owner.database.systems]`和`[owner.vcs.developers]`，它们可以被手动解码为“所有者”的层次结构。Python 标准库为这种格式提供了一个名为`configparser`的解析器。这是标准库中较老的模块之一，它的 API 有点破旧:

```
import configparser

cfg = configparser.ConfigParser()

cfg.read('data/example.ini')

print("Sections:   ", cfg.sections())

print("Owner keys: ", [k for k in cfg['owner']])

print("Owner/name: ", cfg['owner']['name'])

print("Port #:     ", cfg['database'].getint('port')) 
```

```
Sections:    ['owner', 'database']

Owner keys:  ['name', 'organization']

Owner/name:  John Doe

Port #:      143 
```

数据类型也受到限制。特殊方法`.getboolean()`、`.getint()`和`.getfloat()`只是做了显式类型构造函数的等效工作。但是，与方法一起转换的布尔值不区分大小写，并且识别`yes` / `no`、`on` / `off`、`true` / `false`和`1` / `0`。

虽然这个 API 不是最自然的，但至少模块是存在的。当工具定义它们的自己的格式时，你可能需要下降到手工文本处理的层次，例如在 [*第 3 章*](Chapter_3.xhtml#_idTextAnchor005) 、*重新利用数据源*中讨论的，在*自定义文本格式*一节中。例如，在我的系统上，古老的基于文本的 web 浏览器 **w3m** 在`$HOME/.w3m/config`中有一个定制的配置格式，其中包含这样的行(以及大约 150 个其他行):

```
tabstop 8

display_charset UTF-8

cookie_avoid_wrong_number_of_dots

accept_encoding gzip, compress, bzip, bzip2, deflate

extbrowser7 wget -c

extbrowser8 url=%s && printf %s "$url" | xsel && printf %s "$url" | xsel -b & ssl_ca_path /etc/ssl/certs 
```

一般来说，*看起来*密钥是一些字母数字字符后跟一个空格。但是接下来可能什么都不是；它可能是一个字符串或一个数字，它可能是一个带有更多空格的逗号分隔列表，或者它甚至可能是一个涉及管道、进程等的 shell 命令。如果我们想要分析一百万个用户的配置文件，我们将需要使用许多手动试探法，或者找到每个键可以取什么值的明确文档(如果这样的文档存在的话)。

## 汤姆

TOML 形式化了许多被各种工具利用自己的 INI 格式使用的约定。各部分以相同的方式标记，但可能会嵌套不确定的层次结构。合理的数据类型范围由解析器正式指定。不是每个数据结构都可以用 TOML 直接表示，但是大多数最常见的数据结构都可以。许多编程语言都有支持 TOML 的库，尽管在撰写本文时，有些只是在 v0.5.0 级别上支持，而不是 v1.0.0-rc.1(但是差别非常小)。

下面是 TOML 文档中给出的一个示例:

```
# This is a TOML document.

title = "TOML Example"

[owner]

name = "Tom Preston-Werner"

dob = 1979-05-27T07:32:00-08:00 # First class dates

[database]

server = "192.168.1.1"

ports = [ 8001, 8001, 8002 ]

connection_max = 5000

enabled = true

[servers]

  # Indentation (tabs and/or spaces) is allowed but not required

  [servers.alpha]

  ip = "10.0.0.1"

  dc = "eqdc10"

  [servers.beta]

  ip = "10.0.0.2"

  dc = "eqdc10"

[clients]

data = [ ["gamma", "delta"], [1, 2] ]

# Line breaks are OK when inside arrays

hosts = [

  "alpha",

  "omega"

] 
```

拥有一个正式的解析器可以避免大量定制格式的手工逻辑。此外，这里的 API 非常现代，因为它只是将配置文件转换为本地数据结构，不需要特殊的方法来获取底层数据。拥有对 datetime 数据类型的本机支持非常方便(这是 JSON 所缺乏的)；支持字符串、数字(float/int)、列表和字典。每个 TOML 文档的顶层总是一个映射；然而，这可能用一种特定的编程语言来表示。让我们看一个例子:

```
import toml

toml.load(open('data/example.toml')) 
```

```
{'title': 'TOML Example',

 'owner': {'name': 'Tom Preston-Werner',

  'dob': datetime.datetime(1979, 5, 27, 7, 32, tzinfo=<toml.tz.TomlTz object at 0x7fe20bc4e490>)},

 'database': {'server': '192.168.1.1',

  'ports': [8001, 8001, 8002],

  'connection_max': 5000,

  'enabled': True},

 'servers': {'alpha': {'ip': '10.0.0.1', 'dc': 'eqdc10'},

  'beta': {'ip': '10.0.0.2', 'dc': 'eqdc10'}},

 'clients': {'data': [['gamma', 'delta'], [1, 2]],

  'hosts': ['alpha', 'omega']}} 
```

使用解析器的一个很大的好处是，它通常会(相对)有帮助地报告出了什么问题。我创建了同一个 TOML 文件的一个略有错误的版本，旨在模拟人类打字员可能经常犯的错误。错误消息本身可能并没有提供关于哪里出错的完全清晰的信息；但至少它告诉我们去哪里找它:

```
with open('data/example-bad.toml') as fh:

    try:

        cfg = toml.load(fh)

    except Exception as err:

        print_err(err) 
```

```
TomlDecodeError

invalid literal for int() with base 0: '2] []

hosts = [   "alpha"' (line 27 column 1 char 433) 
```

让我们打印 TOML 文件的一部分:

```
!cat -n data/example-bad.toml | tail -8 
```

```
 26  [clients]

    27  data = [ ["gamma", "delta"], [1, 2] []

    28

    29  # Line breaks are OK when inside arrays

    30  hosts = [

    31    "alpha",

    32    "omega"

    33  ] 
```

凭借人类的眼睛，我们可以很容易地发现问题。第 27 行有一些格式问题，尽管确切的意图并不完全清楚。一般需要人工补救才能重建初衷。

只是为了演示另一种编程语言，把 TOML 读成 R 非常类似。具体来说，这也为我们提供了一个(嵌套的)本机数据结构，只需一次调用:

```
%%R

library(RcppTOML)

parseTOML("data/example.toml") 
```

```
List of 5

 $ clients :List of 2

  ..$ data :List of 2

  .. ..$ : chr [1:2] "gamma" "delta"

  .. ..$ : int [1:2] 1 2

  ..$ hosts: chr [1:2] "alpha" "omega"

 $ database:List of 4

  ..$ connection_max: int 5000

  ..$ enabled       : logi TRUE

  ..$ ports         : int [1:3] 8001 8001 8002

  ..$ server        : chr "192.168.1.1"

 $ owner   :List of 2

  ..$ dob : POSIXct[1:1], format: "1979-05-27 15:32:00"

  ..$ name: chr "Tom Preston-Werner"

 $ servers :List of 2

  ..$ alpha:List of 2

  .. ..$ dc: chr "eqdc10"

  .. ..$ ip: chr "10.0.0.1"

  ..$ beta :List of 2

  .. ..$ dc: chr "eqdc10"

  .. ..$ ip: chr "10.0.0.2"

 $ title   : chr "TOML Example" 
```

## 又一种标记语言

YAML 占据了与 JSON 和 XML 相似的空间，但是非常强调人类的可读性和可编辑性。从某种程度上来说，后两者最初都是为了成为人类可读和可编辑的格式，但都没有在这个目标上取得成功；是的，它们是文本，但两者都很容易出现微妙的句法或语法错误。YAML 要近得多。

在他们的基本形式，YAML 文件是非常可读的，并提出了他们的结构直观的看法。使用标记和指令，事情会变得更加复杂，当您使用特定于语言的模式时，大部分通用的可读性会降低。然而，99%的 YAML 文档只利用了非常容易访问的子集，这些子集仍然简单而强大。让我们看一个改编自 YAML 教程的例子:

```
invoice: 34843

date   : 2001-01-23

bill-to: &id001

    given  : Chris

    family : Dumars

    address:

        lines: |

            458 Walkman Dr.

            Suite #292

        city    : Royal Oak

        state   : MI

        postal  : 48046

ship-to: *id001

product:

    - sku         : BL394D

      quantity    : 4

      description : Basketball

      price       : 450.00

    - sku         : BL4438H

      quantity    : 1

      description : Super Hoop

      price       : 2392.00

tax  : 251.42

total: 4443.52

comments:

    Late afternoon is best.

    Backup contact is Nancy

    Billsmer @ 338-4338. 
```

这份简单的文件中有一些微妙之处。基于语法模式可以识别大量不同的数据类型，就像我们可以在编程语言中拼写许多类型的常量一样，解析器可以区分这些常量。很少需要引用，但是引用是允许的(例如，如果一个字符串恰好只包含数字，并且您不希望它被视为数字)。

这个文档的整体结构是从几个名字到它们的值的映射。在某些情况下，这些值本身是序列或映射，在其他情况下，它们是标量。字符串可以是多行的，以竖线(`|`)开始表示应该保留换行符(但忽略其他缩进)。上例中的地址行显示了这一点。在键`comments`的情况下，字符串占据多行，但是不保留换行符。

一个强大的特性是锚和引用的可用性。这些都是从有引用和指针的 C-family 语言中隐约得到的启发。这个想法是文档的一个片段可以被命名为(一个锚)并在其他地方被引用。这避免了重复，但更重要的是，确保了内容的一致性。我们看到，一个有地址的人是相对于`bill-to`定义的，但在键`ship-to`下被引用。

让我们看看数据读入原生 Python 数据结构时的样子:

```
import yaml

order = yaml.load(open('data/example.yaml'))

order 
```

```
{'invoice': 34843,

 'date': datetime.date(2001, 1, 23),

 'bill-to': {'given': 'Chris',

  'family': 'Dumars',

  'address': {'lines': '458 Walkman Dr.\nSuite #292\n',

   'city': 'Royal Oak',

   'state': 'MI',

   'postal': 48046}},

 'ship-to': {'given': 'Chris',

  'family': 'Dumars',

  'address': {'lines': '458 Walkman Dr.\nSuite #292\n',

   'city': 'Royal Oak',

   'state': 'MI',

   'postal': 48046}},

 'product': [{'sku': 'BL394D',

   'quantity': 4,

   'description': 'Basketball',

   'price': 450.0},

  {'sku': 'BL4438H',

   'quantity': 1,

   'description': 'Super Hoop',

   'price': 2392.0}],

 'tax': 251.42,

 'total': 4443.52,

 'comments': 'Late afternoon is best. Backup contact is Nancy Billsmer @ 338-4338.'} 
```

和 TOML 一样，日期也是本地处理的。锚和引用被扩展成对同一嵌套字典的引用。一些数字被解析为浮点数，另一些被解析为整数，使用与大多数编程语言相同的拼写规则。注意，开头的破折号引入了序列/列表中的一项，而不是映射/字典中的一个键。回头看看 YAML 版本的发票就知道了。

我们可以验证被引用的对象只是引用，而不是完整的副本:

```
# Is nested dict same object under different keys?

order['ship-to'] is order['bill-to'] 
```

```
True 
```

记住，有几种不同的增强被用来支持 JSON 流，最常见的是 JSON 行。YAML 在最初的设计中就考虑到了这一点，并为同一个流中的多个文档内置了特定的元素，同时仍然允许每个组件文档使用任何使其可读性最强的空白(显然，受 YAML 语法的限制，但它是灵活的)。比如，这里有一个文件包含多个文档；不过，它同样可以是任何其他带有`.read()`方法的类似 Python 文件的对象(即包括无限流):

```
%YAML 1.1

---

# YAML can contain comments like this

name: David

age: 55

---

name: Mei

age: 50     # Including end-of-line

---

name: Juana  

age: 47

...

---

name: Adebayo

age: 58

... 
```

开始时的版本指令是可选的，但这是一个好习惯。一行中只有三个破折号表示文档的开始。开始一个新文档足以表明上一个文档已经结束。但是，也可以使用三个点来明确标记文档的结尾。我们可能会遍历这些多个文档，并以某种方式处理每个文档，如下面的代码所示。在数据科学环境中，我们通常期望每个文档包含相似的结构和“字段”，但这并不是 YAML 格式本身的约束:

```
with open('data/multidoc.yaml') as stream:

    docs = yaml.load_all(stream)

    print(docs, '\n')

    for doc in docs:

        print(doc) 
```

```
<generator object load_all at 0x7fe20bc2edd0> 

{'name': 'David', 'age': 55}

{'name': 'Mei', 'age': 50}

{'name': 'Juana', 'age': 47}

{'name': 'Adebayo', 'age': 58} 
```

正如我们与 TOML 讨论的那样，使用开发的工具处理正式指定的格式的最大优势之一——甚至，或者特别是，如果它是一种经常由人工编辑的格式——是解析器将有希望产生关于格式问题的有意义的消息，而不需要我们人工捕捉它们:

```
try:

    yaml.load(open('data/example-bad.yaml'))

except Exception as err:

    print_err(err) 
```

```
ScannerError

mapping values are not allowed here   in "data/example-bad.yaml", line

17, column 31 
```

有了错误消息，我们可以查看文档中指出问题的部分。识别第 17 行的问题并不太难。在这种情况下，错误是显而易见的:

```
%%bash

cat -n data/example-bad.yaml | sed '15,19p;d' 
```

```
 15      - sku         : BL394D

    16        quantity    : 4

    17        description : Basketball: ERROR

    18        price       : 450.00

    19      - sku         : BL4438H 
```

类似地，如果我们试图解析一个 YAML 流，它会成功，直到遇到坏文档。这必须是真的，因为在迭代器到达之前，流中语法不正确的文档甚至不会被读取。我们可以通过试着在阅读每份文件时将打印出来来证实这一点:

```
try:

    for doc in yaml.load_all(open('data/multidoc-bad.yaml')):

        print(doc)

except Exception as err:

    print_err(err) 
```

```
{'name': 'David', 'age': 55}

{'name': 'Mei', 'age': 50}

ScannerError

mapping values are not allowed here   in "data/multidoc-bad.yaml",

line 10, column 12 
```

我们已经了解了最重要的配置文件格式；让我们回到大数据。

# NoSQL 数据库

> 这不仅是不对的；这根本不算错！
> 
> –沃尔夫冈·泡利^(没有错)

*没有错*

用英语说:“That 不仅不对；它甚至没有错。”泡利的丰富多彩的短语通常被简单地称为“甚至没有错”在一般的理解中，他的意图被认为是“不可伪造的”

**概念**:

*   图表数据库
*   面向文档的数据库
*   参差不齐的文档中缺少字段
*   反规范化和数据完整性
*   键/值存储
*   非正式等级制度

许多数据库系统避免使用关系模型，通常是为了在特定的领域中获得更好的性能。同样，许多 RBDMSs 现在包含 JSON 和 XML 数据类型。总的来说，这些系统分为面向文档的数据库、图形数据库和键/值存储。特定的服务器软件可能结合了这些元素——或者实际上是关系数据库的元素——并且特定的性能特征、设计理念和一般限制在每个项目中都有所不同。

大多数“NoSQL”数据库系统都有一个名字所暗示的突出属性；即使用 SQL 以外的查询语言。然而，即使这样，他们中的一些人仍然至少实现了 SQL 的一个子集作为访问数据的方法。这些其他的查询语言有时是特定数据库系统所独有的，但是在某些情况下有些是 T2 标准化的。比如图查询语言 [Gremlin](Glossary.xhtml#_idTextAnchor053) 、 [SPARQL (SPARQL 协议和 RDF 查询语言)](Glossary.xhtml#_idTextAnchor128)、 [GQL](Glossary.xhtml#_idTextAnchor052) ( [图查询语言](Glossary.xhtml#_idTextAnchor052))；以前的*密码*分别由几个不同的数据库系统支持。在开源图形数据库中，最著名的可能是 [Neo4j](Glossary.xhtml#_idTextAnchor081) 和 [OrientDB](Glossary.xhtml#_idTextAnchor092) ，但是还有许多其他数据库，包括许多专有数据库。

除了在这里提到它们的存在之外，我不会在本书中讨论任何关于图数据库特有的数据清洁问题的细节。对图表执行的数据分析类型通常有些专门化，超出了我在这里讨论的范围。但是您可能会遇到这些格式的数据。我将更详细地讨论面向文档的数据库和键/值存储，这两者都是您更可能使用的(对于大多数读者；当然，个人需求和工作不同)。

从广义上讲，图数据库由节点和连接节点的边组成；节点和边通常都可以保存属性或特性，或者是每个对象的自由形式，或者是由模式定义的。例如，代表我的节点可能包含我的名字(“David”)、我的职业(“数据科学家”)和我当前的家乡(“缅因州”)。反过来，我有一个“社交图”，其中包括我与节点“Brad”的连接/边，标记为“Friend”(可能包含其他属性)我还有一个标记为“Publisher”的到节点“Packt”的连接一个完整的社交图可能由数百万个节点和边组成，每个节点和边都有不同的属性。

用户 Ahzf 为维基共享创建了一个公共领域的小插图:

![](img/B17126_02_02.png)

图 2.2:社交图示例。来源:https://commons . wikimedia . org/wiki/File:graph database _ property graph . png

## 面向文档的数据库

面向文档的数据库通常使用 XML、JSON 或[二进制 JSON (BSON)](Glossary.xhtml#_idTextAnchor019) 来存储和交流数据。从某种意义上来说，你可以把这些数据库简单地想象成这些格式中的一个巨大的文件，这些文件恰好有索引和优化查询的机制。在实际的实现中，这不是真的，但是作为一个概念模型，它不会偏离太远。在面向文档的数据库中，要理解的关键是它们的数据是分层组织的。这可以使一些访问模式非常有效，但是它也有和其他层次格式一样的缺陷。

流行的 open source 面向文档的数据库包括 [MongoDB](Glossary.xhtml#_idTextAnchor079) 、 [CouchDB](Glossary.xhtml#_idTextAnchor030) 、 [CrateDB](Glossary.xhtml#_idTextAnchor031) 、 [Elasticsearch](Glossary.xhtml#_idTextAnchor042) 和 [Solr](Glossary.xhtml#_idTextAnchor125) 。这个软件空间被很好的占用了，其他的一个大号的工具，无论是开源的还是专有的，都不在我的列表中。从广义上讲，特别是在数据清洁方面，这些不同的项目是相似的。

分层数据中的主要缺陷仅仅是它是粗糙的。特定嵌套级别的特定字段可能会丢失。让我们用一个受 MongoDB 博客帖子启发的例子来说明，这个帖子是关于带评论的餐馆的。对于这些例子，我们使用 MongoDB，它是基于 JSON 的。同样的概念也适用于任何面向文档的数据库。与本书中的其他示例一样，安全配置和登录凭证将是正常使用的一部分，但不在此讨论:

```
# Assume that MongoDB is running on local system

from pymongo import MongoClient

client = MongoClient('mongodb://localhost:27017') 
```

我们可以检查这台服务器上有哪些数据库。除了`"business"`之外，其他的本质上都是管理性的，只是默认存在于每个 MongoDB 安装中。

```
# What databases exist on the local server?

client.database_names() 
```

```
['admin', 'business', 'config', 'local'] 
```

数据库的顶层有两个分支:一个用于评论，另一个用于信息。

面向文档的数据库通常以服务器→数据库→集合→文档的层次结构组织。作为比较，关系数据库被组织为服务器→数据库→表→行。

让我们来看一看每一个的一些文件。一般`"info"`先上几家餐厅:

```
db_biz = client.business

print("Restaurants:", db_biz.info.count())

for biz in db_biz.info.find(limit=3):

    pprint(biz) 
```

```
Restaurants: 50

{'_id': ObjectId('5f30928db504836031a2c2a1'),

 'cuisine': 'Mexican',

 'name': 'Kitchen Tasty Inc.',

 'phone': '+1 524 555 9265'}

{'_id': ObjectId('5f30928db504836031a2c2a2'),

 'cuisine': 'Sandwich',

 'name': 'Sweet Salty Take-Out',

 'phone': '+1 408 555 6924'}

{'_id': ObjectId('5f30928db504836031a2c2a3'),

 'cuisine': 'Vegetarian',

 'name': 'City Kitchen Inc.',

 'phone': '+1 528 555 8923'} 
```

同样，下面是的前几篇评论。每个评论都与`"info"`分支中列出的一家餐厅相关:

```
print("Reviews:", db_biz.reviews.count())

for review in db_biz.reviews.find(limit=3):

    pprint(review) 
```

```
Reviews: 5000

{'_id': ObjectId('5f30928db504836031a2c2d3'),

 'name': 'Tasty Sweet Inc.',

 'price': 'cheap',

 'rating': 1}

{'_id': ObjectId('5f30928db504836031a2c2d4'),

 'name': 'Big Big Restaurant',

 'price': 'cheap',

 'rating': 6}

{'_id': ObjectId('5f30928db504836031a2c2d5'),

 'name': 'Goat Big Take-Out',

 'price': 'reasonable',

 'rating': 8} 
```

我们可以做一个更具体的调查。例如，也许我们对那些考虑价格`"cheap"`的`"City Kitchen Inc."`评论感兴趣。我们可以看到，评价同样价格的不同食客对餐馆的评价是不同的。当然，原则上，其他数据可能会附加到这些文档中。MongoDB 的查询语言本身表示为 JSON(或者来自 Python 接口的 Python 字典):

```
query = {'price': 'cheap', 'name': 'City Kitchen Inc.'}

for review in db_biz.reviews.find(query, limit=4):

    pprint(review) 
```

```
{'_id': ObjectId('5f30928db504836031a2c2ea'),

 'name': 'City Kitchen Inc.',

 'price': 'cheap',

 'rating': 3}

{'_id': ObjectId('5f30928db504836031a2c435'),

 'name': 'City Kitchen Inc.',

 'price': 'cheap',

 'rating': 7}

{'_id': ObjectId('5f30928db504836031a2c553'),

 'name': 'City Kitchen Inc.',

 'price': 'cheap',

 'rating': 3}

{'_id': ObjectId('5f30928db504836031a2c5d6'),

 'name': 'City Kitchen Inc.',

 'price': 'cheap',

 'rating': 1} 
```

### 缺失字段

在我们对`"business"`数据库的一般预览中，一切都完全正常。我们可能会编写一些代码，遍历某种类型的记录，可能会匹配某种过滤器，目的是对相应的数据字段执行聚合或建模。例如，也许我们想生成一个给`"City Kitchen Inc."`的评分直方图。这里的危险是有些评论可能没有*的*评级，我们在下面使用`try/except`块来处理:

```
ratings = []

query = {'name': 'City Kitchen Inc.'}

for review in db_biz.reviews.find(query):

    try:

        ratings.append(review['rating'])

    except KeyError:

        pass

n = len(ratings)

pd.Series(ratings).plot(kind="hist", title=f"{n} ratings"); 
```

![](img/B17126_02_03.png)

图 2.3:评级及其频率的直方图

如果我们向 MongoDB 询问实际的行数，我们可以看到缺少了什么。我们的循环确实跳过了一些数据:

```
db_biz.reviews.find({'name': 'City Kitchen Inc.'}).count() 
```

```
110 
```

MongoDB——或任何其他层次数据库(可能在 API 中有一些变化)——将允许您基于缺失字段匹配文档。在这个小例子中，每个文档中没有太多其他数据需要考虑，但是在现实世界中，相似的文档中可能有许多不同的字段。让我们列出没有相关评级的评论:

```
list(db_biz.reviews.find({'name': 'City Kitchen Inc.', 'rating': None})) 
```

```
[{'_id': ObjectId('5f30928db504836031a2c3fa'),

  'name': 'City Kitchen Inc.',

  'price': 'expensive'},

 {'_id': ObjectId('5f30928db504836031a2c6b6'),

  'name': 'City Kitchen Inc.',

  'price': 'reasonable'}] 
```

您是否需要担心这两个缺失评级的评论是由问题和领域驱动的。你可能想忽略它们。您可能希望执行一些技术，如第 5 章[](Chapter_5.xhtml#_idTextAnchor007)**数据质量*和 [*第 6 章*](Chapter_6.xhtml#_idTextAnchor008)*值插补*中讨论的技术。无论如何，你应该意识到你的数据是不完整的。*

 *### 反规格化及其不满足条件

出于与关系数据库中的类似的性能原因，在面向文档的数据库中，有时数据会被[反规范化](Glossary.xhtml#_idTextAnchor038)。在一个分支内查询会更快，而只查询一个文档也会更快。因此，面向文档的数据库的管理员通常会将信息复制到“更靠近”通常访问它的位置。

在查询面向文档的数据库时，我们可能会使用类似如下的代码:

```
def has_best_review(name, db=db_biz):

    "Return phone if restaurant has at least one 10 rating"

    query = {'name': name, 'rating': 10}

    review = None

    # Fast path has phone in local results

    for review in db.reviews.find(query):

        phone = review.get('phone')

        if phone:

            return f"Call {name} at {phone}! (FAST query)"

    # If there were no ratings of 10, we don't like it!

    if not review:

        return f"Do not bother with {name}!"

    # MUCH SLOWER path is second query

    info = db.info.find_one({'name': name})

    return f"Call {name} at {info['phone']}! (SLOW query)" 
```

也许当一篇综述被多次查阅时(例如，如果它有一个实际的描述字段)，数据库管理员可能会缓存实际综述文档中通常需要的电话号码。

让我们看看几个询问是如何进行的:

```
has_best_review('Salty Big Take-Out') 
```

```
'Call Salty Big Take-Out at +1 354 555 8317! (FAST query)' 
```

```
has_best_review('City Kitchen Inc.') 
```

```
'Call City Kitchen Inc. at +1 528 555 8923! (SLOW query)' 
```

```
has_best_review('Out of Business') 
```

```
'Do not bother with Out of Business!' 
```

从表面上看，这似乎是合理的性能优化。问题是*重复的*信息是*可能与*不一致的信息。这里我们将使用数据库本身来查找非缺失字段(示例函数也可以使用这个查询元素来改进):

```
query = {'name': 'Salty Big Take-Out', 

         'rating': 10, 'phone':{"$ne":None}}

db_biz.reviews.find_one(query) 
```

```
{'_id': ObjectId('5f30928db504836031a2c7c9'),

 'name': 'Salty Big Take-Out',

 'price': 'reasonable',

 'rating': 10,

 'phone': '+1 354 555 8317'} 
```

但是，让我们看看这家餐馆的`"info"`分店，而不是我们到目前为止关注的`"` `reviews"`分店:

```
db_biz.info.find_one({'name': 'Salty Big Take-Out'}) 
```

```
{'_id': ObjectId('5f30928db504836031a2c2aa'),

 'name': 'Salty Big Take-Out',

 'cuisine': 'Mexican',

 'phone': '+1 967 555 5487'} 
```

此时，我们面临着数据完整性问题。据推测，在*的某个时间点*，电话号码被复制到了审查文档中。在创建评论时，电话号码被从`"info"`分支复制到`"reviews"`分支中*似乎是合理的*(或者可能在第一千次访问它时？);这表明`"`和`info"`分支更为流行。然而，*也有可能*输入的电话号码将审核本身作为一个选项。

不幸的是，要确定数据完整性问题的原因，不仅要了解过去可能运行过的代码，还要了解可能发生过的人工或自动输入过程。

## 键/值存储

最简单的数据库系统是键/值存储。这些系统只不过是将一些键(通常是一个字符串)映射到一个值(有时是一个字符串，有时是复合类型)。这些系统通常被用作内存中的数据存储，以实现尽可能快的访问速度，这通常是一种缓存形式。然而，大多数通常在内存中运行的系统——包括分布式服务器——也有一些持久性机制，如虚拟内存或快照。键/值存储的其他变体主要是磁盘格式，但它们也可能主要驻留在高速缓存中，从而获得类似的速度。

[Redis(远程字典服务器)](Glossary.xhtml#_idTextAnchor109)和 [Memcached](Glossary.xhtml#_idTextAnchor075) 是流行的内存系统(附带持久化机制)。顾名思义，Memcached 最常用作缓存，因此很少用作“知识来源”也就是说，缓存经常位于客户端和服务器之间，只记录来自客户端的先前结果。如果再次出现相同的请求(可能限于某个“失效”期)，则可以跳过复杂的数据库查询、困难的计算或对服务器外部的附加资源的访问，而是返回缓存的结果。Redis 有时以这种方式使用，但也经常被用作服务器所需的某些数据元素的权威或唯一的知识来源。

为了用伪代码说明这种缓存，服务器代理可能包含类似如下的代码:

```
request = get_client_request()

key = hash(request)   # Collision resistant hash

# See if FAST PATH is available

if result := check_for_cache(key):

    send_to_client(result)

# SLOW PATH as fallback

else:

    result = expensive_operation(request)

    send_to_client(result)

    store_to_cache(key, result, expiration=duration) 
```

其他的 key/valuestore 都来源于早期的 Unix **数据库管理器** ( **DBM** )系统。其中包括[闪电内存映射数据库(LMDB)](Glossary.xhtml#_idTextAnchor069) 、 [GNU dbm (GDBM)](Glossary.xhtml#_idTextAnchor047) 和 [Berkeley DB](Glossary.xhtml#_idTextAnchor016) 。所有这些只是简单的将字节串映射到其他字节串，而没有处理更复杂的数据结构。相反，例如，Redis 允许值拥有丰富的数据类型集合，包括允许嵌套的集合。然而，实际上，字节足以容纳任何类型的数据；它只是代表某种序列化格式的那些字节的问题，比如 JSON 文本或 Python pickles(例如，Python `shelve`模块基本上只是 DBM 加上 pickles)。

作为磁盘上存储键/值对的主要单个文件，DBM 家族库占据了与 SQLite 单文件数据库相似的应用空间。这两种方法都是将相关数据封装成一种可以通用读取的格式，并且只依赖于要共享的单个文件。显然，关系存储和键/值存储的使用方式是不同的，但是相同的信息可以很容易地在任一种存储中表示，并且两者都提供了自己的查询和更新接口。

从某种意义上说，键/值存储非常简单，不会导致数据完整性问题。显然，无论使用什么格式，总是可以存储简单地*错误的*值。但是映射本身的结构中并没有什么需要特别关注的地方。或者看起来是这样。

在实践中会出现问题，因为用户实际上希望他们的数据中有层次结构。大多数键作为完全扁平的名称是没有用的。开发人员通常会在键/值存储中使用的键中发明特定的层次结构；这不一定或者通常不是开发人员的坏习惯，它通常反映了问题空间的真实需求。然而，这些层次结构可能特别脆弱。

例如，我创建了一个 DBM 文件，其中包含了与上面讨论的 MongoDB 格式的餐馆数据库相似的信息。分支的层次结构在这里用带分隔符的键的命名空间来表示。这种方法在键/值存储系统的创建者中很常见。让我们看看这个键/值存储中的一些键。我使用了一个随机种子，碰巧对一些感兴趣的键进行了采样:

```
biz = dbm.open('data/keyval.db')

seed(6)

# Keys are bytes; could convert to strings if desired

sample(list(biz.keys()), 10) 
```

```
[b'Big Sweet Take-Out::info::phone',

 b'Big Sweet Inc.::ratings',

 b'Goat Sweet Inc.::info::phone',

 b'Fish City Restaurant//ratings',

 b'Delight Goat Inc.::ratings',

 b'DESCRIPTION',

 b'Salty Delight Take-Out::ratings',

 b'Sweet Tasty Restaurant::info::phone',

 b'Delight Salty Restaurant::info::phone',

 b'Tasty Fish Inc.::info::cuisine'] 
```

我们可以查询各种非正式的层次键。

```
name = b"Tasty Fish Inc."

print("Overview:", biz[b"DESCRIPTION"])

print("Cuisine: ", biz[name + b"::info::cuisine"] )

print("Ratings: ", biz[name + b"::ratings"][:30], "...") 
```

```
Overview: b'Restaurant information'

Cuisine:  b'Mexican'

Ratings:  b'2;1;1;10;5;7;1;4;8;10;7;7;6;8;' ... 
```

通常，我通过使用分隔符在`"ratings"`值中创建了一个非正式序列。数据的消费者只需要知道特定的值是这样格式化的。我们甚至可以使用少量的代码从特定的层次结构中提取相关的键:

```
for key, val in biz.items():

    if key.startswith(b'Tasty Fish Inc.::'):

        print(key.decode(), '\t', val[:30].decode()) 
```

```
Tasty Fish Inc.::ratings         2;1;1;10;5;7;1;4;8;10;7;7;6;8;

Tasty Fish Inc.::info::phone     +1 935 555 8029

Tasty Fish Inc.::info::cuisine   Mexican 
```

这里出现的主要问题是在使用数据库的过程中，使用了不一致的层次键约定。这是一个普遍关注的问题，在真实数据中经常出现；这在像 Redis 这样的多用户、多消费者系统中可能尤为突出，随着时间的推移，这些系统可能会与许多人用多种语言编写的工具进行通信。数据完整性故障往往会渗透进来。例如:

```
for key, val in biz.items():

    if key.startswith(b'Fish City Restaurant'):

        print(key, val[:30]) 
```

```
b'Fish City Restaurant::ratings' b'6;10;4;3;10;5;1;4;7;8;5;2;1;5;'

b'Fish City Restaurant//ratings' b'9'

b'Fish City Restaurant::info::phone' b'+1 851 555 1082'

b'Fish City Restaurant::info::cuisine' b'American' 
```

虽然不同分隔的层次键的意图很容易被人类读者识别，但是检测这种不一致可能很费力，并且由于补救措施不充分，您可能会丢失信息。对于这些类型的键/值存储，如果您需要利用它们的数据源，最好首先分析键本身的结构。他们不会总是利用特定的层次结构，但这样做是经常的。即使有数百万个键，而不是我的示例中的数百个，最初的方法至少可以确保使用一致的分隔符(或其他键格式)存在一致的路径组件。

我们已经讨论了大量不同的层次结构格式，尽管如此，我们仍然忽略了对其他格式的讨论。对图形数据库的深入研究需要放在另一本书中。此外，已经有很多卷关于 XML 的各种 API 和方言的文章，这一章只是简单介绍一下。然而，我希望这已经让您对这一系列数据源所产生的各种问题有所了解。

# 练习

这里的第一个练习是改进几种格式的地理数据的处理。第二个练习涉及在键/值和关系模型之间移动数据表示。

## 探索填充区域

使用美国各州的县数据，我们创建了整洁的数据框架，其中包含了作为简单基本方向限制的县的范围；我们还获得了每个县的“人口普查区”。不幸的是，此处可用的数据并没有具体说明水体及其大小，这可能与一些县有关。

人口普查数据可在以下网址找到:

[https://www.gnosis.cx/cleaning/gz_2010_us_050_00_20m.json](https://www.gnosis.cx/cleaning/gz_2010_us_050_00_20m.json)

[https://www.gnosis.cx/cleaning/gz_2010_us_050_00_20m.kml](https://www.gnosis.cx/cleaning/gz_2010_us_050_00_20m.kml)

[https://www.gnosis.cx/cleaning/gz_2010_us_050_00_20m.zip](https://www.gnosis.cx/cleaning/gz_2010_us_050_00_20m.zip)

在本练习中，您将在文本所示的数据框中创建一个额外的列，以保存人口普查区域所占的县“边界框”的百分比。当然，诀窍在于由纬度/经度角包围的表面区域不是简单的矩形，甚至也不是梯形，而是球面的一部分。县形状本身通常不是矩形的，可能包含不连续的区域。

要完成此练习，您可以对该区域进行数学推理(地球是球体的简化假设是可以接受的)，或者确定合适的 GIS 软件来为您进行此计算。您的工作结果将是一个类似于本章中所示的数据框，但是有一个名为`"occupied"`的列，其中包含 3221 个介于 0 和 1 之间的浮点值。

对于额外的学分，您可以调查或改善一些额外的数据完整性问题。ZIP 存档中的 shapefile 是美国人口普查局提供的规范数据。我们在本章中看到的处理 GeoJSON 和 KML 的代码实际上对纬度/经度位置产生了稍微不同的结果，在第三个小数位。据推测，我下载这些转换的独立开发人员允许一些数据错误以某种方式潜入。诊断哪个版本与原始`.shp`文件匹配，并尝试描述差异的原因和程度。

对于额外的加分，修复本章中的`kml_county_summary()`函数，使其正确处理`<MultiGeometry>`县形状，而不是跳过它们。这个问题在美国 3221 个县中出现的频率有多高？

## 创建关系模型

DBM 餐馆数据中的键/值数据的组织方式可以在 Redis 或类似系统中提供非常快速的访问。但是这肯定与隐式数据模型不匹配。键在它们的层次结构中是有结构的，但这是一个有限的、浅层的层次结构。值可以是几种不同的隐式数据类型；特别是，评级被存储为字符串，但它们实际上表示小整数值的序列。其他字段是简单的字符串(尽管在 DBM 中是以字节存储的)。

所示示例中的`dbm`模块使用 Python 的后备“哑 DBM”格式，它不依赖于像 GDBM 或 LDBM 这样的外部驱动程序。对于有数百条记录的例子，这是相当快的；如果您希望使用数百万条记录，那么其他系统也可以很好地扩展，并且是首选。这种“哑”格式实际上由三个独立的文件组成，但是共享`keyval.db`前缀；这三个文件以 ZIP 存档的形式提供:

```
dbm.whichdb('data/keyval.db') 
```

```
'dbm.dumb' 
```

`dbm.dumb`格式不一定能移植到其他编程语言。然而，它非常简单，您可以相当容易地编写一个适配器。为了以更通用的格式提供相同的数据，还提供了相同内容的 CSV:

[https://www.gnosis.cx/cleaning/keyval.zip](https://www.gnosis.cx/cleaning/keyval.zip)

[https://www.gnosis.cx/cleaning/keyval.csv](https://www.gnosis.cx/cleaning/keyval.csv)

对于这个任务，您应该将本例中的键/值数据转换成关系表，在适当的地方使用外键，并对数据类型做出正确的决定。SQLite 是数据库系统的绝佳选择；在第一章 、*表格格式*中讨论。如果您有管理权限(即表创建权限)，任何其他 RDBMS 也是一个不错的选择。在转换数据模型之前，您需要清除本章中讨论的层次键中的不一致。

餐馆的名字被承诺是独特的；但是，对于外键关系，您可能希望使用统一表示餐馆的短索引号来进行规范化。单独的评级应该明确地作为不同的数据项存储在相关的表中。为了感受更充实的数据，为评论发明时间戳，这样每个评论都是不同的。真实世界的数据集通常包含审查日期；例如，不需要具体的日期，只需要日期的形式。

虽然这个数据足够小，性能不会成为问题，但是请考虑一下，在这个数据的一个假设版本中，什么样的索引可能是有用的，这个数据是它的几千倍或几百万倍。假设您正在运行一个流行的餐馆评论服务，您希望您的用户能够快速访问他们的常见查询。

使用数据模型的关系版本，回答一些简单的查询，很可能使用 SQL:

*   哪家餐厅收到的评论最多？
*   在给定时间段内，哪些餐厅收到了 10 条评论(相关范围取决于您选择填充的日期)？
*   什么风格的菜肴得到了最高的评价？

为了额外加分，你可以回到编写代码，只用键/值数据模型来回答同样的问题。

# 结局

> 简单比复杂好。
> 
> 复杂总比复杂好。
> 
> 扁平的比嵌套的好。
> 
> 疏比密好。
> 
> 可读性很重要。
> 
> –蒂姆·彼得斯(Python 之禅)

**本章涉及的主题**:JSON；JSON 行；JSON 模式；XMLYAML 和配置文件；面向文件的数据库；键/值存储。

分层数据通常比平面数据更好地表示具有属性和相互关系的实体。在面向对象编程中，但也仅仅是在普通分类法和[本体论](Glossary.xhtml#_idTextAnchor090)中，关系*是-a* 和*有-a* 通常是基本的，而且这两者都不是表格。或者说充其量就算是*捕捉到的属性也有-a* 关系，参差不齐，而且肯定不整齐。此外， *is-a* 关系的核心是层次化的。

分层数据和数据科学之间经常(甚至通常)存在阻抗不匹配。这些问题大部分归结为*访问模式*。对于许多软件应用程序来说，我们感兴趣的是携带异构数据束的特定实体，每个数据束都与该实体的实例有关。在利用这样的应用程序时，我们在给定的时间只关心一件*事情*(或者少数事情)。在这种情况下，分层数据结构通常更有效，并且在概念上更接近数据所代表的基本思想。

当我们从事数据科学时，无论是一般统计、数据可视化还是机器学习模型，我们都关注与我们的目的或目标相似的记录或样本的集合。是的，可能会有丢失数据的问题，例如那些将在第 4 章*异常检测*和 [*第 5 章*](Chapter_5.xhtml#_idTextAnchor007)*数据质量*中解决的问题，但这些问题并不主导我们的分析。数据科学是关于总结和聚合的。它几乎从来不是关于个体实体本身。

因此，当提供分层数据时，作为数据科学家，我们需要清楚地说明一棵树代表什么，可以用同质样本来表示。哪些字段或特性从层次结构中抽象出来，表达了众多实体之间的共同点？这些实体不必是树叶(尽管这很常见)；它们也可能是可以从不同的分支中提取或推断出来的属性，这些属性对于总结、建模和聚合是有用和有意义的。

在下一章中，我们将探讨一些额外的数据格式，包括 web 数据、pdf、图像、自定义文本和二进制格式。*