<title>Chapter_4</title> <link href="../Styles/epub.css" rel="stylesheet" type="text/css"> <link href="../Styles/syntax-highlighting.css" rel="stylesheet" type="text/css">

# 4

# 异常检测

> 地图不是领地，数据不是观察到的世界。数据是杂乱的、不一致的、不可靠的。这个世界更加混乱，更加不稳定，也更加不可靠。
> 
> –参见阿尔弗雷德·科济布斯基

当我们考虑异常检测时，有两个截然不同且基本上相互独立的概念。这一章的主题也许是两者中不太令人兴奋的。重要的是，安全和加密研究人员寻找可能代表欺诈、伪造和系统入侵企图的异常。根据实施者的意图，正常数据模式中的这些异常值是微妙且难以检测的，并且在希望伪造数据的人和希望检测到该伪造的人之间存在冲突。

在这本书里，我们感兴趣的概念更为常见。我们希望发现数据在收集、整理、传输和转录的日常过程中变质的情况。也许某个仪器在某个时间或所有时间都给出不好的读数。也许在重新编码为不同的数据格式的过程中，一些值被系统地改变了。也许数据子集使用了错误的度量单位。诸如此类。偶然地，这些更广泛的检查可能偶尔会识别出反映实际恶意的变化，但更多情况下，它们只会检测到错误，以及*或许*偏差(但不太常见，因为偏差通常仍倾向于*似是而非的*值)。

异常检测与 [*第五章*](Chapter_5.xhtml#_idTextAnchor007)*数据质量*有着特别紧密的联系，并且经常与 [*第六章*](Chapter_6.xhtml#_idTextAnchor008)*值插补*的题目联系在一起。本章与下一章关于数据质量的松散对比是，异常是可以被诊断为可能错误的单个数据值，而数据质量更广泛地将数据集的*模式*作为一个整体来看待，它们可以呈现或识别问题。

当异常*被*检测到时，有时估算更可能的值比完全丢弃那些观察值更有意义。就本书的结构而言，本章的课程将允许您识别异常并将其标记为“缺失”，而第六章[](Chapter_6.xhtml#_idTextAnchor008)*，*值插补*，将继续填充那些更好的插补值(插补只是用可能的或至少似是而非的值替换错误的数据点)。*

 *这些相互联系的章节——[*4*](#_idTextAnchor006)、 *5* 、[*6*](Chapter_6.xhtml#_idTextAnchor008)——组成了一个更宽泛的单元，大致描述了一个流水线或一系列步骤。也就是说，给定不可避免的有缺陷的数据，您可能首先寻找异常并标记它们缺失。接下来，您可能会寻找数据集的更系统的属性，并以各种方式修复它们。最后，您可能会估算(或丢弃)那些一开始就丢失或由于本章将帮助您检测的属性而被标记为丢失的数据。这个序列的最后一步之后的一步是你执行的实际建模或分析，是许多优秀书籍的主题，但不是这本书的主题。^(清除代码)

*清理代码*

我提到的这些步骤是一个很好的机会来重复一个已经在其他地方发生的告诫。您的数据处理流程的步骤应该仔细地、可重复地进行编码和记录。以探索的方式对数据集进行更改通常很容易，也很有诱惑力——正如本书所做的那样——但在这个过程中，会丢失一个关于具体采取了哪些步骤的良好记录。探索是数据科学不可或缺的一部分，但是在这个过程中不应该失去可再现性。好的做法是保留原始数据集——无论它最初以何种数据格式呈现——并通过脚本(在版本控制中维护)而不是在笔记本或交互式 shells 中生成最终版本。必须始终注意允许其他人重复地从原始数据集移动到输入到机器学习模型或其他分析工具中的版本。对什么工具或功能产生了什么变化进行审计跟踪是卫生实践。

***

在我们开始本章的章节之前，让我们运行我们的标准设置代码:

```
from src.setup import *

%load_ext rpy2.ipython

%%R

library(tidyverse)

require("RPostgreSQL") 
```

# 缺失数据

> 格雷戈里:你还有什么想引起我注意的吗？
> 
> 福尔摩斯:关于晚上狗的奇怪事件。
> 
> 格雷戈里:狗在晚上什么也不做。
> 
> 福尔摩斯:这是一个奇怪的事件。
> 
> 阿瑟·柯南·道尔

**概念**:

*   哨兵与明确缺席
*   空、难和不适用的语义
*   SQL 中可为空的列
*   层次结构中的缺席
*   哨兵的陷阱

一些数据格式明确支持缺失数据，而其他格式使用一种排序的特殊值**标记值**来表示缺失。非表格格式可能仅仅通过在可能出现数据的位置不包含任何值来指示丢失的数据。然而，遗憾的是，哨兵值有时是不明确的。

特别是，在许多数据格式和大多数数据框库中，缺失的数值由特殊的 IEEE-754 浮点值 NaN(非数字)表示。这里的问题是，通过设计和意图，NaN 可以作为一些并非明显不合理的计算尝试的结果而出现。虽然这样一个不可表示的值实际上是*不可用的*，但这在语义上可能不同于一开始就没有收集的数据。作为一个小题外话，让我们看看哄骗一个 NaN 出现在一个“普通的”计算中(尽管是一个人造的)。

```
for n in range(7, 10):

    exp1 = 2**n

    a = (22/7) ** exp1 

    b = π ** exp1

    # Compute answer in two "equivalent" ways

    res1 = (a * a) / (b * b)

    res2 = (a / b) * (a / b)

    print(f"n={n}:\n  "

          f"method1: {res1:.3f}\n  "

          f"method2: {res2:.3f}") 
```

```
n=7:

  method1: 1.109

  method2: 1.109

n=8:

  method1: 1.229

  method2: 1.229

n=9:

  method1: nan

  method2: 1.510 
```

与丢失的浮点数被表示为 nan 的缺陷类似，丢失的字符串几乎总是被表示为*字符串*。通常，当缺少字符串值时，会使用一个或多个保留值，如“N/A”或空字符串。然而，那些哨兵并没有明确区分“不适用”和“不可用”，这两者是有细微差别的。

作为一个玩具例子，我们可能收集了人名，包括“中间名”具有“中间名”的警戒值不会区分没有中间名的调查对象和仅仅没有提供中间名的调查对象。稍微触及一下数据科学的目的:也许我们希望找到某些中间名和人口统计特征之间的关联。例如，在美国，中间名“圣地亚哥”会与西班牙裔家庭密切相关；没有提供中间名的调查对象可能仍然有中间名。原则上，一个字符串字段可以包含不同的标记，例如“没有中间名”和“没有响应”，但是数据集很少*仔细区分这些区别。*

## 结构化查询语言

在 SQL 数据库中，显式 NULL 可用于所有列类型。一个特定的列是否“可空”是由数据库管理员决定的(或者是任何具有该功能角色的人，不管他们有多合格)。原则上，这可以区分数值字段的显式 n an 和缺失值的 NULL。

不幸的是，许多或大多数实际的数据库表未能利用这些可用的区别(即，特定配置和填充的表)。实际上，您可能会看到空字符串、nan、实际 NULLs 或其他标记的许多组合，甚至在 SQL 数据库中也是如此。这并不是因为任何广泛使用的 RDBMS 都不支持这些不同的值和类型；相反，在各种客户端使用各种代码库将数据放入其中的历史中，做出了非最优的选择。

要在下一个单元中运行代码，您需要获得对 RDBMS 的访问权。特别是，在我的本地系统上运行的 PostgreSQL 服务器有一个名为`dirty`的数据库，该数据库又包含一个名为`missing`的表。如果您使用不同的 RDBMS，您的驱动程序将具有不同的名称，并且您的引擎将在其连接 URL 中使用不同的方案。特定的用户、密码、主机和端口也会有所不同。数据库服务器也经常使用认证方法而不是密码来授权访问。然而，Python DB-API ( *数据库 API* )非常一致，当您访问其他 RDBMSs 时，您将以相同的方式使用连接对象和引擎。为了便于说明，我们展示了我们的 PostgreSQL 配置函数`connect_local()`，它包含在`setup.py`中。

```
# PostgreSQL configuration

def connect_local():

    user = 'cleaning'

    pwd = 'data'

    host = 'localhost'

    port = '5432'  

    db = 'dirty'

    con = psycopg2.connect(database=db, host=host, user=user, password=pwd)

    engine = create_engine(f'postgresql://{user}:{pwd}@{host}:{port}/{db}')

    return con, engine 
```

随着连接的建立，我们可以在 Python 中检查一些数据。

```
con, engine = connect_local()

cur = con.cursor()

# Look at table named "missing"

cur.execute("SELECT * FROM missing")

for n, (a, b) in enumerate(cur):

    print(f"{n+1} | {str(a):>4s} | {b}") 
```

```
1 |  nan | Not number

2 | 1.23 | A number  

3 | None | A null    

4 | 3.45 | Santiago  

5 | 6.78 |           

6 | 9.01 | None 
```

作为 Python 对象，SQL NULL 被表示为 singleton `None`，这是一个合理的选择。让我们回顾一下这种友好的数据表示。

*   第 1 行包含一个 NaN(不可计算)和一个描述该行的字符串
*   第 2 行包含一个常规浮点值和描述它的字符串
*   第 3 行包含一个 SQL NULL(不可用)和一个字符串
*   第 4 行包含一个常规浮点值和一个常规字符串
*   第 5 行包含一个常规浮点值和一个空字符串(“不适用”)
*   第 6 行包含一个常规浮点值和一个空值(“不可用”)

在实际支持真空值和像 NaN 这样的标记值之间的区别方面，库的质量参差不齐。Pandas 在 1.0 版本中取得了一些进展，引入了特殊的 singleton `pd.NA`作为跨数据类型的“缺失”指示符，而不是`np.nan`、`None`和`pd.NaT`(不是时间)。然而，在撰写本文时，任何标准的数据读取器都没有使用 singleton，将值转换成数据需要特别的努力。我希望当你读到这封信的时候，情况已经有所改善。

R 的 Tidyverse 做得更好，因为 R 本身有一个`NA`特殊值。稍微令人困惑的是，R 还包含一个*甚至更特殊的*伪值`NULL`，它用来表示某些东西是未定义的(与简单的缺失相对)。r 的`NULL`可以由一些表达式和函数调用产生，但不能是数组或数据帧中的元素。

```
%%R

# Notice NULL is simply ignored in the construction

tibble(val = c(NULL, NA, NaN, 0), 

       str = c("this", "that", NA)) 
```

```
# A tibble: 3 x 2

     val  str  

   <dbl>  <chr>

1     NA  this 

2    NaN  that 

3      0  NA 
```

SQL 调用 NULL 的东西，R 调用 NA；NaN 仍然是一个单独的值，表示“不可计算”这使得 R 能够正确无误地与 SQL 接口，或者偶尔与其他格式接口，这些格式也以非标记的方式明确地标记为“丢失”。

*楠*

事实上，IEEE-754 标准保留了大量的位模式作为 nan:其中 1600 万用于 32 位浮点，更多用于 64 位浮点。此外，这些众多的“南”被分成一个大数目，分别对应于发出信号的“T3”和安静的“T5”南。在概念上，当标准被开发时，选择数百万个可用的 NaN 中的哪一个(“有效载荷”)可以用来记录关于哪种操作导致 NaN 发生的确切信息。也就是说，没有任何用于数据科学的软件——几乎没有任何用于数组和数值计算的软件——真正利用了许多 nan 之间的区别。实际上，NaN 相当于一个单体，就像 R 的`NA`，Python 的`None`，或者 JavaScript 的`null`。

这段 R 代码假设与 Python 示例中使用的是同一个 PostgreSQL 数据库。与 Python 代码一样，不同的 RDBMS 需要不同的驱动程序名称，并且用户、密码、主机和端口在您的配置中也会有所不同:

```
%%R

drv <- dbDriver("PostgreSQL")

con <- dbConnect(drv, dbname = "dirty",

                 host = "localhost", port = 5432,

                 user = "cleaning", password = "data")

sql <- "SELECT * FROM missing"

data <- tibble(dbGetQuery(con, sql))

data 
```

```
# A tibble: 6 x 2

         a   b           

     <dbl>   <chr>       

1   NaN      "Not number"

2     1.23   "A number  "

3    NA      "A null    "

4     3.45   "Santiago  "

5     6.78   "          "

6     9.01    NA 
```

相比之下，Pandas 1.0 生成的数据帧不太正确。用`connect_local()`函数配置了`engine`对象，并在上面讨论了:

```
pd.read_sql("SELECT * FROM missing", engine) 
```

```
 A            b

0     NaN   Not number

1    1.23     A number

2     NaN       A null

3    3.45     Santiago

4    6.78             

5    9.01         None 
```

## 分层格式

在像 JSON 这样灵活嵌套数据的格式中，有一种明显的方式来表示丢失的数据:根本不表示它。如果您执行分层处理，您将需要检查在给定的级别上是否存在给定的字典键。JSON 规范本身并不处理`NaN`值，这意味着一些生成数据的系统可能会选择使用 JavaScript `null`值来代替它，从而产生我们上面讨论过的模糊性。然而，许多特定的库扩展了定义，将`NaN`(有时还有`inf`，也是一个浮点数)识别为一个值。举例说明:

```
json.loads('[NaN, null, Infinity]')  # null becomes Python None 
```

```
[nan, None, inf] 
```

让我们以一种(相对)紧凑的方式来表示上述 SQL 表中的相同数据。然而，请注意，由于在 Python `json`库中`NaN`是一个可识别的值，我们*可以*显式地表示所有丢失的键，并根据需要将它们与`null`进行匹配。显然，我们数据科学家通常不会生成我们需要消费的数据；所以我们得到的格式是我们需要处理的格式。

我们可以很容易地将这个特定的数据读入熊猫数据帧，但要受到 sentinel 的限制。由于数据帧采用表格格式，缺失的行/列位置必须用某个值填充，在这种情况下用 a `NaN`作为标记。当然，正如在 [*第二章*](Chapter_2.xhtml#_idTextAnchor004) 、*层次格式*中所讨论的，嵌套数据可能根本不适合用表格的方式来表示。

```
json_data = '''

{"a": {"1": NaN, "2": 1.23, "4": 3.45, "5": 6.78, "6": 9.01},

 "b": {"1": "Not number", "2": "A number", "3": "A null",

       "4": "Santiago", "5": ""}

}'''

pd.read_json(json_data).sort_index() 
```

```
 A            b

1     NaN   Not number

2    1.23     A number

3     NaN       A null

4    3.45     Santiago

5    6.78             

6    9.01          NaN 
```

为了便于说明，让我们以一种更加层次化和程序化的方式处理这些 JSON 数据，在遇到特殊/缺失值时对它们进行分类。对于这个例子，我们假设顶层是字典的字典，但是显然如果需要，我们也可以遍历其他结构:

```
data = json.loads(json_data)

rows = {row for dct in data.values() 

            for row in dct.keys()}

for row in sorted(rows):

    for col in data.keys():

        val = data[col].get(row)

        if val is None:

            print(f"Row {row}, Col {col}: Missing")

        elif isinstance(val, float) and math.isnan(val):

            print(f"Row {row}, Col {col}: Not a Number")

        elif not val:

            print(f"Row {row}, Col {col}: Empty value {repr(val)}") 
```

```
Row 1, Col a: Not a Number

Row 3, Col a: Missing

Row 5, Col b: Empty value ''

Row 6, Col b: Missing 
```

## 哨兵

在文本数据格式中，主要是分隔和固定宽度的文件，缺失的数据通过缺失或标记来指示。定界格式和固定宽度格式都能够省略一行中的某个字段，尽管在固定宽度格式中，这并不区分空字符串、空格字符串和缺失值。CSV 中相邻的两个逗号应该明确表示“无值”

理想情况下，这种缺失应该用于指示缺失，并潜在地允许一些其他标记来指示“不适用”、“不可计算”、“没有中间名”或变量域之外的已知值的其他特定标记。然而，在实践中，我在这里推荐的“最佳实践”通常不是您实际需要处理的数据集中所使用的。

标记的使用不仅限于文本格式。例如，通常在 SQL 中，`TEXT`或`CHAR`列原则上可以为空，并使用`NULL`来指示缺少的值，而不是使用标记(并不总是单个标记；实际上，他们经常在多代软件变更中获得多个标记)。有时，像 JSON 这样可以保存文本值的格式同样使用 sentinels，而不是省略键。即使在像 HDF5 这样强制数据类型化的格式中，有时也会使用 sentinel 数值来指示缺失值，而不是依赖`NaN`作为特殊标记(上面讨论过，它有自己的问题)。

特别是在 Pandas 中，从 1.0 版本开始，当读取分隔或固定宽度的文件时，以下 sentinel 值被默认识别为表示“丢失”:`' '`、`'#N/A'`、`'#N/A N/A'`、`'#NA'`、`'-1.#IND'`、`'-1.#QNAN'`、`'-NaN'`、`'-nan'`、`'1.#IND'`、`'1.#QNAN'`、`'N/A'`、`'NA'`、`'NULL'`、`'NaN'`、`'n/a'`、`'nan'`和`'null'`。其中一些肯定来自我个人不熟悉的领域或工具，但许多我已经见过了。然而，我也遇到了许多不在名单上的哨兵。您将需要为您的特定数据集考虑哨兵，这些默认值只是该工具提供的一些初步猜测。其他工具会有不同的默认值。

用于处理数据集(通常作为数据框)的库将具有指定特定值的机制，这些值将被视为缺失数据的哨兵。让我们来看一个例子，它是基于从美国国家海洋和大气管理局(NOAA)获得的真实数据。事实上，这些数据是以 CSV 文件的形式提供的；这里使用了更具描述性的文件名，并且省略了许多列。但是在该示例中只有一个数据值被改变。换句话说，这是我在写作本书之外不得不使用的数据集，讨论的问题不是我在写作之前就知道的。

我们下面读到的数据集是关于一个特定气象站的天气测量。挪威索斯托肯站是从成千上万个可供选择的站中随机选择的。其他电台使用相同的编码，但没有明显的记录。不幸的是，未记录或记录不足的字段约束在已发布的数据中是常见的，而不是例外。列名略显缩写，但不难猜测其含义:温度(F)、最大阵风速度(mph)等:

```
sorstokken = pd.read_csv('data/sorstokken-no.csv.gz')

sorstokken 
```

```
 STATION         DATE    TEMP   VISIB   GUST   DEWP

0    1001499999   2019-01-01    39.7     6.2   52.1   30.4

1    1001499999   2019-01-02    36.4     6.2  999.9   29.8

2    1001499999   2019-01-03    36.5     3.3  999.9   35.6

3    1001499999      UNKNOWN    45.6     2.2   22.0   44.8

...         ...          ...     ...     ...    ...    ...

295  1001499999   2019-12-17    40.5     6.2  999.9   39.2

296  1001499999   2019-12-18    38.8     6.2  999.9   38.2

297  1001499999   2019-12-19    45.5     6.1  999.9   42.7

298  1001499999   2019-12-20    51.8     6.2   35.0   41.2

299 rows × 6 columns 
```

我们注意到在选择表格的视图中的一些事情。未知的日期值包括在内(由我的构造)。此外，一些阵风值为 999.9(在原始数据中)。使用几个 9 位数作为标记是一种常见的惯例。然而，使用的 9 的数目是变化的，如果使用小数点的话，小数点的位置也是变化的。另一个常见的约定是使用-1 作为数值的标记，对于合法的值，语义上必须是正数。例如,-1 约定可能合理地用于阵风速度，但它不能用于华氏度或摄氏度，这对于普通的地球表面温度来说完全可以具有值-1。另一方面，如果我们使用相同的单位来测量铁炉内的温度(铁的熔点是 2800 F/1538 C)，-1 将安全地超出可能的工作范围。

查看给定变量的最小值和最大值通常是关于所使用的哨兵的线索。对于数字和日期来说，大得不合理的*或小得不合理的*的值通常被用作标记。当合法的测量值后来超出其最初预期的范围时，这可能会出错:**

```
pd.DataFrame([sorstokken.min(), sorstokken.max()]) 
```

```
 STATION         DATE    TEMP    VISIB    GUST    DEWP

0    1001499999   2019-01-01    27.2      1.2    17.1    16.5

1    1001499999      UNKNOWN    88.1    999.9   999.9    63.5 
```

这里我们看到温度和 DEWP 似乎总是落在一个“合理的”范围内。日期以这种方式提醒我们一个问题值；它也可能这样做，但可能更微妙，如果哨兵是 1900 年 1 月 1 日，这是一个实际的日期，但在 NOAA 测量之前。同样，VISIB 和 GUST 的值高得不合理，而且看起来很特别。对于字符串值，标记很可能出现在有效值的正中间。“没有中间名”按字母顺序排在“内奥米”和“尼可”之间让我们用哨兵更仔细地看看这些变量。

异常值和标准差(σ)将在后面的章节中详细讨论:

```
print("Normal max:")

for col in ['VISIB', 'GUST']:

        s = sorstokken[col]

        print(col, s[s < 999.9].max(), 

              "...standard deviation w/ & w/o sentinel:",

              f"{s.std():.1f} / {s[s < 999.9].std():.1f}") 
```

```
Normal max:

VISIB 6.8 ...standard deviation w/ & w/o sentinel: 254.4 / 0.7

GUST 62.2 ...standard deviation w/ & w/o sentinel: 452.4 / 8.1 
```

我相信 VISIB 是以英里来衡量的，看到一千英里是不合理的。阵风风速以英里/小时为单位，同样 999.9 也不会发生在地球上。然而，当哨兵值在实际值的三个数量级之内时应该担心，就像这里一样。对于幂律分布值，即使是关于数量级的经验法则也没什么帮助。

在熊猫和其他工具中，我们可以指示工具寻找特定的哨兵，并替换特定的值。当然，我们可以在使用常规的数据帧过滤和操作技术将数据读入数据结构之后这样做。如果我们能在阅读时这样做，那就更好了。这里，我们在特定于列的基础上寻找哨兵:

```
sorstokken = pd.read_csv('data/sorstokken-no.csv.gz', 

                         na_values={'DATE': 'UNKNOWN', 

                                    'VISIB': '999.9',

                                    'GUST': '999.9'},

                         parse_dates=['DATE'])

sorstokken.head() 
```

```
 STATION         DATE    TEMP    VISIB    GUST    DEWP

0    1001499999   2019-01-01    39.7      6.2    52.1    30.4

1    1001499999   2019-01-02    36.4      6.2     NaN    29.8

2    1001499999   2019-01-03    36.5      3.3     NaN    35.6

3    1001499999          NaT    45.6      2.2    22.0    44.8

4    1001499999   2019-01-06    42.5      1.9     NaN    42.5 
```

本节中的主题主要由数据格式本身驱动。让我们来看看更常见的由收集过程引起的异常情况。

# 错误编码的数据

> “当我使用一个词时，”汉仆·邓普蒂用相当轻蔑的语气说，“它的意思就是我选择的意思——不多也不少。”
> 
> 刘易斯·卡罗尔

**概念**:

*   分类和顺序约束
*   编码值和元数据定义
*   稀有类别

当我在本节讨论错误编码的数据时，我主要指的是[分类的](Glossary.xhtml#_idTextAnchor022)数据，也称为 R 中的“因子”(有时在其他地方)。[有序的](Glossary.xhtml#_idTextAnchor091)数据也可能包括在内，因为它有已知的界限。例如，如果等级范围被指定为从 1 到 10，则该数值范围之外的任何值(或者如果真正是序数，则是任何非整数的值)都必须以某种方式进行错误编码。

从某种意义上说，定量数据显然也可能编码错误。打算输入值 55 的数据条目可能被不小心输入为 555。但是同样地，一个本应是 55 的值可能会被误输入为 54，这不太可能被认为是明显错误的。无论如何，在本章的后面几节中，我们将讨论错误的定量特征的检查。数字，尤其是实数(或复数、整数、分数等。)，不呈现为立即错误，而仅呈现在它们的分布或域约束中。

对于序数值，在大多数情况下，验证其类型和范围应该确保编码的有效性(以非连续整数作为有效值的序数值有时会出现，但不太常见)。在可从 UCI 机器学习资料库获得的[皮肤病学数据集](https://archive.ics.uci.edu/ml/datasets/Dermatology)中，大多数字段被编码为 0、1、2 或 3。一个字段只有 0 或 1；年龄和目标(皮肤状况)分别是[连续](Glossary.xhtml#_idTextAnchor027)和因子变量。

在这个例子中，没有错误编码；请注意，验证是*不是*等同于知道所有值都是*正确的*:

```
from src.dermatology import *

(pd.DataFrame(

    [derm.min(), derm.max(), derm.dtypes])

     .T

     .rename(columns={0:'min', 1:'max', 2:'dtype'})

) 
```

```
 Min                  max      dtype

erythema                              0                    3      int64

scaling                               0                    3      int64

definite borders                      0                    3      int64

itching                               0                    3      int64

...                                 ...                  ...        ...

inflammatory                          0                    3      int64

monoluclear infiltrate

band-like infiltrate                  0                    3      int64

Age                                   0                   75    float64

TARGET                cronic dermatitis  seboreic dermatitis     object

35 rows × 3 columns 
```

最小值、最大值和验证整数数据类型的使用足以确保序数不被错误编码。分类变量*有时*以有序的方式编码，但通常由命名其值的单词组成。例如，下面的数据集与第 6 章 、*值插补*的 [*练习中使用的数据集非常相似。但是，在这个版本中，存在一些错误，我们将在接下来的几节中讨论。该数据包含 25，000 名调查对象的(假设的)身高、体重、头发长度和最喜欢的颜色:*](Chapter_6.xhtml#_idTextAnchor008)

```
humans = pd.read_csv('data/humans-err.csv')

# random_state for deterministic sample

humans.sample(5, random_state=1) 
```

```
 Height        Weight   Hair_Length    Favorite

21492  176.958650     72.604585          14.0         red

9488   169.000221     79.559843           0.0        blue

16933  171.104306     71.125528           5.5         red

12604  174.481084     79.496237           8.1        blue

8222   171.275578     77.094118          14.6       green 
```

从语义上来看，`Favorite`是一个分类值，有少量合法值。通常，检查此类要素的错误编码的方法是从检查它所取的唯一值开始。显然，如果文档中存在可以帮助我们的期望值。然而，请记住软件开发人员的座右铭，“文档”是“*谎言*的同义词。”它可能无法准确反映数据本身:

```
humans.Favorite.unique() 
```

```
array(['red', 'green', 'blue', 'Red', ' red', 'grееn', 'blüe',

       'chartreuse'], dtype=object) 
```

在最初查看唯一值时，我们已经看到了几个可能的问题。例如，以空格开头的`' red'`是一种常见的数据输入错误，我们很可能认为它只是简单地表示为`'red'`。另一方面，`'` `Red'`大写和小写哪个正确不一定不言自明。字符串`'blüe'`看起来像是英语单词的另一个拼写错误。`'green'`仍然发生着奇怪的事情；我们将回头讨论这个问题。

为了了解数据的意图，我们可以检查一些变化是否罕见，而其他变化是否常见。这通常是一个强烈的暗示:

```
humans.Favorite.value_counts() 
```

```
red           9576

blue          7961

green         7458

Red              1

chartreuse       1

 red             1

grееn            1

blüe             1

Name: Favorite, dtype: int64 
```

这些数字告诉了我们很多。颜色`'chartreuse'`是一个非常好的颜色名称，尽管是一个不太常用的词。它*可能*是一个合法的值，但最有可能的是它的稀有性表明某种不正确的输入，因为只有三种颜色(模一些拼写问题，我们正在工作)似乎是可用的。最有可能的情况是，我们希望将这个值标记为 missing，以便以后处理。但是*只有*最有可能；可能有领域知识表明，尽管它很罕见，但它是我们希望考虑的价值。如果存在描述它的文档，那么简单地保留它的选择就更有价值了。

罕见的以空格开头的`'` `red'`和大写的`'Red'`给了我们强有力的支持，假设它们只是`'red'`的错别字版本。然而，如果我们在大写和小写版本上大致平分秋色，或者即使*和*都不罕见，正确的行动就不那么清楚了。尽管如此，在许多情况下，对一个特定案例进行规范化或标准化(案例折叠)将是一种很好的做法，而数据框工具使其易于对大型数据集进行矢量化。但是，有时大写字母表示有意的差异，例如，在不同的家族中，相同的姓氏具有不同的大写字母。同样，在许多科学领域，简称或公式可以区分大小写，不应该是大小写混合的。了解内容领域仍然很重要。

我们只剩下两个格林的奇怪案例。他们看起来一模一样；同样，例如，上述分类值中的尾随空格在屏幕上是不可见的。这里需要手动查看这些值:

```
for color in sorted(humans.Favorite.unique()):

    print(f"{color:>10s}", [ord(c) for c in color]) 
```

```
 red [32, 114, 101, 100]

       Red [82, 101, 100]

      blue [98, 108, 117, 101]

      blüe [98, 108, 252, 101]

chartreuse [99, 104, 97, 114, 116, 114, 101, 117, 115, 101]

     green [103, 114, 101, 101, 110]

     grееn [103, 114, 1077, 1077, 110]

       red [114, 101, 100] 
```

我们在这里从 Unicode 代码点发现，我们的一个 greens 实际上有两个西里尔“ye”字符，而不是罗马“e”字符。这种几乎相同的字形的替换通常是恶意或欺骗的结果，就像这个狡猾的作者的例子一样。然而，在人类语言的大千世界中，确实可能发生这样的情况:某个特定的字符串无意中与另一个并非如此的字符串相似。除了可能使在您熟悉的特定键盘上键入一些字符串变得更加困难之外，这种视觉上的相似性并不符合阿瑟数据完整性问题。然而，在这里，一个混合语言的版本也很少见，显然这是为了纠正罗马字母中的常规英语单词。

一旦我们对所需的补救措施做出了决定——以一种对领域知识敏感的方式——我们就可以翻译麻烦的值。例如:

```
humans.loc[humans.Favorite.isin(['Red', ' red']), 'Favorite'] = 'red'

humans.loc[humans.Favorite == 'chartreuse', 'Favorite'] = None

humans.loc[humans.Favorite == 'blüe', 'Favorite'] = 'blue'

humans.loc[humans.Favorite == 'grееn', 'Favorite'] = 'green'

humans.Favorite.value_counts() 
```

```
red      9578

blue     7962

green    7459

Name: Favorite, dtype: int64 
```

让我们转向领域知识可以通知异常检测的领域。

# 固定界限

> “板球是一门艺术。像所有艺术一样，它有一个技术基础。享受它不需要技术知识，但没有技术基础的分析仅仅是印象主义。”
> 
> 《超越边界》

**概念**:

*   域与测量极限
*   输入和剪辑
*   不可能与不可能
*   探索数据错误的假设

基于我们对问题和现有数据集的领域知识，我们可能知道特定变量的固定界限。例如，我们可能知道世界上最高的人是罗伯特·潘兴·瓦德罗，身高 271 厘米，而最矮的成年人是钱德拉·巴哈杜尔·唐吉，身高 55 厘米。在我们的数据集中，超出这个范围的值可能是不合理的。事实上，我们可能希望假设更严格的界限；举个例子，让我们在 92 厘米和 213 厘米之间选择(这将包括所有成年人中的*绝大多数*)。让我们检查一下我们的`humans`数据集是否符合这些界限:

```
((humans.Height < 92) | (humans.Height > 213)).any() 
```

```
False 
```

那么，对于高度，在数据集中不会超过我们的特定于域的固定界限。变量`Hair_Length`呢？从测量的实际物理意义来看，头发长度不能为负。

然而，让我们也规定，用于我们观察的卷尺是 120 厘米长(即假设的领域知识)，因此，超过这个长度不可能是完全合法的(这样的长度是罕见的，但在人类中并非不可能)。首先，让我们看看超过测量仪器的头发长度:

```
humans.query('Hair_Length > 120') 
```

```
 Height     Weight  Hair_Length   Favorite

1984   165.634695  62.979993        127.0        red

8929   175.186061  73.899992        120.6       blue

14673  174.948037  77.644434        130.1       blue

14735  176.385525  68.735397        121.7      green

16672  173.172298  71.814699        121.4        red

17093  169.771111  77.958278        133.2       blue 
```

只有少数样本的头发长度超过了可能的测量值。然而，所有这些数字只是稍微长于测量仪器或标尺。如果没有收集过程的更多信息，就不可能确定错误的来源。也许一些受试者自己估算了他们的长发长度，而不是使用仪器。也许一个数据收集站点实际上有一个更长的卷尺，这在我们的元数据或数据描述中没有记录。又或许是有抄写错误，比如加了小数点；例如，可能 124.1 厘米的头发实际上是 24.1 厘米。或者可能单位混淆了，实际测量的是毫米而不是厘米(这是理发推子和其他理发设备的标准)。

无论如何，这个问题只影响 25，000 次观测中的 6 次。删除这些行不会丢失大量数据，所以这是可能的。输入值可能是合理的(例如，假设这 6 名受试者有平均头发长度)。第六章 的主题是 [*，选项在那里有更详细的讨论；在这个阶段，第一遍可能会将这些值标记为缺失。*](Chapter_6.xhtml#_idTextAnchor008)

但是，对于这些相对接近合法值的超出范围的值，将这些值剪切到记录的最大值可能也是一种合理的方法。“裁剪”操作有时也称为“夹紧”、“裁剪”或“修剪”，这取决于您正在使用的库。一般的想法是简单的，一个在某个界限之外的值被当作它自己就是那个界限。我们可以在修改数据时对其进行版本控制:

```
humans2 = humans.copy()  # Retain prior versions of dataset

humans2['Hair_Length'] = humans2.Hair_Length.clip(upper=120)

humans2[humans2.Hair_Length > 119] 
```

```
 Height     Weight  Hair_Length   Favorite

1984   165.634695  62.979993        120.0        red

4146   173.930107  72.701456        119.6        red

8929   175.186061  73.899992        120.0       blue

9259   179.215974  82.538890        119.4      green

14673  174.948037  77.644434        120.0       blue

14735  176.385525  68.735397        120.0      green

16672  173.172298  71.814699        120.0        red

17093  169.771111  77.958278        120.0       blue 
```

略低的阈值表示 119.6 保持不变，但是超过 120.0 的值都被精确地设置为 120。

太大的价值观不难调整。接下来让我们看看零的物理下限。正好为零的值是完全合理的。许多人剃光头或秃顶。这是虚构的数据，来自一个作者觉得模糊合理的分布，所以不要太看重长度的精确分布。请注意，零长度在现实生活中是相对常见的:

```
humans2[humans2.Hair_Length == 0] 
```

```
 Height      Weight   Hair_Length   Favorite

6      177.297182   81.153493           0.0       blue

217    171.893967   68.553526           0.0       blue

240    161.862237   76.914599           0.0       blue

354    172.972247   73.175032           0.0        red

...           ...         ...           ...        ...

24834  170.991301   67.652660           0.0      green

24892  177.002643   77.286141           0.0      green

24919  169.012286   74.593809           0.0       blue

24967  169.061308   65.985481           0.0      green

517 rows × 4 columns 
```

然而，不可能的负长度怎么办？我们可以轻松地创建一个过滤器来查看这些内容:

```
neg_hair = humans2[humans2.Hair_Length < 0]

neg_hair 
```

```
 Height       Weight   Hair_Length   Favorite

493    167.703398    72.567763          -1.0       blue

528    167.355393    60.276190         -20.7      green

562    172.416114    60.867457         -68.1      green

569    177.644146    74.027147          -5.9      green

...           ...          ...           ...        ...

24055  172.831608    74.096660         -13.3        red

24063  172.687488    69.466838         -14.2      green

24386  176.668430    62.984811          -1.0      green

24944  172.300925    72.067862         -24.4        red

118 rows × 4 columns 
```

有一定数量的这些明显编码错误的行。和其他地方一样，简单地删除有问题的行通常是一种合理的方法。然而，快速浏览一下表格数据，以及一些轻微的取证，表明很可能许多合理的值中有一个负号。这些量是正确的，但仅仅带有一个相反的符号，这至少是可信的。让我们来看看问题值的一些统计数据。为了好玩，我们将使用 R 和 Pandas 来查看非常相似的摘要:

```
%%R -i neg_hair

summary(neg_hair$Hair_Length) 
```

```
 Min.   1st Qu.   Median     Mean   3rd Qu.   Max. 

 -95.70    -38.08   -20.65   -24.35    -5.60   -0.70 
```

```
neg_hair.Hair_Length.describe() 
```

```
count    118.000000

mean     -24.348305

std       22.484691

min      -95.700000

25%      -38.075000

50%      -20.650000

75%       -5.600000

max       -0.700000

Name: Hair_Length, dtype: float64 
```

一般的统计数据并不与这个符号倒置假说相矛盾。然而，在我们得出结论之前，让我们继续在这个练习中更仔细地看看这些不好的价值观。可能还有其他模式:

```
plt.hist(neg_hair.Hair_Length, bins=30)

plt.title("Distribution of invalid negative hair length"); 
```

![](img/B17126_04_01.png)

图 4.1:显示负头发长度值分布的直方图

负值的分布大致与正值的分布相匹配。有更多的人留着长短不一的短发，而留着长发的人则更少。然而，一眼看去，这个接近于零的区域似乎有点*太多*的一个峰值。对于示例中的大约 100 行数据，您可以手动观察它们，但是对于更大的数据集或更大的边界冲突集，以编程方式关注细微差别更为普遍:

```
neg_hair.Hair_Length.value_counts() 
```

```
-1.0     19

-41.6     2

-6.8      2

-30.1     2

         ..

-3.3      1

-51.4     1

-25.1     1

-4.8      1

Name: Hair_Length, Length: 93, dtype: int64 
```

这里确实有一种模式。*有 19 个值正好是* -1，只有一两个彼此无效的负值出现。很可能在-1 错误和其他负值错误之间发生了一些不同的事情。例如，也许-1 曾被用作哨兵。当然，也有可能-1 是由规定的符号反转误差产生的；我们不能完全区分这两种可能性。

我可能会使用工作假设来处理数据集中的这个问题(如果不是简单地完全丢弃所有有问题的内容)，将-1 值标记为*缺失*，但反转其他负值的符号:

```
humans3 = humans2.copy()     # Versioned changes to data

# The "sentinel" negative value means missing

humans3.loc[humans3.Hair_Length == -1, 'Hair_Length'] = None

# All other values simply become non-negative

humans3['Hair_Length'] = humans3.Hair_Length.abs()

plt.hist(humans3.Hair_Length, bins=30)

plt.title("Distribution of corrected hair lengths"); 
```

![](img/B17126_04_02.png)

图 4.2:显示校正后头发长度的直方图

我们已经执行了一个典型的有界值清理。让我们来看看没有严格界限的值，但有一般分布统计。

# 极端值

> 如果国会有意限制这一法案，它肯定会用文字来表达这一意思。
> 
> –田纳西流域管理局。“诉希尔”,载于《美国最高法院判例汇编》第 437 卷，第 153 页(1978)

**概念**:

*   z 分数和意外值
*   四分位间距
*   标准偏差和出现频率

在连续数据中，落在标准范围内的值在那些有限的期望值内可能仍然非常不典型。在最简单的情况下，当一个值与同一个变量的其他值非常不同时，就会出现这种情况。表征一个值的期望值的标准方法是一种称为 z 值的度量。这个值就是每个点离变量平均值的距离，除以变量的标准偏差。

![](img/B17126_04_004.png)

其中![](img/B17126_04_005.png)是样本均值，![](img/B17126_04_006.png)是标准差。

这种方法对于遵循正态分布的数据最为精确，但通常对于任何单峰(有一个峰值)、稍微对称且与比例相关的数据都很有用。用更普通的语言来说，我们只想寻找一个数据变量的直方图，它有一个峰值，两边以大致相同的速度逐渐变小。完全正态分布在真实世界的数据中是不常见的。

还经常使用一种稍微不同的方法来识别异常值。盒须图(通常简称为盒图)通常包含异常值作为单独的视觉元素。虽然在这种可视化中可以使用 z 分数，但这些图更经常利用**四分位距** ( **IQR** )和固定乘数来定义异常值。不同的技术会产生相似但不完全相同的答案。

## z 分数

通过可视化，我们可以看到数据集中的身高和体重总体上遵循类似正态分布的。我们已经看到，修正后的头发长度是严格的单尾。然而，从 0 处的模式的单侧下降足够接近正态分布的一个尾部，z 得分仍然是合理的考虑。

```
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

ax1.hist(humans3.Height, bins=50)

ax2.hist(humans3.Weight, bins=50)

ax1.set_title("Distribution of Height")

ax2.set_title("Distribution of Weight"); 
```

![](img/B17126_04_03.png)

图 4.3:显示身高和体重分布的直方图

如果我们希望更精确地量化变量的正态性，我们可以使用统计检验，如安德森-达林，夏皮罗-维尔克，或偏度-峰度。这些技术中的每一种都试图拒绝正态分布的假设。对于不同的 p 值(概率)，不同的检验统计决定了这种拒绝的阈值(尽管对于大样本，即使与正态性的小偏差也会拒绝假设，但从 z 得分有用的角度来看并不重要)。在 Anderson-Darling 中，如果检验统计量不超过 1.0，则曲线绝对正常，足以用 z 值来衡量异常值。然而，反之则不成立；许多非正态曲线仍然可以合理地使用 z 分数。本质上，对于幂律或指数分布，以及强多峰曲线，我们只需要避免这种测量。让我们对身高、体重和头发长度变量进行 Anderson-Darling 测试:

```
from scipy.stats import anderson

for var in ('Height', 'Weight', 'Hair_Length'):

    data = humans3[var][humans3[var].notnull()]

    stat = anderson(data, 'norm').statistic

    print(f"Anderson-Darling statistic for {var:<12s}: {stat:6.2f}") 
```

```
Anderson-Darling statistic for Height      :   0.24

Anderson-Darling statistic for Weight      :   0.54

Anderson-Darling statistic for Hair_Length : 578.19 
```

认识到头发长度不正常，但它显示了沿线性比例的单边衰减，我们可以将所有定量变量的 z 分数添加到工作数据框架中。和以前一样，作为保留修改版本的良好实践，我们在下一次转换之前将数据复制到一个新的数据框中。

我们在计算标准偏差时忽略了自由度参数 delta ,因为它对于 25，000 个样本来说是微不足道的(如果我们只有 10 或 20 个样本，这可能很重要)。自由度涉及基于样本的总人口内的预期方差；但是，只有当样本是几十个观察值而不是几万个观察值时，这些差异才会显著:

```
humans4 = humans3.copy()

for var in ('Height', 'Weight', 'Hair_Length'):

    zscore = (humans4[var] - humans4[var].mean()) / humans4[var].std()

    humans4[f"zscore_{var}"] = zscore

humans4.sample(5, random_state=1) 
```

```
 Height      Weight   Hair_Length   Favorite   zscore_Height

21492   176.958650   72.604585          14.0        red        0.880831

9488    169.000221   79.559843           0.0       blue       -0.766210

16933   171.104306   71.125528           5.5        red       -0.330758

12604   174.481084   79.496237           8.1       blue        0.368085

8222    171.275578   77.094118          14.6      green       -0.295312

        zscore_Weight   zscore_Hair_Length

21492       -0.042032            -0.568786

9488         0.997585            -1.225152

16933       -0.263109            -0.967294

12604        0.988078            -0.845397

8222         0.629028            -0.540656 
```

z 分数阈值的选择非常依赖于领域和问题。经验法则通常是使用绝对值大于 3 的 z 值作为界定异常值的分界点。但是*所期望的*在很大程度上取决于数据集的大小。

在统计学中，我们有时会想起*68–95–99.7 规则*，它列出了在正态分布中落入一个、两个或三个标准差范围内的观察值的百分比。

在离平均值的任何距离处，如果足够多的话，一些*观察值是可以预期的，但是随着标准偏差距离的增加，数量会迅速减少。*

让我们看看常见 z 值阈值 3。请记住，我们在这里处理 25，000 个样本，因此一般来说，我们希望在上面讨论的*68–95–99.7 规则*下，找到大约 75 个样本，不在 3 个标准偏差之内。让我们看一下高度表，但只需检查其他变量超出此界限的行数:

```
humans4[humans4.zscore_Height.abs() > 3] 
```

```
 Height      Weight   Hair_Length   Favorite   zscore_Height

138     187.708718   86.829633          19.3      green        3.105616

174     187.537446   79.893761          37.5       blue        3.070170

412     157.522316   62.564977           6.8       blue       -3.141625

1162    188.592435   86.155948          53.1        red        3.288506

...            ...         ...           ...        ...             ...

22945   157.293031   44.744929          18.4        red       -3.189077

23039   187.845548   88.554510           6.9       blue        3.133934

24244   158.153049   59.725932          13.8      green       -3.011091

24801   189.310696   85.406727           2.3      green        3.437154

          zscore_Weight    zscore_Hair_Length

138            2.084216             -0.320304

174            1.047496              0.532971

412           -1.542673             -0.906345

1162           1.983518              1.264351

...                 ...                   ...

22945         -4.206272             -0.362499

23039          2.342037             -0.901657

24244         -1.967031             -0.578162

24801          1.871531             -1.117320

51 rows × 7 columns 
```

```
print("Outlier weight:", (humans4.zscore_Weight.abs() > 3).sum())

print("Outlier hair length:", (humans4.zscore_Hair_Length.abs() > 3).sum()) 
```

```
Outlier weight: 67

Outlier hair length: 285 
```

我们已经注意到头发长度是单尾的，所以我们可能会预期大约两倍的异常值。实际数字是这个数字的两倍多，但这本身并不是价值观的极端分歧。实际上，身高和体重的峰度比我们从正态分布中预期的要低一些(尾巴变细的速度稍微快一点)。

无论如何，z 值 3 可能太小，对我们的样本量没有用。4 sigma 可能与我们区分仅仅是不寻常的和可能是错误的观察结果的目的更相关，对于单尾头发长度可能是 4.5。

一天一次超出给定标准偏差(σ)的观察频率表提供了一个有用的直觉。记住西格玛效果的一个简单方法是前面提到的*68–95–99.7 规则*；也就是说，在一个、两个和三个标准差范围内的事物的百分比:

| 范围 | 观察比例 | 每日事件的频率 |
| ± 1σ | 三分之一 | 每周两次 |
| ± 2σ | 22 分之一 | 每三周一次 |
| ± 3σ | 370 分之一 | 每年的 |
| ± 4σ | 15787 分之一 | 每 43 年(一生中两次) |
| ± 5σ | 1 比 1，744，278 | 每 5000 年(有记载的历史中有一次) |
| ± 6σ | 506797346 分之一 | 每 140 万年(人类历史上的两次) |
| ± 7σ | 390，682，215，445 中的 1 | 每 10 亿年(地球历史上的四次) |

让我们看看给定更宽的 z 得分界限的异常值:

```
cond = (

    (humans4.zscore_Height.abs() > 4) |

    (humans4.zscore_Weight.abs() > 4) |

    (humans4.zscore_Hair_Length.abs() > 4.5))

humans4[cond] 
```

```
 Height      Weight   Hair_Length   Favorite   zscore_Height

13971   153.107034   63.155154           4.4      green       -4.055392

14106   157.244415   45.062151          70.7        red       -3.199138

22945   157.293031   44.744929          18.4        red       -3.189077

          zscore_Weight     zscore_Hair_Length

13971         -1.454458              -1.018865

14106         -4.158856               2.089496

22945         -4.206272              -0.362499 
```

使用人类身体特征的适度领域知识，即使他们在“标准”之外，153cm 或 45kg 的人是小的，但是没有超出我们预期的范围。根据数据，少量的 4 sigma 异常值既短又轻，我们期望这些数据与相对较高的程度相关，从而为测量提供合理性。

此外，我们在上一节中讨论的固定边界的高度边界比 4 sigma(甚至 5 sigma)检测要宽得多。因此，虽然我们*可以*丢弃或标记这些异常值行中的缺失值，但是分析似乎并没有激励我们这样做。

## 四分位间距

使用 IQR 而不是 z 值会减少对分布正态性的假设。然而，这种技术也不能为幂律或指数数据分布提供有意义的答案。如果您可以将分布范围确定为多个数量级，那么查看原始数据的 n 次方根或对数的四分位数可能仍然会产生合理的结果。事实上，如果使用 z 分数分析，同样的转换也同样适用。

IQR 的想法是简单地查看变量中的四分位数临界值，并测量第一个和第三个四分位数之间的数值距离，即 25%和 75%百分位数之间的距离。恰好一半的数据在该范围内，但我们通常也期望大多数数据在这些临界值之外的某个距离内，临界值定义为临界值之间范围的倍数。最常见的是，选择乘数 1.5；这仅仅是一个惯例，通常很有用，但缺乏更深层次的含义。

我在本文中加入了一个简单的函数来可视化显示 IQR 定义的异常值的箱线图。通常情况下，这种功能只包含在本书的源代码库中，但在这里，我认为读者可以看看 [Matplotlib](Glossary.xhtml#_idTextAnchor073) 中这几行代码的配置(其他可视化库也有类似的功能；事实上，通常是更高层次的抽象，更具视觉冲击力):

```
# Function defined but not run in this cell

def show_boxplots(df, cols, whis=1.5):

    # Create as many horizontal plots as we have columns

    fig, axes = plt.subplots(len(cols), 1, figsize=(10, 2*len(cols)))

    # For each one, plot the non-null data inside it

    for n, col in enumerate(cols):

        data = df[col][df[col].notnull()]

        axes[n].set_title(f'{col} Distribution')

        # Extend whiskers to specified IQR multiplier

        axes[n].boxplot(data, whis=whis, vert=False, sym='x')

        axes[n].set_yticks([])

    # Fix spacing of subplots at the end

    fig.tight_layout() 
```

虽然默认乘数(胡须宽度)是 1.5，但我们已经看到，人类数据足够大，其值必须相对极端，才能看起来真正*不可能*是真实的。因此，我们选择晶须宽度为 2.5:

```
show_boxplots(humans4, ["Height", "Weight", "Hair_Length"], 2.5) 
```

![](img/B17126_04_04.png)

图 4.4:显示身高、体重和头发长度分布的箱线图

中间的方框代表 IQR，从 25%到 75%。在盒子的上方/下方，须状物延伸到 IQR 的倍数倍。一个 *x* 标记了胡须后面的异常值。

只有一个异常值出现在这个身高阈值，在短端。同样，只有两个重量出现，都在轻端。这与我们在 z 值上发现的模式相同。更多的“异常”长发长度出现，但是我们已经使用了一个更大的 z 值来更严格地过滤。如果我们愿意，我们同样可以使用更大的*胡须*宽度来过滤掉更多的头发长度。

虽然可视化很方便，但我们希望找到图中用 *x* 标记的实际数据行。让我们编码。我们找到四分位数，计算 IQR，然后显示内层范围:

```
quartiles = (

    humans4[['Height', 'Weight']]

    .quantile(q=[0.25, 0.50, 0.75, 1.0]))

quartiles 
```

```
 Height       Weight

0.25    169.428884    68.428823

0.50    172.709078    72.930616

0.75    175.953541    77.367039

1.00    190.888112    98.032504 
```

```
IQR = quartiles.loc[0.75] - quartiles.loc[0.25]

IQR 
```

```
Height    6.524657

Weight    8.938216

dtype: float64 
```

```
for col, length in IQR.iteritems():

    high = quartiles.loc[0.75, col] + 2.5*IQR[col]

    low = quartiles.loc[0.25, col] - 2.5*IQR[col]

    print(f"Inliers for {col}: [{low:.3f}, {high:.3f}]") 
```

```
Inliers for Height: [153.117, 192.265]

Inliers for Weight: [46.083, 99.713] 
```

实际上，在这种情况下，使用 inlier 范围过滤会给我们与 z-score 方法相同的答案。无论我们使用哪种异常检测技术，最矮的人必然是最矮的。但是，根据实际数据分布情况，选择基于领域的 IQR 乘数可能比使用基于领域的 z 值识别出更多或更少的异常值:

```
cond = (

    (humans4.Height > 192.265) |

    (humans4.Height < 153.117) |

    (humans4.Weight > 99.713)  |

    (humans4.Weight < 46.083))

humans4[cond] 
```

```
 Height     Weight  Hair_Length  Favorite    zscore_Height

13971  153.107034  63.155154          4.4     green        -4.055392

14106  157.244415  45.062151         70.7       red        -3.199138

22945  157.293031  44.744929         18.4       red        -3.189077

       zscore_Weight  zscore_Hair_Length

13971      -1.454458           -1.018865

14106      -4.158856            2.089496

22945      -4.206272           -0.362499 
```

检测单变量异常值可能很重要，但有时是一些特征的组合变得异常。

# 多元异常值

> 如果你不是解决方案的一部分，你就是沉淀的一部分。
> 
> –匿名

**概念**:

*   确定性综合特征的变化
*   相对稀有的期望

有时单变量特征可以落在相对适中的 z 得分边界内，然而这些特征的组合不太可能或不合理。也许一个实际的机器学习模型可以预测特征的组合可能是错误的。在这一节中，我们只看更简单的特征组合来识别有问题的样本。

在第七章[](Chapter_7.xhtml#_idTextAnchor009)**特征工程*中我们讨论多项式特征。该技术将属于同一观测值的两个或多个变量的值相乘，并将结果视为一个新特征。例如，在我们的工作示例中，也许身高和体重都没有超出合理的范围，但它们的乘积却超出了合理的范围。虽然这肯定是可能的，但我们通常希望这些特征一开始就是正相关的，因此乘法可能只会产生一些新的东西*稍微超出单变量异常值检测已经检测到的界限*。*

 *然而，让我们考虑一个由特定领域驱动的派生特性。身体质量指数(身体质量指数)是一种常用来测量人们健康体重的方法，定义为:

![](img/B17126_04_007.png)

也就是说，体重和身高在这个导出量中是反比关系，而不是相乘组合。也许这个多元衍生特征显示了一些问题异常值。让我们构建另一个数据框版本，该版本丢弃了之前计算的列，但添加了身体质量指数及其 z 值:

```
humans5 = humans4[['Height', 'Weight']].copy()

# Convert weight from cm to m

humans5['BMI'] = humans5.Weight / (humans5.Height/100)**2

humans5["zscore_BMI"] = (

    (humans5.BMI - humans5.BMI.mean()) / 

     humans5.BMI.std()

)

humans5 
```

```
 Height         Weight           BMI        zscore_BMI

0        167.089607      64.806216     23.212279         -0.620410

1        181.648633      78.281527     23.724388         -0.359761

2        176.272800      87.767722     28.246473          1.941852

3        173.270164      81.635672     27.191452          1.404877

...             ...            ...           ...               ...

24996    163.952580      68.936137     25.645456          0.618008

24997    164.334317      67.830516     25.117048          0.349063

24998    171.524117      75.861686     25.785295          0.689182

24999    174.949129      71.620899     23.400018         -0.524856

25000 rows × 4 columns 
```

在衍生特征中寻找异常值，我们看到了强信号。如前所述，在 z 值为 4 且数据集包含 25，000 条记录的情况下，我们预计会有略多于一条记录通过自然随机分布显示为异常值。事实上，我们在下面看到的绝对值仅略大于 4 的两个 z 值出现在数据集中，然后才被设计为强调本节的教训:

```
humans5[humans5.zscore_BMI.abs() > 4] 
```

```
 Height       Weight          BMI     zscore_BMI

21388   165.912597    90.579409    32.905672       4.313253

23456   187.110000    52.920000    15.115616      -4.741383

23457   158.330000    92.780000    37.010755       6.402625

24610   169.082822    47.250297    16.527439      -4.022805 
```

除了高身体质量指数和低身体质量指数的中度异常值的一个示例之外，我们在每一侧还有两个更多的极端值。在这种情况下，这些是为截面构建的，但类似的多元异常值也会在野外出现。在 25，000 个样本中，z 值为-4.74 并不是我们预期的极端值，但也不是完全不可信。然而，+6.4 的 z 分数在天文上不太可能在没有数据错误(或书籍作者的构建)的情况下出现。由于身体质量指数是一个结合了身高和体重的派生特征，而且每一项都在合理的范围内，正确的方法几乎肯定是简单地丢弃这些有问题的行。数据本身没有引导我们去了解体重或身高是否是问题的关键，也没有合理的补救措施。

幸运的是，对于这个特定的数据集，只有 2 个(或者*可能是* 4 个)样本显示了正在讨论的问题。我们这里有大量的数据，丢弃这些行并没有什么实际的危害。显然，本节和最后几节中说明的关于 z 分数阈值和特定数据行的处置的特定决策只是示例。您将需要在您的问题和领域内决定什么是最相关的级别和测试，以及要执行什么补救措施。

# 练习

本章中的两个练习要求你首先在定量数据中寻找异常，然后在分类数据中寻找异常。

## 一个著名的实验

迈克尔逊-莫雷实验是 19 世纪后期的一次尝试，目的是探测发光的以太 T2 的存在，这是一种被广泛认为可以携带光波的介质。这是物理学史上最著名的“失败实验”，因为它没有探测到它在寻找的东西——我们现在知道根本不存在的东西。

总的想法是在设备相对于地球运动方向的不同方位下测量光速，因为以太介质的相对运动会增加或减少波的速度。是的，根据相对论，这是行不通的，但在 150 年前这是一个合理的猜测。

除了物理问题之外，迈克尔逊-莫雷实验得出的数据集也广泛可用，包括 r 中内置的样本。相同的数据可从以下网址获得:

[https://www.gnosis.cx/cleaning/morley.dat](https://www.gnosis.cx/cleaning/morley.dat)

弄清楚这种并不复杂的格式是这个练习的第一步(也是真正的数据科学工作的典型)。

该数据中的具体数字是以千米/秒为单位的光速测量值，零点为 299，000。例如，实验 1 中的平均测量值为 299，909 km/s，让我们看看 R 束中的数据:

```
%%R -o morley

data(morley)

morley %>%

    group_by('Expt') %>%

    summarize(Mean = mean(Speed), Count = max(Run)) 
```

```
'summarise()' ungrouping output (override with '.groups' argument)

# A tibble: 5 x 3

   Expt  Mean Count

  <int> <dbl> <int>

1     1  909     20

2     2  856     20

3     3  845     20

4     4  820\.    20

5     5  832\.    20 
```

总之，我们只需要查看每个实验设置的运行次数，以及该设置的平均值。原始数据在每个设置中有 20 次测量。

使用您喜欢的任何编程语言和工具，首先在每个设置(由一个`Expt`数字定义)中识别异常值，然后在整个数据集合中识别异常值。在最初的实验中，我们希望每种设置都显示出显著的集中趋势差异，实际上它们的平均值有些不同。

这本书和这一章没有详细探讨置信水平和无效假设，而是创建了一个可视化工具，帮助您直观地了解几个设置之间存在的明显差异。

如果您丢弃每个设置中的异常值，设置之间的差异是增加了还是减少了？用可视化的方式或通过查看减少的组的统计数据来回答。

## 拼写错误的单词

在这个练习中，我们回到我们用来说明一些概念的 25，000 次人体测量。然而，在这个数据集的变体中，每一行都有一个人的名字(取自美国社会安全局上个世纪常见名字的列表；抱歉，由于美国人口和移民趋势的历史，这些名字倾向于盎格鲁中心。

本练习的数据集可在以下位置找到:

[https://www.gnosis.cx/cleaning/humans-names.csv](https://www.gnosis.cx/cleaning/humans-names.csv)

不幸的是，我们为这个数据集假设的数据收集者简直是糟糕的打字员，当他们以惊人的频率输入姓名时会出现拼写错误。在这个数据集中有一些预期的名字，但是也有一些名字的简单错误编码。问题是:我们如何区分真实姓名和错别字？

有多种方法可以衡量字符串的相似性，并提供可能的错别字线索。一种通用的方法是根据字符串之间的*编辑距离*。以为例，R 包 **stringdist** 提供了 Damerau–Levenshtein、Hamming、Levenshtein 和 optimal string alignment 作为编辑距离的度量。较少特定于编辑的模糊匹配技术利用“n-gram 包”方法，包括 q-gram、余弦距离和 Jaccard 距离。像 Jaro 和 Jaro-Winkler 这样的启发式度量标准也包含在`stringdist`中，还有其他提到的度量标准。Soundex、soundex 变体和[变音位](Glossary.xhtml#_idTextAnchor076)寻找发音相似的单词，但因此特定于语言甚至地区方言。

与更常见的 Python 和 R 库的模式相反，Python 是将字符串相似性度量分散在多个库中的一种库，每个库中只包含几个度量。然而， **python-Levenshtein** 是一个非常好的包，包含了大部分这些措施。如果你想余弦相似，你可能必须使用`sklearn.metrics.pairwise`或另一个模块。对于语音对比， **fonetika** 和 **soundex** 都支持多种语言(但每种语言不同；几乎所有的包都通用英文)。

在我的个人系统上，我有一个名为`similarity`的命令行工具，我用它来测量字符串之间的距离。这个特殊的几行脚本测量 Levenshtein 距离，但也将其规范化为较长字符串的长度。短名称将具有较小的距离数值度量，即使在不同的字符串之间也是如此，而总体上接近的长字符串在规范化之前可以具有较大的度量(取决于选择了什么度量，但对于大多数字符串来说)。几个例子说明了这一点:

| 字符串 1 | 字符串 2 | 莱文斯坦距离 | 相似比 |
| 大卫 | 戴维恩 | one | Zero point eight |
| 大卫 | 专家 | three | Zero point four |
| 那只敏捷的棕色狐狸跳了起来 | 这份快速的棕色传真混在一起 | five | 0.814814814815 |

在这个练习中，你的目标是识别每一个真正的*名字，并将所有拼错的名字更正为正确的规范拼写。请记住，有时多个合法名称在相似性度量方面实际上彼此很接近。然而，假设*罕见的*拼写是错别字可能是合理的，至少如果它们也相对类似于常见的拼写。您可以使用任何您认为对任务最有用的编程语言、库和度量标准。*

在阅读数据时，我们看到它类似于我们以前看到的人类测量:

```
names = pd.read_csv('data/humans-names.csv')

names.head() 
```

```
 Name        Height     Weight

0     James    167.089607  64.806216

1     David    181.648633  78.281527

2   Barbara    176.272800  87.767722

3      John    173.270164  81.635672

4   Michael    172.181037  82.760794 
```

很容易看出，有些“名字”出现的频率很高，而有些则很少出现。在做这个练习时，也看看中间值:

```
names.Name.value_counts() 
```

```
Elizabeth    1581

Barbara      1568

Jessica      1547

Jennifer     1534

             ... 

ichael          1

Wlliam          1

Richrad         1

Mray            1

Name: Name, Length: 249, dtype: int64 
```

# 结局

> 当你排除了不可能，剩下的，不管多么不可能，一定是事实。
> 
> 阿瑟·柯南·道尔

**本章涉及的主题**:缺失数据；哨兵；编码错误的数据；固定界限；离群值。

我们在本章中讨论的异常现象分为几个相对不同的类别。对于第一种类型，有明确标记缺失数据的特殊值，尽管这些标记有时容易出错。然而，丢失的明确指示可能是最直接的异常。第二种异常是错误编码的分类值；一些有限数量的值是适当的(尽管并不总是清楚地记录在案)，任何不是这几个值之一的值都是异常的。

第三种异常是连续的——或者至少是一定范围内的——数据值超出了我们的预期范围。这些也被称为*异常值*，尽管一个值必须在典型值之外多少才会成为问题是非常依赖于领域和问题的。期望可以采取源自测量领域知识的先验假设的形式。它们也可能产生于总体变量内的数据分布，以及某个特定值与作为该变量测量的其他值的偏差。有时，我们对界限的期望甚至可以是多元的，一些多元变量的数字组合会产生一个超出期望界限的值。

对于所有这些异常情况，我们可以采取两种措施。如果一个观察或样本有这些问题之一，我们可以决定完全放弃它。或者，我们可以简单地更明确地将观察中的一个特征标记为缺失，因为它的值不可靠。当我们将值修改为“缺失的”特殊值时，跟踪我们的更改和数据版本是极其重要的实践。我们选择如何处理那些被标记为显式缺失的值是一个下游决策，我们将在后面的章节中详细讨论。

在下一章，我们将从寻找特定数据点的问题转移到寻找数据集整体“形状”的问题。****