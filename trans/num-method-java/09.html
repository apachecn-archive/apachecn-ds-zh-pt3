<html lang="en">
<head><title>Unconstrained Optimization</title>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<link href="../css/springer_epub.css" rel="styleSheet" type="text/css"/>
</head>
<body>
 
<!--Begin Abstract--><h1 class="ChapterTitle" lang="en">9.无约束最优化</h1>

 
<!--End Abstract--><p>无约束优化寻找一个变量(或一组变量),使目标函数最小化或最大化，而对它们的值没有限制。数学上，它说，求<em class="EmphasisTypeItalic ">x</em><sup>∫</sup>使得函数<em class="EmphasisTypeItalic ">f</em>(<em class="EmphasisTypeItalic ">x</em><sup>∫</sup>)取最小值。<p> <img alt="$$ {x}^{\ast }=\underset{x}{\min }f(x) $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equa.png" style="width:5.98em"/> </p></p>
<p class="Para" id="Par3"><em class="EmphasisTypeItalic "> x </em>可能是标量(在<em class="EmphasisTypeItalic ">ℝ</em>T4【1】t5)也可能是矢量(在<em class="EmphasisTypeItalic "> ℝ </em> <sup> <em class="EmphasisTypeItalic "> n </em> </sup>)。<em class="EmphasisTypeItalic "> f </em> ( <em class="EmphasisTypeItalic "> x </em>)始终是标量(在<em class="EmphasisTypeItalic ">ℝ</em>1)。无约束最优化在许多科学分支中都有应用。在金融学中，构建收益率曲线包括寻找一组参数，以最小化债券的理论价格和市场价格之间的差异。通过将约束优化问题的约束转换成等价的无约束优化问题的目标函数中的罚项，约束优化问题也可以公式化为无约束优化问题。此外，许多约束优化解算器或算法涉及将无约束优化问题作为子问题来求解。NM Dev 有一套优化器或最小化器来解决无约束优化问题，包括单变量和多变量问题，有或没有导数信息。我们的实施和讨论基本上是基于&amp;陆(2007)。</p>
<h2 class="Heading">9.1 暴力搜索</h2>
<p class="Para" id="Par4">最简单也可能是最容易理解的求解器是强力搜索。也就是说，我们简单地遍历函数定义域中所有可能的值，<em class="EmphasisTypeItalic "> x </em>，枚举出<em class="EmphasisTypeItalic "> f </em> ( <em class="EmphasisTypeItalic "> x </em>)所有可能的值。然后我们挑最小的<em class="EmphasisTypeItalic "> f </em> ( <em class="EmphasisTypeItalic "> x </em>)对应的<em class="EmphasisTypeItalic "> x </em>。可能有不止一个解决方案。但是，这种方法很少使用，因为它不实用。一个要求是域必须是可枚举的。大多数实际的优化问题都是在一个连续的区域上解决的，比如实数。我们无法列举无限多的数字。解决这个问题的一个方法是离散化。我们从无限域中选择有限数量的点。比如我们可以把单位区间[0，1]离散成 11 个离散值，0，0.1，0.2，…，1.0。离散化最明显的缺点是，当选择的点不包含最优解时，我们可能(很可能)得到一个次优解。事实上，如果区域是无界的，(∞，+∞)，如何选择包含解的点并不明显。此外，对于除了小问题之外的任何问题，性能都非常慢。假设我们有 10 个变量需要求解；那么问题的维度就是<em class="EmphasisTypeItalic ">n</em>T14】10，其中<em class="EmphasisTypeItalic "> n </em>是每个维度选择的离散点数。问题的复杂性呈指数增长。尽管如此，强力搜索还是很容易理解的。因此，我们先从它开始。</p>
<p>下面的 NM Dev 代码大体上说明了我们如何使用这个库来解决一个优化问题。假设我们要最小化函数<em class="EmphasisTypeItalic ">f</em>(<em class="EmphasisTypeItalic ">x</em>)=<em class="EmphasisTypeItalic ">x</em><sup>2</sup>—4。</p>
<pre>// define the optimization problem using an objective function
OptimProblem problem = new OptimProblem() {

    @Override
    public int dimension() {
        return 1;
    }

    @Override
    public RealScalarFunction f() {
        return new RealScalarFunction() {

            // the objective function
            @Override
            public Double evaluate(Vector v) {
                double x = v.get(1);
                Polynomial polynomial = new Polynomial(1, 0, -4); // f(x) = x^2 - 4
                double fx = polynomial.evaluate(x);
                return fx;
            }

            @Override
            public int dimensionOfDomain() {
                return 1;
            }

            @Override
            public int dimensionOfRange() {
                return 1;
            }
        };
    }
};

// set up the solver to use and the solution
DoubleBruteForceMinimizer solver = new DoubleBruteForceMinimizer(true);
BruteForceMinimizer.Solution soln = solver.solve(problem);

// for brute force search, we need to explicitly enumerate the values in the domain
List&lt;Vector&gt; domain = new ArrayList&lt;&gt;();
domain.add(new DenseVector(-2.));
domain.add(new DenseVector(-1.));
domain.add(new DenseVector(0.)); // the minimizer
domain.add(new DenseVector(1.));
domain.add(new DenseVector(2.));
soln.setDomain(domain);

System.out.println(String.format("f(%s) = %f", soln.minimizer(), soln.min()));

</pre>
<p>输出如下所示:</p>
<pre>f([0.000000] ) = -4.000000

</pre>
<p>使用 NM Dev 中的求解器解决优化问题有许多步骤。</p>
<ol><li class="ListItem"><p class="Para" id="Par8">通过实现接口<code>OptimProblem</code>定义一个优化问题。我们在实现中给出了目标函数。</p>
 </li>
<li class="ListItem"><p class="Para" id="Par9">构建一个求解器来使用，如<code>DoubleBruteForceMinimizer</code>。</p>
 </li>
<li class="ListItem"><p class="Para" id="Par10">调用<code>solve</code>函数，使用优化问题从求解器构建一个<code>Solution</code>对象。换句话说，求解器对象是一个为优化问题<code>OptimProblem</code>创建解决方案<code>Solution</code>的工厂。虽然规划求解可重复用于其他问题，但一个解决方案仅对应于一个问题。</p>
 </li>
<li class="ListItem"><p class="Para" id="Par11">向<code>Solution</code>对象提供任何与问题相关的信息。在这个强力搜索的例子中，我们需要给出一个可能的搜索值列表。</p>
 </li>
<li class="ListItem"><p class="Para" id="Par12">调用<code>minimizer</code>函数运行算法计算最小值<em class="EmphasisTypeItalic ">x</em><sup>∫</sup>，或者调用<code>min</code>函数返回函数<em class="EmphasisTypeItalic ">f</em>(<em class="EmphasisTypeItalic ">x</em><sup>∫</sup>)的最小值。</p>
 </li>
</ol>

<p>前面的过程适用于 NM Dev 库中的所有解算器。所有 NM 解算器都做最小化。要实现最大化，我们只需将目标函数乘以-1 即可。一般来说，<code>OptimProblem</code>、解算器和<code>Solution</code>的签名如下:</p>
<pre>public interface OptimProblem {

    /**
     * Get the number of variables.
     *
     * @return the number of variables.
     */
    public int dimension();

    /**
     * Get the objective function.
     *
     * @return the objective function
     */
    public RealScalarFunction f();
}

public interface MultivariateMinimizer&lt;P extends OptimProblem, S extends MinimizationSolution&lt;Vector&gt;&gt; extends Minimizer&lt;P, S&gt; {

    /**
     * Solve an optimization problem, e.g., {@link OptimProblem}.
     *
     * @param problem an optimization problem
     * @return a solution to the optimization problem
     * @throws Exception when there is an error solving the problem
     */
    public S solve(P problem) throws Exception;
}

public interface MinimizationSolution&lt;T&gt; {

    /**
     * Get the (approximate) minimum found.
     *
     * @return the (approximate) minimum found
     */
    public double minimum();

    /**
     * Get the minimizer (solution) to the minimization problem.
     *
     * @return the minimizer
     */
    public T minimizer();
}

</pre>
<p><code>DoubleBruteForceMinimizer</code>有以下签名:</p>
<pre>public DoubleBruteForceMinimizer(boolean isParallel);

@Override
public BruteForceMinimizer.Solution solve(OptimProblem problem) throws Exception

</pre>
<p class="Para" id="Par15">当<code>isParallel</code>标志被设置为<code>true</code>时，代码使用多线程代码并行运行搜索，以在所有可用线程上搜索所有提供的域值。</p>
<p class="Para" id="Par11221">强力搜索可用于解决单变量优化问题，如在前面的例子中，以及多变量优化问题。以下是求解的示例:</p>
<p><em class="EmphasisTypeItalic "> f </em> ( <em class="EmphasisTypeItalic "> x </em>、<em class="EmphasisTypeItalic ">y</em>)=<em class="EmphasisTypeItalic ">x</em><sup>2</sup>+<em class="EmphasisTypeItalic ">y</em><sup>2</sup></p>
<pre>OptimProblem problem = new OptimProblem() {

    @Override
    public int dimension() {
        return 2;
    }

    @Override
    public RealScalarFunction f() {
        return new RealScalarFunction() {

            @Override
            public Double evaluate(Vector v) {
                double x = v.get(1);
                double y = v.get(2);

                double fx = x * x + y * y;
                return fx;
            }

            @Override
            public int dimensionOfDomain() {
                return 2;
            }

            @Override
            public int dimensionOfRange() {
                return 1;
            }
        };
    }
};

DoubleBruteForceMinimizer bf = new DoubleBruteForceMinimizer(true);
BruteForceMinimizer.Solution soln = bf.solve(problem);
List&lt;Vector&gt; domain = new ArrayList&lt;&gt;();
domain.add(new DenseVector(-2., -2.));
domain.add(new DenseVector(-1., -1.));
domain.add(new DenseVector(0., 0.)); // the minimizer
domain.add(new DenseVector(1., 1.));
domain.add(new DenseVector(2., 2.));
soln.setDomain(domain);

System.out.println(String.format("f(%s) = %f", soln.minimizer(), soln.min()));

</pre>
<p>输出如下所示:</p>
<pre>f([0.000000, 0.000000] ) = 0.000000

</pre>
<p class="Para" id="Par18">如本例所示，除非我们神奇地知道解，在本例中为(0，0)，并将其放入域列表中，否则暴力搜索不会找到解。但是在这种情况下，如果我们知道答案，我们就不需要优化器了。</p>

<h2 class="Heading">9.2 C <sup> 2 </sup>最优压缩问题</h2>
<p><code>OptimProblem</code>类设置起来有些困难，并且在大多数情况下不方便使用。一个 C <sup> 0 </sup>函数是一个连续函数。A C <sup> 1 </sup>函数是一个平滑函数。C <sup> 2 </sup>函数是连续的、光滑的、有二阶导数的函数。它也被称为二次可微函数。如果我们要优化这样一个函数，<code>C2OptimProblemImpl</code>提供了一个方便的包装器来创建一个<code>OptimProblem</code>对象。其签名如下:</p>
<pre>/**
 * Construct an optimization problem with an objective function.
 * This uses a numerical gradient and a numerical Hessian, if needed.
 *
 * @param f the objective function to be minimized
 */
public C2OptimProblemImpl(RealScalarFunction f)

</pre>
<p class="Para" id="Par20">我们可以用更简单的代码重写前面的例子。</p>
<p>对于一元函数<em class="EmphasisTypeItalic ">f</em>(<em class="EmphasisTypeItalic ">x</em>)=<em class="EmphasisTypeItalic ">x</em>T6】24，我们有这个:</p>
<pre>// set up the solver to use and the solution
DoubleBruteForceMinimizer solver = new DoubleBruteForceMinimizer(false);
BruteForceMinimizer.Solution soln
        = solver.solve(new C2OptimProblemImpl(new Polynomial(1, 0, -4))); // f(x) = x^2 - 4

// for brute force search, we need to explicitly enumerate the values in the domain
List&lt;Vector&gt; domain = new ArrayList&lt;&gt;();
domain.add(new DenseVector(-2.));
domain.add(new DenseVector(-1.));
domain.add(new DenseVector(0.)); // the minimizer
domain.add(new DenseVector(1.));
domain.add(new DenseVector(2.));
soln.setDomain(domain);

System.out.println(String.format("f(%s) = %f", soln.minimizer(), soln.min()));

</pre>
<p>输出如下所示:</p>
<pre>f([0.000000] ) = -4.000000

</pre>
<p>对于一个多元函数<em class="EmphasisTypeItalic "> f </em> ( <em class="EmphasisTypeItalic "> x </em>，<em class="EmphasisTypeItalic ">y</em>)=<em class="EmphasisTypeItalic ">x</em><sup>2</sup>+<em class="EmphasisTypeItalic ">y</em><sup>2</sup>，我们有这样的:</p>
<pre>DoubleBruteForceMinimizer bf = new DoubleBruteForceMinimizer(true);
BruteForceMinimizer.Solution soln = bf.solve(
        new C2OptimProblemImpl(
                new AbstractBivariateRealFunction() {
            @Override
            public double evaluate(double x, double y) {
                double fx = x * x + y * y;
                return fx;
            }
        }));

List&lt;Vector&gt; domain = new ArrayList&lt;&gt;();
domain.add(new DenseVector(-2., -2.));
domain.add(new DenseVector(-1., -1.));
domain.add(new DenseVector(0., 0.)); // the minimizer
domain.add(new DenseVector(1., 1.));
domain.add(new DenseVector(2., 2.));
soln.setDomain(domain);

System.out.println(String.format("f(%s) = %f", soln.minimizer(), soln.min()));

</pre>
<p>输出如下所示:</p>
<pre>f([0.000000, 0.000000] ) = 0.000000

</pre>

<h2 class="Heading">9.3 分类方法</h2>
<p class="Para" id="Par25">一元方法解决一元无约束优化问题。快速单变量方法在最优化中是至关重要的。一个多变量约束优化问题通常可以转化为一个等价的多变量无约束优化问题。求解多变量无约束优化问题通常涉及求解多个单变量无约束优化问题，例如线搜索。在后面的章节中讨论的许多多元无约束优化算法使用线搜索。</p>
<p class="Para" id="Par26">一种最简单的单变量算法是括号搜索法。假设我们有一个包含最小值的三点包围区间，<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">L</em></sub>&lt;<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">a</em></sub>&lt;<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">U</em></sub>，并假设[<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">L</em></sub><em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">U</em>我们计算第四个点，<em class="EmphasisTypeItalic "> x </em> <sub> <em class="EmphasisTypeItalic "> b </em> </sub>，根据一个区间划分表，形成两个重叠的子区间，<em class="EmphasisTypeItalic ">x</em><sub>T39】L</sub>，<em class="EmphasisTypeItalic "> x </em> <sub> <em class="EmphasisTypeItalic "> a </em> </sub>和<em class="EmphasisTypeItalic ">x</em><sub>b</sub>，<em class="EmphasisTypeItalic ">x<em class="EmphasisTypeItalic ">选择包含新最小值的较小子区间。重复该过程以划分区间，直到收敛，即当所选择的子区间足够小时。该算法对于区间[<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">L</em></sub>，<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">U</em></sub>]内的单峰函数最为有效。单峰函数是从<em class="EmphasisTypeItalic "> x </em> <sub> <em class="EmphasisTypeItalic "> L </em> </sub>增加(或减少)到某一点，然后向<em class="EmphasisTypeItalic "> x </em> <sub> <em class="EmphasisTypeItalic "> U </em> </sub>减少(增加)的函数。它在区间中正好包含一个最小值(或最大值)。</em></em></sub></p>
<p>考虑在[ <em class="EmphasisTypeItalic "> x </em> <sub> <em class="EmphasisTypeItalic "> L </em> </sub>，<em class="EmphasisTypeItalic "> x </em> <sub> <em class="EmphasisTypeItalic "> U </em> </sub> ]中有最小值的单峰函数。这个区间称为不确定度范围。用四个点，<em class="EmphasisTypeItalic ">f</em>(<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">L</em></sub>)，<em class="EmphasisTypeItalic ">f</em>(<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">a</em></sub>)，<em class="EmphasisTypeItalic ">f</em>(<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">b</em></sub>)，以及<em class="EmphasisTypeItalic "> f </em> ( <em class="EmphasisTypeItalic "> x 【T39)一个二分法搜索说如果<em class="EmphasisTypeItalic ">f</em>(<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">a</em></sub>)&lt;<em class="EmphasisTypeItalic ">f</em>(<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">b</em></sub>)，<em class="EmphasisTypeItalic ">x</em><sup>∫</sup>位于<em class="EmphasisTypeItalic "> x </em> <sub> <em class="EmphasisTypeItalic ">有两种可能:要么是蓝色曲线中的<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">L</em></sub>&lt;<em class="EmphasisTypeItalic ">x</em><sup>∫</sup>&lt;<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">b</em></sub>要么是蓝色曲线中的<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">a</em></sub>&lt;<em class="EmphasisTypeItalic ">x</em>将它们组合起来就得到<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">L</em></sub>&lt;<em class="EmphasisTypeItalic ">x</em><sup>∵</sup>&lt;<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">b</em></sub>。见图<a href="#Fig1"> 9-1 </a>。</em></sub></em></p>
<p><img alt="../images/500382_1_En_9_Chapter/500382_1_En_9_Fig1_HTML.png" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Fig1_HTML.png" style="width:26.3em"/></p>
<p>图 9-1</p><p class="SimplePara">左侧音程中的<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>∫</sup></p>


<p>同样，如果<em class="EmphasisTypeItalic ">f</em>(<em class="EmphasisTypeItalic ">x</em><sub>T5】a</sub>)&gt;<em class="EmphasisTypeItalic ">f</em>(<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">b</em></sub>)，我们就有<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">a</em></sub>&lt;<em class="EmphasisTypeItalic ">x</em><sup>∵</sup>&lt;<em class="EmphasisTypeItalic ">x</em><sub><sub/></sub></p>
<p><img alt="../images/500382_1_En_9_Chapter/500382_1_En_9_Fig2_HTML.png" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Fig2_HTML.png" style="width:26.05em"/></p>
<p>图 9-2</p><p class="SimplePara"><em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup>∫</sup>在正确的区间</p>


<p>如果<em class="EmphasisTypeItalic ">f</em>(<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">【a】</em></sub>=<em class="EmphasisTypeItalic ">【f】</em>(<em class="EmphasisTypeItalic ">x</em><em class="EmphasisTypeItalic ">见图<a href="#Fig3"> 9-3 </a>。</em></p>
<p><img alt="../images/500382_1_En_9_Chapter/500382_1_En_9_Fig3_HTML.png" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Fig3_HTML.png" style="width:26.05em"/></p>
<p>图 9-3</p><p class="SimplePara"><em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">【a】</strong></em></sub>&lt;<strong class="EmphasisTypeBoldItalic "/></p>


<p class="Para" id="Par30">一个简单的二分法搜索每次可以简单地将区间粗略地分成两部分。(子)间隔的大小呈指数下降(每次迭代减半)。在大约七次迭代之后，间隔大小减小到小于初始间隔的 1%。由于每次迭代需要两次计算，总共有 14 次函数求值，<em class="EmphasisTypeItalic ">f</em>(<em class="EmphasisTypeItalic ">x</em><sub>T5】a</sub>)和<em class="EmphasisTypeItalic ">f</em>(<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">b</em></sub>)。NM Dev 实现了两个更复杂的时间间隔划分计划。</p>
<h3 class="Heading">9.3.1 斐波那契搜索法</h3>
<p>让我们从分析区间大小开始。为了简单起见，让我们假设左右子区间长度相同。参见图<a href="#Fig4"> 9-4 </a>。</p>
<p><img alt="../images/500382_1_En_9_Chapter/500382_1_En_9_Fig4_HTML.jpg" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Fig4_HTML.jpg" style="width:25.8em"/></p>
<p>图 9-4</p><p class="SimplePara">迭代 k，k+1，k+2 中的子间隔</p>


<p>我们有以下:<p> <img alt="$$ {I}_{k+1}={I}_{k+1}^L={I}_{k+1}^R $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equb.png" style="width:7.64em"/> </p> <p> <img alt="$$ {I}_k={I}_{k+1}^L+{I}_{k+2}^R={I}_{k+1}+{I}_{k+2} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equc.png" style="width:12.33em"/> </p></p>
<p>对于<em class="EmphasisTypeItalic "> n </em>次迭代，我们有如下递归关系:<p><img alt="$$ {I}_1={I}_2+{I}_3 $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equd.png" style="width:5.06em"/></p><p><img alt="$$ {I}_2={I}_3+{I}_4 $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Eque.png" style="width:5.06em"/></p><p><img alt="$$ \cdots $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equf.png" style="width:1.59em"/></p><p><img alt="$$ {I}_n={I}_{n+1}+{I}_{n+2} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equg.png" style="width:6.75em"/></p></p>
<p class="Para" id="Par34">有<em class="EmphasisTypeItalic "> n </em>个方程，我们假设<em class="EmphasisTypeItalic ">I</em>T4】1 已知。有<em class="EmphasisTypeItalic "> n </em> + 2 个变量。我们可以附加一个条件来产生一系列的间隔。</p>
<p class="Para" id="Par35">特别是，如果我们假设最后一次迭代中的区间为零，就会生成斐波那契数列。即<em class="EmphasisTypeItalic "> I </em> <sub> <em class="EmphasisTypeItalic "> n </em> + 2 </sub> = 0。</p>
<p>求解前面的方程组给出如下:<p><img alt="$$ {I}_{n+1}={F}_0{I}_n $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equh.png" style="width:5.07em"/></p><p><img alt="$$ {I}_n={F}_0{I}_n $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equi.png" style="width:4.22em"/></p><p><img alt="$$ {I}_k={F}_{n-k+1}{I}_n $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equj.png" style="width:5.85em"/></p><p><img alt="$$ {I}_1={F}_n{I}_n $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equk.png" style="width:4.22em"/></p></p>
<p>其中<p> <img alt="$$ {F}_k={F}_{k-1}+{F}_{k-2},k\ge 2 $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equl.png" style="width:10.41em"/> </p></p>
<p class="Para" id="Par38"><em class="EmphasisTypeItalic ">F</em><sub>T3】kT5】是斐波那契数列的定义，{1，1，2，3，5，8，13，…}。</sub></p>
<p class="Para" id="Par39">例如，如果<em class="EmphasisTypeItalic "> n </em> = 11，<em class="EmphasisTypeItalic ">F</em><sub>T5】n</sub>= 144。<em class="EmphasisTypeItalic ">I</em><sub><em class="EmphasisTypeItalic ">n</em></sub>=<em class="EmphasisTypeItalic ">I</em><sub>1</sub>/<em class="EmphasisTypeItalic ">F</em><sub><em class="EmphasisTypeItalic ">n</em></sub>。<em class="EmphasisTypeItalic "> I </em> <sub> <em class="EmphasisTypeItalic "> n </em> </sub>降为<em class="EmphasisTypeItalic "> I </em> <sub> 1 </sub>的 0.69%。此外，通过选择左子区间或右子区间，我们已经有了两个端点和中间的一个点，{<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">L</em></sub>，<em class="EmphasisTypeItalic "> x </em> <sub> <em class="EmphasisTypeItalic "> a </em> </sub>，<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">b</em></sub>}或{<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">a</em></sub>，<em class="EmphasisTypeItalic ">我们只需要多一个点，因此只需要计算一次函数。对于 11 次迭代，我们总共需要 14 次计算，而在简单的二分法搜索中需要 22 次。</em></p>
<p class="Para" id="Par40">使用斐波那契数列的一个主要缺点是它需要预知<em class="EmphasisTypeItalic "> n </em>。如果我们想让<em class="EmphasisTypeItalic ">x</em><sup>∫</sup>在预定的精度内，这是可以的。我们用<em class="EmphasisTypeItalic ">I</em><sub><em class="EmphasisTypeItalic ">n</em></sub>=<em class="EmphasisTypeItalic ">I</em><sub>1</sub>/<em class="EmphasisTypeItalic ">F</em><sub><em class="EmphasisTypeItalic ">n</em></sub>确定<em class="EmphasisTypeItalic "> n </em>。但是，如果我们要使<em class="EmphasisTypeItalic ">f</em>(<em class="EmphasisTypeItalic ">x</em><sup>∫</sup>)在一定精度内，我们不知道从什么<em class="EmphasisTypeItalic "> n </em>开始。</p>

<h3 class="Heading">黄金分割搜索</h3>
<p>与斐波纳契曲线不同，黄金分割搜索会进行迭代，直到达到函数精度。递归关系与斐波那契方法相同:<em class="EmphasisTypeItalic ">I</em><sub><em class="EmphasisTypeItalic ">k</em></sub>=<em class="EmphasisTypeItalic ">I</em><sub><em class="EmphasisTypeItalic ">k</em>—1</sub>+<em class="EmphasisTypeItalic ">I</em><sub><em class="EmphasisTypeItalic ">k</em>—2</sub>。附加条件是保持两个相邻区间的比值不变。<p> <img alt="$$ \frac{I_k}{I_{k+1}}=K,\forall k $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equm.png" style="width:5.6em"/> </p></p>
<p>利用这两个方程，我们可以解出<em class="EmphasisTypeItalic "> K </em>，如下图:<p> <img alt="$$ {K}^2=K+1 $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equn.png" style="width:5.24em"/> </p> <p> <img alt="$$ K=\frac{1+\sqrt{5}}{2}\approx 1.618034 $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equo.png" style="width:10.96em"/> </p></p>
<p>这个数字被称为黄金比例。音程顺序如下:<p> <img alt="$$ \left\{\ {I}_1,\frac{I_1}{K},\frac{I_1}{K^2},\dots \dots, \frac{I_1}{K^{n-1}}\right\} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equp.png" style="width:11.26em"/> </p></p>
<p class="Para" id="Par44">举个例子:{100，61.8，38.2，23.6，14.6，9，…}。这可以在没有预定的<em class="EmphasisTypeItalic "> n </em>的情况下继续进行。</p>
<p>我们可以比较黄金分割搜索和斐波那契搜索的效率。<em class="EmphasisTypeItalic ">F</em><sub>T3】nT5】和<em class="EmphasisTypeItalic "> K </em>的关系为<img alt="$$ {F}_n\approx \frac{K^{n+1}}{\sqrt{5}} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_IEq1.png" style="width:4.4em"/>。斐波纳契搜索中的不确定区域如下:<p> <img alt="$$ {\Lambda}_F={I}_n=\frac{I_1}{F_n}\approx \frac{\sqrt{5}}{K^{n+1}}{I}_1 $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equq.png" style="width:10.47em"/> </p></sub></p>
<p>对于黄金分割搜索，如下:<p> <img alt="$$ {\Lambda}_G={I}_n=\frac{I_1}{K^{n+1}} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equr.png" style="width:7.16em"/> </p></p>
<p>于是，我们有了下面的:<p> <img alt="$$ \frac{\Lambda_G}{\Lambda_F}=\frac{K^2}{\sqrt{5}}=1.17 $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equs.png" style="width:7.8em"/> </p></p>
<p class="Para" id="Par48">因此，黄金分割搜索需要更多的迭代才能达到与斐波那契搜索相同的精度。斐波那契搜索更有效，但是黄金分割搜索不需要先验知识。</p>

<h3 class="Heading">布伦特的搜索</h3>
<p>黄金分割搜索具有线性收敛速度。我们即将讨论的抛物搜索的收敛速度为 1.325；因此，效率更高。在每个包围区间，我们再次选择三个点:<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">L</em></sub>&lt;<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">m</em></sub>&lt;<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">U</em></sub>。我们用二次多项式(又名抛物线)来近似这个区间中的函数。<p> <img alt="$$ p(x)={a}_0+{a}_1x+{a}_2{x}^2 $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equt.png" style="width:10.02em"/> </p></p>
<p>有了这三点，我们就可以通过求解三个线性方程组来确定多项式系数。<em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">x</em>)<img alt="$$ \overline{x} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_IEq2.png" style="width:0.78em"/>的最小值可以通过求解其一阶导数等于零来容易地确定。<p> <img alt="$$ \overline{x}=-\frac{a_1}{2{a}_2} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equu.png" style="width:4.36em"/> </p></p>
<p>借助泰勒展开，<img alt="$$ \overline{x} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_IEq3.png" style="width:0.78em"/>将充分接近<em class="EmphasisTypeItalic ">x</em><sup>√</sup>；事实上，如果函数是二次的，它将是精确的。通过检查<img alt="$$ \overline{x} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_IEq4.png" style="width:0.78em"/>，我们可以通过拒绝<em class="EmphasisTypeItalic "> x </em> <sub> <em class="EmphasisTypeItalic "> L </em> </sub>或<em class="EmphasisTypeItalic "> x </em> <sub> <em class="EmphasisTypeItalic "> U </em> </sub>并保留<img alt="$$ \overline{x} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_IEq5.png" style="width:0.78em"/>来选择新的间隔。重复该过程，直到所有三个点足够接近(到<em class="EmphasisTypeItalic ">x</em>T21】∫)。见图<a href="#Fig5"> 9-5 </a>。</p>
<p><img alt="../images/500382_1_En_9_Chapter/500382_1_En_9_Fig5_HTML.png" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Fig5_HTML.png" style="width:26.05em"/></p>
<p>图 9-5</p><p class="SimplePara">曲线段的抛物线近似</p>


<p class="Para" id="Par52">虽然抛物线搜索具有超线性收敛速度，但它并不总是收敛(甚至收敛到局部最小值)。例如，如果三个点共线，则生成的抛物线会退化，因此不会提供新的候选点。因此，这种方法很少单独使用。</p>
<p>布伦特的方法结合了抛物线搜索和黄金分割搜索。Brent 的方法使用抛物线搜索(例如，没有两个点是相同的)。当条件失败时，它切换到使用黄金分割搜索。换句话说，布伦特的方法结合了两个世界的优点:超线性收敛和找到最小值的保证。因此，在实践中，我们几乎总是使用 Brent 的搜索作为单变量优化的默认。参见图<a href="#Fig6"> 9-6 </a>。</p>
<p><img alt="../images/500382_1_En_9_Chapter/500382_1_En_9_Fig6_HTML.jpg" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Fig6_HTML.jpg" style="width:23.88em"/></p>
<p>图 9-6</p><p class="SimplePara">对数伽马函数</p>


<p>以下 NM Dev 代码使用斐波那契搜索、黄金分割搜索和布伦特搜索来求解 log-gamma 函数:</p>
<pre>LogGamma logGamma = new LogGamma(); // the log-gamma function

BracketSearchMinimizer solver1 = new FibonaccMinimizer(1e-8, 15);
UnivariateMinimizer.Solution soln1 = solver1.solve(logGamma);
double x_min_1 = soln1.search(0, 5);
System.out.println(String.format("f(%f) = %f", x_min_1, logGamma.evaluate(x_min_1)));

BracketSearchMinimizer solver2 = new GoldenMinimizer(1e-8, 15);
UnivariateMinimizer.Solution soln2 = solver2.solve(logGamma);
double x_min_2 = soln2.search(0, 5);
System.out.println(String.format("f(%f) = %f", x_min_2, logGamma.evaluate(x_min_2)));

BracketSearchMinimizer solver3 = new BrentMinimizer(1e-8, 10);
UnivariateMinimizer.Solution soln3 = solver3.solve(logGamma);
double x_min_3 = soln3.search(0, 5);
System.out.println(String.format("f(%f) = %f", x_min_3, logGamma.evaluate(x_min_3)));

</pre>
<p>输出如下所示:</p>
<pre>f(1.463682) = -0.121484
f(1.460813) = -0.121486
f(1.461632) = -0.121486

</pre>
<p>他们的签名如下:</p>
<pre>/**
 * Construct a univariate minimizer using the Fibonacci method.
 *
 * @param epsilon       a precision parameter: when a number |x| &amp;le;
 *                      &amp;epsilon;, it is considered 0
 * @param maxIterations the maximum number of iterations
 */
public FibonaccMinimizer(double epsilon, int maxIterations)

/**
 * Construct a univariate minimizer using the Golden method.
 *
 * @param epsilon       a precision parameter: when a number |x| &amp;le;
 *                      &amp;epsilon;, it is considered 0
 * @param maxIterations the maximum number of iterations
 */
public GoldenMinimizer(double epsilon, int maxIterations)

/**
 * Construct a univariate minimizer using Brent's algorithm.
 *
 * @param epsilon       a precision parameter: when a number |x| &amp;le;
 *                      &amp;epsilon;, it is considered 0
 * @param maxIterations the maximum number of iterations
 */
public BrentMinimizer(double epsilon, int maxIterations)

/**
 * This is the solution to a univariate minimization problem.
 */
public static interface Solution extends MinimizationSolution&lt;Double&gt; {

    /**
     * Search for a minimum within the interval &lt;i&gt;[lower, upper]&lt;/i&gt;.
     *
     * @param lower   the lower bound for the bracketing interval which
     *                contains a minimum
     * @param initial an initial guess
     * @param upper   the upper bound for the bracketing interval which
     *                contains a minimum
     * @return an approximate minimizer
     */
    public double search(double lower, double initial, double upper);

    /**
     * Search for a minimum within the interval &lt;i&gt;[lower, upper]&lt;/i&gt;.
     *
     * @param lower the lower bound for the bracketing interval which
     *              contains a minimum
     * @param upper the upper bound for the bracketing interval which
     *              contains a minimum
     * @return an approximate minimizer
     */
    public double search(double lower, double upper);
}

</pre>
<p class="Para" id="Par57">我们称之为<code>search</code>方法，寻找一元函数最小化的极小值。</p>


<h2 class="Heading">9.4 最速下降法</h2>
<p class="Para" id="Par58">搜索最小化的一般思想通常从一个初始点开始，然后修改一些参数来寻找下一个(希望)给出更小函数值的点。如果我们没有信息，盲目的搜索是没有效率的。两条信息是有用的:梯度<em> <strong class="EmphasisTypeBoldItalic "> g </strong> </em>和黑森<em> <strong class="EmphasisTypeBoldItalic "> H </strong> </em>。</p>
<p>最速下降算法是一系列算法，使用梯度和 Hessian 信息在函数值减少最大的方向上进行搜索。我们首先讨论只使用梯度信息的一阶方法。考虑以下优化问题:<p> <img alt="$$ \min F=f(x)\kern0.75em for\ x\in {E}^n $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equv.png" style="width:11.52em"/> </p></p>
<p class="Para" id="Par60">泰勒展开式给出了以下内容:</p>
<p><em class="EmphasisTypeItalic ">f</em>+f<em class="EmphasisTypeItalic "/>=<em class="EmphasisTypeItalic ">f</em>(<em><strong class="EmphasisTypeBoldItalic "/></em>+<strong class="EmphasisTypeBoldItalic "/> as【<em><strong class="EmphasisTypeBoldItalic "/></em>→【0.0】<em class="EmphasisTypeItalic ">【f】</em><em><strong class="EmphasisTypeBoldItalic ">【g】</strong></em><sup><em class="EmphasisTypeItalic ">】</em></sup></p>
<p>对于任意矢量<em> <strong class="EmphasisTypeBoldItalic "> δ </strong> </em>，当<em class="EmphasisTypeItalic "> θ </em> = 0 时，δ<em class="EmphasisTypeItalic ">F</em>最大，这意味着<em> <strong class="EmphasisTypeBoldItalic "> δ </strong> </em>与<em> <strong class="EmphasisTypeBoldItalic "> g </strong> </em>同向。当<em class="EmphasisTypeItalic "> θ </em> = <em class="EmphasisTypeItalic "> π </em>时，δ<em class="EmphasisTypeItalic ">F</em>最小，这意味着<em> <strong class="EmphasisTypeBoldItalic "> δ </strong> </em>与<em> <strong class="EmphasisTypeBoldItalic "> g </strong> </em>方向相反或者与—<em><strong class="EmphasisTypeBoldItalic ">g</strong></em>方向相反。据说它们分别是最陡上升和最陡下降方向，如图<a href="#Fig7"> 9-7 </a>所示。</p>
<p><img alt="../images/500382_1_En_9_Chapter/500382_1_En_9_Fig7_HTML.jpg" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Fig7_HTML.jpg" style="width:26.75em"/></p>
<p>图 9-7</p><p class="SimplePara">最陡下降和最陡上升方向</p>


<p>最速下降算法的一般框架如下:</p>
<ol><li class="ListItem"><p class="Para" id="Par64">指定一个初始点<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub><strong class="EmphasisTypeBold ">0</strong></sub>和公差<em class="EmphasisTypeItalic "> ε </em>。</p>
 </li>
<li class="ListItem"><p class="Para" id="Par65">计算渐变<em><strong class="EmphasisTypeBoldItalic ">g</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">k</strong></em></sub>并设置牛顿方向向量<em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">k</strong></em></sub>=<em><strong class="EmphasisTypeBoldItalic ">g</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">k</strong></em></sub>。</p>
 </li>
<li class="ListItem"><p class="Para" id="Par66">最小化<em class="EmphasisTypeItalic ">f</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">k</strong></em></sub>+<em class="EmphasisTypeItalic ">α</em><em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">k</strong></em></sub>)相对于<em class="EmphasisTypeItalic "> α </em>计算增量<em class="EmphasisTypeItalic "> α </em> <sub> <em class="EmphasisTypeItalic "> k </em> </sub>。</p>
 </li>
<li class="ListItem"><p class="Para" id="Par67">设置下一个点<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub>T5】k+1</sub>=<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub><em class="EmphasisTypeItalic ">k</em></sub>+<em class="EmphasisTypeItalic ">α</em><sub><em class="EmphasisTypeItalic ">k</em></sub><em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><em class="EmphasisTypeItalic ">k</em></sub>并计算下一个函数值<em class="EmphasisTypeItalic "> f </em> <sub/></p>
 </li>
<li class="ListItem">如果∩<em class="EmphasisTypeItalic ">α</em><sub><em class="EmphasisTypeItalic ">k</em></sub><em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><em class="EmphasisTypeItalic ">k</em></sub>∩&lt;<em class="EmphasisTypeItalic ">ε</em>，那么<ol><li class="ListItem"><p class="Para" id="Par69">完成了。<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup><code>∗</code></sup>=<em><strong class="EmphasisTypeBoldItalic "/><sub><sub/></sub></em>+1【t】</p>
 </li>
<li class="ListItem"><p class="Para" id="Par70">否则，重复步骤 2。</p>
 </li>
</ol>

 </li>
</ol>

<p class="Para" id="Par71">在步骤 3 中，<em class="EmphasisTypeItalic "> α </em> <sub> <em class="EmphasisTypeItalic "> k </em> </sub>可以通过使用单变量优化算法的线搜索来计算。在我们的实现中，我们使用弗莱彻的不精确线搜索方法。许多优化算法被证明对线搜索的不精确性相当宽容。或者，<em class="EmphasisTypeItalic "> α </em> <sub> <em class="EmphasisTypeItalic "> k </em> </sub>可以解析计算。</p>
<p>以下 NM Dev 代码使用一阶最速下降法最小化该函数:<p> <img alt="$$ f\left({x}_1,{x}_2,{x}_3,{x}_4\right)={\left({x}_1-4{x}_2\right)}^4+12{\left({x}_3-{x}_4\right)}^4+3{\left({x}_2-10{x}_3\right)}^2+55{\left({x}_1-2{x}_4\right)}^2 $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equx.png" style="width:33.07em"/> </p></p>
<p>
 
</p>
<pre>// the objective function
//  the global minimizer is at x = [0,0,0,0]
RealScalarFunction f = new RealScalarFunction() {

    @Override
    public Double evaluate(Vector x) {
        double x1 = x.get(1);
        double x2 = x.get(2);
        double x3 = x.get(3);
        double x4 = x.get(4);

        double result = pow(x1 - 4 * x2, 4);
        result += 12 * pow(x3 - x4, 4);
        result += 3 * pow(x2 - 10 * x3, 2);
        result += 55 * pow(x1 - 2 * x4, 2);

        return result;
    }

    @Override
    public int dimensionOfDomain() {
        return 4;
    }

    @Override
    public int dimensionOfRange() {
        return 1;
    }
};

// the gradient function
RealVectorFunction g = new RealVectorFunction() {

    @Override
    public Vector evaluate(Vector x) {
        double x1 = x.get(1);
        double x2 = x.get(2);
        double x3 = x.get(3);
        double x4 = x.get(4);

        double[] result = new double[4];
        result[0] = 4 * pow(x1 - 4 * x2, 3) + 110 * (x1 - 2 * x4);
        result[1] = -16 * pow(x1 - 4 * x2, 3) + 6 * (x2 - 10 * x3);
        result[2] = 48 * pow(x3 - x4, 3) - 60 * (x2 - 10 * x3);
        result[3] = -48 * pow(x3 - x4, 3) - 220 * (x1 - 2 * x4);
        return new DenseVector(result);
    }

    @Override
    public int dimensionOfDomain() {
        return 4;
    }

    @Override
    public int dimensionOfRange() {
        return 4;
    }
};

C2OptimProblem problem = new C2OptimProblemImpl(f, g); // only gradient information
SteepestDescentMinimizer firstOrderMinimizer
        = new FirstOrderMinimizer(
                FirstOrderMinimizer.Method.IN_EXACT_LINE_SEARCH, // FirstOrderMinimizer.Method.ANALYTIC
                1e-8,
                40000
        );
IterativeSolution&lt;Vector&gt; soln = firstOrderMinimizer.solve(problem);

Vector xmin = soln.search(new DenseVector(new double[]{1, -1, -1, 1}));
double f_xmin = f.evaluate(xmin);
System.out.println(String.format("f(%s) = %f", xmin.toString(), f_xmin));

</pre>
<p>输出如下所示:</p>
<pre>f([0.046380, 0.016330, 0.001634, 0.023189] ) = 0.000003

</pre>
<p>大约需要 40，000 次迭代，这太慢了。</p>
<pre>The signature of FirstOrderMinimizer is as follows.
/**
 * Construct a multivariate minimizer using the First-Order method.
 *
 * @param method        the method to do line search
 * @param epsilon       a precision parameter: when a number |x| &amp;le;
 *                      &amp;epsilon;, it is considered 0
 * @param maxIterations the maximum number of iterations
 */
public FirstOrderMinimizer(Method method, double epsilon, int maxIterations)

</pre>
<p class="Para" id="Par76"><code>Method</code>指定我们是使用线搜索还是解析表达式来计算<em class="EmphasisTypeItalic "> α </em> <sub> <em class="EmphasisTypeItalic "> k </em> </sub>。</p>
<h3 class="Heading">9.4.1 牛顿-拉夫森法</h3>
<p>二阶方法除了梯度之外还使用(非奇异的)Hessian 来确定最速下降框架中的方向向量<em><strong class="EmphasisTypeBoldItalic "/></em><sub><em class="EmphasisTypeItalic ">k</em></sub>和增量<em class="EmphasisTypeItalic "> α </em> <sub> <em class="EmphasisTypeItalic "> k </em> </sub>以加速收敛。牛顿-拉夫森方法将泰勒级数展开到二阶。设<em> <strong class="EmphasisTypeBoldItalic "> δ </strong> </em>为<em> <strong class="EmphasisTypeBoldItalic "> x </strong> </em>的变化，我们有如下:<p> <img alt="$$ f\left(x+\delta \right)\approx f(x)+\sum \limits_{j=1}^n\frac{\partial f}{\partial {x}_i}{\delta}_i+\frac{1}{2}\sum \limits_{i=1}^n\sum \limits_{j=1}^n\frac{\partial^2f}{\partial {x}_i\partial {x}_j}{\delta}_i{\delta}_j $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equy.png" style="width:21.81em"/> </p></p>
<p>区分<em class="EmphasisTypeItalic ">f</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>+<em><strong class="EmphasisTypeBoldItalic ">δ</strong></em>)相对于<em class="EmphasisTypeItalic "> δ </em> <sub> <em class="EmphasisTypeItalic "> k </em> </sub>为了最小化<em class="EmphasisTypeItalic ">f</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>+<em><strong class="EmphasisTypeBoldItalic ">δ</strong>【T25)，我们有如下:<p> <img alt="$$ \frac{\partial f}{\partial {x}_k}+\sum \limits_{i=1}^n\frac{\partial^2f}{\partial {x}_i\partial {x}_j}{\delta}_i=0\kern0.75em for\ k=1,2,\dots, n $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equz.png" style="width:18.84em"/> </p></em></p>
<p>用矩阵形式改写如下:<p> <img alt="$$ g=-\boldsymbol{H}\delta $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equaa.png" style="width:4.16em"/> </p></p>
<p><em> <strong class="EmphasisTypeBoldItalic "> x </strong> </em>中的最优变化如下:<p> <img alt="$$ \delta =-{\boldsymbol{H}}^{-\mathbf{1}}g $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equab.png" style="width:5.04em"/> </p></p>
<p>使用线搜索来计算沿着方向的<em class="EmphasisTypeItalic "> f </em> ( <em> <strong class="EmphasisTypeBoldItalic "> x </strong> </em>)的最大减少量。下一点<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">k+1</strong></em></sub>如下:<p> <img alt="$$ {x}_{k+1}={x}_k+{\delta}_k={x}_k+{\alpha}_k{d}_k $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equac.png" style="width:11.5em"/> </p></p>
<p>以下哪一项是正确的:<p> <img alt="$$ {d}_k=-{\boldsymbol{H}}_k^{-1}{g}_k $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equad.png" style="width:5.82em"/> </p></p>
<p class="Para" id="Par83"><em class="EmphasisTypeItalic ">α</em><sub>T3】kT5】就是最小化<em class="EmphasisTypeItalic "> f </em>的<em class="EmphasisTypeItalic ">α</em>(<em class="EmphasisTypeItalic ">x</em><sub><em class="EmphasisTypeItalic ">k</em></sub>+<em class="EmphasisTypeItalic ">α</em><em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><em class="EmphasisTypeItalic ">k</em>【T25)。</sub></sub></p>
<p>与一阶方法相比，牛顿-拉夫森方法使用 Hessian 信息来加速收敛，特别是当它接近解时。以下 NM Dev 代码使用 Newton-Raphson 方法最小化相同的函数:</p>
<pre>// the objective function
// the global minimizer is at x = [0,0,0,0]
RealScalarFunction f = new RealScalarFunction() {

    @Override
    public Double evaluate(Vector x) {
        double x1 = x.get(1);
        double x2 = x.get(2);
        double x3 = x.get(3);
        double x4 = x.get(4);

        double result = pow(x1 - 4 * x2, 4);
        result += 12 * pow(x3 - x4, 4);
        result += 3 * pow(x2 - 10 * x3, 2);
        result += 55 * pow(x1 - 2 * x4, 2);

        return result;
    }

    @Override
    public int dimensionOfDomain() {
        return 4;
    }

    @Override
    public int dimensionOfRange() {
        return 1;
    }
};

// the gradient function
RealVectorFunction g = new RealVectorFunction() {

    @Override
    public Vector evaluate(Vector x) {
        double x1 = x.get(1);
        double x2 = x.get(2);
        double x3 = x.get(3);
        double x4 = x.get(4);

        double[] result = new double[4];
        result[0] = 4 * pow(x1 - 4 * x2, 3) + 110 * (x1 - 2 * x4);
        result[1] = -16 * pow(x1 - 4 * x2, 3) + 6 * (x2 - 10 * x3);
        result[2] = 48 * pow(x3 - x4, 3) - 60 * (x2 - 10 * x3);
        result[3] = -48 * pow(x3 - x4, 3) - 220 * (x1 - 2 * x4);
        return new DenseVector(result);
    }

    @Override
    public int dimensionOfDomain() {
        return 4;
    }

    @Override
    public int dimensionOfRange() {
        return 4;
    }
};

C2OptimProblem problem = new C2OptimProblemImpl(f, g); // use numerical Hessian
SteepestDescentMinimizer newtonRaphsonMinimizer
        = new NewtonRaphsonMinimizer(
                1e-8,
                20
        );
IterativeSolution&lt;Vector&gt; soln = newtonRaphsonMinimizer.solve(problem);

Vector xmin = soln.search(new DenseVector(new double[]{1, -1, -1, 1}));
double f_xmin = f.evaluate(xmin);
System.out.println(String.format("f(%s) = %f", xmin.toString(), f_xmin));

</pre>
<p>输出如下所示:</p>
<pre>f([0.000134, -0.000009, -0.000001, 0.000067] ) = 0.000000

</pre>
<p class="Para" id="Par86">值得指出的是，牛顿-拉夫逊方法的收敛速度比一阶方法快得多。与 40，000 次迭代相比，它仅使用 20 次迭代就获得了更高的精度。</p>
<p><code>NewtonRaphsonMinimizer</code>的签名如下:</p>
<pre>/**
 * Construct a multivariate minimizer using the Newton-Raphson method.
 *
 * @param epsilon       a precision parameter: when a number |x| &amp;le;
 *                      &amp;epsilon;, it is considered 0
 * @param maxIterations the maximum number of iterations
 */
public NewtonRaphsonMinimizer(double epsilon, int maxIterations)

</pre>

<h3 class="Heading">高斯-牛顿法</h3>
<p>高斯-牛顿法以如下形式最小化目标函数:<p> <img alt="$$ \boldsymbol{f}={\left[{f}_1(x)\kern0.5em {f}_2(x)\dots \dots \kern0.5em {f}_m(x)\right]}^T $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equae.png" style="width:13.88em"/> </p></p>
<p class="Para" id="Par89">解决方法是一个点<em><strong class="EmphasisTypeBoldItalic ">×点</strong>×点</em>使得所有的<em class="EmphasisTypeItalic ">f</em><sub><em class="EmphasisTypeItalic ">p</em></sub>(<em><strong class="EmphasisTypeBoldItalic ">×点</strong> </em>)同时归零。</p>
<p>我们可以构造一个实值函数<em class="EmphasisTypeItalic "> F </em>，使得如果<em class="EmphasisTypeItalic "> F </em>被最小化，那么函数<em class="EmphasisTypeItalic ">F</em><sub><em class="EmphasisTypeItalic ">p</em></sub>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>)在最小二乘意义上也被最小化。<p> <img alt="$$ F=\sum \limits_{p=1}^m{f}_p{(x)}^2=\boldsymbol{f}{\boldsymbol{f}}^{\boldsymbol{T}} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equaf.png" style="width:9.52em"/> </p></p>
<p>梯度的雅可比矩阵如下:<p> <img alt="$$ \mathbf{J}=\left[\begin{array}{ccc}\frac{\partial {f}_1}{\partial {x}_1}&amp;amp; \cdots &amp;amp; \frac{\partial {f}_m}{\partial {x}_1}\\ {}\vdots &amp;amp; \ddots &amp;amp; \vdots \\ {}\frac{\partial {f}_1}{\partial {x}_n}&amp;amp; \cdots &amp;amp; \frac{\partial {f}_m}{\partial {x}_n}\end{array}\right] $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equag.png" style="width:8.44em"/> </p></p>
<p>区分<em class="EmphasisTypeItalic "> F </em>关于<em class="EmphasisTypeItalic ">x</em><sub>T5】I</sub>，我们有如下:<p> <img alt="$$ \frac{\partial F}{\partial {x}_i}=\sum \limits_{p=1}^m2{f}_p(x)\frac{\partial {f}_p}{\partial {x}_i} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equah.png" style="width:8.92em"/> </p></p>
<p>或者，在矩阵形式中，我们有这样的:<p> <img alt="$$ {g}_F=2{\mathbf{J}}^{\boldsymbol{T}}f $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equai.png" style="width:4.82em"/> </p></p>
<p>再区分一下，我们有以下:<p> <img alt="$$ \frac{\partial^2F}{\partial {x}_i\partial {x}_j}\approx 2\sum \limits_{p=1}^m\frac{\partial {f}_p}{\partial {x}_i}\frac{\partial {f}_p}{\partial {x}_j} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equaj.png" style="width:9.99em"/> </p></p>
<p>黑森<strong class="EmphasisTypeBold ">H</strong><sub>T3】FT5 如下。它必须是非奇异和正定的。<p> <img alt="$$ {\mathbf{H}}_F\approx 2{\mathbf{J}}^{\boldsymbol{T}}\mathbf{J} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equak.png" style="width:5.04em"/> </p></sub></p>
<p>下一个点是由递归关系给出的:<p> <img alt="$$ {x}_{k+1}={x}_k-{\alpha}_k{\left(2{\mathbf{J}}^{\boldsymbol{T}}\mathbf{J}\right)}^{-1}\left(2{\mathbf{J}}^{\boldsymbol{T}}\mathbf{f}\right) $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equal.png" style="width:13.1em"/> </p></p>
<p>以下代码使用高斯-牛顿方法解决了相同的最小化问题:</p>
<pre>// the objective function
// the global minimizer is at x = [0,0,0,0]
RealVectorFunction f = new RealVectorFunction() {

    @Override
    public Vector evaluate(Vector x) {
        double x1 = x.get(1);
        double x2 = x.get(2);
        double x3 = x.get(3);
        double x4 = x.get(4);

        double[] fx = new double[4];
        fx[0] = pow(x1 - 4 * x2, 2);
        fx[1] = sqrt(12) * pow(x3 - x4, 2);
        fx[2] = sqrt(3) * (x2 - 10 * x3);
        fx[3] = sqrt(55) * (x1 - 2 * x4);

        return new DenseVector(fx);
    }

    @Override
    public int dimensionOfDomain() {
        return 4;
    }

    @Override
    public int dimensionOfRange() {
        return 4;
    }
};

// the Jacobian
RntoMatrix J = new RntoMatrix() {

    @Override
    public Matrix evaluate(Vector x) {
        double x1 = x.get(1);
        double x2 = x.get(2);
        double x3 = x.get(3);
        double x4 = x.get(4);

        Matrix Jx = new DenseMatrix(4, 4);

        double value = 2 * (x1 - 4 * x2);
        Jx.set(1, 1, value);

        value = -8 * (x1 - 4 * x2);
        Jx.set(1, 2, value);

        value = 2 * sqrt(12) * (x3 - x4);
        Jx.set(2, 3, value);
        Jx.set(2, 4, -value);

        Jx.set(3, 2, sqrt(3));
        Jx.set(3, 3, -10 * sqrt(3));

        Jx.set(4, 1, sqrt(55));
        Jx.set(4, 4, -2 * sqrt(55));

        return Jx;
    }

    @Override
    public int dimensionOfDomain() {
        return 4;
    }

    @Override
    public int dimensionOfRange() {
        return 1;
    }
};

GaussNewtonMinimizer optim1 = new GaussNewtonMinimizer(1e-8, 10);

IterativeSolution&lt;Vector&gt; soln = optim1.solve(f, J);//analytical gradient

Vector xmin = soln.search(new DenseVector(new double[]{1, -1, -1, 1}));
System.out.println(String.format("f(%s) = %s", xmin.toString(), f.evaluate(xmin).toString()));

</pre>
<p>输出如下所示:</p>
<pre>f([0.000007, -0.000000, -0.000000, 0.000003] ) = [0.000000, 0.000000, -0.000000, -0.000000]

</pre>
<p class="Para" id="Par99">高斯-牛顿法比牛顿-拉夫逊法更有效。前一个问题大约需要 10 次迭代，而前一个问题需要 20 次。</p>
<p><code>GaussNewtonMinimizer</code>的签名如下:</p>
<pre>/**
 * Construct a multivariate minimizer using the Gauss-Newton method.
 *
 * @param epsilon       a precision parameter: when a number |x| &amp;le;
 *                      &amp;epsilon;, it is considered 0
 * @param maxIterations the maximum number of iterations
 */
public GaussNewtonMinimizer(double epsilon, int maxIterations)

</pre>


<h2 class="Heading">9.5 共轭方向法</h2>
<p>最速下降法中的连续搜索方向可能彼此相关，也可能不相关。而且，它们完全由目标函数的局部性质决定，即梯度和 Hessian (Hestenes &amp; Stiefel，1952)。另一方面，在共轭方向法中，连续搜索方向之间有严格的数学关系。共轭方法被开发用于解决具有以下目标函数的二次优化问题:<p> <img alt="$$ \min {\boldsymbol{x}}^{\boldsymbol{T}}\mathbf{b}+\frac{1}{2}{\mathbf{x}}^{\boldsymbol{T}}\boldsymbol{H}\mathbf{X} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equam.png" style="width:8.4em"/> </p></p>
<p class="Para" id="Par102">其中<em> <strong class="EmphasisTypeBoldItalic "> H </strong> </em>为一个<em class="EmphasisTypeItalic "> n </em>变量函数的一个<em class="EmphasisTypeItalic ">n</em>∫<em class="EmphasisTypeItalic ">n</em>对称正定矩阵。</p>
<p class="Para" id="Par103">对于二次问题，搜索将在有限次迭代中收敛到最小值。共轭方法已经被扩展到解决更一般的最优化问题。</p>
<h3 class="Heading">9.5.1 共轭方向</h3>
<p class="Para" id="Par104">对于一个对称矩阵<em> <strong class="EmphasisTypeBoldItalic "> H </strong> </em>，向量的有限集{<em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><strong class="EmphasisTypeBold ">0</strong></sub>，<em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><strong class="EmphasisTypeBold ">1</strong></sub>，<em><strong class="EmphasisTypeBoldItalic "/></em>，<em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">k</strong>如果<em> <strong class="EmphasisTypeBoldItalic "> H </strong> </em>是正定的，那么向量是线性无关的。如果<em> <strong class="EmphasisTypeBoldItalic "> H = I </strong> </em>，则矢量正交。</em></sub></p>
<p>对于之前所示的二次最小化问题，当<em> <strong class="EmphasisTypeBoldItalic "> H </strong> </em>为正定时，简单的微积分表明解是唯一的，并且可以通过解线性方程组来解析计算。<p> <img alt="$$ \boldsymbol{H}{\mathbf{x}}^{\ast}=\mathbf{b} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equan.png" style="width:3.96em"/> </p></p>
<p>因为共轭向量组是线性独立的，因此跨越了<em class="EmphasisTypeItalic "> n </em>维空间<em class="EmphasisTypeItalic ">E</em><sup>T5】n</sup>，<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup><em><strong class="EmphasisTypeBoldItalic ">∑</strong></em></sup>可以写成这些向量的线性组合。<p> <img alt="$$ {\boldsymbol{x}}^{\ast }={\alpha}_0{\boldsymbol{d}}_{\mathbf{0}}+\dots +{\alpha}_{n-1}{\boldsymbol{d}}_{\boldsymbol{n}-\mathbf{1}} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equao.png" style="width:11.83em"/> </p></p>
<p>两边乘以<em> <strong class="EmphasisTypeBoldItalic "> H </strong> </em>并取与<em><strong class="EmphasisTypeBoldItalic "/></em><sub><strong class="EmphasisTypeBoldItalic ">I</strong></sub>的标量积得出:<p> <img alt="$$ {\alpha}_i=-\frac{{\boldsymbol{d}}_{\boldsymbol{i}}^{\boldsymbol{T}}\boldsymbol{H}{\boldsymbol{x}}^{\ast }}{{\boldsymbol{d}}_{\boldsymbol{i}}^{\boldsymbol{T}}\boldsymbol{H}{\boldsymbol{d}}_{\boldsymbol{i}}}=-\frac{{\boldsymbol{d}}_{\boldsymbol{i}}^{\boldsymbol{T}}\boldsymbol{b}}{{\boldsymbol{d}}_{\boldsymbol{i}}^{\boldsymbol{T}}\boldsymbol{H}{\boldsymbol{d}}_{\boldsymbol{i}}} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equap.png" style="width:11.26em"/> </p></p>
<p>或者等价地，<p> <img alt="$$ {\boldsymbol{x}}^{\ast }=\sum \limits_{\boldsymbol{i}=\mathbf{0}}^{\boldsymbol{n}-\mathbf{1}}\frac{{\boldsymbol{d}}_{\boldsymbol{i}}^{\boldsymbol{T}}\boldsymbol{b}}{{\boldsymbol{d}}_{\boldsymbol{i}}^{\boldsymbol{T}}\boldsymbol{H}{\boldsymbol{d}}_{\boldsymbol{i}}}{\boldsymbol{d}}_{\boldsymbol{i}} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equaq.png" style="width:8.02em"/> </p></p>
<p class="Para" id="Par109">这种<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sup><em><strong class="EmphasisTypeBoldItalic ">∫</strong></em></sup>的展开，可以认为是一个求和或者一个<em class="EmphasisTypeItalic ">n</em>-相加<em class="EmphasisTypeItalic ">α</em><sub><em class="EmphasisTypeItalic ">I</em></sub><em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">I</strong></em></sub>的分步迭代过程。</p>
<p class="Para" id="Par110"><strong class="EmphasisTypeBold ">共轭方向定理</strong></p>
<p>设<img alt="$$ {\left\{{\boldsymbol{d}}_{\boldsymbol{i}}\right\}}_{i=0}^{n-1} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_IEq7.png" style="width:3.06em"/>为一组非零的<em><strong class="EmphasisTypeBoldItalic ">H</strong></em>T5】-正交向量。对于任意一个<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub>0</sub>∈<em class="EmphasisTypeItalic ">E</em><sup><em class="EmphasisTypeItalic ">n</em></sup>，序列{<em><strong class="EmphasisTypeBoldItalic ">×x</strong></em><sub><em class="EmphasisTypeItalic ">I</em></sub>}按照以下方式生成:<p> <img alt="$$ {\boldsymbol{x}}_{\boldsymbol{k}+\mathbf{1}}={\boldsymbol{x}}_{\boldsymbol{k}}+{\alpha}_k{\boldsymbol{d}}_{\boldsymbol{k}},k\ge 0 $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equar.png" style="width:10.48em"/> </p></p>
<p>同下面:<p> <img alt="$$ {\alpha}_k=-\frac{{\boldsymbol{g}}_{\boldsymbol{k}}^{\boldsymbol{T}}{\boldsymbol{d}}_{\boldsymbol{k}}}{{\boldsymbol{d}}_{\boldsymbol{k}}^{\boldsymbol{T}}\boldsymbol{H}{\boldsymbol{d}}_{\boldsymbol{k}}} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equas.png" style="width:6.48em"/> </p></p>
<p>还有下面的:<p> <img alt="$$ {\boldsymbol{g}}_{\boldsymbol{k}}=\boldsymbol{b}+\boldsymbol{H}{\boldsymbol{x}}_{\boldsymbol{k}} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equat.png" style="width:6.34em"/> </p></p>
<p class="Para" id="Par114">收敛到唯一解<strong class="EmphasisTypeBold ">x</strong><sup>T3】∫T5<strong class="EmphasisTypeBold ">HX = b</strong>经过<em class="EmphasisTypeItalic "> n </em>步，即<strong class="EmphasisTypeBold ">x</strong><sub><strong class="EmphasisTypeBold ">n</strong></sub><strong class="EmphasisTypeBold ">= x</strong><sup><strong class="EmphasisTypeBold ">∫</strong></sup>。</sup></p>
<p class="Para" id="Par115">利用这个定理和许多产生共轭向量的方法，可以设计出许多共轭方向法。我们还需要扩展这个定理来解决非二次问题。共轭方向方法非常适合最速下降框架。它们的区别仅在于如何计算方向向量<em><strong class="EmphasisTypeBoldItalic ">【d】</strong></em><sub><em class="EmphasisTypeItalic ">k</em></sub>以及随后的星等<em class="EmphasisTypeItalic "> α </em> <sub> <em class="EmphasisTypeItalic "> k </em> </sub>。这里我们用共轭方向代替最陡下降方向。迭代添加一个增量<em class="EmphasisTypeItalic ">α</em><sub><em class="EmphasisTypeItalic ">k</em></sub><em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><em class="EmphasisTypeItalic ">k</em></sub>到一个<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub><em class="EmphasisTypeItalic ">k</em></sub>生成<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub><em class="EmphasisTypeItalic ">k</em>+1</sub>的思路也是一样的。</p>

<h3 class="Heading">共轭梯度法</h3>
<p>Hestenes &amp; Stiefel (1952)提出了共轭梯度法。在这种方法中，新方向是上一个方向和负梯度的加权和。<p><img alt="$$ {\boldsymbol{d}}_{\mathbf{0}}=-{\boldsymbol{g}}_{\mathbf{0}}=-\left(\boldsymbol{b}+\boldsymbol{H}{\boldsymbol{x}}_{\mathbf{0}}\right) $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equau.png" style="width:10.8em"/></p><p><img alt="$$ {\boldsymbol{d}}_{\boldsymbol{k}}=-{\boldsymbol{g}}_{\boldsymbol{k}}+{\beta}_k{\boldsymbol{d}}_{\boldsymbol{k}-\mathbf{1}} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equav.png" style="width:8.38em"/></p><p><img alt="$$ {\boldsymbol{g}}_{\boldsymbol{k}}=\boldsymbol{b}+\boldsymbol{H}{\boldsymbol{x}}_{\boldsymbol{k}} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equaw.png" style="width:6.34em"/></p><p><img alt="$$ {\beta}_k=\frac{{\boldsymbol{g}}_k^TH{d}_k}{{\boldsymbol{d}}_k^TH{d}_k} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equax.png" style="width:5.54em"/></p><p><img alt="$$ {\alpha}_k=-\frac{{\boldsymbol{g}}_k^T{d}_k}{{\boldsymbol{d}}_k^TH{d}_k} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equay.png" style="width:6.28em"/></p></p>
<p>这种方法的优点如下:</p>
<ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par118">梯度是有限的，并且与所有先前的方向向量线性无关，除非找到解。</p></li>
<li><p class="Para" id="Par119">计算新的方向向量相对容易。</p></li>
<li><p class="Para" id="Par120">没有线搜索。</p></li>
<li><p class="Para" id="Par121">对于二次问题，它在<em class="EmphasisTypeItalic "> n </em>步中收敛。</p></li>
<li><p class="Para" id="Par122">第一个方向与最速下降中的方向相同，<em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><strong class="EmphasisTypeBold ">0</strong></sub><em><strong class="EmphasisTypeBoldItalic ">= g</strong></em><sub><strong class="EmphasisTypeBold ">0</strong></sub>，它给出了第一个良好的缩减。</p></li>
<li><p class="Para" id="Par123">没有黑森的反转。</p></li>
</ul>

<p>以下是缺点:</p>
<ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par125">粗麻布必须得到供应、储存和处理。</p></li>
<li><p class="Para" id="Par126">对于非二次问题，不能保证收敛。事实上，如果初始点离一个解很远，搜索可能会徘徊在某个次优区域，因为不可靠的方向会随着迭代而增加。正如我们将在后面的代码示例中看到的，共轭梯度法对于 Himmelblau 函数来说需要最多的迭代次数来收敛。</p></li>
</ul>


<h3 class="Heading">弗莱彻-里维斯法</h3>
<p>弗莱彻-里维斯方法是共轭梯度法的一种变体。主要区别在于，<em class="EmphasisTypeItalic "> α </em> <sub> <em class="EmphasisTypeItalic "> k </em> </sub>是通过最小化<em class="EmphasisTypeItalic ">f</em>(<em><strong class="EmphasisTypeBoldItalic ">X</strong></em>+<em class="EmphasisTypeItalic ">α</em><sub><em class="EmphasisTypeItalic ">k</em></sub><em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">k</strong></em></sub>)使用线搜索来计算的 而那个<em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">k</strong></em></sub>是共轭方向相对于{<em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><strong class="EmphasisTypeBold ">0</strong></sub>，<em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><strong class="EmphasisTypeBold ">1</strong></sub>，<em><strong class="EmphasisTypeBoldItalic "/></em>，<em> 因此，该算法类似于共轭梯度法，但是由于线搜索而需要更多的计算，这是一个缺点。然而，有两个优点。</em></p>
<ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par128">该修改对于非二次函数的最小化工作得更好，因为在解的邻域之外的点处，可以在沿着<em><strong class="EmphasisTypeBoldItalic "/></em><sub><em class="EmphasisTypeItalic ">k</em></sub>的<em class="EmphasisTypeItalic "> f </em> ( <em> <strong class="EmphasisTypeBoldItalic "> x </strong> </em>)中实现更大的缩减。</p></li>
<li><p class="Para" id="Par129">这个算法不需要 Hessian。</p></li>
</ul>


<h3 class="Heading">鲍威尔法</h3>
<p class="Para" id="Par130">Powell 算法使用线搜索生成一系列共轭方向。</p>
<p>设<img alt="$$ {x}_a^{\ast } $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_IEq8.png" style="width:1.17em"/>和<img alt="$$ {x}_b^{\ast } $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_IEq9.png" style="width:1.17em"/>为若下列凸二次函数得到的极小值:<p> <img alt="$$ f(x)={x}^Tb+\frac{1}{2}{x}^T\boldsymbol{H}x $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equaz.png" style="width:9.36em"/> </p></p>
<p>相对于行<em> <strong class="EmphasisTypeBoldItalic ">上的<em class="EmphasisTypeItalic ">α</em>d</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">a</strong></em></sub>和<em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">b</strong></em></sub>:<p><img alt="$$ x={x}_a+\alpha {\boldsymbol{d}}_{\boldsymbol{a}} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equba.png" style="width:5.85em"/></p></p>
<p>还有下面的:<p> <img alt="$$ x={x}_b+\alpha {\boldsymbol{d}}_{\boldsymbol{b}} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equbb.png" style="width:5.85em"/> </p></p>
<p>如果说<em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">a</strong></em></sub><strong class="EmphasisTypeBold ">=</strong><em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">b</strong></em></sub>，那么矢量<img alt="$$ {x}_b^{\ast }-{x}_a^{\ast } $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_IEq10.png" style="width:3.27em"/>相对于<em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">a</strong></em></sub>。参见图<a href="#Fig8"> 9-8 </a>。</p>
<p><img alt="../images/500382_1_En_9_Chapter/500382_1_En_9_Fig8_HTML.jpg" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Fig8_HTML.jpg" style="width:22.12em"/></p>
<p>图 9-8</p><p class="SimplePara">共轭方向的生成</p>


<p>在这个算法中，一个初始点为<em class="EmphasisTypeItalic "> x </em> <sub> 00 </sub>和<em class="EmphasisTypeItalic "> n </em>线性独立的方向{<em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><strong class="EmphasisTypeBold ">0</strong></sub>，<em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><strong class="EmphasisTypeBold ">1</strong></sub>，<em><strong class="EmphasisTypeBoldItalic "/></em>，<em> <strong class="EmphasisTypeBoldItalic "> d </strong> </em>在第一次迭代中，<em class="EmphasisTypeItalic ">f</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>)在方向{<em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><strong class="EmphasisTypeBold ">0</strong></sub>，<em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><strong class="EmphasisTypeBold ">1</strong><em><strong class="EmphasisTypeBoldItalic ">⋯</strong></em>， <em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">n</strong></em></sub>}从点<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub>00<em><strong class="EmphasisTypeBoldItalic ">x</strong><sub>01</sub>，<em> <strong class="EmphasisTypeBoldItalic "> x </strong> </em> <sub> 然后一个新的方向<em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub>0(<em class="EmphasisTypeItalic ">n</em>+1)</sub>生成如下:<p> <img alt="$$ {\boldsymbol{d}}_{0\left(n+1\right)}={x}_{0n}-{x}_{00} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equbc.png" style="width:8.04em"/> </p></sub></em></sub></sub></p>
<p>并且<em class="EmphasisTypeItalic "> f </em> ( <em> <strong class="EmphasisTypeBoldItalic "> x </strong> </em>)在这个方向上被最小化以产生新的点<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub>0(<em class="EmphasisTypeItalic ">n</em>+1)</sub>。通过设置以下内容更新方向(参见图<a href="#Fig9">9-9</a>):<p><img alt="$$ {\boldsymbol{d}}_{11}={\boldsymbol{d}}_{02} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equbd.png" style="width:4.3em"/></p><p><img alt="$$ {\boldsymbol{d}}_{12}={\boldsymbol{d}}_{03} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Eqube.png" style="width:4.3em"/></p><p><img alt="$$ \cdots $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equbf.png" style="width:1.59em"/></p><p><img alt="$$ {\boldsymbol{d}}_{1\left(n-1\right)}={\boldsymbol{d}}_{0n} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equbg.png" style="width:5.64em"/></p><p><img alt="$$ {\boldsymbol{d}}_{1n}={\boldsymbol{d}}_{0\left(n+1\right)} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equbh.png" style="width:5.64em"/></p></p>
<p><img alt="../images/500382_1_En_9_Chapter/500382_1_En_9_Fig9_HTML.jpg" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Fig9_HTML.jpg" style="width:35.45em"/></p>
<p>图 9-9</p><p class="SimplePara">鲍威尔算法的第一次迭代</p>


<p>同样的过程在从点<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub>10</sub>=<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub>0(<em class="EmphasisTypeItalic ">n</em>+1)</sub>开始的第二次迭代中重复。<em class="EmphasisTypeItalic ">f</em>(<em><strong class="EmphasisTypeBoldItalic ">x</strong></em>)按方向依次最小化{<em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><strong class="EmphasisTypeBold ">11</strong></sub>，<em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub><strong class="EmphasisTypeBold ">12</strong></sub>，<em><strong class="EmphasisTypeBoldItalic ">【⋯</strong></em>，<em> <strong class="EmphasisTypeBoldItalic "> d </strong> </em>然后生成一个新的方向<em><strong class="EmphasisTypeBoldItalic ">d</strong></em><sub>1(<em class="EmphasisTypeItalic ">n</em>+1)</sub>(见图<a href="#Fig10"> 9-10 </a> ): <p> <img alt="$$ {\boldsymbol{d}}_{1\left(n+1\right)}={x}_{1n}-{x}_{10} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equbi.png" style="width:8.04em"/> </p></p>
<p><img alt="../images/500382_1_En_9_Chapter/500382_1_En_9_Fig10_HTML.jpg" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Fig10_HTML.jpg" style="width:35.43em"/></p>
<p>图 9-10</p><p class="SimplePara">鲍威尔算法的第二次迭代</p>


<p class="Para" id="Par139">以同样的方式进行，每个新的迭代将增加一个共轭方向并去除另一个方向，因此是一个替换。由于每次迭代需要(<em class="EmphasisTypeItalic "> n </em> + 1)次线搜索，鲍威尔方法在<em class="EmphasisTypeItalic "> n </em>次迭代中执行<em class="EmphasisTypeItalic "> n </em> ( <em class="EmphasisTypeItalic "> n </em> + 1)次线搜索。</p>
<p class="Para" id="Par140">鲍威尔方法的优点是它不供应、储存或操纵麻袋。它甚至不需要梯度。但是，它可能不会生成一组跨越<em class="EmphasisTypeItalic "> E </em> <sup> <em class="EmphasisTypeItalic "> n </em> </sup>的线性独立方向，算法可能不会收敛到一个解。</p>

<h3 class="Heading">赞威尔法</h3>
<p class="Para" id="Par141">赞威尔算法是鲍威尔算法的改进版本。它的修改产生了一组总是线性独立的共轭方向。因此，它消除了鲍威尔算法中的缺点。</p>
<p class="Para" id="Par142">修改如下:首先，选择坐标方向组，使得行列式为 1。第二，新方向被归一化为单位长度。第三，方向子站必须保持方向向量矩阵的行列式为正且仍小于或等于 1。最后一项确保方向向量总是线性独立的。</p>
<p>以下代码最小化 Himmelblau 函数(见图<a href="#Fig11"> 9-11 </a> ): <p> <img alt="$$ f\left({x}_1,{x}_2\right)={\left({x_1}^2+{x}_2-11\right)}^2+{\left({x}_1+{x_2}^2-7\right)}^2 $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equbj.png" style="width:19.68em"/> </p></p>
<p> 
</p>
<p><img alt="../images/500382_1_En_9_Chapter/500382_1_En_9_Fig11_HTML.jpg" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Fig11_HTML.jpg" style="width:29.6em"/></p>
<p>图 9-11</p><p class="SimplePara">具有四个最小值的 Himmelblau 函数</p>


<p>
 
</p>
<pre>/**
 * The Himmelblau function:
 * f(x) = (x1^2 + x2 - 11)^2 + (x1 + x2^2 - 7)^2
 */
RealScalarFunction f = new RealScalarFunction() {
    @Override
    public Double evaluate(Vector x) {
        double x1 = x.get(1);
        double x2 = x.get(2);

        double result = pow(x1 * x1 + x2 - 11, 2);
        result += pow(x1 + x2 * x2 - 7, 2);

        return result;
    }

    @Override
    public int dimensionOfDomain() {
        return 2;
    }

    @Override
    public int dimensionOfRange() {
        return 1;
    }
};

RealVectorFunction g = new RealVectorFunction() {
    @Override
    public Vector evaluate(Vector x) {
        double x1 = x.get(1);
        double x2 = x.get(2);

        double w1 = x1 * x1 + x2 - 11;
        double w2 = x1 + x2 * x2 - 7;

        double[] result = new double[2];
        result[0] = 4 * w1 * x1 + 2 * w2;
        result[1] = 2 * w1 + 4 * w2 * x2;
        return new DenseVector(result);
    }

    @Override
    public int dimensionOfDomain() {
        return 2;
    }

    @Override
    public int dimensionOfRange() {
        return 2;
    }
};

RntoMatrix H = new RntoMatrix() {
    @Override
    public Matrix evaluate(Vector x) {
        double x1 = x.get(1);
        double x2 = x.get(2);

        Matrix result = new DenseMatrix(2, 2);
        result.set(1, 1, 12 * x1 * x1 + 4 * x2 - 42);
        result.set(1, 2, 4 * (x1 + x2));
        result.set(2, 1, result.get(1, 2));
        result.set(2, 2, 4 * x1 + 12 * x2 * x2 - 26);

        return result;
    }

    @Override
    public int dimensionOfDomain() {
        return 2;
    }

    @Override
    public int dimensionOfRange() {
        return 1;
    }
};
C2OptimProblemImpl problem = new C2OptimProblemImpl(f, g, H);

ConjugateGradientMinimizer ConjugateGradientMinimizer
        = new ConjugateGradientMinimizer(1e-16, 40);
IterativeSolution&lt;Vector&gt; soln1 = ConjugateGradientMinimizer.solve(problem);
Vector xmin1 = soln1.search(new DenseVector(new double[]{6, 6}));
double f_xmin1 = f.evaluate(xmin1);
System.out.println(String.format("f(%s) = %.16f", xmin1.toString(), f_xmin1));

ConjugateGradientMinimizer fletcherReevesMinimizer
        = new FletcherReevesMinimizer(1e-16, 20);
IterativeSolution&lt;Vector&gt; soln2 = fletcherReevesMinimizer.solve(problem);
Vector xmin2 = soln2.search(new DenseVector(new double[]{6, 6}));
double f_xmin2 = f.evaluate(xmin2);
System.out.println(String.format("f(%s) = %.16f", xmin2.toString(), f_xmin2));

SteepestDescentMinimizer powellMinimizer
        = new PowellMinimizer(1e-16, 20);
IterativeSolution&lt;Vector&gt; soln3 = powellMinimizer.solve(problem);
Vector xmin3 = soln3.search(new DenseVector(new double[]{6, 6}));
double f_xmin3 = f.evaluate(xmin3);
System.out.println(String.format("f(%s) = %.16f", xmin3.toString(), f_xmin3));

SteepestDescentMinimizer zangwillMinimizer
        = new ZangwillMinimizer(1e-16, 1e-16, 20);
IterativeSolution&lt;Vector&gt; soln4 = zangwillMinimizer.solve(problem);
Vector xmin4 = soln4.search(new DenseVector(new double[]{6, 6}));
double f_xmin4 = f.evaluate(xmin4);
System.out.println(String.format("f(%s) = %.16f", xmin4.toString(), f_xmin4));

</pre>
<p>输出如下所示:</p>
<pre>f([3.000000, 2.000000] ) = 0.0000000000013999
f([3.000002, 1.999998] ) = 0.0000000001278725
f([-2.805114, 3.131310] ) = 0.0000000007791914
f([-2.805117, 3.131311] ) = 0.0000000001359077

</pre>
<p class="Para" id="Par146">找到的两个最小值是(3，2)和(2.805118，3.131312)。</p>
<p>这些最小化者的签名如下:</p>
<pre>/**
 * Construct a multivariate minimizer using the Conjugate-Gradient method.
 *
 * @param epsilon       a precision parameter: when a number |x| &amp;le;
 *                      &amp;epsilon;, it is considered 0
 * @param maxIterations the maximum number of iterations
 */
public ConjugateGradientMinimizer(double epsilon, int maxIterations)

/**
 * Construct a multivariate minimizer using the Fletcher-Reeves method.
 *
 * @param epsilon       a precision parameter: when a number |x| &amp;le;
 *                      &amp;epsilon;, it is considered 0
 * @param maxIterations the maximum number of iterations
 */
public FletcherReevesMinimizer(double epsilon, int maxIterations)

/**
 * Construct a multivariate minimizer using the Powell method.
 *
 * @param epsilon       a precision parameter: when a number |x| &amp;le;
 *                      &amp;epsilon;, it is
 *                      considered 0
 * @param maxIterations the maximum number of iterations
 */
public PowellMinimizer(double epsilon, int maxIterations)

/**
 * Construct a multivariate minimizer using the Zangwill method.
 *
 * @param epsilon       a precision parameter: when a number |x| &amp;le;
 *                      &amp;epsilon;, it is considered 0
 * @param epsilon2      a precision parameter to decide whether there is a
 *                      linear dependence among the conjugate directions
 * @param maxIterations the maximum number of iterations
 */
public ZangwillMinimizer(double epsilon, double epsilon2, int maxIterations)

</pre>


<h2 class="Heading">9.6 拟牛顿法</h2>
<p>拟牛顿法是另一种不需要 Hessian 显式表达式的算法框架。它建立在第 9.4.1 节中的牛顿法的基础上，除了搜索方向是基于一个<em class="EmphasisTypeItalic ">n</em>∫<em class="EmphasisTypeItalic ">n</em>矩阵<strong class="EmphasisTypeBold "> S </strong>之外，它的作用与牛顿法中的逆 Hessian 矩阵相同。也就是<p> <img alt="$$ {x}_{k+1}={x}_k-{\alpha}_k{\boldsymbol{S}}_{\boldsymbol{k}}{g}_k $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equbk.png" style="width:8.34em"/> </p></p>
<p class="Para" id="Par149">当<em><strong class="EmphasisTypeBoldItalic ">S</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">k</strong></em></sub>=<em><strong class="EmphasisTypeBoldItalic ">I</strong></em>时，是最速下降算法。当<em><strong class="EmphasisTypeBoldItalic ">S</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">k</strong></em></sub>=<strong class="EmphasisTypeBold ">H</strong><sup>—1</sup>时，就是牛顿法。当 Hessian 不可用时，<strong class="EmphasisTypeBold "> S </strong>从可用数据中得出，是<strong class="EmphasisTypeBold ">H</strong>T32】1 的近似值。其实可以看出，前面的算法更新<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub><em class="EmphasisTypeItalic ">k</em>+1</sub>在<em><strong class="EmphasisTypeBoldItalic ">S</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">k</strong></em></sub>=<strong class="EmphasisTypeBold ">H</strong><sup>—1</sup>时收敛最快。拟牛顿法结合了最速下降法和共轭方向法的优点以及有效的方向更新规则。在本章讨论的所有方法中，它们是最有效的，并在许多应用中广泛使用。</p>
<p>假设逐次<em><strong class="EmphasisTypeBoldItalic ">x</strong></em><sub><em class="EmphasisTypeItalic ">k</em></sub>的差异如下:<p> <img alt="$$ {\delta}_k={x}_{k+1}-{x}_k $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equbl.png" style="width:6.22em"/> </p></p>
<p>逐次梯度的区别<em class="EmphasisTypeItalic "> g </em> <sub> <em class="EmphasisTypeItalic "> k </em> </sub>如下:<p> <img alt="$$ {\gamma}_k={g}_{k+1}-{g}_k $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equbm.png" style="width:6.3em"/> </p></p>
<p>于是我们有了这个:<p> <img alt="$$ {\gamma}_k=\boldsymbol{H}{\delta}_k $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equbn.png" style="width:4.24em"/> </p></p>
<p>牛顿法确定方向向量如下:<p> <img alt="$$ {d}_k=-{\boldsymbol{S}}_{\boldsymbol{k}}{g}_k $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equbo.png" style="width:5.1em"/> </p></p>
<p>幅度<em class="EmphasisTypeItalic "> α </em> <sub> <em class="EmphasisTypeItalic "> k </em> </sub>可以通过最小化函数<p> <img alt="$$ f\left({x}_k+\alpha {d}_k\right) $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equbp.png" style="width:5.37em"/> </p>的线搜索来确定</p>
<p>我们要避免对一个矩阵求逆，比如计算<em><strong class="EmphasisTypeBoldItalic ">S</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">k</strong></em></sub><strong class="EmphasisTypeBold ">= H</strong><sup>—1</sup>并检查<em><strong class="EmphasisTypeBoldItalic ">S</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">k</strong></em></sub>在每次迭代中都是正定的。我们要一个简单的更新规则为<em><strong class="EmphasisTypeBoldItalic ">S</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">k+</strong></em><strong class="EmphasisTypeBold ">1</strong></sub>。<p><img alt="$$ {\boldsymbol{S}}_{\boldsymbol{k}+\mathbf{1}}={\boldsymbol{S}}_{\boldsymbol{k}}+{\boldsymbol{C}}_{\boldsymbol{k}} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equbq.png" style="width:6.84em"/>T38】</p></p>
<p><strong class="EmphasisTypeBold ">C</strong><sub><em><strong class="EmphasisTypeBoldItalic ">k</strong></em></sub>是一个<em class="EmphasisTypeItalic ">n</em>∑<em class="EmphasisTypeItalic ">n</em>校正矩阵，可以从现有数据中计算出来。在<em> <strong class="EmphasisTypeBoldItalic ">的任何定义中，C</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">k</strong></em></sub>，<em><strong class="EmphasisTypeBoldItalic ">S</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">k+</strong></em><strong class="EmphasisTypeBold ">【1</strong></sub>必须满足这三个性质:</p>
<ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par157"><strong class="EmphasisTypeBold ">s</strong><sub><em><strong class="EmphasisTypeBoldItalic ">【k+</strong></em><strong class="EmphasisTypeBoldItalic "><strong class="EmphasisTypeBoldItalic "><strong class="EmphasisTypeBoldItalic "><sub/></strong></strong></strong></sub></p></li>
<li><p class="Para" id="Par158">向量{ <em> <strong class="EmphasisTypeBoldItalic "> δ </strong> </em> <sub> 0 </sub>，<em> <strong class="EmphasisTypeBoldItalic "> δ </strong> </em> <sub> 1 </sub>，<em><strong class="EmphasisTypeBoldItalic ">δ</strong></em><em class="EmphasisTypeItalic ">n</em>—1}必须是共轭方向。</p></li>
<li><p class="Para" id="Par159">一个正定的<em><strong class="EmphasisTypeBoldItalic ">S</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">k</strong></em></sub>必然产生一个正定的<em><strong class="EmphasisTypeBoldItalic ">S</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">k+</strong></em><strong class="EmphasisTypeBold ">1</strong></sub>。</p></li>
</ul>

<p class="Para" id="Par160">第二个性质保证了共轭方向搜索的优良性质也适用于准牛顿搜索。第三个属性确保方向向量在每次迭代中都有效。定义<em><strong class="EmphasisTypeBoldItalic ">【C】</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">k</strong></em></sub>的方法有很多种，它们产生了拟牛顿法的不同变体。在本章中，我们根据它们如何计算<em><strong class="EmphasisTypeBoldItalic ">C</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">k</strong></em></sub>或者它们如何更新<em><strong class="EmphasisTypeBoldItalic ">S</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">k</strong></em></sub>来介绍几种变体。</p>
<h3 class="Heading">等级一方法</h3>
<p class="Para" id="Par161">秩一法的特点是校正矩阵<strong class="EmphasisTypeBold ">C</strong><sub><em><strong class="EmphasisTypeBoldItalic ">k</strong></em></sub>具有一个秩。</p>
<p>假设如下:<p> <img alt="$$ {\mathbf{S}}_{\boldsymbol{k}+\mathbf{1}}{\gamma}_k={\delta}_k $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equbr.png" style="width:5.24em"/> </p></p>
<p>让<p> <img alt="$$ {\mathbf{S}}_{\boldsymbol{k}+\mathbf{1}}={\mathbf{S}}_{\boldsymbol{k}}+\beta {\xi}_k{\xi}_k^T $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equbs.png" style="width:8.08em"/> </p></p>
<p><em><strong class="EmphasisTypeBoldItalic ">ξ</strong></em><sub><em><strong class="EmphasisTypeBoldItalic ">k</strong></em></sub>为列向量，<em class="EmphasisTypeItalic "> β </em> <sub> <em class="EmphasisTypeItalic "> k </em> </sub>为常数。校正矩阵<img alt="$$ \beta {\xi}_k{\xi}_k^T $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_IEq11.png" style="width:2.67em"/>是对称的，并且秩为 1。结合这两个等式，我们得到如下:<p> <img alt="$$ {\delta}_k={\mathbf{S}}_{\boldsymbol{k}}{\gamma}_k+\beta {\xi}_k{\xi}_k^T{\gamma}_k $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equbt.png" style="width:8.91em"/> </p></p>
<p>而我们有以下:<p> <img alt="$$ {\gamma}_k^T\left({\delta}_k-{\mathbf{S}}_{\boldsymbol{k}}{\gamma}_k\right)={\beta}_k{\gamma}_k^T{\xi}_k{\xi}_k^T{\gamma}_k={\beta}_k{\left({\xi}_k^T{\gamma}_k\right)}^2 $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equbu.png" style="width:17.74em"/> </p> <p> <img alt="$$ \left({\delta}_k-{\mathbf{S}}_{\boldsymbol{k}}{\gamma}_k\right){\left({\delta}_k-{\mathbf{S}}_{\boldsymbol{k}}{\gamma}_k\right)}^T={\beta}_k{\left({\xi}_k^T{\gamma}_k\right)}^2{\beta}_k{\xi}_k{\xi}_k^T $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equbv.png" style="width:18.6em"/> </p></p>
<p>于是我们有了以下:<p> <img alt="$$ {\beta}_k{\xi}_k{\xi}_k^T=\frac{\left({\delta}_k-{\mathbf{S}}_{\boldsymbol{k}}{\gamma}_k\right){\left({\delta}_k-{\mathbf{S}}_{\boldsymbol{k}}{\gamma}_k\right)}^T}{\gamma_k^T\left({\delta}_k-{\mathbf{S}}_{\boldsymbol{k}}{\gamma}_k\right)} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equbw.png" style="width:14.68em"/> </p></p>
<p>因此，我们有以下:<p> <img alt="$$ {\mathbf{S}}_{\boldsymbol{k}+\mathbf{1}}={\mathbf{S}}_{\boldsymbol{k}}+\frac{\left({\delta}_k-{\mathbf{S}}_{\boldsymbol{k}}{\gamma}_k\right){\left({\delta}_k-{\mathbf{S}}_{\boldsymbol{k}}{\gamma}_k\right)}^T}{\gamma_k^T\left({\delta}_k-{\mathbf{S}}_{\boldsymbol{k}}{\gamma}_k\right)} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equbx.png" style="width:16em"/> </p></p>
<p class="Para" id="Par168">这种方法有两个严重的问题。一、一个正定<strong class="EmphasisTypeBold ">S</strong><sub><em><strong class="EmphasisTypeBoldItalic ">k</strong></em></sub>未必给一个正定<strong class="EmphasisTypeBold ">S</strong><sub><em><strong class="EmphasisTypeBoldItalic ">k+</strong></em><strong class="EmphasisTypeBold ">1</strong></sub>。如果出现这种情况，下一个方向就不是好的方向。第二，修正公式中的分母可能趋近于零或等于零。如果发生这种情况，该方法就会失败，因为<strong class="EmphasisTypeBold ">S</strong><sub><em><strong class="EmphasisTypeBoldItalic ">k+</strong></em><strong class="EmphasisTypeBold ">1</strong></sub>未定义。</p>

<h3 class="Heading">9.6.2 达维顿-弗莱彻-鲍威尔法</h3>
<p class="Para" id="Par169">大卫顿-弗莱彻-鲍威尔(DPF)方法类似于秩一方法，但它有一个重要的优点:如果初始矩阵<strong class="EmphasisTypeBold "> S </strong> <sub> <strong class="EmphasisTypeBold "> 0 </strong> </sub>是正定的，则后续矩阵总是正定的。与秩一方法不同，每个新方向都是下降方向。</p>
<p>DPF 更新公式如下:<p> <img alt="$$ {\mathbf{S}}_{\boldsymbol{k}+\mathbf{1}}={\mathbf{S}}_{\boldsymbol{k}}+\frac{\delta_k{\delta}_k^T}{\delta_k^T{\gamma}_k}-\frac{{\mathbf{S}}_{\boldsymbol{k}}{\gamma}_k{\gamma}_k^T{\mathbf{S}}_{\boldsymbol{k}}}{\gamma_k^T{\mathbf{S}}_{\boldsymbol{k}}{\gamma}_k} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equby.png" style="width:13.25em"/> </p></p>
<p class="Para" id="Par171">校正是一个秩为二的<em class="EmphasisTypeItalic ">n</em>∫<em class="EmphasisTypeItalic ">n</em>对称矩阵。</p>

<h3 class="Heading">9.6.3 布罗伊登-弗莱彻-戈德法布-尚诺方法</h3>
<p>BFGS 更新公式如下:<p> <img alt="$$ {\mathbf{S}}_{\boldsymbol{k}+\mathbf{1}}={\mathbf{S}}_{\boldsymbol{k}}+\left(1+\frac{\gamma_k^T{\mathbf{S}}_{\boldsymbol{k}}{\gamma}_k}{\gamma_k^T{\delta}_k}\right)\frac{\delta_k{\delta}_k^T}{\gamma_k^T{\delta}_k}-\frac{\delta_k{\gamma}_k^T{\mathbf{S}}_{\boldsymbol{k}}+{\mathbf{S}}_{\boldsymbol{k}}{\gamma}_k{\delta}_k^T}{\gamma_k^T{\delta}_k} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equbz.png" style="width:22.32em"/> </p></p>
<p>BFGS 方法具有以下特性:</p>
<ol><li class="ListItem"><p class="Para" id="Par174"><strong class="EmphasisTypeBold ">S</strong><sub><em><strong class="EmphasisTypeBoldItalic ">k+</strong></em><strong class="EmphasisTypeBold ">1</strong></sub>变得与<strong class="EmphasisTypeBold ">H</strong><sup><strong class="EmphasisTypeBold ">—1</strong>—1<em class="EmphasisTypeItalic ">k</em><em><strong class="EmphasisTypeBoldItalic ">=</strong></em><em class="EmphasisTypeItalic ">n</em>—1。</sup></p>
 </li>
<li class="ListItem"><p class="Para" id="Par175">方向{<em><strong class="EmphasisTypeBoldItalic ">δ</strong></em><sub>T5】I</sub>}<sub><em class="EmphasisTypeItalic ">I</em>= 1，2，…，<em class="EmphasisTypeItalic ">n</em>—1</sub>形成共轭集。</p>
 </li>
<li class="ListItem"><p class="Para" id="Par176"><strong class="EmphasisTypeBold ">S</strong><sub><em><strong class="EmphasisTypeBoldItalic ">k+</strong></em><strong class="EmphasisTypeBold ">1</strong></sub>为正定如果<strong class="EmphasisTypeBold ">S</strong><sub><em><strong class="EmphasisTypeBoldItalic ">k</strong></em></sub>为正定。</p>
 </li>
<li class="ListItem"><p class="Para" id="Par177"><img alt="$$ {\delta}_k^T{\gamma}_k={\delta}_k^T{g}_{k+1}-{\delta}_k^T{g}_k&amp;gt;0 $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_IEq12.png" style="width:11.22em"/>。</p>
 </li>
</ol>

<p class="Para" id="Par178">BFGS 求解器是所有求解器中最好的。在许多 NM 无约束优化应用中默认使用它。</p>

<h3 class="Heading">9.6.4 黄家族(排名第一，DFP，，Pearson，McCormick)</h3>
<p>黄更新公式是包含秩一、DFP、、Pearson 和 McCormick 方法的一般公式。它有这样的形式:<p> <img alt="$$ {\mathbf{S}}_{\boldsymbol{k}+\mathbf{1}}={\mathbf{S}}_{\boldsymbol{k}}+\frac{\delta_k{\left(\theta {\delta}_k+\phi {\mathbf{S}}_k^T{\gamma}_k\right)}^T}{{\left(\theta {\delta}_k+\phi {\mathbf{S}}_k^T{\gamma}_k\right)}^T{\gamma}_k}-\frac{{\mathbf{S}}_{\boldsymbol{k}}{\gamma}_k{\left(\varphi {\delta}_k+\omega {\mathbf{S}}_k^T{\gamma}_k\right)}^T}{{\left(\varphi {\delta}_k+\omega {\mathbf{S}}_k^T{\gamma}_k\right)}^T{\gamma}_k} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equca.png" style="width:23.28em"/> </p></p>
<p class="Para" id="Par180"><em class="EmphasisTypeItalic "> θ </em>、<em class="EmphasisTypeItalic "> ϕ </em>、<em class="EmphasisTypeItalic "> φ </em>、<em class="EmphasisTypeItalic "> ω </em>为独立参数。</p>
<p class="Para" id="Par181">通过让<em class="EmphasisTypeItalic "> θ </em> = 1，<em class="EmphasisTypeItalic ">ϕ</em>= 1，<em class="EmphasisTypeItalic "> ψ </em> = 1，<em class="EmphasisTypeItalic ">ω</em>= 1，我们就有了秩一公式。</p>
<p class="Para" id="Par182">让<em class="EmphasisTypeItalic "> θ </em> = 1，<em class="EmphasisTypeItalic "> ϕ </em> = 0，<em class="EmphasisTypeItalic "> ψ </em> = 0，<em class="EmphasisTypeItalic "> ω </em> = 1，我们就有了 DFP 公式。</p>
<p class="Para" id="Par183">通过让<img alt="$$ \frac{\phi }{\theta }=\frac{-{\gamma}_k{\delta}_k^T}{\gamma_k{\delta}_k^T+{\gamma}_k^T{S}_k{\gamma}_k},\psi =1,\mathrm{and}\ \omega =0 $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_IEq13.png" style="width:14.34em"/>我们有了 BFGS 公式。</p>
<p>设<em class="EmphasisTypeItalic "> θ </em> = 1，<em class="EmphasisTypeItalic "> ϕ </em> = 0，<em class="EmphasisTypeItalic "> ψ </em> = 1，<em class="EmphasisTypeItalic "> ω </em> = 0，我们就有了麦考密克公式。<p> <img alt="$$ {\mathbf{S}}_{\boldsymbol{k}+\mathbf{1}}={\mathbf{S}}_{\boldsymbol{k}}+\frac{\left({\delta}_k-{\mathbf{S}}_{\boldsymbol{k}}{\gamma}_k\right){\delta}_k^T}{\delta_k^T{\gamma}_k} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equcb.png" style="width:11.76em"/> </p></p>
<p>设<em class="EmphasisTypeItalic "> θ </em> = 0，<em class="EmphasisTypeItalic "> ϕ </em> = 1，<em class="EmphasisTypeItalic "> ψ </em> = 0，<em class="EmphasisTypeItalic "> ω </em> = 1，我们就有了皮尔逊公式。<p> <img alt="$$ {\mathbf{S}}_{\boldsymbol{k}+\mathbf{1}}={\mathbf{S}}_{\boldsymbol{k}}+\frac{\left({\delta}_k-{\mathbf{S}}_{\boldsymbol{k}}{\gamma}_k\right){\gamma}_k^T{\mathbf{S}}_{\boldsymbol{k}}}{\gamma_k^T{\mathbf{S}}_{\boldsymbol{k}}{\gamma}_k} $$" src="../images/500382_1_En_9_Chapter/500382_1_En_9_Chapter_TeX_Equcc.png" style="width:12.88em"/> </p></p>
<p>NM Dev 有一套准牛顿方法。以下代码使用不同的拟牛顿法求解图<a href="#Fig11"> 9-11 </a>中的 Himmelblau 函数:</p>
<pre>/**
 * The Himmelblau function:
 * f(x) = (x1^2 + x2 - 11)^2 + (x1 + x2^2 - 7)^2
 */
RealScalarFunction f = new RealScalarFunction() {
    @Override
    public Double evaluate(Vector x) {
        double x1 = x.get(1);
        double x2 = x.get(2);

        double result = pow(x1 * x1 + x2 - 11, 2);
        result += pow(x1 + x2 * x2 - 7, 2);

        return result;
    }

    @Override
    public int dimensionOfDomain() {
        return 2;
    }

    @Override
    public int dimensionOfRange() {
        return 1;
    }
};

RealVectorFunction g = new RealVectorFunction() {
    @Override
    public Vector evaluate(Vector x) {
        double x1 = x.get(1);
        double x2 = x.get(2);

        double w1 = x1 * x1 + x2 - 11;
        double w2 = x1 + x2 * x2 - 7;

        double[] result = new double[2];
        result[0] = 4 * w1 * x1 + 2 * w2;
        result[1] = 2 * w1 + 4 * w2 * x2;
        return new DenseVector(result);
    }

    @Override
    public int dimensionOfDomain() {
        return 2;
    }

    @Override
    public int dimensionOfRange() {
        return 2;
    }
};
C2OptimProblemImpl problem = new C2OptimProblemImpl(f, g);

QuasiNewtonMinimizer rankOneMinimizer = new RankOneMinimizer(1e-16, 15);
IterativeSolution&lt;Vector&gt; soln1 = rankOneMinimizer.solve(problem);
Vector xmin = soln1.search(new DenseVector(new double[]{6, 6}));
double f_xmin = f.evaluate(xmin);
System.out.println(String.format("f(%s) = %.16f", xmin.toString(), f_xmin));

QuasiNewtonMinimizer dfpMinimizer = new DFPMinimizer(1e-16, 15);
IterativeSolution&lt;Vector&gt; soln2 = dfpMinimizer.solve(problem);
xmin = soln2.search(new DenseVector(new double[]{6, 6}));
f_xmin = f.evaluate(xmin);
System.out.println(String.format("f(%s) = %.16f", xmin.toString(), f_xmin));

QuasiNewtonMinimizer bfgsMinimizer = new BFGSMinimizer(false, 1e-16, 15);
IterativeSolution&lt;Vector&gt; soln3 = bfgsMinimizer.solve(problem);
xmin = soln3.search(new DenseVector(new double[]{6, 6}));
f_xmin = f.evaluate(xmin);
System.out.println(String.format("f(%s) = %.16f", xmin.toString(), f_xmin));

QuasiNewtonMinimizer huangMinimizer = new HuangMinimizer(0, 1, 0, 1, 1e-16, 15);
IterativeSolution&lt;Vector&gt; soln4 = huangMinimizer.solve(problem);
xmin = soln4.search(new DenseVector(new double[]{6, 6}));
f_xmin = f.evaluate(xmin);
System.out.println(String.format("f(%s) = %.16f", xmin.toString(), f_xmin));

QuasiNewtonMinimizer pearsonMinimizer = new PearsonMinimizer(1e-16, 15);
IterativeSolution&lt;Vector&gt; soln5 = pearsonMinimizer.solve(problem);
xmin = soln5.search(new DenseVector(new double[]{6, 6}));
f_xmin = f.evaluate(xmin);
System.out.println(String.format("f(%s) = %.16f", xmin.toString(), f_xmin));

</pre>
<p>输出如下所示:</p>
<pre>f([3.000000, 2.000000] ) = 0.0000000000000000
f([3.000000, 2.000000] ) = 0.0000000000000000
f([3.000000, 2.000000] ) = 0.0000000000000000
f([3.000000, 2.000000] ) = 0.0000000000000000
f([3.000000, 2.000000] ) = 0.0000000000000000

</pre>
<p class="Para" id="Par188">拟牛顿法似乎比任何共轭方向法更有效。它们需要的迭代次数少得多，15 次对 20 次，而且精度更高。</p>
<p>这些解算员的签名如下:</p>
<pre>/**
 * Construct a multivariate minimizer using the Rank One method.
 *
 * @param epsilon       a precision parameter: when a number |x| &amp;le;
 *                      &amp;epsilon;, it is considered 0
 * @param maxIterations the maximum number of iterations
 */
public RankOneMinimizer(double epsilon, int maxIterations)

/**
 * Construct a multivariate minimizer using the DFP method.
 *
 * @param epsilon       a precision parameter: when a number |x| &amp;le;
 *                      &amp;epsilon;, it is considered 0
 * @param maxIterations the maximum number of iterations
 */
public DFPMinimizer(double epsilon, int maxIterations)

/**
 * Construct a multivariate minimizer using the BFGS method.
 *
 * @param epsilon          a precision parameter: when a number |x| &amp;le;
 *                         &amp;epsilon;, it is considered 0
 * @param maxIterations    the maximum number of iterations
 * @param isFletcherSwitch indicate whether to use the Fletcher switch
 */
public BFGSMinimizer(boolean isFletcherSwitch, double epsilon, int maxIterations)

/**
 * Construct a multivariate minimizer using Huang's method.
 *
 * @param theta         &amp;theta; in Huang's formula
 * @param phi           &amp;phi; in Huang's formula
 * @param psi           &amp;psi; in Huang's formula
 * @param omega         &amp;omega; in Huang's formula
 * @param epsilon       a precision parameter: when a number |x| &amp;le;
 *                      &amp;epsilon;, it is considered 0
 * @param maxIterations the maximum number of iterations
 */
public HuangMinimizer(
        double theta,
        double phi,
        double psi,
        double omega,
        double epsilon,
        int maxIterations
)

/**
 * Construct a multivariate minimizer using the Pearson method.
 *
 * @param epsilon       a precision parameter: when a number |x| &amp;le;
 *                      &amp;epsilon;, it is considered 0
 * @param maxIterations the maximum number of iterations
 */
public PearsonMinimizer(double epsilon, int maxIterations)

</pre>




</body>
</html>