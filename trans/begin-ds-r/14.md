托马斯 mailund 2017 年

Thomas Mailund,《在 R 开始数据科学》, 10.1007/978-1-4842-2671-1_14

# 14.分析和优化

Thomas Mailund <sup class="calibre6">1</sup> 的缩写形式

①丹麦奥胡斯

在这最后一章中，我们简要地考虑当你发现你的代码运行太慢时该怎么办，特别是，如何找出*为什么*它运行太慢。

不过，在开始担心代码的性能之前，重要的是要考虑是否值得加速。提高性能需要花费时间，如果包括这种额外的编程，提高的性能能为您节省时间，这才是值得的。对于一个可以在一天内完成的分析，花一天的时间让它变得更快，甚至更快是没有意义的，因为你最终还是要花同样的时间，甚至更多的时间来完成分析。

任何在分析过程中只需要运行几次的代码通常都不值得优化。我们很少需要只运行*一次*的分析——乐观地说，我们可能希望如此，但实际上，当数据或想法发生变化时，我们通常不得不一次又一次地运行它——但我们不期望运行数百次或数千次。因此，即使重新运行一个分析需要几个小时，你的时间最好还是花在它运行的时候做点别的事情上。花很多时间让它变得更快是不值得的。CPU 时间比你自己的便宜。

但是，如果您正在开发一个包，您通常必须在某种程度上考虑性能。如果一个包值得开发，它将会有更多的用户，运行你的代码所花费的总时间在某种程度上使代码变得更快是值得的。

## 压型

在你能让你的代码变得更快之前，你首先需要弄清楚它为什么慢。您可能对代码慢的地方有一些想法，但是实际上很难猜测。我发现，大部分时间实际上并不在我想象的地方。在两个不同的场合，我曾努力加速一个算法，但后来发现我的程序慢的原因是用于读取程序输入的代码。解析器很慢。相比之下，这种算法简直快如闪电。那是在 C 语言中，抽象层次很低，从代码中很容易看出运行需要多少时间。在 R 中，抽象是非常高级的，很难猜测一行代码运行需要多长时间。

关键是，如果你发现你的代码很慢，你不应该猜测哪里慢。你应该测量一下运行时间，确定一下。您需要分析您的代码，以了解哪些部分占用了大部分运行时间。否则，您可能会优化只使用总运行时间的几个百分比的代码，而忽略真正的时间浪费者。

在普通代码中，真正的瓶颈只有几个。如果你能发现这些并提高他们的表现，你的工作就完成了。剩下的会跑得足够快。找出那些瓶颈在哪里需要剖析。

我们将使用[provis](https://rstudio.github.io/profvis/)包进行概要分析。在 RStudio 的最新版本中，有这种支持，如果您的版本有，您应该在主菜单中有一个配置文件项目。我们将在 R 代码中使用这个包。

### 图流算法

对于一些代码的例子，假设您想要分析一个小图算法。这是一种算法，用于平滑图中节点的权重。它是一种方法的一部分，用于传播图中节点的证据权重，并已用于使用基因-基因相互作用网络来促进疾病-基因关联的搜索。这个想法是，如果一个基因是这个相互作用网络中另一个基因的邻居，那么它更有可能与另一个基因有类似的疾病关联。因此，已知关联的基因被赋予一个初始权重，而其他基因如果与这些基因有关联，就会比没有关联的基因获得更高的权重。

不过，该算法的具体用途并不重要。它所做的只是平滑节点之间的权重。最初，所有节点 *n* 被分配一个权重 *w* ( *n* )。然后，在平滑的一次迭代中，该权重被更新为![$$ {w}^{\hbox{'}}(n)=\alpha w(n)+\left(1-\alpha \right)\frac{1}{\left| N(n)\right|}{\displaystyle {\sum}^{v\in N(n)} w(v)} $$](Images/A439481_1_En_14_Chapter_IEq1.gif)，其中 *α* 是零和一之间的数字，并且 *N* ( *n* )表示节点 *n* 的邻居。如果这被迭代足够多次，则对于图中所有连接的节点，图中的权重变得相等，但是如果较早停止，这只是轻微的平滑，取决于 *α* 的值。

为了实现这一点，我们需要图形的表示和平滑算法。我们从表示图表开始。有许多方法可以做到这一点，但一种简单的格式是所谓的*关联矩阵*。这是一个矩阵，如果节点 *i* 和 *j* 不直接相连，则具有条目*M*<sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">T5】I，j</sub> = 0，如果它们直接相连，则具有条目 *M* <sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">*i，j*</sub> = 1。由于我们想在这个算法中处理一个无向图，我们将有 *M* <sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">*i，j*</sub>=*M*<sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">*j，i*</sub> 。

我们可以使用如下所示的构造函数来实现这种表示:

```
graph <- function(n, edges) {
  m <- **matrix**(0, nrow = n, ncol = n)

  no_edges <- **length**(edges)
  if (no_edges >= 1) {
    for (i in **seq**(1, no_edges, by = 2)) {
      m[edges[i], edges[i+1]] <- m[edges[i+1], edges[i]] <- 1
    }
  }

  **structure**(m, class = "graph")
}
```

在这里，我要求节点的数量作为参数 *n* 给出，并且边被指定为一个向量，其中每一对对应于一条边。如果图形应该手工编码，这不是表示边的最佳方式，但是由于该算法应该用于非常大的图形，我假设我们可以在其他地方编写代码来读取图形表示并创建这样的边向量。

没有太多的功能。它只是创建关联矩阵，然后遍历边来设置它。如果边向量为空，则有一种特殊情况需要处理。然后 seq()调用将返回一个从 1 到 0 的列表。所以我们避免这样。我们可能还想检查边向量的长度是否是 2 的倍数，但我不在乎。我假设生成向量的代码会处理好这个问题。

尽管图形表示只是一个矩阵，我还是给了它一个类，以防以后我想为它编写通用函数。

使用这种图形表示，平滑函数可以如下所示:

```
smooth_weights <- function(graph, node_weights, alpha) {
  if (**length**(node_weights) != **nrow**(graph))
    **stop**("Incorrect number of nodes")

  no_nodes <- **length**(node_weights)
  new_weights <- **vector**("numeric", no_nodes)

  for (i in 1:no_nodes) {
    neighbour_weights <- 0
    n <- 0
    for (j in 1:no_nodes) {
      if (i != j && graph[i, j] == 1) {
        neighbour_weights <- neighbour_weights + node_weights[j]
        n <- n + 1
      }
    }

    if (n > 0) {
      new_weights[i] <-
        alpha * node_weights[i] +
        (1 - alpha) * neighbour_weights / n
    } else {
      new_weights[i] <- node_weights[i]
    }

  }
  new_weights
}
```

它创建了我们应该返回的新的权重向量，然后在嵌套循环中遍历矩阵。如果关联矩阵说 *i* 和 *j* ，和*I*≦*j*之间有联系——如果有自循环，我们不想加上节点自身的权重——我们用它来计算均值。如果有要更新的东西——如果节点 *i* 有邻居，就会有更新，我们就进行更新。

代码并不特别优雅，但它是这一思想的直接实现。

为了分析这段代码，我们使用了来自 prov vis 的 prov vis()函数。它将一个表达式作为它的单个参数，所以为了分析不止一个函数调用，我们给它一个代码块，将语句序列转换成一个表达式。

我只是生成一个有 1000 个节点，300 条边和随机权重的随机图。我们在这里不是测试代码，只是分析它。如果这是真实的代码而不仅仅是一个例子，我们当然应该进行单元测试——如果你开始重写代码来优化它，这一点尤其重要。否则，您可能会得到更快但不正确的代码。

```
profvis::**profvis**({
  n <- 1000
  nodes <- 1:n
  edges <- **sample**(nodes, 600, replace = TRUE)
  weights <- **rnorm**(n)
  g <- **graph**(n, edges)
  **smooth_weights**(g, weights, 0.8)
})
```

运行这段代码将打开一个显示结果的新标签，如图 [14-1](#Fig1) 所示。选项卡的上半部分显示了您的代码，注释以水平条的形式首先显示内存使用情况，然后显示时间使用情况。窗口的下半部分显示时间使用和调用堆栈。

![A439481_1_En_14_Fig1_HTML.jpg](Images/A439481_1_En_14_Fig1_HTML.jpg)

###### 图 14-1。显示配置文件结果的窗口

我们可以看到，总执行时间大约为 1800 毫秒。阅读图表的方式是，从左到右，您可以看到在运行中的任何点执行了什么，在底部的代码块中直接调用了我们给 profvis()的函数，它们调用的代码直接在上面，进一步的函数调用堆叠得更高。

我们还可以看到，到目前为止，大多数时间都花在了 smooth_weights()函数上，因为它几乎从图形的最左边一直延伸到最右边。

如果您将鼠标指针移动到窗口中，无论是在代码中还是在底部图形中，它都会突出显示您所指向的内容，如图 [14-2](#Fig2) 所示。你可以用它来计算时间都花在哪里了。

![A439481_1_En_14_Fig2_HTML.jpg](Images/A439481_1_En_14_Fig2_HTML.jpg)

###### 图 14-2。从分析窗口突出显示正在执行的代码

在这种特殊情况下，看起来大部分时间都花在了内部循环上，检查是否存在边。由于这是一个双循环的内部部分，这可能并不令人惊讶。它不是内部循环的全部，而是 if 语句的原因可能是，我们在每次迭代中检查 if 表达式，但我们不执行它的主体，除非它为真。对于 1000 个节点和 300 条边，它仅在概率约为 300/(1000*1000) = 3 × 10-4 时成立(它可以更小，因为一些边可能是相同的或自循环的)。

现在，如果我们对这段代码有性能问题，这就是我们应该集中精力进行优化的地方。有了 1000 个节点，我们真的没有问题。1800 ms 毕竟不是很长的时间。但是我想到的应用程序有大约 30，000 个节点，所以可能值得稍微优化一下。

如果你需要优化一些东西，你首先应该考虑的是——有没有更好的算法或者更好的数据结构？与仅仅改变实现的细节相比，算法的改进更有可能带来实质性的性能改进。

在这种情况下，如果我们正在处理的图是稀疏的，这意味着与所有可能的边相比，它们几乎没有实际的边，那么关联矩阵不是一个好的表示。我们*可以*通过使用向量表达式来代替内部循环和类似的 hacks 来加速代码，但是我们最好考虑图的另一种表示。

这里当然要先搞清楚我们所用的模拟数据是否代表了我们需要分析的实际数据。如果实际的数据是一个密集的图形，而我们在一个稀疏的图形上进行性能分析，我们就不能得到正确的印象，即时间花在了哪里，我们可以在哪里进行合理的优化。但是我声称，我心目中的应用是使用稀疏图的应用。

对于稀疏图，我们应该用不同的格式来表示边。我们将边表示为一个列表，而不是矩阵，对于每个节点，我们都有一个该节点邻居的向量。

我们可以这样实现这种表示:

```
graph <- function(n, edges) {
  neighbours <- **vector**("list", length = n)

  for (i in **seq_along**(neighbours)) {
    neighbours[[i]] <- **vector**("integer", length = 0)
  }

  no_edges <- **length**(edges)
  if (no_edges >= 1) {
    for (i in **seq**(1, no_edges, by = 2)) {
      n1 <- edges[i]
      n2 <- edges[i+1]
      neighbours[[n1]] <- **c**(n2, neighbours[[n1]])
      neighbours[[n2]] <- **c**(n1, neighbours[[n2]])
    }
  }

  for (i in **seq_along**(neighbours)) {
    neighbours[[i]] <- **unique**(neighbours[[i]])
  }

  **structure**(neighbours, class = "graph")
}
```

我们首先生成边向量列表，然后将它们全部初始化为空整数向量。然后，我们遍历输入边并更新边向量。我们更新向量的方式在计算上可能很慢，因为我们在每次更新中都强制使用前一个向量的副本，但是我们事先不知道这些向量的长度，所以这是一个简单的解决方案，如果分析认为这是一个问题，我们可以稍后再担心。

现在，如果我们作为输入得到的边包含相同的节点对两次，我们将得到相同的边表示两次。这意味着在计算邻居权重的平均值时，节点的同一个邻居将被使用两次。如果我们希望在应用程序中允许这样的多条边，这很好，但我们没有这样做，所以我们明确地确保同一个邻居只被表示一次，方法是在最后对所有向量调用 unique()函数。

有了这个图形表示，我们可以将平滑函数更新为:

```
smooth_weights <- function(graph, node_weights, alpha) {
  if (**length**(node_weights) != **length**(graph))
    **stop**("Incorrect number of nodes")

  no_nodes <- **length**(node_weights)
  new_weights <- **vector**("numeric", no_nodes)

  for (i in 1:no_nodes) {
    neighbour_weights <- 0
    n <- 0
    for (j in graph[[i]]) {
      if (i != j) {
        neighbour_weights <- neighbour_weights + node_weights[j]
        n <- n + 1
      }
    }

    if (n > 0) {
      new_weights[i] <-
        alpha * node_weights[i] +
        (1 - alpha) * neighbour_weights / n
    } else {
      new_weights[i] <- node_weights[i]
    }

  }
  new_weights
}
```

变化很小。我们只是确保 *j* 只遍历我们知道是节点 *i* 的邻居的节点。

概要分析代码与之前相同，如果我们运行它，我们会得到如图 [14-3](#Fig3) 所示的结果。

![A439481_1_En_14_Fig3_HTML.jpg](Images/A439481_1_En_14_Fig3_HTML.jpg)

###### 图 14-3。第一次更改后的分析结果

我们看到我们获得了显著的性能提升。执行时间现在是 20 毫秒，而不是 1800 毫秒。我们还可以看到，一半时间用于构建图形，另一半时间用于平滑图形。在构造中，几乎所有的时间都花在 unique()上，而在 smoothing 函数中，时间都花在实际计算邻居的平均值上。

不过，这里应该说明的是，分析器是通过在特定时间点对正在执行的代码进行采样来工作的。它没有无限的分辨率，正如左下角所说，它每 10 毫秒采样一次，所以实际上，它在这次运行中只采样了两次。我们看到的结果只是因为样本碰巧分别击中了图形构造和平滑中的这两个位置。我们在这里看不到细节。

为了获得更多的细节，并更接近实际输入的大小，我们可以尝试将图的大小增加到 10，000 个节点和 600 条边。

```
profvis::**profvis**({
  n <- 10000
  nodes <- 1:n
  edges <- **sample**(nodes, 1200, replace = TRUE)
  weights <- **rnorm**(n)
  g <- **graph**(n, edges)
  **smooth_weights**(g, weights, 0.8)
})
```

该分析的结果如图 [14-4](#Fig4) 所示。

![A439481_1_En_14_Fig4_HTML.jpg](Images/A439481_1_En_14_Fig4_HTML.jpg)

###### 图 14-4。用更大的图形分析结果

令我们惊讶的是，我们发现对于较大的图形，我们实际上花在构建图形上的时间比平滑图形的时间要多。我们还看到，这段时间花在了调用 unique()函数上。

现在，这些调用是避免重复边所必需的，但它们不一定会是我们经常看到的东西——至少在随机图中，它们不太可能发生——所以这些调用中的大多数并没有真正做任何事情。

如果我们可以在一次调用 unique()中删除所有重复的边，我们应该可以节省一些时间。我们可以做到这一点，但它需要在构造函数中多做一点工作。

我们想使边缘独特，这里有两个问题。一个是我们实际上没有将它们表示为可以对其调用 unique()的对，并且对 edges 向量调用 unique()肯定不是一个解决方案。另一个问题是同一条边可以用两种不同的方式表示:( *i，j* )和( *j，i* )。

我们可以通过将向量转化为矩阵来解决第一个问题。如果我们在一个矩阵上调用 unique()，我们会得到唯一的行，所以我们只是用那种方式来表示对。第二个问题我们可以通过确保以规范的形式表示边来解决，比如要求边使用*I*<*j*(*I*， *j* )。

```
graph <- function(n, edges) {
  neighbours <- **vector**("list", length = n)

  for (i in **seq_along**(neighbours)) {
    neighbours[[i]] <- **vector**("integer", length = 0)
  }

  no_edges <- **length**(edges)
  if (no_edges >= 1) {
    sources <- **seq**(1, no_edges, by = 2)
    destinations <- **seq**(2, no_edges, by = 2)

    edge_matrix <- **matrix**(NA, nrow = **length**(sources), ncol = 2)
    edge_matrix[,1] <- edges[sources]
    edge_matrix[,2] <- edges[destinations]

    for (i in 1:**nrow**(edge_matrix)) {
      if (edge_matrix[i,1] > edge_matrix[i,2]) {
        edge_matrix[i,] <- **c**(edge_matrix[i,2], edge_matrix[i,1])
      }
    }

    edge_matrix <- **unique**(edge_matrix)

    for (i in **seq**(1, **nrow**(edge_matrix))) {
      n1 <- edge_matrix[i, 1]
      n2 <- edge_matrix[i, 2]
      neighbours[[n1]] <- **c**(n2, neighbours[[n1]])
      neighbours[[n2]] <- **c**(n1, neighbours[[n2]])
    }
  }

  **structure**(neighbours, class = "graph")
}
```

与以前相比，运行时间减少了一半，并且花在构建图上的时间相对较少。执行代码所花费的时间也很短，所以我们不能对分析样本说太多。

当我编写这段代码时，图形的大小并不完全符合应用程序的预期大小。我们可以将其提升到大约 20，000 个节点和 50，000 个边的完整大小，并分析该大小。结果如图 [14-5](#Fig5) 所示。

![A439481_1_En_14_Fig5_HTML.jpg](Images/A439481_1_En_14_Fig5_HTML.jpg)

###### 图 14-5。全时图上的性能分析结果

在一个全尺寸的图形上，我们仍然花费大部分时间来构建图形，而不是平滑它——大约一半的构建时间是在 unique()函数中——但是这有点误导。我们不期望在一个图上只调用一次平滑函数。对平滑函数的每一次调用都会使权重变得更加平滑，我们希望在实际应用中运行十次左右。

我们可以将该函数重命名为 flow_weights_iteration()，然后编写一个 smooth_weights()函数，使其运行多次迭代:

```
flow_weights_iteration <- function(graph, node_weights, alpha) {
  if (**length**(node_weights) != **length**(graph))
    **stop**("Incorrect number of nodes")

  no_nodes <- **length**(node_weights)
  new_weights <- **vector**("numeric", n)

  for (i in 1:no_nodes) {
    neighbour_weights <- 0
    n <- 0
    for (j in graph[[i]]) {
      if (i != j) {
        neighbour_weights <- neighbour_weights + node_weights[j]
        n <- n + 1
      }
    }

    if (n > 0) {
      new_weights[i] <- (alpha * node_weights[i] + (1 - alpha)
                           * neighbour_weights / n)
    } else {
      new_weights[i] <- node_weights[i]
    }

  }
  new_weights
}
smooth_weights <- function(graph, node_weights, alpha, no_iterations) {
  new_weights <- node_weights
  **replicate**(no_iterations, {
      new_weights <- **flow_weights_iteration**(graph, new_weights, alpha)
  })
  new_weights
}
```

然后，我们可以通过 10 次迭代进行分析:

```
profvis::**profvis**({
  n <- 20000
  nodes <- 1:n
  edges <- **sample**(nodes, 100000, replace = TRUE)
  weights <- **rnorm**(n)
  g <- **graph**(n, edges)
  **smooth_weights**(g, weights, 0.8, 10)
})
```

结果如图 [14-6](#Fig6) 所示。显然，如果我们运行平滑函数更多次，平滑将占用更多的总时间，所以这里没有真正的惊喜。已经没有什么明显的热点可以挖掘了。我在迭代中使用了 replicate()函数，它确实有一点开销，因为它不仅仅是循环，它还创建了一个结果向量，通过用显式循环替换它，我可以多获得几毫秒的时间:

![A439481_1_En_14_Fig6_HTML.jpg](Images/A439481_1_En_14_Fig6_HTML.jpg)

###### 图 14-6。使用多次平滑迭代分析结果

```
smooth_weights <- function(graph, node_weights,
                               alpha, no_iterations) {
  new_weights <- node_weights
  for (i in 1:no_iterations) {
    new_weights <-
      **smooth_weights_iteration**(graph, new_weights, alpha)
  }
  new_weights
}
```

我还没有展示结果，所以你必须相信我。然而，现在已经没有什么可攻击的了。

如果您处于没有更明显的东西可以尝试加速的情况下，您必须考虑是否真的有必要进行更多的优化。从这一点开始，除非你能想出一个更好的算法，这是很难的，进一步的优化将是非常困难的，不太可能值得努力。在计算运行时，你最好把时间花在别的事情上，而不是浪费几天时间试图从中获得更多的性能。

当然，在某些情况下，你真的有*更多的*来提高性能，以便在合理的时间内做你的分析，并且有一些最后的手段你可以去做，比如并行化你的代码或者把它的时间关键部分转移到 C++。但是现在，我们可以在不到两秒钟的时间内分析完整的图形，所以我们绝对不应该在优化这个特定的代码上花费更多的时间。

## 加速您的代码

如果你真的有性能问题，你会怎么做？我将假设您没有在处理一个其他人已经解决的问题——如果已经有一个您可以使用的包，那么您应该使用它，而不是编写您自己的代码，当然。但是可能会有类似的问题，你可以适应你的需要，所以在你做任何事情之前，做一点研究，看看是否有人已经解决了类似的问题，如果是，他们是怎么做的。生活中很少有真正独特的问题，不借鉴别人的经验是很傻的。

不过，可能需要一点时间来弄清楚要搜索什么，因为类似的问题会出现在非常不同的领域。可能会有一个解决方案，只是你不知道如何搜索，因为它是用完全不同于你自己领域的术语描述的。询问邮件列表或堆栈溢出可能会有所帮助(见[http://stackoverflow.com](http://stackoverflow.com)),但是不要在每一件小事上都寻求帮助，你应该做一点工作就能弄清楚。

如果你真的不能找到一个现有的解决方案，下一步就是开始考虑算法和数据结构。改善这些通常比微优化对性能的影响更大。您在任何优化中的第一次尝试应该是弄清楚是否可以使用更好的数据结构或更好的算法。

当然，重新实现复杂的数据结构或算法是一项更艰巨的任务——如果您能找到已经实现的解决方案，就不应该这样做——但这通常是您获得最高性能的地方。当然，在你花费在重新实现一个算法上的时间和你获得的收益之间总是有一个权衡，但是随着经验的积累，你会更好地判断这一点。嗯，稍微好一点。如果有疑问，忍受缓慢的代码可能比花费大量时间试图改进它更好。

在你做任何事情之前，确保你有单元测试来确保新的实现不会破坏旧的功能！你的新代码可以像闪电一样快，如果不正确，它就一文不值。

如果您已经研究了现有的包和新的算法和数据结构，并且仍然存在性能问题，那么您就达到了微优化的水平。这是您使用稍微不同的函数和表达式来尝试提高性能的地方，在这种级别的更改中，您不太可能获得巨大的改进。但是如果你的代码被执行了数千次或数百万次，这些小的收益仍然可以累积起来。因此，如果您的分析突出了几个性能热点，您可以尝试在那里重写代码。

在这种级别的优化中，采样分析器并不是非常有用。它以毫秒级进行采样，这通常是一个比您在这里需要的更粗粒度的测量。相反，您可以使用 microbenchmark 包来评估和比较表达式。microbenchmark()函数多次运行一系列表达式，并以纳秒为单位计算执行时间的统计数据。如果您想通过微优化获得一些性能，您可以使用它来评估您的计算的不同替代方案。

例如，我们可以用它来比较 sum()的 R 实现和内置的 sum()函数:

```
**library**(microbenchmark)
mysum <- function(sequence) {
  s <- 0
  for (x in sequence) s <- s + x
  s
}

**microbenchmark**(
  **sum**(1:10),
  **mysum**(1:10)
)
## Unit: nanoseconds
##         expr  min   lq    mean median     uq
##    sum(1:10)  194  202  300.10  233.5  349.5
##  mysum(1:10) 1396 1592 2280.47 1750.0 1966.5
##    max neval cld
##   2107   100  a
##  11511   100   b
```

输出中的第一列是计算的表达式，然后是计算时观察到的最小值、下四分之一、平均值、中值、上四分之一和最大值时间，以及使用的计算次数。最后一列是性能排名，这里显示 sum()是 a，mysum()是 b，所以前者更快。这种排序考虑了评估时间的变化，而不仅仅是根据平均值进行排序。

在微优化中有一些加快代码速度的经验法则，但是你应该经常测量。直觉通常是衡量的一个很糟糕的替代品。

一个经验法则是尽可能使用内置函数。sum()之类的函数实际上是用 C 实现的，并且经过了高度优化，所以您自己的实现将很难与之竞争，正如您之前看到的那样。

另一个经验法则是使用最简单的函数来完成工作。更通用的函数会引入各种开销，而更简单的函数可以避免这些开销。

您可以使用 Reduce()将一个序列中的所有数字相加，但是与专用函数相比，使用这样一个通用函数会相对较慢。

```
**microbenchmark**(
  **sum**(1:10),
  **mysum**(1:10),
  **Reduce**(`+`, 1:10, 0)
)
## Unit: nanoseconds
##                  expr  min   lq    mean median
##             sum(1:10)  207  258  356.03  324.5
##           mysum(1:10) 1611 1892 2667.25 2111.0
##  Reduce(`+`, 1:10, 0) 4485 5285 6593.07 6092.0
##      uq   max neval cld
##   409.0  1643   100 a
##  2369.0 11455   100  b
##  6662.5 15497   100   c
```

为了编程方便，我们使用这样的通用函数。他们给了我们抽象的积木。我们很少能从它们那里获得性能提升，有时它们会降低速度。

第三，做尽可能少的事情。R 中的许多函数比我们想象的有更多的功能。像 read.table()这样的函数不仅可以读入数据，还可以计算出每一列应该是什么类型。如果您使用 colClasses 参数告诉它每一列的类型，它会变得更快，因为它不需要自己找出答案。对于 factor()，您可以使用 levels 参数给它允许的类别，这样它就不必自己计算了。

```
x <- **sample**(LETTERS, 1000, replace = TRUE)
**microbenchmark**(
  **factor**(x, levels = LETTERS),
  **factor**(x)
)
## Unit: microseconds
##                         expr    min      lq
##  factor(x, levels = LETTERS) 19.211 20.8975
##                    factor(x) 59.458 61.9575
##      mean  median     uq     max neval cld
##  22.03447 21.6175 22.610  32.981   100  a
##  66.70901 62.9135 67.946 132.306   100   b
```

不仅仅是在提供输入的时候，帮助函数避免计算出一些东西，这是有效的。函数也经常返回比您感兴趣的更多的内容。例如，像 unlist()这样的函数会将列表的名称保存到结果向量中。除非你真的需要这些名字，否则你应该去掉它们，因为拖着这些名字走是很费钱的。如果你只是对一个数字向量感兴趣，你应该使用 use.names = FALSE:

```
x <- **rnorm**(1000)
**names**(x) <- **paste**("n", 1:1000)
**microbenchmark**(
  **unlist**(**Map**(function(x) x**2, x), use.names = FALSE),
  **unlist**(**Map**(function(x) x**2, x))
)
## Unit: microseconds
##                                                expr
##  unlist(Map(function(x) x^2, x), use.names = FALSE)
##                     unlist(Map(function(x) x^2, x))
##      min      lq     mean   median       uq
##  484.866 574.248 704.2379 660.3140 716.2325
##  659.355 722.974 825.7712 813.3855 891.4630
##       max neval cld
##  3141.598   100  a
##  1477.028   100   b
```

第四，尽可能使用向量表达式，而不是循环。不仅仅是因为这使得代码更容易阅读，还因为 R 的运行时系统处理向量表达式中的隐式循环比处理显式循环要快得多。

然而，最重要的是，当你试图提高性能时，要*始终*衡量，只有当有实质性的改进值得时，才使用更复杂的代码替换简单的代码。

## 并行执行

有时候你可以加快速度，不是通过做得更快，而是通过并行做很多事情。当今大多数计算机都有不止一个内核，这意味着您应该能够并行运行更多的计算。

这些通常基于 lapply()或 Map()或类似的一些变体，请参见 parallel 包作为示例，但也要查看 foreach 包，它提供了更高级别的循环结构，也可用于并行运行代码。

如果我们考虑我们的图形平滑，我们可以认为，由于每个节点是一个独立的计算，我们应该能够通过并行运行这些计算来加速功能。如果我们将内部循环移到局部函数中，我们可以用对 Map()的调用来替换外部外观:

```
smooth_weights_iteration_map <- function(graph, node_weights, alpha) {
  if (**length**(node_weights) != **length**(graph))
    **stop**("Incorrect number of nodes")

  handle_i <- function(i) {
    neighbour_weights <- 0
    n <- 0
    for (j in graph[[i]]) {
      if (i != j) {
        neighbour_weights <- neighbour_weights + node_weights[j]
        n <- n + 1
      }
    }

    if (n > 0) {
      alpha * node_weights[i] + (1 - alpha) * neighbour_weights / n
    } else {
      node_weights[i]
    }
  }

  **unlist**(**Map**(handle_i, 1:**length**(node_weights)))
}
```

这不太可能加快速度——高级 Map()函数中的额外开销会起到相反的作用——但是它允许我们用并行函数中的一个函数替换 Map()，例如 clusterMap():

```
  **unlist**(**clusterMap**(cl, inner_loop, 1:**length**(node_weights)))
```

这里，cl 是“集群”,仅由我笔记本电脑上的两个内核组成:

```
cl <- **makeCluster**(2, type = "FORK")
**microbenchmark**(
  **original_smooth**(),
  **using_map**(),
  **using_cluster_map**(),
  times = 5
)
```

这三个函数指的是算法的三个不同版本，给了我这些结果。在我的双核笔记本电脑上，我们可以预期并行版本的运行速度会快两倍。事实上，它的运行速度慢了几个数量级:

```
Unit: milliseconds
                expr         min          lq
   original_smooth()    33.58665    33.73139
         using_map()    33.12904    34.84107
 using_cluster_map() 14261.97728 14442.85032
        mean      median          uq         max
    35.88307    34.25118    36.62977    41.21634
    38.31528    40.50315    41.28726    41.81587
 15198.55138 14556.09176 14913.24566 17818.59187
 neval cld
     5  a
     5  a
     5   b
```

使用类型 FORK 来设置集群只能在 UNIX 机器上工作，所以在 Windows 上您必须使用另一种类型。使用默认参数 PSOCK，您可以设置并行化，但是运行您的计算的不同内核不会知道您已经导入的库或您在主脚本中定义的函数。如果你不能使用类型 FORK，你将需要明确地通知内核他们应该知道哪些值和函数。查看函数 clusterExport 和 clusterCall 的文档。

我不完全确定我们在这里看到的问题是什么，但最有可能的是单个任务非常短，线程(这里实际上是进程)之间的通信开销最终比实际计算花费的时间多得多。至少我的侧写显示了这一点。使用真正轻量级的线程，可以避免一些通信，但这不是我们这里所拥有的。

当每个任务运行更长时间时，并行化效果会更好，因此线程不必经常通信。

对于并行化效果更好的示例，我们可以考虑在训练数据上拟合模型，并在测试数据上测试其准确性。我们可以使用之前看过的 cars 数据和第 [6](06.html) 章中的 partition()函数。

我们编写一个函数来评估一个单独的训练/测试分区，然后顺序或并行地调用它 n 次。

```
test_rmse <- function(data) {
  model <- data$training %>% **lm**(dist ∼ speed, data = .)
  predictions <- data$test %>% **predict**(model, data = .)
  **rmse**(data$test$dist, predictions)
}

sample_rmse <- function (n) {
  random_cars <- cars %>%
    **partition**(n, **c**(training = 0.5, test = 0.5))
  **unlist**(**Map**(test_rmse, random_cars))
}

sample_rmse_parallel <- function (n) {
  random_cars <- cars %>%
    **partition**(n, **c**(training = 0.5, test = 0.5))
  **unlist**(**clusterMap**(cl, test_rmse, random_cars))
}
```

当我对 10 个训练/测试分区这样做时，这两个函数花费的时间大致相同。也许水货版慢了*一点点*，但这次开销并不大。

```
**microbenchmark**(
  **sample_rmse**(10),
  **sample_rmse_parallel**(10),
  times = 5
)
Unit: milliseconds
                     expr      min       lq
          sample_rmse(10) 28.72092 29.62857
 sample_rmse_parallel(10) 26.08682 27.15047
     mean   median       uq      max neval cld
 31.57316 33.05759 33.21979 33.23894     5   a
 34.75991 28.17528 29.37144 63.01556     5   a
```

然而，如果我创建 1000 个训练/测试分区，并行版本开始比顺序版本运行得更快。

```
**microbenchmark**(
  **sample_rmse**(1000),
  **sample_rmse_parallel**(1000),
  times = 5
)
Unit: seconds
                       expr      min       lq
          sample_rmse(1000) 3.229113 3.277292
 sample_rmse_parallel(1000) 2.570328 2.595402
     mean   median       uq      max neval cld
 3.459333 3.508533 3.536792 3.744934     5   b
 2.921574 2.721095 3.185070 3.535977     5  a
```

由于我的笔记本电脑只有两个内核，它永远无法运行两倍以上的速度，并且通过并行化实现可能的最佳加速几乎是不可能的。线程间的通信开销增加了并行版本所用的时间，而且有些代码必须是连续的，比如准备所有线程都应该处理的数据。

如果您有一台具有许多内核的机器，并且您可以将您的分析分成相当大的独立块，那么通常会有所收获。

## 切换到 C++

这是一个激烈的步骤，但是通过切换到 C++这样的语言，您可以对计算机进行更细粒度的控制，因为您可以在更低的级别上编程，并且您没有 R 所具有的来自运行时系统的开销。当然，这也意味着您没有 R 所具有的许多特性，所以您不希望用 C++编写整个分析，但是您可能希望将时间关键的代码翻译成 C++。

幸运的是，Rcpp 包使得集成 R 和 C++变得非常简单。当然，前提是你可以用两种语言编程。唯一需要小心的是 C++从 0 开始索引，R 从 1 开始。Rcpp 负责转换这一点，所以在 C++中可以将 R 中的索引为 1 的向量作为索引为 0 的向量来访问，但是在翻译代码时，您必须记住这一点。

对 C++和 R 之间通信框架的完整描述远远超出了本书的范围。为此，我会向您推荐一本优秀的书，Dirk Eddelbuettel 的《R 和 C++与 Rcpp 的无缝集成》。在这里，我将向您展示如何使用 Rcpp 来加速一个函数。

我们将再次关注平滑函数。这是一个相对简单的函数，没有使用 R 的任何高级特性，所以非常适合翻译成 C++。我们几乎可以一字不差地这样做，只要记住我们应该从零而不是从一开始索引。

```
NumericVector
smooth_weights_iteration_cpp(List g,
                             NumericVector node_weights,
                             double alpha)
{
  NumericVector new_weights(g.length());

  **for** (int i = 0; i < g.length(); ++i) {

    IntegerVector neighbours = g[i];
    double neighbour_sum = 0.0;
    int n = 0;

    **for** (int j = 0; j < neighbours.length(); ++j) {
      neighbour_sum += node_weights[j];
      ++n;
    }

    **if** (n > 0) {
      new_weights[i] = alpha * node_weights[i] +
        (1-alpha) * (neighbour_sum / n);
    } **else** {
      new_weights[i] = node_weights[i];
    }
  }

  **return** new_weights;
}
```

类型 List、NumericVector 和 IntegerVector 对应于 R 类型，除了我们如何创建 new_weights 向量之外，代码非常接近 R 代码。

有几种方法可以编译这个函数并把它包装成一个 R 函数，但是最简单的方法是把它放在一个字符串中并把它交给函数 cppFunction():

```
**cppFunction**("
NumericVector
smooth_weights_iteration_cpp(List g,
                             NumericVector node_weights,
                             double alpha)
{
  NumericVector new_weights(g.length());

  for (int i = 0; i < g.length(); ++i) {

    // The body here is just the C++ code
    // shown above...

  }

  return new_weights;
}
")
```

这就创建了一个与 C++函数同名的函数，可以直接从 R 中调用，Rcpp 会根据需要负责类型转换。

```
smooth_weights_cpp <- function(graph, node_weights,
                               alpha, no_iterations) {
  new_weights <- node_weights
  for (i in 1:no_iterations) {
    new_weights <-
      **smooth_weights_iteration_cpp**(graph, new_weights, alpha)
  }
  new_weights
}
```

如果我们比较一下 R 和 C++函数，我们会发现我们从中获得了实质性的性能提升。

```
**microbenchmark**(
  **smooth_weights**(g, weights, 0.8, 10),
  **smooth_weights_cpp**(g, weights, 0.8, 10),
  times = 5
)
Unit: milliseconds
                                    expr
     smooth_weights(g, weights, 0.8, 10)
 smooth_weights_cpp(g, weights, 0.8, 10)
        min         lq       mean    median
 1561.78635 1570.23346 1629.12784 1572.3979
   32.77344   33.38822   35.57017   36.5103
         uq        max neval cld
 1694.31571 1746.90573     5   b
   37.29083   37.88803     5  a
```

要将一个函数翻译成 C++，您不一定会被阻止使用 R 的更高级的特性。您可以像从 R 中调用 C++函数一样轻松地从 C++中调用 R 函数。使用翻译成 C++的 R 类型在许多情况下可以像在 R 中一样用于向量表达式。但是，请注意，当您在 C++中使用高级功能时，它们的运行时开销与在 R 中相同。您可能不会从翻译此类函数中获得太多性能。不过，翻译嵌套循环之类的低级代码通常会给你带来实质性的性能提升。如果您的代码中有一些相对简单的性能热点，只是因为它们做了大量的工作而非常耗时，那么值得考虑将它们转换为 C++，Rcpp 使之变得容易。

不过，不要太过火。在 C++中分析和调试代码更难，如果是 C++和 r 的混合，重构代码更难。使用它，但只在真正需要时使用它。

## 练习

找一些你写的代码，试着分析一下。如果您可以识别性能热点，那么尝试优化它们。首先，考虑算法的变化，然后是 R 表达式的变化——使用 microbenchmark()检查——如果其他都失败了，尝试用 C++并行化或实现它们。

## 项目 2

本章的项目是为贝叶斯线性回归构建一个 R 包。我们将使用的模型在某种程度上是一个我们可以想象我们可以为之构建 R 包的玩具示例，我们的目标不是开发贝叶斯线性回归的所有花哨功能。我们将构建足够多的东西来看看构建一个真正的 R 包的各个方面。

## 贝叶斯线性回归

在线性回归中我们假设我们有预测变量 *x* 和目标变量 *y* 其中*y*=*w*<sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">0</sub>+*w*<sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">1</sub>*x+ε*其中ε∾N(0，σ <sup xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre6">2</sup> )。也就是说，我们有一条截距为*w*T20】0 且倾斜度为*w*T24】1 的直线，使得目标变量正态分布在直线给出的点周围。我们有时把*σ*T28】2 写成 1/ *β* ，把 *β* 称为精度。我将在这里这样做，并假设 *β* 是一个已知量；我们将考虑估计权重 w<sup xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre6">*T*</sup>=(*w*<sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">0</sub>， *w* <sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">1</sub> )的贝叶斯方法。

既然我们假设我们知道精度参数 *β* ，*如果*我们知道模型的真实权重，那么无论何时我们有一个 *x* 值，我们就会知道 *y* 值的分布:*y*∨N(*w*<sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">0</sub>+*w*<sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">1</sub>*x*，1/*β【1*

出于符号化的目的，我将定义一个函数，将 *x* 映射到一个向量: *ϕ* : *x* ↦ (1， *x* ) <sup xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre6">*T*</sup> 。然后我们有**w**<sup xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre6">*t*</sup>*ϕ*(*x*)=*w*<sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">0</sub>+*w*<sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">1</sub>*x*和*y*∾n(**w**<sup xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre6">*t*</sup>

当然，我们不知道权重的值，但必须估计它们。在贝叶斯方法中，我们不认为权重是固定但未知的值；我们认为它们是随机变量，来自我们部分了解的分布。了解权重意味着根据观察到的 *x* 和 *y* 值来估计向量 **w** 的后验分布。

我们将为 **w** 、 *p* ( **w** )假设一些先验分布。如果我们观察一系列匹配的 *x* 和 *y* 值，**x**<sup xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre6">*T*</sup>=(*x*<sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">1</sub>， *x* <sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">2</sub> ，…， *x* <sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">*n*</sub> )和 **y** <sup xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre6">*y* <sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">*n*</sub> )，我们希望将权重 **w** 的先验分布更新为后验分布 *p* ( **w | x** ， **y** )</sup>

我们将假设 **w** 的先验分布是均值为零且对角协方差矩阵具有某种(已知超参数)精度的正态分布 *α* ，如:
![$$ p\left(\boldsymbol{w}\left|\alpha \right.\right)= N\left(\mathbf{0},{\alpha}^{-1}\boldsymbol{I}\right) $$](Images/A439481_1_En_14_Chapter_Equa.gif)

由于我没有时间或空间在这里讨论的原因，这是线性模型的一个很好的先验选择，因为它意味着后验也将是正态分布。这也意味着，给定 **x** 和 **y** ，我们可以通过一些简单的矩阵运算来计算后验的均值和协方差矩阵。

但是首先，我们需要定义我们的模型矩阵。这是一个矩阵，它捕捉到我们试图寻找的线性模型是一条线，即*y = w*<sub class="calibre10">0</sub>+*w*<sub class="calibre10">1</sub>*x*。我们为观测向量 **x** 定义模型矩阵如下:
![$$ {\boldsymbol{\phi}}_x=\left[\begin{array}{cc}\hfill 1\hfill & \hfill {x}_1\hfill \\ {}\hfill 1\hfill & \hfill {x}_2\hfill \\ {}\hfill 1\hfill & \hfill {x}_3\hfill \\ {}\hfill \vdots \hfill & \hfill \vdots \hfill \\ {}\hfill 1\hfill & \hfill {x}_n\hfill \end{array}\right] $$](Images/A439481_1_En_14_Chapter_Equb.gif)

一般来说，我们会为每个观察值设置一行，其中包含我们希望包含在模型中的观察值的各种特征，但对于简单的线来说，它是倾斜和截距，因此对于观察值 *i* 它是 1，而*x*<sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">T5】I</sub>。

权重的后验分布 *p* ( **w | x** 、 **y** 、 *α* 、 *β* )，则由
![$$ p\left(\boldsymbol{w}\boldsymbol{\Big|}\boldsymbol{x},\ \boldsymbol{y},\ \alpha,\ \beta \right)= N\left({\boldsymbol{m}}_{x, y},{\boldsymbol{S}}_{x, y}\right) $$](Images/A439481_1_En_14_Chapter_Equc.gif)给出

在哪里

![$$ {\boldsymbol{m}}_{\boldsymbol{x},\boldsymbol{y}}=\beta\;{\boldsymbol{S}}_{\boldsymbol{x},\boldsymbol{y}}{\boldsymbol{\varPhi}}_{\boldsymbol{x}}^T\boldsymbol{y} $$](Images/A439481_1_En_14_Chapter_Equd.gif)

和

![$$ {\boldsymbol{S}}_{\boldsymbol{x},\ \boldsymbol{y}}^{-1}=\alpha\;\boldsymbol{I} +\beta\;{\boldsymbol{\varPhi}}_{\boldsymbol{x}}^T\;{\boldsymbol{\varPhi}}_{\boldsymbol{x}} $$](Images/A439481_1_En_14_Chapter_Eque.gif)

### 练习:先验和后验

尝试以下练习，以便更好地理解本章中讨论的概念。

#### 多元正态分布的样本

如果要从多元正态分布中进行采样，MASS 包中的 mvrnorm 函数就是您想要的。

```
**library**(MASS)
**mvrnorm**(n = 5, mu = **c**(0,0), Sigma = **diag**(1, nrow = 2))
##            [,1]       [,2]
## [1,]  0.6420163 -0.9853573
## [2,]  0.2112605  1.0362092
## [3,]  2.2689703 -0.1181916
## [4,] -0.9177489  0.6836801
## [5,] -0.8123927  0.7117685
```

你需要给它提供一个平均向量μ和一个协方差矩阵适马。

我们的权重向量的先验分布是 N( **0** ， **S** ， <sub class="calibre10">0</sub> )，其中
![$$ \mathbf{0}=\left({}_0^0\right) $$](Images/A439481_1_En_14_Chapter_Equf.gif)

和![$$ {\boldsymbol{S}}_0=\frac{1}{\alpha}\boldsymbol{I} =\left(\begin{array}{cc}\hfill {\scriptscriptstyle \raisebox{1ex}{$1$}\!\left/ \!\raisebox{-1ex}{$\alpha $}\right.}\hfill & \hfill 0\hfill \\ {}\hfill 0\hfill & \hfill {\scriptscriptstyle \raisebox{1ex}{$1$}\!\left/ \!\raisebox{-1ex}{$\alpha $}\right.}\hfill \end{array}\right) $$](Images/A439481_1_En_14_Chapter_Equg.gif)

您可以使用 diag 函数创建对角协方差矩阵。

编写一个名为 make_prior(alpha)的函数来构造这个先验分布，另一个名为 sample_from_prior(n，alpha)的函数从中抽取 *n* 个权重向量**w**<sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">T5】I</sub>。我的版本将样本作为数据帧返回，其中一列用于*w*T10】1 样本，另一列用于相应的*w*T14】0 样本。你可以用任何对你方便的方式退回样品。

如果你能从先验中采样，你就能绘制出结果，既可以是在 **w** 空间中的点，也可以是在( *x* ， *y* )空间中的线，如图 [14-7](#Fig7) 和 [14-8](#Fig8) 所示。

![A439481_1_En_14_Fig7_HTML.jpg](Images/A439481_1_En_14_Fig7_HTML.jpg)

###### 图 14-7。从先验分布中采样的权重向量

![A439481_1_En_14_Fig8_HTML.gif](Images/A439481_1_En_14_Fig8_HTML.gif)

###### 图 14-8。从表示为线的先验分布中采样的权重向量

#### 计算后验分布

如果我们固定模型的参数， *β* 和**w**=(*w*T6】0，*w*<sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">1</sub>)<sup xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre6">*T*</sup>，就可以模拟出( *x* ， *y* )值。我们可以先随机选取一些 *x* 值，然后模拟相应的 *y* 值(见图 [14-9](#Fig9) )。

![A439481_1_En_14_Fig9_HTML.jpg](Images/A439481_1_En_14_Fig9_HTML.jpg)

###### 图 14-9。随机采样的(x，y)值

```
w0 <- 0.3 ; w1 <- 1.1 ; beta <- 1.3
x <- **rnorm**(50)
y <- **rnorm**(50, w1 * x + w0, 1/beta)
```

编写一个函数 make_posterior(x，y，alpha，beta)来计算权重的后验分布，并编写一个函数 sample_from_posterior 来从后验分布中进行采样。

使用这个采样函数，我们可以看到，随着( *x，y* )点的数量增加，后验分布如何以真实值为中心。在这里的图中，我在每种情况下从后验概率中采样了 10 个权重，但是增加了后验概率所基于的( *x，y* )点的数量。见图 [14-10](#Fig10) 和 [14-11](#Fig11) 。

![A439481_1_En_14_Fig10_HTML.gif](Images/A439481_1_En_14_Fig10_HTML.gif)

###### 图 14-10。从后面取样。真实值显示为十字。

![A439481_1_En_14_Fig11_HTML.jpg](Images/A439481_1_En_14_Fig11_HTML.jpg)

###### 图 14-11。从后面画的线。真正的线用红色显示。

### 预测新预测值的目标变量

给定一个新的值 *x* ，我们要对相应的 *y* 进行预测。对于一个固定的 **w** ，同样，我们有 *p* ( *y* | *x，* **w** ，*β*)= n(**w**<sup xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre6">*t*</sup>*ϕ*(*x*)，1/ *β* )，但是既然我们不知道训练数据改进我们预测的方式是，我们对由 **w** 的后验分布而不是先验分布
w 加权的 **w** 进行积分

这种对两个正态分布乘积的积分给出了另一个正态分布，并且可以证明它是:![$$ p\left( y\Big| x,\boldsymbol{x},\boldsymbol{y},\alpha, \beta \right)= N\left({m}_{\boldsymbol{x},\boldsymbol{y}}^T\phi (x),{\sigma}_{\boldsymbol{x},\boldsymbol{y}}^2(x)\right) $$](Images/A439481_1_En_14_Chapter_Equi.gif)

其中 m <sub class="calibre10">**x，y**</sub> 是来自 **w** 的后验分布的均值，其中
![$$ {\sigma}_{\boldsymbol{x},\boldsymbol{y}}^2(x)=\frac{1}{\beta}+\phi {(x)}^T{\boldsymbol{S}}_{\boldsymbol{x},\boldsymbol{y}}\phi (x) $$](Images/A439481_1_En_14_Chapter_Equj.gif)

其中**S**<sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">T3】x，yT5 是来自 **w** 的后验分布的协方差矩阵。</sub>

有了目标值的完整分布， *y* ，给定预测值， *x* ，我们当然可以进行预测。对于 *y* 的点预测当然是这个正态分布的均值，所以 m <sub class="calibre10">**x** ，**y**</sub><sup class="calibre6">*t*</sup>*ϕ*(*x*)。但是我们可以做的不仅仅是预测最可能的值，我们当然也可以得到置信值，因为我们知道分布。

针对给定的 *x* 值，编写一个预测最可能的 *y* 值的函数。使用它来绘制相对于( *x，y* )点的推断模型。见图 [14-12](#Fig12) 。

![A439481_1_En_14_Fig12_HTML.jpg](Images/A439481_1_En_14_Fig12_HTML.jpg)

###### 图 14-12。红色表示真实线性模型，蓝色表示预测值

利用您也有预测的 *y* 分布这一事实，编写一个函数，为您提供该分布的分位数，并使用它绘制预测值周围的 95%区间。见图 [14-13](#Fig13) 。

![A439481_1_En_14_Fig13_HTML.jpg](Images/A439481_1_En_14_Fig13_HTML.jpg)

###### 图 14-13。95%支持区间的预测

如果我们仅在用于训练模型的点周围绘制线，则仅绘制具有 95%支持区间的线不会直接显示点周围的不确定性如何取决于模型权重的不确定性。在这里，所有点的支持都大致相同。但是，如果我们远离用于训练的点，我们将会看到不同。权重——通过点的线——中有小的不确定性，放大并扩展了区间。参见图 [14-14](#Fig14) 。

![A439481_1_En_14_Fig14_HTML.jpg](Images/A439481_1_En_14_Fig14_HTML.jpg)

###### 图 14-14。支持区间为 95%的预测，范围更广

## 公式及其模型矩阵

我们继续研究贝叶斯线性回归，我们将归纳出适合我们的公式。

回想一下第 [6](06.html) 章，当用我们的模型拟合数据时，我们使用了一个所谓的模型矩阵(或设计矩阵)，其形式为:
![$$ {\varPhi}_{\boldsymbol{x}}=\left[\begin{array}{cc}\hfill 1\hfill & \hfill {x}_1\hfill \\ {}\hfill 1\hfill & \hfill {x}_2\hfill \\ {}\hfill 1\hfill & \hfill {x}_3\hfill \\ {}\hfill \vdots \hfill & \hfill \vdots \hfill \\ {}\hfill 1\hfill & \hfill {x}_n\hfill \end{array}\right] $$](Images/A439481_1_En_14_Chapter_Equk.gif)

此矩阵中的第*行 i* 包含向量(1，*x*T4】t5】IT7)=*ϕ*(*x*<sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">*I*</sub>)<sup xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre6">*t*</sup>捕捉预测变量为第 *i* 次观察， *x* <sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">*i*</sub> 它实际上有两个预测变量，只是其中一个是常数 1。因此，它捕获的是我们需要的输入，以预测预测变量在给定点的目标参数，因为我们有一个既有 *y* 轴截距又有倾斜度的模型，所以它需要两个变量。为了预测目标，我们使用该行的内积和拟合模型的权重:

![$$ {y}_i={\boldsymbol{w}}^T\phi \left({x}_i\right)+\mathit{\in}={w}_0\cdot 1+{w}_1\cdot {x}_i+\mathit{\in} $$](Images/A439481_1_En_14_Chapter_IEq2.gif)。

我们称 *ϕ* ( *x* )点 *x* 的*特征向量*和*模型矩阵*包含我们想要拟合或预测的每个数据点的一行，这样行 *i* 就是*ϕ*(*x*<sub class="calibre10">*I*</sub>)<sup class="calibre6">*t*</sup>使用我们在这里使用的形式上的特征向量，*ϕ*(*x*)<sup class="calibre6">*t*</sup>=(1， *x* )，我们正在拟合一条线，但是特征不一定要具有这种形式。我们可以做出更复杂的特征向量。

如果我们改为使用特征向量*ϕ*(*x*)<sup xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre6">*t*</sup>=(1， *x，x* <sup xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre6">*2*</sup> )并向 **w** 添加另一个权重，那么它现在有三个维度，(*w*t18】0，t20】w<sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">1</sub>， *w *y =***w**<sup xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre6">*t*</sup>*ϕ*(*x*<sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">*I*</sub>)+*ε*，当然除了现在的**w**<sup xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre6">*t*</sup>ϕ*(*x*

如果你现在在想“嘿，那不再是线性模型了！”你错了。模型中的线性实际上从未与 *x* 中的线性相关。在 **w** 中的线性使得模型是线性的，只要我们得到预测值作为像这样的权重向量和一些特征向量的内积，它就是我们正在处理的线性模型。你可以让特征向量 *ϕ* ( *x* )像你喜欢的那样疯狂。

如果你以同样的方式构建模型矩阵:![$$ {\varPhi}_x=\left[\begin{array}{c}\hfill \phi {\left({x}_1\right)}^T\hfill \\ {}\hfill \phi {\left({x}_2\right)}^T\hfill \\ {}\hfill \phi {\left({x}_3\right)}^T\hfill \\ {}\hfill \vdots \hfill \\ {}\hfill \phi {\left({x}_n\right)}^T\hfill \end{array}\right] $$](Images/A439481_1_En_14_Chapter_Equl.gif)

用于拟合权重和预测新目标值的数学原理是相同的，当然除了权重向量 **w** 具有反映特征向量中的维度的维度数量。

特征向量也不必是单个变量 *x* 的函数。如果你想在两个变量中拟合一个线性模型——比如一个平面——那么你可以只让特征向量依赖于两个变量: *ϕ* ( *x* ，*z*)<sup class="calibre6">*t*</sup>=(1， *x，z* )。与权重向量的线性组合将是**w**<sup class="calibre6">*t*</sup>*ϕ*(*x*，*z*)=*w*<sub class="calibre10">0</sub>+*w*<sub class="calibre10">1</sub>*x+w*<sub class="calibre10">2</sub>z，这将完全是两个中的线性模型

### 在 R 中使用模型矩阵

我们在 R 中指定特征向量和模型矩阵的方式是一个*公式*。一个公式被创建为一个包含波浪号“∞”的表达式，目标变量应该放在左边，解释变量放在右边。

R 有相当丰富的指定公式的语法，如果您感兴趣，您应该通过在 R shell 中编写以下内容来阅读文档:

```
?formula
```

对于线性模型，我们写 y∞x。截距变量隐含在那里；您不需要告诉 R 您希望特征向量包含“-1”，相反，您必须显式地删除它。您也可以指定多项式特征向量，但是 R 将乘法*解释为涉及变量之间交互的东西。 [<sup class="calibre6">1</sup>](#Fn1) 要指定你想要的是 *x* 的二阶多项式，你需要写出 y∞i(x^2)+x .函数 I 是恒等式函数，在这里使用它使 r 将 x^2 解释为对数字 x 求平方，而不是试图将其解释为公式规范的一部分。如果你只想要拟合 x 的平方，你可以写 y i(x^2).对于一般的 *n* 次多项式，可以使用 y∞poly(x，n，raw=TRUE)。

为了符合我们的线性模型，我们需要两件事的数据。在我们已经实现的模型中，我们有向量 **x** 和 **y** ，但是在一般情况下，预测变量 **x** 应该用模型矩阵**φ**代替。从**φ**和 **y，**我们可以拟合模型。

r 具有从公式和数据中获取数据的功能。然而，由于作用域规则的原因，这并不十分简单。如果您在代码中的某个地方编写了一个公式，您希望公式中的变量引用您所在范围内的变量。而不是代码*可能*查看公式的其他地方。因此公式需要捕捉当前的范围——类似于闭包捕捉它周围的范围。另一方面，您还希望能够通过数据框直接向模型提供数据。通常，您希望适合的数据是作为数据框中的列出现的，而不是作为范围内的单个变量出现的。有时甚至是混合。

model.frame 函数可让您获取采集与公式相关的数据所需的信息。它将知道公式的范围，但您也可以通过数据框添加数据。可以把它想象成一个 data.frame，只是多了一些从分析公式中获得的数据信息。

我们可以在这个小例子中看到所有这些:

```
predictors <- **data.frame**(x = **rnorm**(5), z = **rnorm**(5))
y <- **with**(predictors, **rnorm**(5, mean = 3*x + 5*z + 2))

model <- y ∼ x + z

**model.frame**(model, data = predictors)
##            y           x           z
## 1  0.9532092  0.02035145 -0.11778756
## 2 -2.7934758 -1.06963074  0.03347605
## 3  3.3926486  1.11133916 -0.43176349
## 4 10.8303077  1.68233471  1.27280461
## 5 -2.0380722 -0.32349845 -0.59252024
```

这里，我们在一个数据框中有两个预测变量 x 和 z，我们在全局范围内模拟了响应变量 y。我们使用公式 y∞x+z(这意味着 *ϕ* ( *x* ， *z* ) <sup class="calibre6">*T*</sup> = (1， *x，* z))创建的模型，并由此构建一个包含公式中使用的所有变量的数据的模型框架。

创建模型框架的方式是，R 首先在获得的数据框架中查找变量，如果存在，它使用该数据，如果不存在，它使用在公式范围内找到的数据。如果它根本找不到，它当然会报告一个错误。

数据框也用于从变量构造表达式。在作用域中，你可能有变量 *x* ，但没有变量*x*T4】2，后者是构建模型矩阵所需要的。model.frame 函数将为您构建它。

```
x <- **runif**(10)
**model.frame**(∼ x + **I**(x^2))
##             x       I(x^2)
## 1  0.35846214 0.128495....
## 2  0.76297492 0.582130....
## 3  0.42375496 0.179568....
## 4  0.09579368 0.009176....
## 5  0.48314622 0.233430....
## 6  0.56738521 0.321925....
## 7  0.89860842 0.807497....
## 8  0.51414054 0.264340....
## 9  0.15684623 0.024600....
## 10 0.30248554 0.091497....
```

在这个例子中，我们没有公式的响应变量；你不一定需要。你需要它能够提取向量 **y** 当然，我们需要一个用于线性模型拟合，但是 R 不一定需要。

一旦有了模型框架，就可以用函数 model.matrix 得到模型矩阵，它需要知道公式和模型框架(前者知道特征函数 *ϕ* ，后者知道我们正在拟合的数据)。

接下来，我们建立两个模型，一个是拟合一条穿过 *y* = 0 的线，另一个是让这条线在任意点与 *y* 轴相交。

请注意，数据框是相同的-两个模型中使用的变量是相同的-但是模型矩阵不同。

```
x <- **runif**(10)
y <- **rnorm**(10, mean=x)

model.no.intercept <- y ∼ x + 0
(frame.no.intercept <- **model.frame**(model.no.intercept))
##             y         x
## 1  -0.6475994 0.2170210
## 2   2.3601909 0.7161212
## 3   0.2708529 0.7415493
## 4   1.3094623 0.5982522
## 5   1.6820729 0.7725481
## 6  -0.3574741 0.3912436
## 7   0.5509808 0.4675246
## 8   0.8465933 0.6210651
## 9   1.6873893 0.7360315
## 10  1.3658199 0.8070293
**model.matrix**(model.no.intercept, frame.no.intercept)
##            x
## 1  0.2170210
## 2  0.7161212
## 3  0.7415493
## 4  0.5982522
## 5  0.7725481
## 6  0.3912436
## 7  0.4675246
## 8  0.6210651
## 9  0.7360315
## 10 0.8070293
## attr(,"assign")
## [1] 1
model.with.intercept <- y ∼ x
(frame.with.intercept <- **model.frame**(model.with.intercept))
##             y         x
## 1  -0.6475994 0.2170210
## 2   2.3601909 0.7161212
## 3   0.2708529 0.7415493
## 4   1.3094623 0.5982522
## 5   1.6820729 0.7725481
## 6  -0.3574741 0.3912436
## 7   0.5509808 0.4675246
## 8   0.8465933 0.6210651
## 9   1.6873893 0.7360315
## 10  1.3658199 0.8070293
**model.matrix**(model.with.intercept, frame.with.intercept)
##    (Intercept)         x
## 1            1 0.2170210
## 2            1 0.7161212
## 3            1 0.7415493
## 4            1 0.5982522
## 5            1 0.7725481
## 6            1 0.3912436
## 7            1 0.4675246
## 8            1 0.6210651
## 9            1 0.7360315
## 10           1 0.8070293
## attr(,"assign")
## [1] 0 1
```

目标向量或响应变量 **y** 也可以从数据帧中提取。这次不需要公式，因为数据框实际上记住了哪个变量是响应变量。您可以使用函数 model 从模型框架中获取它。响应:

```
**model.response**(frame.with.intercept)
##          1          2          3          4
## -0.6475994  2.3601909  0.2708529  1.3094623
##          5          6          7          8
##  1.6820729 -0.3574741  0.5509808  0.8465933
##          9         10
##  1.6873893  1.3658199
```

### 练习

尝试以下练习，以便更好地理解本章中讨论的概念。

#### 构建模型矩阵

构建一个函数，该函数将一个公式和一个数据框(可选)作为输入，并根据公式和可选数据构建模型矩阵。

#### 拟合通用模型

将您之前编写的拟合直线的函数扩展为可以拟合任何公式的函数。

### 没有响应变量的模型矩阵

当您拥有模型框架所需的所有变量时，以这种方式构建模型矩阵是很好的，但是当您没有目标值时会发生什么呢？当然，您需要目标值来拟合您的模型的参数，但是稍后，您想要预测您不知道目标的新数据点的目标，那么您如何构建模型矩阵呢？

对于一些明显伪造的数据，情况可能是这样的:

```
training.data <- **data.frame**(x = **runif**(5), y = **runif**(5))
frame <- **model.frame**(y ∼ x, training.data)
**model.matrix**(y ∼ x, frame)
##   (Intercept)         x
## 1           1 0.6983229
## 2           1 0.2849977
## 3           1 0.1836589
## 4           1 0.2277518
## 5           1 0.2773418
## attr(,"assign")
## [1] 0 1
predict.data <- **data.frame**(x = **runif**(5))
frame <- **model.frame**(y ∼ x, predict.data)
## Error in model.frame.default(y ∼ x, predict.data): variable lengths differ (found for 'x')
```

当然，当我们试图在不知道目标变量 y 的情况下构建框架时，会遇到一个问题。

要是有办法从公式中去掉响应变量就好了！确实有。

函数 delete.response 就是这样做的。您不能在公式中直接调用它。与您看到的其他函数不同，r 首先需要为这个函数的工作收集一些信息。但是您可以将它与函数项结合起来，得到一个没有响应变量的公式，然后您可以使用该公式为您不知道目标值的数据构建模型矩阵。

```
responseless.formula <- **delete.response**(**terms**(y ∼ x))
frame <- **model.frame**(responseless.formula, predict.data)
**model.matrix**(responseless.formula, frame)
##   (Intercept)          x
## 1           1 0.05530272
## 2           1 0.34011728
## 3           1 0.23095021
## 4           1 0.29074418
## 5           1 0.37240380
## attr(,"assign")
## [1] 0 1
```

### 练习

尝试以下练习，以便更好地理解本章中讨论的概念。

#### 新数据的模型矩阵

编写一个函数，将一个公式和一个数据框作为输入，该数据框不包含响应变量，并为其构建模型矩阵。

#### 预测新目标

更新您之前编写的用于预测新变量的值的函数，以处理符合通用公式的模型。如果它还不允许这样做，您还应该扩展它，以便它可以接受多个这样的数据点。使新数据点的输入以数据框的形式出现。

## blm 类的接口

到目前为止，我们已经实现了贝叶斯线性回归，但不一定是易于重用的形式。将与拟合模型相关的数据包装到一个类中，并提供各种方法来访问它，这使得重用模型/类变得容易。

一般来说，您希望尽可能多地通过函数来访问对象。如果您知道该类有哪些$字段，就很容易编写访问这些字段的代码，但是这使得以后很难更改该类的实现。许多假设对象外观的代码会崩溃。这也使得在以后的某个时间点很难改变分析中的模型/类，因为不同的类通常在内部看起来不一样。

为了让其他人——以及您未来的自己——更容易使用贝叶斯线性回归模型，我们将为它创建一个类，并提供使用它的函数。

这既包括编写特定于您自己的类的函数，也包括编写多态函数，一般来说，人们期望一个合适的模型来实现这些函数。正是后者使得用你的 blm 类替换另一个合适的模型成为可能。

一般来说，如何设计类和实现函数，以及选择实现哪些函数，都取决于您。当然，除了当您实现已经存在的多态函数的 blm 特定版本时；在这种情况下，您需要遵循现有的接口。

你如何选择表示你的类的对象，以及你选择为它实现哪些函数，通常取决于你自己。不过，R 中有一个通用的约定，即通过调用与给定类同名的函数来创建该类的对象。所以我建议你写一个叫 blm 的构造函数。

实际上没有任何明显的类可以继承，所以 blm 对象的类应该仅仅是“blm ”,而不是类的向量。如果你想在你的实现中建立一个类的层次结构，或者实现多个类来处理你的模型接口的不同方面，你应该尽力而为。

### 构造器

一个*构造函数*就是我们所说的创建一个给定类的对象的函数。在一些编程语言中，*创建*和*初始化*一个对象是有区别的。当你不得不担心内存管理时，这是最相关的，这样会变得非常复杂，这不是我们在 r 中所担心的事情。这就是为什么在 Python 中构造函数被称为 __init__ 并且它实际上处理的是初始化。Java 也是如此，它强制执行构造函数必须与类同名的规则，而对于 R 来说，这只是一个约定。在 Java 中，创建新对象有一个特殊的语法:new ClassName()。在 Python 中，你必须使用类名来创建对象——class name()——但是语法看起来就像函数调用。在 R 中，只有*约定*规定类名和构造函数应该相同。创建一个对象的语法看起来像一个函数调用，因为它*是*一个函数调用，除了它返回一个我们已经设置了 class 属性的对象之外，这个函数中没有什么特别的东西。

因此，您应该编写一个名为 blm 的函数，该函数返回一个将 class 属性设置为“blm”的对象。创建对象时，可以使用 class

### 更新发行版:界面示例

让我们考虑一个案例，我们可以把它作为贝叶斯线性模型的接口。这不是你必须要做的事情，但这是一个很好的尝试。

我们在贝叶斯统计中拟合模型时所做的事情是，当观察数据 *D* 时，我们获取模型参数的先验分布 *P* ( *θ* ，并将它们更新为后验分布 *P* ( *θ* | *D* )。可以这样想:先验分布就是我们所知道的参数。好吧，通常我们只是基于数学上的便利来编造先验，但是你应该把它想成我们从对宇宙如何运作的理解中所知道的参数，以及先验经验教给我们的东西。然后，当你观察更多的时候，你添加了关于世界的信息，这些信息改变了给定你所观察到的参数的条件概率。

在这里，我们所说的先验和后验并没有什么神奇之处。两者都是模型参数的分布。如果先验是基于先前的经验，那么它就真的是那些经验的后验。我们只是没有以那种方式建模。

假设我们已经观测到数据 *D* <sub class="calibre10">1</sub> ，并得到一个后验*P*(*θ*|*D*<sub class="calibre10">1</sub>)。如果我们随后观察更多的数据， *D* <sub class="calibre10">2</sub> ，我们会获得关于我们的参数的更多信息，并且可以将它们的分布更新为*P*(*θ*|*D*<sub class="calibre10">1</sub>， *D* <sub class="calibre10">2</sub> )。

当然，我们总是可以通过获取所有旧数据和所有新数据来计算这种分布，并将其推入我们的拟合代码中。但是，如果我们已经根据模型的可能性仔细选择了先验分布——我说的仔细是指我们有一个所谓的*共轭*先验——那么我们就可以用不同的先验来拟合新数据:旧的后验概率。

共轭先验是这样一种先验分布，它被选择成使得先验和后验都来自同一类分布(只是具有不同的参数)。在我们的贝叶斯线性模型中，先验和后验都是正态分布，所以我们有一个共轭先验。这意味着，原则上，我们可以通过使用相同的拟合代码但使用不同的先验，用更多的观察值更新我们的拟合模型。

我在之前的练习中暗示了一点，但是现在你可以更正式地处理它了。您需要一种表示多元正态分布的方法——但是您无论如何都需要它来表示您的 blm 对象——以及一种在您的 blm 对象中找到一个合适的正态分布来提取后验分布的方法。

有很多方法可以实现这个特性，所以你可以尝试一下。您可以拥有一个更新函数，它将先前的和新的观察值作为参数，并输出(更新后的)后验值。在这里，你需要以某种方式包括公式，以建立模型矩阵。或者，您可以让 update 将拟合对象与新数据一起获取，并从拟合对象中获取公式和先验信息。当然，如果你这样做，你需要把没有任何观察的先验(T0)作为一个特例对待——并且先验不知道任何公式或模型矩阵。

我们可以尝试这样的界面:

```
update <- function(model, data, prior) { ... }
```

其中模型是公式，数据是新的数据集，在用于拟合之前。这大致是构造函数的接口，除了您不一定需要将数据作为显式参数(毕竟，您希望能够适应数据框中没有数据的模型)，并且您根本不需要将 prior 作为参数。

思考几秒钟，并意识到无论我们在这里放入什么模型拟合，都将与 blm 中的完全相同，我们可以更改接口以去掉显式数据参数。如果我们让参数通过...相反，我们可以使用与 blm 中完全相同的代码(稍后通过调用 update 从 blm 中删除代码)。

```
update <- function(model, prior, ...) { ... }
blm <- function(model, ...) {
*# some code here...* 
    prior <- **make_a_prior_distribution_somehow**()
    posterior <- **update**(model, prior, ...)
*# some code that returns an object here...* 
}
```

要让这个版本的 blm 工作，您需要以一种可以传递更新的形式获得 prior，但是如果您之前做了练习，您应该已经有了一个这样做的函数(尽管您可能希望为这些发行版创建一个类，并同样地返回它们，以便您可以通过一个接口操纵它们，如果您想更进一步的话)。

当然，除了去掉 blm 主体中的模型拟合代码之外，您还可以去掉 update，通过让该函数接受 prior 参数，将该功能放入 blm 中。但是，如果这样做，您需要给它一个默认值，这样如果没有指定，您就可以使用原来的值。

```
blm <- function(model, prior = NULL, ...) {
*# some code here...* 
    if (**is.null**(prior)) {
        prior <- **make_a_prior_distribution_somehow**()
    }
    posterior <- **update**(model, prior, ...)
*# some code that returns an object here...* 
}
```

不过，现在让我们继续更新吧。我们如何对拟合的模型使用 update？

```
fit1 <- **blm**(y ∼ x)
fit2 <- **update**(y ∼ x, new_data, fit1)
```

这是行不通的，因为 fit1 是 blm 对象，不是正态分布。您需要从拟合的模型中提取分布。

如果你已经在对象中存储了分布——你应该这样做，因为否则，你不能使用对象做任何事情，因为拟合*是*后验分布——你应该能够得到它。然而，你不想做的是直接从对象中访问后验，比如 fit1$posterior 或类似的东西。是的，这是可行的，访问对象的内部使得以后改变表示更加困难。我知道我在这里重复我自己，但是它值得重复。你不希望在没有必要的情况下访问对象的内部，因为这会增加改变表示的难度。

相反，写一个后验函数，给出后验概率。

```
posterior <- function(fit) fit$posterior
```

*这个*函数必须访问内部——最终你将不得不获取信息——但是如果这是唯一一个这样做的函数，并且每个其他函数都使用这个函数，那么如果你改变对象的表示，你只需要改变这个函数。

有了这个函数，您可以这样做:

```
fit2 <- **update**(y ∼ x, new_data, **posterior**(fit1))
```

您还可以编写 update，以便它可以将拟合模型和分布作为其输入。然后你只需要一种方法来获得前一个对象(可能是一个分布，也可能是一个拟合模型的后验分布)，这两种方法都可以。

一种方法是直接测试先验参数的类别。

```
update <- function(model, prior, ...) {
    if (**class**(prior) == "blm") {
        prior <- **posterior**(prior)
    }
*# fitting code here* 
}
```

然而，由于种种原因，这是一个糟糕的解决方案。首先，它只在你要么得到一个先验分布，要么得到一个带有类“blm”的对象的情况下才起作用。如果有人后来写了一个扩展你的 blm 的类呢？它们的类属性可能是 c(“my blm”，“blm”)，这与“BLM”不同，所以这个测试会失败，下面的代码也会失败，因为这里假设您有一个发行版，但您拥有的是一个非常不同的类的对象。

要解决这个问题，您可以使用 inherits 函数。它测试一个给定的类名是否在 class 属性中，所以如果有人给你的 update 函数一个专门用于你的 blm 类的类，它就能工作。

```
update <- function(model, prior, ...) {
    if (**inherits**(prior, "blm")) {
        prior <- **posterior**(prior)
    }
*# fitting code here* 
}
```

这是一个不错的解决方案——如果你开始阅读面向对象的代码，你会在很多代码中看到它——但是它仍然有一些缺点。它假设唯一可以提供您可以用作先验的分布的对象要么是您默认实现先验的方式(并且您没有在上面测试它)，要么是“blm”类的对象(或其专门化)。

当然，你可以测试先验，如果它不是一个合适的对象，是否是你为你的分布定义的类，这将解决第一个问题。但是你如何处理其他种类的物体，它们也可能给你一个先验/后验分布？

每当你编写这样一个可以提供它的类时，你也可以更新你的更新函数，但是其他人不能以这种方式为你提供一个发行版(除非他们改变你的代码)。以这种方式显式测试对象的类型不是一个好的代码设计。解决这个问题的方法和访问对象内部是一样的:通过函数来访问。

如果我们要求我们给定的任何对象更新为先验参数，如果我们要求它，我们可以给我们一个分布，我们可以更新代码

```
update <- function(model, prior, ...) {
    prior <- **posterior**(prior)
*# fitting code here* 
}
```

这要求我们为后验函数创建一个多态函数，也可能要求我们为分布对象编写一个版本。这里我将采用一种快捷方式，将默认实现设为 identity 函数。

```
posterior <- function(x) **UseMethod**("posterior")
posterior.default <- function(x) x
posterior.blm <- function(x) x$posterior
```

现在唯一的烦恼是我们称之为后验。当我们有一个合适的对象时，这是后验分布，否则就不是了。让我们把它改成分布:

```
distribution <- function(x) **UseMethod**("distribution")
distribution.default <- function(x) x
distribution.blm <- function(x) x$posterior
```

然后相应地更新更新:

```
update <- function(model, prior, ...) {
    prior <- **distribution**(prior)
*# fitting code here* 
}
```

这样它在更新函数中看起来更好。

### 设计您的 blm 类

当你在实现你的 blm 类的时候，想想你正在创建的接口，各种函数是如何组合在一起的，以及你认为其他人如何能够重用你的模型。请记住，“未来的你”也是“其他人”，所以你这样做是在帮助自己。

我们开发的 update 函数是一个例子，说明我们可以在类设计中加入什么功能，以及我们如何使它可重用。您应该考虑访问对象的其他函数，并设计它们。

一个例子可以是提取给定输入点的分布。您已经实现了一个从预测变量预测响应变量的函数，接下来您将在预测函数中再次实现该函数，但是如果您希望获得给定输入下响应分布的全部好处，您需要获得该分布。你将如何向用户提供这些？如何在自己的函数中使用这种功能？

当你开发你的类的时候，用它来玩。每当你改变一些东西时，想想这是否能使其他功能更简单，或者事情是否能被一般化以使你的代码更可重用。

### 模型方法

有一些多态函数通常由表示拟合模型的类提供。不是所有的模型都实现了所有的，但是你实现的越多，现有的代码就越能操作你的新类；只通过函数向对象提供接口的另一个原因。

以下部分包括我认为您的 blm 类应该实现的函数列表。这些函数是按字母顺序列出的，但其中许多函数通过使用一个或多个其他函数更容易实现。所以在你开始编程之前通读这个列表。如果您认为通过调用其他函数中的一个可以更简单地实现一个函数，那么就这样实现它。

在所有情况下，首先阅读通用函数的 R 文档。无论如何，您需要文档来实现每个函数的正确接口，这样您至少可以阅读全部内容。本说明中的描述只是这些函数应该做什么的概述。

#### 系数

该函数应返回模型的拟合参数。对于我们的贝叶斯模型来说，解释这意味着什么并不完全简单，因为拟合的模型是分布而不是单点参数。我们可以让该函数返回拟合的分布，但是该函数通常的使用方式会使现有代码无法访问该模型的拟合参数，例如，作为 lm 模型中相应参数的替换。相反，返回参数的点估计可能更好，这将是拟合时计算的后验平均值。

以数值向量的形式返回结果，参数名为。这将符合你从 lm 得到的。

#### 限制

函数 confint 给出了拟合参数的置信区间。这里我们有与系数相同的问题:我们推断一个完整的分布，而不是一个参数(在任何情况下，我们的参数没有置信区间；它们具有联合分布)。然而，我们可以从我们推断的分布中计算出置信区间的模拟值。

如果我们的后验分布为**w**∾*N*(**m**， **S** )，那么权重向量的分量 *i* 分布为*w*<sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">*I*</sub>∾*N*(*m*<sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">*I*</sub>，S <sub xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="calibre10">【T25 由此，以及你想要的密度分数，你可以得出与你需要的分位数相匹配的阈值。</sub>

利用正态分布是对称的，您可以获取函数的水平参数并获得阈值分位数。所以你希望分位数是 c(level/2，1-level/2)。由此，您可以使用函数 qnorm 获得阈值。

#### 异常

该函数只计算从预测响应变量到观测值的距离平方和。如果你能得到距离的平方，或者即使你只有距离并且必须自己平方，这应该很容易计算。也许有一个函数可以给你这个答案？

#### 合适的

这个函数应该给你合适的响应变量。这是*而不是*你拟合模型的数据中的响应变量，而是模型做出的预测。

#### 情节

此函数绘制您的模型。你完全可以自由决定如何绘制它，但是我可以想象，看到一个 x-y 图，并有一条线穿过它来拟合，会很有用。但是，如果有一个以上的预测变量，我不确定什么是可视化拟合模型的好方法。对于 plot 函数应该做什么没有明确的规则，除了绘制一些东西，这样你就可以发挥你的想象力了。

#### 预测

该函数应根据拟合的模型进行预测。它的界面是

```
**predict**(object, ...)
```

但是惯例是你在变量 new data 中给它新的数据。如果您不提供新数据，它会根据用于拟合模型的数据给出预测。

#### 打印

如果您显式打印一个对象，或者如果您只是编写一个表达式，计算 R 终端中的类的对象，这个函数就会被调用。通常，它会打印对象的简短描述。

对于拟合的对象，它通常会打印拟合函数是如何调用的，以及拟合的系数是多少，或者拟合有多好。你可以看看 lm 对象是如何被打印出来的。

如果你想打印拟合函数是如何被调用的，你需要从你在 blm 构造函数中拟合对象时得到它。毕竟，令人感兴趣的是如何调用构造函数。在该函数内部，您可以通过使用函数 sys.call 来获得它被调用的方式。

#### 残差

该函数返回拟合的残差。这是响应变量的预测值和观察值之间的差异。

#### 摘要

此功能通常用作较长版本的打印。它为您提供了更多关于合身模型的信息。

然而，它的作用不止于此。它返回一个带有摘要信息的对象。这实际上意味着什么取决于模型实现，所以在这里做你喜欢的。

## 为 blm 构建 R 包

我们已经把贝叶斯线性回归软件的大部分内容放在了一起，现在是我们把它收集到 R 包中的时候了。这是我们项目的下一步。

您已经用一个类、blm 和各种用于访问这种类型的对象的函数实现了贝叶斯线性回归。现在是时候将这些函数收集到一个包中了。

### 决定包接口

当你设计你的类的功能和接口时，你必须决定你的类的对象应该有什么功能，以及你的所有函数如何组合在一起，使代码易于扩展和使用。制作包装也有类似的设计过程。

当然，你为设计类所做的一切对于一个包来说都是一样的，但是对于这个包来说，你必须决定哪些函数应该被导出，哪些应该保留在内部。

只有导出的函数才能被加载您的包的其他人使用，因此您可能会想尽可能地导出所有的函数。然而，这是一个糟糕的选择。你的包的接口是导出的函数，如果你导出太多，你需要维护一个巨大的接口。如果你改变了一个包的界面，那么每个使用你的包的人都必须更新他或她的代码来适应变化的界面。您希望将对包接口的更改保持在最低限度。

您应该弄清楚哪些功能是软件包功能的基本部分，哪些是内部帮助函数，并且只导出软件包接口的一部分。

### 源文件的组织

r 并不真正关心你使用了多少个文件来存放你的源代码，或者源代码是如何组织的，但是你可能会关心。在将来的某个时候，您将需要能够找到相关的函数来修复 bug 或者扩展您的包的功能。

决定如何组织您的源代码。你希望每个文件都有一个函数吗？是否有某种逻辑方法将代码的功能分成不同的类别，每个类别有一个文件？

### 很好地记录您的包接口

至少，您从包中导出的函数应该有文档记录。没有文档，用户(将来可能就是你)就不知道一个功能应该如何使用。

该文档主要用于在线帮助，即您使用？—所以它不应该太长，但应该能让你很好地了解一个函数应该如何使用。

要给出整个软件包的总体描述，以及各种函数如何组合在一起以及应该如何使用它们，您可以为整个软件包编写文档。

就像包数据一样，确实没有地方可以这样做，但是您可以使用与数据相同的技巧。将文档放在 R/目录下的源代码文件中。

以下是我的混合图包文档:

```
*#' admixturegraph: Visualising and analysing admixture graphs.* 
*#'* 
*#' The package provides functionality to analyse and test admixture graphs* 
*#' against the \eqn{f} statistics described in the paper* 
*#' \href{http://tinyurl.com/o5a4kr4}{Ancient Admixture in Human History},* 
*#' Patternson \emph{et al.}, Genetics, Vol. 192, 1065--1093, 2012.* 
*#'* 
*#' The \eqn{f} statistics -- \eqn{f_2}, \eqn{f_3}, and \eqn{f_4} -- extract* 
*#' information about correlations between gene frequencies in different* 
*#' populations (or single diploid genome samples), which can be informative* 
*#' about patterns of gene flow between these populations in form of admixture* 
*#' events. If a graph is constructed as a hypothesis for the relationship* 
*#' between the populations, equations for the expected values of the \eqn{f}* 
*#' statistics can be extracted, as functions of edge lengths -- representing* 
*#' genetic drift -- and admixture proportions.* 
*#'* 
*#' This package provides functions for extracting these equations and for* 
*#' fitting them against computed \eqn{f} statistics. It does not currently* 
*#' provide functions for computing the \eqn{f} statistics -- for that we refer* 
*#' to  the \href{https://github.com/DReichLab/AdmixTools}{ADMIXTOOLS} software* 
*#' package.* 
*#'* 
*#' @docType package* 
*#' @name admixturegraph* 
NULL
```

@docType 和@name 标签告诉 Roxygen 我正在为整个包编写文档。

### 将自述文件和新闻文件添加到包中

习惯上，在您的包中还有一个自述文件和一个新闻文件。自述文件描述了您的软件包的作用以及如何和可以被认为是软件包的简短广告，而新闻文件描述了随着时间的推移您对软件包所做的更改。

许多开发人员更喜欢使用“markdown”作为这些文件的格式——在这种情况下，它们通常被命名为 README.md 和 news . MD——特别是如果您将您的软件包放在 GitHub 上，拥有 README.md 文件是一个好主意，因为当人们转到 GitHub 上的软件包主页时，它会突出显示。

#### 自述文件

在自述文件中写什么由你自己决定，但是习惯上应该简单描述一下这个包的功能，并给出一两个使用的例子。

如果你把它写在 markdown——一个名为 README.md 的文件中——如果你把你的包放在 GitHub 上，它将是主页。

您可能希望用 R markdown 来编写它，以获得 knitr 的所有好处。在这种情况下，您应该将文件命名为 README。Rmd，并将其放在标题中:

```
---
output:
  md_document:
    variant: markdown_github
---
```

这告诉 knitr 应该生成一个 markdown 文件作为输出——它将被命名为 README.md。

#### 新闻ˌ消息

这个文件应该只包含一个列表，列出了一段时间以来您对包所做的更改。为了让人们更容易看到包的哪些版本有哪些变化，您可以将它分成几个部分，每个部分对应一个版本。

## 测试

在包中，我们现在应该确保我们所有的功能都经过了至少一次单元测试，并且我们的包能够通过包测试。

## 开源代码库

注册 GitHub 并为项目创建一个资源库。将代码移到那里。

## 结论

嗯，这是本书的结尾，但希望不是你数据科学生涯的结尾。我在这本书里已经说了我想说的一切。我漏掉了很多东西。例如文本处理。r 不是我最喜欢的处理文本的语言，所以我不使用它，但是它有它的功能。这超出了我们在这里看到的数据。如果你想处理文本，比如基因组或自然语言，你需要不同于我在本书中提到的工具。我假设您只是在处理数据框。这使得这本书更容易写。但它并没有涵盖数据科学的所有内容。对于更专业的数据分析，你需要去别处看看。有许多好书，我甚至可能在以后的某个时候写它。这不在本书的讨论范围之内。

这是本书的结尾，但我想给你一些提示，告诉你在哪里可以了解更多关于数据科学和 r 的知识。根据你是对分析数据更感兴趣还是对开发方法更感兴趣，你可能会有不同的方向。两者 r 都是不错的选择。从长远来看，你可能会两者都想做。下面列出的书会让你朝着你想去的方向开始。

### 数据科学

*   *数据科学的艺术*罗杰·彭和伊丽莎白·松井

这是对数据科学的基本步骤和理念的概述。它描述了项目经历的各个阶段——探索性分析、拟合模型等。—虽然它没有涵盖任何技术细节，但它是一个很好的概述。

### 机器学习

*   Christopher Bishop 的*模式匹配和机器学习*

这是我多年来一直用来教机器学习课的一本书。它涵盖了监督学习和非监督学习的许多不同的算法——也是本书没有涵盖的分析类型。它相当数学化，侧重于方法，但如果你对底层的机器学习感兴趣，这是一个很好的介绍。

### 数据分析

*   *R*中的线性模型

*   *用 R 扩展线性模型:广义线性、混合效应和非参数回归模型*

线性模型和广义线性模型是我最先尝试的东西。差不多一直都是。这些伟大的书籍，看看这些模型是如何在 r。

*   保罗·穆雷尔的作品

*   *ggplot2:优雅的数据分析图形*作者:Hadley Wickham

第一本书描述了基本的图形包和作为 ggplot2 基础的网格系统。显然，第二本书是了解更多关于 ggplot2 的最佳书籍。

### r 编程

*   哈雷·威克姆的《高级研究》

*   哈雷·威克姆的 R 包包

如果你想学习更多更高级的 R 编程和包开发，这些都是很棒的书。

*   Dirk Eddelbuettel 开发的 R 和 C++与 Rcpp 的无缝集成

如果您对集成 C++和 R 感兴趣，那么 Rcpp 是一个不错的选择，这是对 Rcpp 的一个很好的介绍。

## 结束了

这是我离开你的地方。我希望你觉得这本书有用，如果你想给我留下任何评论和批评，请这样做。这将有助于我在未来的版本中改进它。如果有你认为应该添加的东西，请告诉我，我会添加一两章来涵盖它。当然，如果你在书中发现任何错误，请告诉我。如果你能指出书中代码中的任何错误，我将不胜感激。

## 承认

我要感谢 Asger Hobolth 对这篇手稿早期版本的许多有用的评论，帮助我改进了材料的写作和陈述。

# 脚注

[1](#Fn1_source) 在公式中，x*z 表示 x + z + x:z，而 x:z 是 x 和 z 之间的相互作用——实际上是它们数的乘积——所以 y∾x * z 表示 *ϕ* ( *x* ，z) = (1， *x，* z *，x* ⋅ z)。