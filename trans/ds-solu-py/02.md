# 二、大数据、机器学习和深度学习框架

本章详细介绍了用于并行数据处理的大数据框架 Apache Spark。它还涵盖了几个对构建可伸缩应用程序有用的机器学习(ML)和深度学习(DL)框架。阅读本章后，您将理解如何使用弹性和容错技术来收集、处理和检查大数据。它讨论了 Scikit-Learn、Spark MLlib 和 XGBoost 框架。它还涵盖了一个名为 Keras 的深度学习框架。本文最后讨论了建立和管理这些框架的有效方法。

大数据框架支持并行数据处理。它们使您能够包含跨多个集群的大数据。最流行的大数据框架是 Apache Spark，它是基于 Hadoop 框架构建的。

## 大数据

大数据对不同的人意味着不同的东西。在本书中，我们将大数据定义为使用传统方法无法充分处理和操纵的大量数据。毫无疑问，我们必须使用可扩展的框架和现代技术来处理这些数据，并从中获得洞察力。当数据不适合当前的内存存储空间时，我们通常认为它是“大”的。例如，如果你有一台个人电脑，而你能处理的数据超过了你电脑的存储容量，这就是大数据。这同样适用于拥有大型存储空间集群的大型企业。当我们将堆栈与 Hadoop/Spark 结合使用时，我们经常谈到大数据。

### 大数据特性

大数据的特征被描述为四个 v——速度、数量、多样性和准确性。表 [2-1](#Tab1) 强调了大数据的这些特征。

表 2-1

大数据特性

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

元素

 | 

描述

 |
| --- | --- |
| 速度 | 现代技术和改进的连通性使您能够以前所未有的速度生成数据。速度的特征包括批量数据、近实时数据和流。 |
| 卷 | 数据增长的规模。数据源和基础设施的性质会影响数据量。卷的特征包括 EB、zettabyte 等。 |
| 多样 | 数据可以来自唯一的来源。现代技术设备到处留下数字足迹，增加了企业和个人获取数据的来源数量。多样性的特征包括数据的结构和复杂性。 |
| 诚实 | 数据必须来自可靠的来源。此外，它必须是高质量的、一致的和完整的。 |

## 大数据对业务和人员的影响

毫无疑问，大数据影响着我们思考和开展业务的方式。数据驱动的组织通常为基于证据的管理奠定基础。大数据涉及使用定量方法衡量业务的关键方面。它有助于支持决策。接下来的章节将讨论大数据对企业和人们的影响。

### 更好的客户关系

来自大数据的洞察力有助于管理客户关系。拥有客户大数据的组织可以研究客户的行为模式，并使用描述性分析来推动客户管理策略。

### 精细产品开发

数据驱动型组织使用大数据分析和预测分析来推动产品开发。和管理策略。这种方法对于应用程序的增量和迭代交付非常有用。

### 改进决策

当企业拥有大数据时，它可以利用大数据来揭示现象的复杂模式，从而影响战略。这种方法有助于管理层根据证据而不是主观推理做出明智的决策。数据驱动的组织培养基于证据的管理文化。

我们还在生命科学、物理学、经济学和医学等领域使用大数据。大数据影响世界的方式有很多。本章没有考虑所有因素。接下来的部分解释了大数据仓库和 ETL 活动。

## 大数据仓库

在过去的几十年里，各种组织纷纷投资本地数据库，包括 Microsoft Access、Microsoft SQL Server、SAP Hana、Oracle 数据库等等。最近，像微软 Azure SQL 和 Oracle XE 这样的云数据库被广泛采用。还有标准的大数据(分布式)数据库，如 Cassandra 和 HBase 等。企业正在转向可扩展的基于云的数据库，以利用与不断增长的计算能力、容错技术和可扩展解决方案相关的优势。

### 大数据 ETL

尽管数据库管理有了显著的进步，但是人们操作数据库数据的方式仍然是一样的。提取、转换和加载(ETL)仍然是分析和报告中不可或缺的一部分。表 [2-2](#Tab2) 讨论了 ETL 活动。

表 2-2

ETL 活动

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

活动

 | 

描述

 |
| --- | --- |
| 提取 | 包括从一些数据库中获取数据。 |
| 转换 | 包括将数据库中的数据转换成适合分析和报告的格式 |
| 装货 | 包括在数据库管理系统中存储数据。 |

要执行 ETL 活动，您必须使用查询语言。最流行的查询语言是 SQL(标准查询语言)。随着开源运动的发展，还出现了其他查询语言，比如 HiveQL 和 BigQuery。Python 编程语言支持 SQL。Python 框架可以通过实现库来连接数据库，比如 SQLAlchemy、pyodbc、SQLite、SparkSQL 和 pandas 等等。

## 大数据框架

大数据框架使开发人员能够收集、管理和操作分布式数据。大多数开源大数据框架使用内存集群计算。最流行的框架包括 Hadoop、Spark、Flink、Storm 和 Samza。这本书使用 PySpark 执行 ETL 活动，探索数据，构建机器学习流水线。

### 阿帕奇火花

Apache Spark 执行内存集群计算。它使开发人员能够使用 Java、Scala、Python、R 和 SQL 构建可伸缩的应用程序。它包括集群组件，如驱动程序、集群管理器和执行器。您可以将它用作独立的集群管理器，也可以将其置于 Mesos、Hadoop、YARN 或 Baronets 之上。您可以使用它来访问 Hadoop 文件系统(HDFS)、Cassandra、HBase 和 Hive 以及其他数据源中的数据。Spark 数据结构被认为是一个弹性分布式数据集。这本书介绍了一个集成了 Python 和 Apache Spark (PySpark)的框架。书上用它来操作 Spark MLlib。要理解这个框架，首先需要理解弹性分布式数据集背后的思想。

#### 弹性分布式数据集

弹性分布式数据集(rdd)是用于并行化数据或转换现有数据的不可变元素。首席 RDD 运营包括转型和行动。我们将它们存储在 Hadoop 支持的任何存储中。例如，在 Hadoop 分布式文件系统(HDF)，Cassandra，HBase，亚马逊 S3 等。

#### 火花配置

Spark 配置的领域包括 Spark 属性、环境变量和日志记录。默认的配置目录是`SPARK_HOME/conf`。

您可以使用`pip install findspark`在您的环境中安装`findspark`库，并使用`pip install pyspark`安装`pyspark`库。

清单 [2-1](#PC1) 使用`findspark`框架准备 PySpark 框架。

```py
import findspark as initiate_pyspark
initiate_pyspark.init("filepath\spark-3.0.0-bin-hadoop2.7")

Listing 2-1Prepare the PySpark Framework

```

清单 [2-2](#PC2) 使用`SparkConf()`方法规定了 PySpark 应用程序。

```py
from pyspark import SparkConf
pyspark_configuration = SparkConf().setAppName("pyspark_linear_method").setMaster("local")

Listing 2-2Stipulate the PySpark App

```

清单 [2-3](#PC3) 用`SparkSession()`方法准备 PySpark 会话。

```py
from pyspark.sql import SparkSession
pyspark_session = SparkSession(pyspark_context)

Listing 2-3Prepare the Spark Session

```

#### Spark 框架

Spark 框架扩展了 Spark API 的核心。有四个主要的 Spark 框架——Spark SQL、Spark Streaming、Spark MLlib 和 GraphX。

##### SparkSQL

SparkSQL 使您能够使用关系查询语言，如 SQL、HiveQL 和 Scala。它包括一个包含行对象和模式的`schemaRDD`。您可以使用现有的 RDD、拼花文件或 JSON 数据集来创建它。您执行 Spark 上下文来创建 SQL 上下文。

##### 火花流

Spark streaming 是一个可扩展的流框架，支持 Apache Kafka、Apache Flume、HDFS 和 Apache Kensis 等。它使用 HDFS、数据库和仪表板以小批量的方式使用 DStream 处理输入数据。Python 的最新版本不支持 Spark 流。因此，我们不在这本书里讨论这个框架。您可以使用 Spark 流应用程序从任何数据源读取输入，并将数据的副本存储在 HDFS 中。这允许您构建和启动 Spark 流应用程序，该应用程序处理传入的数据并在其上运行算法。

##### Spark MLlib(消歧义)

MLlib 是一个 ML 框架，允许你开发和测试 ML 和 DL 模型。在 Python 中，这些框架与 NumPy 框架一起工作。Spark MLlib 可以与多个 Hadoop 数据源一起使用，并与 Hadoop 工作流结合在一起。常见的算法包括回归、分类、聚类、协同过滤和降维。关键的工作流程工具包括特征转换、标准化和规范化、流水线开发、模型评估和超参数优化。

##### 图形 x

GraphX 是一个可扩展的容错框架，用于迭代和快速图形并行计算、社交网络和语言建模。它包括图形算法，如用于估计图形中每个顶点的重要性的 PageRank、用于用编号最低的顶点的 ID 标记图形的连接组件的连接组件，以及用于查找通过每个顶点的三角形数量的三角形计数。

## ML 框架

要解决 ML 问题，你需要有一个支持构建和缩放 ML 模型的框架。不缺少 ML 模型——有数不清的 ML 框架。您可以使用几个 ML 框架。后续章节涵盖了 Scikit-Learn、Spark MLlib、H2O 和 XGBoost 等框架。

### Scikit-Learn

Scikit-Learn 框架包括 ML 算法，如回归、分类和聚类等。您可以将它与 NumPy 和 SciPy 等其他框架一起使用。它可以执行 ML 项目所需的大多数任务，如数据处理、转换、数据分割、标准化、超参数优化、模型开发和评估。Scikit-Learn 附带了大多数支持 Python 的发行包。使用`pip install sklearn`将其安装到您的 Python 环境中。

### H2O

H2O 是一个使用无人驾驶技术的 ML 框架。它使您能够加快人工智能解决方案的采用。它非常容易使用，并且不需要任何专业技术知识。不仅如此，它还支持数字和分类数据，包括文本。在训练 ML 模型之前，必须首先将数据加载到 H2O 分类中。它支持 CSV、Excel 和拼花文件。默认数据源包括本地文件系统、远程文件、亚马逊 S3、HDFS 等。它有 ML 算法，如回归、分类、聚类分析和降维。它还可以执行 ML 项目所需的大多数任务，如数据处理、转换、数据分割、标准化、超参数优化、模型开发、检查点、评估和生产。使用`pip install h2o`在您的环境中安装软件包。

清单 [2-4](#PC4) 准备 H2O 框架。

```py
import h2o
h2o.init()

Listing 2-4Initializing the H2O Framework

```

### XGBoost

XGBoost 是一个 ML 框架，支持编程语言，包括 Python。它执行可扩展的梯度提升模型，并在不牺牲内存效率的情况下学习快速并行和分布式计算。不仅如此，它还是一个合奏学习者。如第一章所述，集成学习者可以解决回归和分类问题。XGBoost 使用 boosting 从前面的树中提交的错误中学习。当基于树的模型过度拟合时，这是有用的。使用`pip install xgboost`在您的 Python 环境中安装模型。

## DL 框架

DL 框架提供了一种支持扩展人工神经网络的结构。您可以单独使用它，也可以与其他型号配合使用。它通常包括程序和代码框架。主要的 DL 框架包括 TensorFlow、PyTorch、Deeplearning4j、微软认知工具包(CNTK)和 Keras。

### 硬

Keras 是用 Python 写的高级 DL 框架；它运行在一个叫做 TensorFlow 的 ML 平台之上。这对于 DL 模型的快速成型是有效的。您可以在张量处理单元或大规模图形处理单元上运行 Keras。主要的 Keras APIs 包括模型、层和回调。第 7 章[涵盖了这个框架。执行`pip install Keras`和`pip install tensorflow`来使用 Keras 框架。](07.html)