# 9.例子

本章包括几个较大的 web 刮刀示例。与前几章展示的大多数例子相反，这里的例子有双重目的。首先，他们展示了更多使用现实生活中的网站而不是精心策划的安全环境的例子。到目前为止，我们还没有使用很多现实生活中的例子的原因是由于网络的动态特性。可能这里涉及的例子不再提供完全相同的结果，或者在你阅读它们的时候已经被破坏了。也就是说，我们已经试着选择了一些相对友好并且不太容易改变的网站。这些例子的第二个目的是强调贯穿全书的各种概念是如何“结合”和交互的，以及暗示一些有趣的面向数据科学的用例。

本章包含以下示例:

*   抓取黑客新闻:这个例子使用请求和漂亮的汤来抓取黑客新闻首页。
*   使用 Hacker News API:这个例子通过展示如何在请求中使用 API 提供了一种替代方法。
*   引用:这个例子使用请求和漂亮的汤，并引入“数据集”库作为存储数据的简单方法。
*   要抓取的书籍:这个例子使用了请求和漂亮的 Soup，以及数据集库，演示了如何再次运行一个抓取器而不存储重复的结果。
*   抓取 GitHub Stars:这个例子使用请求和漂亮的 Soup 来抓取 GitHub 库，并展示如何使用请求执行登录，重申我们关于法律问题的警告。
*   抓取抵押贷款利率:这个例子使用一个特别复杂的站点请求抓取抵押贷款利率。
*   抓取和可视化 IMDB 收视率:这个示例使用请求和 Beautiful Soup 来获取电视剧集的 IMDB 收视率列表。我们还引入了“matplotlib”库来用 Python 创建绘图。
*   抓取 IATA 航空公司信息:这个例子使用请求和漂亮的汤从一个使用复杂 web 表单的站点抓取航空公司信息。还提供了使用硒的替代方法。使用“pandas”库将抓取的结果转换成表格格式，这个例子中也介绍了这个库。
*   抓取和分析 web 论坛交互:这个示例使用请求和 Beautiful Soup 来抓取 Web 论坛帖子，并使用 dataset 库存储它们。根据收集到的结果，我们使用 pandas 和 matplotlib 创建显示用户活动的热图。
*   收集和聚类一个时尚数据集:这个例子使用请求和 Beautiful Soup 下载一组时尚图片。然后使用“scikit-learn”库对图像进行聚类。
*   抓取的亚马逊评论的情感分析:这个例子使用请求和 Beautiful Soup 从亚马逊抓取用户评论列表，使用 dataset 库存储。然后，我们使用 Python 中的“nltk”和“vaderSentiment”库对这些进行分析，并使用 matplotlib 绘制结果。
*   抓取和分析新闻文章:这个示例使用 Selenium 抓取新闻文章列表，该列表使用 dataset 库存储。然后，我们通过使用 nltk 构建一个主题模型，将这些与一系列主题关联起来。
*   抓取和分析维基百科图表:在这个例子中，我们扩展了我们的维基百科爬虫，使用请求和漂亮的汤抓取页面，使用数据集库存储，然后使用“NetworkX”创建一个图表，并用 matplotlib 可视化。
*   抓取和可视化董事会成员图:这个例子使用请求和漂亮的汤为标准普尔 500 公司抓取董事会成员。使用 NetworkX 创建图形，并使用“Gephi”可视化。
*   使用深度学习破解验证码:这个例子展示了如何使用卷积神经网络来破解验证码。

Source Code

本书的配套网站 [`http://www.webscrapingfordatascience.com`](http://www.webscrapingfordatascience.com/) 也提供了所有示例的源代码。

## 9.1 抓取黑客新闻

我们要刮去 [`https://news.ycombinator.com/news`](https://news.ycombinator.com/news) 首页，使用请求和漂亮的汤。如果您还没有听说过这个页面，请花些时间浏览一下。黑客新闻(Hacker News)是一个受欢迎的新闻文章聚合网站，黑客(计算机科学家、企业家、数据科学家)会对此感兴趣。

在这个例子中，我们将把收集到的信息存储在一个简单的 Python 字典对象列表中。抓取该页面的代码如下所示:

```py
import requests
import re
from bs4 import BeautifulSoup

articles = []

url = 'https://news.ycombinator.com/news'

r = requests.get(url)
html_soup = BeautifulSoup(r.text, 'html.parser')

for item in html_soup.find_all('tr', class_="athing"):
    item_a = item.find('a', class_="storylink")
    item_link = item_a.get('href') if item_a else None
    item_text = item_a.get_text(strip=True) if item_a else None
    next_row = item.find_next_sibling('tr')
    item_score = next_row.find('span', class_="score")
    item_score = item_score.get_text(strip=True) if item_score else '0 points'
    # We use regex here to find the correct element
    item_comments = next_row.find('a', string=re.compile('\d+(&nbsp;|\s)comment(s?)'))
    item_comments = item_comments.get_text(strip=True).replace('\xa0', ' ') \
                        if item_comments else '0 comments'
    articles.append({
        'link' : item_link,
        'title' : item_text,
        'score' : item_score,
        'comments' : item_comments})

for article in articles:
    print(article)

```

这将输出以下内容:

```py
{'link': 'http://moolenaar.net/habits.html', 'title': 'Seven habits of    effective text editing (2000)', 'score': '44 points', 'comments':     '9 comments'}
{'link': 'https://www.repository.cam.ac.uk/handle/1810/251038', 'title': 'Properties of expanding universes (1966)', 'score': '52 points',     'comments': '8 comments'}
[...]

```

尝试扩展这段代码，也可以抓取到评论页面的链接。想想当你也收集评论本身时可能出现的潜在用例(例如，在文本挖掘的上下文中)。

## 9.2 使用黑客新闻 API

注意，黑客新闻还提供了一个 API，提供结构化的、JSON 格式的结果(见 [`https://github.com/HackerNews/API`](https://github.com/HackerNews/API) )。让我们重新编写我们的 Python 代码，现在它可以作为一个 API 客户端，而不需要依赖漂亮的 Soup 来进行 HTML 解析:

```py
import requests

articles = []

url = 'https://hacker-news.firebaseio.com/v0'

top_stories = requests.get(url + '/topstories.json').json()

for story_id in top_stories:
    story_url = url + '/item/{}.json'.format(story_id)
    print('Fetching:', story_url)
    r = requests.get(story_url)
    story_dict = r.json()
    articles.append(story_dict)

for article in articles:
    print(article)

```

这将输出以下内容:

```py
Fetching: https://hacker-news.firebaseio.com/v0/item/15532457.json
Fetching: https://hacker-news.firebaseio.com/v0/item/15531973.json
Fetching: https://hacker-news.firebaseio.com/v0/item/15532049.json
[...]
{'by': 'laktak', 'descendants': 30, 'id': 15532457, 'kids': [15532761,   
     15532768, 15532635, 15532727, 15532776, 15532626, 15532700, 15532634], 'score': 60, 'time': 1508759764, ‘title': ‘Seven habits of effective   
     text editing (2000)', 'type': 'story', 'url': 'http://moolenaar.net/    habits.html'}
[...]

```

## 9.3 要刮的报价

我们要刮 [`http://quotes.toscrape.com`](http://quotes.toscrape.com/) ，用请求和漂亮的汤。本页面由 Scrapinghub 提供，作为一个更真实的 scraping playground。花些时间浏览一下这个页面。我们会刮出所有的信息，那就是:

*   引文及其作者和标签；
*   和作者信息，即出生日期和地点，以及描述。

我们将把这些信息存储在 SQLite 数据库中。我们将使用“数据集”库(参见 [`https://dataset.readthedocs.io/en/latest/`](https://dataset.readthedocs.io/en/latest/) )，而不是使用“记录”库和编写手动 SQL 语句。这个库提供了一个简单的抽象层，删除了大多数直接的 SQL 语句，而不需要完整的 ORM 模型，因此我们可以像使用 CSV 或 JSON 文件一样使用数据库来快速存储一些信息。通过 pip 可以轻松安装数据集:

```py
pip install -U dataset

```

Not a Full ORM

请注意，dataset 不想取代 SQLAlchemy 这样的成熟的 ORM(对象关系映射)库(即使它在幕后使用 SQLAlchemy)。它只是意味着在数据库中快速存储大量数据，而不必定义模式或编写 SQL。对于更高级的用例，考虑使用真正的 ORM 库或者手动定义数据库模式并手动查询是一个好主意。

抓取该站点的代码如下所示:

```py
import requests
import dataset
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

db = dataset.connect('sqlite:///quotes.db')

authors_seen = set()

base_url = 'http://quotes.toscrape.com/'

def clean_url(url):
    # Clean '/author/Steve-Martin' to 'Steve-Martin'
    # Use urljoin to make an absolute URL
    url = urljoin(base_url, url)
    # Use urlparse to get out the path part
    path = urlparse(url).path
    # Now split the path by '/' and get the second part
    # E.g. '/author/Steve-Martin' -> ['', 'author', 'Steve-Martin']
    return path.split('/')[2]

def scrape_quotes(html_soup):

    for quote in html_soup.select('div.quote'):
        quote_text = quote.find(class_='text').get_text(strip=True)
        quote_author_url = clean_url(quote.find(class_='author') \
                                     .find_next_sibling('a').get('href'))
        quote_tag_urls = [clean_url(a.get('href'))
                          for a in quote.find_all('a', class_="tag")]
        authors_seen.add(quote_author_url)
        # Store this quote and its tags
        quote_id = db['quotes'].insert({ 'text' : quote_text,
                                         'author' : quote_author_url })
        db['quote_tags'].insert_many(
                 [{'quote_id' : quote_id, 'tag_id' : tag} for tag in quote_tag_urls])

def scrape_author(html_soup, author_id):
    author_name = html_soup.find(class_='author-title').get_text(strip=True)
    author_born_date = html_soup.find(class_='author-born-date').get_text(strip=True)
    author_born_loc = html_soup.find(class_='author-born-location').get_text(strip=True)
    author_desc = html_soup.find(class_='author-description').get_text(strip=True)
    db['authors'].insert({ 'author_id' : author_id,
                           'name' : author_name,
                           'born_date' : author_born_date,
                           'born_location' : author_born_loc,
                           'description' : author_desc})

# Start by scraping all the quote pages
url = base_url
while True:

    print('Now scraping page:', url)
    r = requests.get(url)
    html_soup = BeautifulSoup(r.text, 'html.parser')
    # Scrape the quotes
    scrape_quotes(html_soup)
    # Is there a next page?
    next_a = html_soup.select('li.next > a')
    if not next_a or not next_a[0].get('href'):
        break

    url = urljoin(url, next_a[0].get('href'))

# Now fetch out the author information
for author_id in authors_seen:
    url = urljoin(base_url, '/author/' + author_id)
    print('Now scraping author:', url)
    r = requests.get(url)
    html_soup = BeautifulSoup(r.text, 'html.parser')
    # Scrape the author information
    scrape_author(html_soup, author_id)

```

这将输出以下内容:

```py
Now scraping page: http://quotes.toscrape.com/
Now scraping page: http://quotes.toscrape.com/page/2/
Now scraping page: http://quotes.toscrape.com/page/3/
Now scraping page: http://quotes.toscrape.com/page/4/
Now scraping page: http://quotes.toscrape.com/page/5/
Now scraping page: http://quotes.toscrape.com/page/6/
Now scraping page: http://quotes.toscrape.com/page/7/
Now scraping page: http://quotes.toscrape.com/page/8/
Now scraping page: http://quotes.toscrape.com/page/9/
Now scraping page: http://quotes.toscrape.com/page/10/
Now scraping author: http://quotes.toscrape.com/author/Ayn-Rand
Now scraping author: http://quotes.toscrape.com/author/E-E-Cummings
[...]

```

请注意，仍然有许多方法可以使代码更加健壮。当抓取报价或作者页面时，我们不会检查无结果。此外，我们在这里使用“dataset”简单地在三个表中插入行。在这种情况下，数据集将自动增加一个主“id”键。如果您想再次运行这个脚本，那么您必须首先清理数据库以重新开始，或者修改脚本以允许恢复其工作或正确地更新结果。在后面的例子中，我们将使用 dataset 的`upsert`方法来实现。

一旦脚本完成，您可以使用 SQLite 客户端查看数据库(“quotes.db”)，例如“SQLite 的 db 浏览器”，它可以从 [`http://sqlitebrowser.org/`](http://sqlitebrowser.org/) 获得。图 [9-1](#Fig1) 显示了该工具的运行情况。

![A463931_1_En_9_Fig1_HTML.jpg](A463931_1_En_9_Fig1_HTML.jpg)

图 9-1

Exploring an SQLite database with “DB Browser for SQLite”

## 9.4 要刮的书

我们要刮 [`http://books.toscrape.com`](http://books.toscrape.com/) ，用请求和漂亮的汤。本页面由 Scrapinghub 提供，作为一个更真实的 scraping playground。花些时间浏览一下这个页面。我们将收集所有信息，也就是说，对于每本书，我们将获得:

*   它的标题；
*   它的形象；
*   其价格和库存情况；
*   其评级；
*   其产品说明；
*   其他产品信息。

我们将把这些信息存储在 SQLite 数据库中，再次使用“dataset”库。然而，这一次我们要以这样一种方式编写我们的程序，它考虑到了更新——这样我们就可以多次运行它，而不会在数据库中插入重复的记录。

抓取该站点的代码如下所示:

```py
import requests
import dataset
import re
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

db = dataset.connect('sqlite:///books.db')

base_url = 'http://books.toscrape.com/'

def scrape_books(html_soup, url):
    for book in html_soup.select('article.product_pod'):
        # For now, we'll only store the books url
        book_url = book.find('h3').find('a').get('href')
        book_url = urljoin(url, book_url)
        path = urlparse(book_url).path
        book_id = path.split('/')[2]
        # Upsert tries to update first and then insert instead
        db['books'].upsert({'book_id' : book_id,
                            'last_seen' : datetime.now()
                            }, ['book_id'])

def scrape_book(html_soup, book_id):
    main = html_soup.find(class_='product_main')
    book = {}

    book['book_id'] = book_id
    book['title'] = main.find('h1').get_text(strip=True)
    book['price'] = main.find(class_='price_color').get_text(strip=True)
    book['stock'] = main.find(class_='availability').get_text(strip=True)
    book['rating'] = ' '.join(main.find(class_='star-rating') \
                        .get('class')).replace('star-rating', '').strip()
    book['img'] = html_soup.find(class_='thumbnail').find('img').get('src')
    desc = html_soup.find(id='product_description')
    book['description'] = ''
    if desc:
        book['description'] = desc.find_next_sibling('p') \
                                  .get_text(strip=True)
    info_table = html_soup.find(string='Product Information').find_next('table')
    for row in info_table.find_all('tr'):
        header = row.find('th').get_text(strip=True)
        # Since we'll use the header as a column, clean it a bit
        # to make sure SQLite will accept it
        header = re.sub('[^a-zA-Z]+', '_', header)
        value = row.find('td').get_text(strip=True)
        book[header] = value
    db['book_info'].upsert(book, ['book_id'])

# Scrape the pages in the catalogue
url = base_url
inp = input('Do you wish to re-scrape the catalogue (y/n)? ')
while True and inp == 'y':
    print('Now scraping page:', url)
    r = requests.get(url)
    html_soup = BeautifulSoup(r.text, 'html.parser')
    scrape_books(html_soup, url)
    # Is there a next page?
    next_a = html_soup.select('li.next > a')
    if not next_a or not next_a[0].get('href'):

break

    url = urljoin(url, next_a[0].get('href'))

# Now scrape book by book, oldest first
books = db['books'].find(order_by=['last_seen'])
for book in books:
    book_id = book['book_id']
    book_url = base_url + 'catalogue/{}'.format(book_id)
    print('Now scraping book:', book_url)
    r = requests.get(book_url)
    r.encoding = 'utf-8'
    html_soup = BeautifulSoup(r.text, 'html.parser')
    scrape_book(html_soup, book_id)
    # Update the last seen timestamp
    db['books'].upsert({'book_id' : book_id,
                        'last_seen' : datetime.now()
                        }, ['book_id'])

```

一旦脚本完成，记住您可以使用例如“DB Browser for SQLite”来查看数据库(“books.db”)注意在这个例子中数据集的`upsert`方法的使用。该方法将尝试更新已经存在的记录(通过将现有记录与给定字段名列表进行匹配)，否则插入新记录。

## 9.5 刮 GitHub 星星

我们要刮 [`https://github.com`](https://github.com/) ，用请求和漂亮的汤。我们的目标是，对于一个给定的 GitHub 用户名，比如 [`https://github.com/google`](https://github.com/google) ，获得一个包含 GitHub 指定的编程语言的存储库列表，以及存储库的星级数。

这种铲运机的基本结构很简单:

```py
import requests
from bs4 import BeautifulSoup
import re

session = requests.Session()

url = 'https://github.com/{}'
username = 'google'

r = session.get(url.format(username), params={'page': 1, 'tab': 'repositories'})
html_soup = BeautifulSoup(r.text, 'html.parser')
repos = html_soup.find(class_='repo-list').find_all('li')
for repo in repos:
    name = repo.find('h3').find('a').get_text(strip=True)
    language = repo.find(attrs={'itemprop': 'programmingLanguage'})
    language = language.get_text(strip=True) if language else 'unknown'
    stars = repo.find('a', attrs={'href': re.compile('\/stargazers')})
    stars = int(stars.get_text(strip=True).replace(',', '')) if stars else 0
    print(name, language, stars)

```

运行此命令将输出:

```py
sagetv Java 192
ggrc-core Python 233
gapid Go 445
certificate-transparency-rfcs Python 55
mtail Go 936
[...]

```

然而，如果我们试图抓取普通用户的页面，这将会失败。谷歌的 GitHub 账户是一个企业账户，显示与普通用户账户略有不同。您可以通过将“username”变量设置为“Macuyiko”(本书的作者之一)来尝试一下。因此，我们需要调整代码来处理这两种情况:

```py
import requests
from bs4 import BeautifulSoup
import re

session = requests.Session()

url = 'https://github.com/{}'
username = 'Macuyiko'

r = session.get(url.format(username), params={'page': 1, 'tab': 'repositories'})
html_soup = BeautifulSoup(r.text, 'html.parser')

is_normal_user = False
repos_element = html_soup.find(class_='repo-list')
if not repos_element:
    is_normal_user = True
    repos_element = html_soup.find(id='user-repositories-list')

repos = repos_element.find_all('li')
for repo in repos:
    name = repo.find('h3').find('a').get_text(strip=True)
    language = repo.find(attrs={'itemprop': 'programmingLanguage'})
    language = language.get_text(strip=True) if language else 'unknown'
    stars = repo.find('a', attrs={'href': re.compile('\/stargazers')})
    stars = int(stars.get_text(strip=True).replace(',', '')) if stars else 0
    print(name, language, stars)

```

运行此命令将输出:

```py
macuyiko.github.io HTML 0
blog JavaScript 1
minecraft-python JavaScript 14
[...]

```

作为一个额外的练习，如果存储库页面是分页的(就像 Google 帐户的情况一样)，尝试修改这段代码来删除所有页面。

作为最后的补充，你会注意到像 [`https://github.com/Macuyiko?tab=repositories`](https://github.com/Macuyiko?tab=repositories) 这样的用户页面也有一个简短的简历，包括(在某些情况下)一个电子邮件地址。但是，这个电子邮件地址只有在我们登录 GitHub 后才可见。在接下来的内容中，我们也将尝试获取这些信息。

Warning

这种寻找高星级 GitHub 档案并提取联系信息的做法经常被招聘公司采用。话虽如此，请注意我们现在要登录 GitHub，我们正在跨越公共信息和私人信息的界限。这是一个实践练习，演示了如何用 Python 实现这一点。谨记法律方面，建议你只收集自己的个人资料，在了解自己的情况之前，不要大规模设置这种收集器。有关刮擦合法性的详细信息，请参考法律问题一章。

如果您还没有创建 GitHub 概要文件，那么您需要创建一个。让我们从登录页面的登录表单开始:

```py
import requests
from bs4 import BeautifulSoup

session = requests.Session()

url = 'https://github.com/{}'
username = 'Macuyiko'

# Visit the login page
r = session.get(url.format('login'))
html_soup = BeautifulSoup(r.text, 'html.parser')

form = html_soup.find(id='login')
print(form)

```

运行此命令将输出:

```py
<div class="auth-form px-3" id="login"> <!-- '"` --><!-- </textarea></xmp> --></div>

```

这不是我们所期望的。如果我们看一下页面源代码，我们会发现页面的格式有点奇怪:

```py
<div class="auth-form px-3" id="login">

    <!-- '"` --><!-- </textarea></xmp> --></option></form>

    <form accept-charset="UTF-8" action="/session" method="post">
    <div style="margin:0;padding:0;display:inline">
    <input name="utf8" type="hidden" value="✓" />
    <input name="authenticity_token" type="hidden" value="AtuMda[...]zw==" />
    </div>

    <div class="auth-form-header p-0">
        <h1>Sign in to GitHub</h1>
    </div>

    <div id="js-flash-container">
</div>
[...]

</form>

```

下面的修改确保了我们得到页面中的表单:

```py
import requests
from bs4 import BeautifulSoup

session = requests.Session()

url = 'https://github.com/{}'
username = 'Macuyiko'

# Visit the login page
r = session.get(url.format('login'))
html_soup = BeautifulSoup(r.text, 'html.parser')

data = {}
for form in html_soup.find_all('form'):
    # Get out the hidden form fields
    for inp in form.select('input[type=hidden]'):
        data[inp.get('name')] = inp.get('value')

# SET YOUR LOGIN DETAILS:

data.update({'login': '', 'password': ''})

print('Going to login with the following POST data:')
print(data)

if input('Do you want to login (y/n): ') == 'y':
    # Perform the login
    r = session.post(url.format('session'), data=data)
    # Get the profile page
    r = session.get(url.format(username))
    html_soup = BeautifulSoup(r.text, 'html.parser')
    user_info = html_soup.find(class_='vcard-details')
    print(user_info.text)

```

Even Browsers Have Bugs

如果你一直在使用 Chrome，你可能想知道为什么在使用 Chrome 的开发工具进行登录时看不到表单数据。原因是 Chrome 包含一个错误，当帖子的状态代码对应重定向时，该错误会阻止表单数据出现在开发人员工具中。帖子数据仍在发送中；然而，你不会在开发者工具标签中看到它。当你读到这篇文章的时候，这个 bug 可能已经被修复了，但这只是表明浏览器中也会出现 bug。

运行此命令将输出:

```py
Going to login with the following POST data:
{'utf8': 'V',
 'authenticity_token': 'zgndmzes [...]',
 'login': 'YOUR_USER_NAME',
 'password': 'YOUR_PASSWORD'}
Do you want to login (y/n): y

KU Leuven

Belgium

macuyiko@gmail.com

http://blog.macuyiko.com

```

Plain Text Passwords

不言而喻，在 Python 文件(以及其他程序)中以明文形式硬编码密码对于现实生活中的脚本来说是不可取的。在实际的部署设置中，您的代码可能会与其他人共享，请确保修改您的脚本，以便它从安全的数据存储中检索存储的凭据(例如，从操作系统环境变量、文件或数据库中，最好是加密的)。例如，看看 pip 中提供的“secureconfig”库，了解如何做到这一点。

![A463931_1_En_9_Fig2_HTML.jpg](A463931_1_En_9_Fig2_HTML.jpg)

图 9-2

The Barclays mortgage simulator submits a POST request using JavaScript and embeds the request data in a JSON format

## 9.6 降低抵押贷款利率

我们将在 [`https://www.barclays.co.uk/mortgages/mortgage-calculator/`](https://www.barclays.co.uk/mortgages/mortgage-calculator/) 推出巴克莱的抵押贷款模拟器。我们选择这家金融服务提供商并没有什么特别的原因，除了它应用了一些有趣的技术作为一个很好的例子。

花些时间探索一下这个网站(使用“它的成本是多少？”).我们被要求填写几个参数，之后我们会得到一个我们想要刮出的可能产品的概述。

如果您使用浏览器的开发工具，您会注意到一个 POST 请求被发送到 [`https://www.barclays.co.uk/dss/service/co.uk/mortgages/costcalculator/productservice`](https://www.barclays.co.uk/dss/service/co.uk/mortgages/costcalculator/productservice) ，它有一个有趣的属性:执行 POST 的页面上的 JavaScript 使用“内容类型”头的“应用程序/json”值，并将 POST 数据作为普通的 JSON；见图 [9-2](#Fig2) 。在这种情况下，依赖于请求的参数' T1 '将不起作用，因为它将对 POST 数据进行编码。相反，我们需要使用`json`参数，它将基本上指示请求将 POST 数据格式化为 JSON。

此外，您会注意到结果页面被格式化为一个看起来相对复杂的表(每个条目都有“Show more”链接)，尽管 POST 请求返回的响应看起来像一个格式良好的 JSON 对象；参见图 [9-3](#Fig3) ，因此我们可能甚至不需要漂亮的 Soup 来访问这个“内部 API”。

![A463931_1_En_9_Fig3_HTML.jpg](A463931_1_En_9_Fig3_HTML.jpg)

图 9-3

The POST response data also comes back as nicely formatted JSON

让我们看看用 Python 实现它会得到什么样的响应:

```py
import requests

url = 'https://www.barclays.co.uk/dss/service/co.uk/mortgages/' + \
      'costcalculator/productservice'

session = requests.Session()

estimatedPropertyValue = 200000
repaymentAmount = 150000
months = 240
data = {"header": {"flowId":"0"},
        "body":
        {"wantTo":"FTBP",
         "estimatedPropertyValue":estimatedPropertyValue,
         "borrowAmount":repaymentAmount,
         "interestOnlyAmount":0,
         "repaymentAmount":repaymentAmount,
         "ltv":round(repaymentAmount/estimatedPropertyValue*100),
         "totalTerm":months,
         "purchaseType":"Repayment"}}

r = session.post(url, json=data)

print(r.json())

```

运行此命令将输出:

```py
{'header':
{'result': 'error', 'systemError':
  {'errorCode': 'DSS_SEF001', 'type': 'E',
   'severity': 'FRAMEWORK',
   'errorMessage': 'State details not found in database',
   'validationErrors': [],
   'contentType': 'application/json', 'channel': '6'}
}}

```

这看起来不太好。请记住，当我们没有得到预期的结果时，我们可以做很多事情:

*   检查我们是否忘了包括一些饼干。例如，我们可能需要首先访问入口页面，或者可能有 JavaScript 设置的 cookies。如果您在浏览器中检查请求，您会注意到有很多 cookies。
*   检查我们是否忘记包含一些标题，或者我们是否需要伪造一些标题。
*   如果其他方法都失败了，就求助于 Selenium 来实现一个完整的浏览器。

在这种特殊的情况下，请求中包含了许多 Cookie，其中一些是通过普通的“Set-Cookie”头设置的，尽管许多也是通过页面中包含的大量 JavaScript 文件设置的。这些肯定很难弄清楚，因为 JavaScript 很混乱。然而，JavaScript 在 POST 请求中设置并包含了一些有趣的头，这些头似乎与错误消息有关。让我们试着包括这些，以及欺骗“用户代理”和“Referer”头:

```py
import requests

url = 'https://www.barclays.co.uk/dss/service/co.uk/mortgages/' + \
      'costcalculator/productservice'

session = requests.Session()

session.headers.update({
    # These are non-typical headers, let's include them
    'currentState': 'default_current_state',
    'action': 'default',
    'Origin': 'https://www.barclays.co.uk',
    # Spoof referer, user agent, and X-Requested-With
    'Referer': 'https://www.barclays.co.uk/mortgages/mortgage-calculator/',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ' + ' (KHTML, like Gecko) Chrome/62.0.3202.62
    Safari/537.36',
    'X-Requested-With': 'XMLHttpRequest',
    })
estimatedPropertyValue = 200000
repaymentAmount = 150000
months = 240
data = {"header": {"flowId":"0"},
        "body":

        {"wantTo":"FTBP",
         "estimatedPropertyValue":estimatedPropertyValue,
         "borrowAmount":repaymentAmount,
         "interestOnlyAmount":0,
         "repaymentAmount":repaymentAmount,
         "ltv":round(repaymentAmount/estimatedPropertyValue*100),
         "totalTerm":months,
         "purchaseType":"Repayment"}}

r = session.post(url, json=data)

# Only print the header to avoid text overload
print(r.json()['header'])

```

这个好像管用！在这种情况下，事实证明我们根本不需要包含任何 cookies。我们现在可以清理这段代码了:

```py
import requests

def get_mortgages(estimatedPropertyValue, repaymentAmount, months):
    url = 'https://www.barclays.co.uk/dss/service/' + \
          'co.uk/mortgages/costcalculator/productservice'
    headers = {
        # These are non-typical headers, let's include them
        'currentState': 'default_current_state',
        'action': 'default',
        'Origin': 'https://www.barclays.co.uk',
        # Spoof referer, user agent, and X-Requested-With
        'Referer': 'https://www.barclays.co.uk/mortgages/mortgage-calculator/',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ' + ' (KHTML, like Gecko) Chrome/62.0.3202.62
        Safari/537.36',

        'X-Requested-With': 'XMLHttpRequest',
        }
    data = {"header": {"flowId":"0"},
            "body":
            {"wantTo":"FTBP",
             "estimatedPropertyValue":estimatedPropertyValue,
             "borrowAmount":repaymentAmount,
             "interestOnlyAmount":0,
             "repaymentAmount":repaymentAmount,
             "ltv":round(repaymentAmount/estimatedPropertyValue*100),
             "totalTerm":months,
             "purchaseType":"Repayment"}}
    r = requests.post(url, json=data, headers=headers)
    results = r.json()
    return results['body']['mortgages']
mortgages = get_mortgages(200000, 150000, 240)

# Print the first mortgage info
print(mortgages[0])

```

运行此命令将输出:

```py
{'mortgageName': '5 Year Fixed', 'mortgageId': '1321127853346',
 'ctaType': None, 'uniqueId': '590b357e295b0377d0fb607b',
 'mortgageType': 'FIXED',
 'howMuchCanBeBorrowedNote': '95% (max) of the value of your home',

 'initialRate': 4.99, 'initialRateTitle': '4.99%',
 'initialRateNote': 'until 31st January 2023',
[...]

```

## 9.7 抓取和可视化 IMDB 评级

接下来的一系列示例将继续包括一些更加面向数据科学的用例。我们将从简单的开始，使用 IMDB(互联网电影数据库)搜集一个电视连续剧集的评论列表。我们以《权力的游戏》为例，其剧集列表可以在 [`http://www.imdb.com/title/tt0944947/episodes`](http://www.imdb.com/title/tt0944947/episodes) 找到。请注意，IMDB 的概述分布在多个页面上(每个季节或每年)，因此我们使用一个额外的循环来迭代我们想要检索的季节:

```py
import requests
from bs4 import BeautifulSoup

url = 'http://www.imdb.com/title/tt0944947/episodes'

episodes = []
ratings = []

# Go over seasons 1 to 7
for season in range(1, 8):
    r = requests.get(url, params={'season': season})
    soup = BeautifulSoup(r.text, 'html.parser')
    listing = soup.find('div', class_="eplist")
    for epnr, div in enumerate(listing.find_all('div', recursive=False)):
        episode = "{}.{}".format(season, epnr + 1)
        rating_el = div.find(class_='ipl-rating-star__rating')
        rating = float(rating_el.get_text(strip=True))
        print('Episode:', episode, '-- rating:', rating)
        episodes.append(episode)
        ratings.append(rating)

```

然后，我们可以使用“matplotlib”绘制抓取的评级，这是一个著名的 Python 绘图库，可以使用 pip 轻松安装:

```py
pip install -U matplotlib

```

Plotting with Python

当然，您也可以使用例如 Excel 来复制下面的图，但是这个示例只是一个简单的介绍，因为我们将继续使用 matplotlib 来完成后面的一些示例。请注意，这当然不是唯一的——甚至不是最用户友好的 Python 绘图库，尽管它仍然是最流行的绘图库之一。看看 Seaborn ( [`https://seaborn.pydata.org/`](https://seaborn.pydata.org/) )、Altair ( [`https://altair-viz.github.io/`](https://altair-viz.github.io/) )和 ggplot ( [`http://ggplot.yhathq.com/`](http://ggplot.yhathq.com/) )其他一些优秀的库。

在我们的脚本中添加以下几行，将结果绘制在一个简单的条形图中，如图 [9-4](#Fig4) 所示。

![A463931_1_En_9_Fig4_HTML.jpg](A463931_1_En_9_Fig4_HTML.jpg)

图 9-4

Plotting IMDB ratings per episode using “matplotlib” Adding in the following

```py
import matplotlib.pyplot as plt

episodes = ['S' + e.split('.')[0] if int(e.split('.')[1]) == 1 else '' \
                                  for e in episodes]

plt.figure()
positions = [a*2 for a in range(len(ratings))]
plt.bar(positions, ratings, align="center")
plt.xticks(positions, episodes)
plt.show()

```

## 9.8 搜集 IATA 航空公司信息

我们将使用 [`http://www.iata.org/publications/Pages/code-search.aspx`](http://www.iata.org/publications/Pages/code-search.aspx) 上的搜索表单收集航空公司信息。这是一个有趣的案例，说明了一些网站的“肮脏”，尽管我们想要使用的表单看起来非常简单(页面上只有一个下拉框和一个文本字段)。正如 URL 已经显示的，驱动这个页面的 web 服务器是建立在 ASP.NET(“。aspx”)，它对如何处理表单数据有非常独特的见解。

尝试使用您的浏览器提交这个表单，并使用它的开发工具看看会发生什么，这是一个好主意。正如您在图 [9-5](#Fig5) 中看到的，似乎 POST 请求中包含了大量的表单数据——比我们的两个字段多得多。

![A463931_1_En_9_Fig5_HTML.jpg](A463931_1_En_9_Fig5_HTML.jpg)

图 9-5

Submitting the IATA form includes lots of form data.

当然，在我们的 Python 脚本中手动包含所有这些字段看起来并不可行。例如，“__VIEWSTATE”字段保存了随每个请求而变化的会话信息。甚至一些字段名似乎包含了我们不能确定将来不会改变的部分，导致我们的脚本中断。此外，似乎我们也需要跟踪 cookies。最后，看看 POST 请求返回的响应内容。这看起来像是部分响应(将由 JavaScript 解析和显示),而不是完整的 HTML 页面:

```py
1|#||4|1330|updatePanel|ctl00_SPWebPartManager1_g_e3b09024_878e
[...]
MSOSPWebPartManager_StartWebPartEditingName|false|5|hiddenField|
MSOSPWebPartManager_EndWebPartEditing|false|

```

为了解决这些问题，我们将努力使我们的代码尽可能健壮。首先，我们将从使用请求的会话机制对搜索页面执行 GET 请求开始。接下来，我们将使用 Beautiful Soup 获取所有表单元素及其名称和值:

```py
import requests
from bs4 import BeautifulSoup

url = 'http://www.iata.org/publications/Pages/code-search.aspx'

session = requests.Session()
# Spoof the user agent as a precaution
session.headers.update({
    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ' + ' (KHTML, like Gecko) Chrome/62.0.3202.62
    Safari/537.36'
    })

# Get the search page
r = session.get(url)
html_soup = BeautifulSoup(r.text, 'html.parser')
form = html_soup.find(id='aspnetForm')

# Get the form fields

data = {}
for inp in form.find_all(['input', 'select']):
    name = inp.get('name')
    value = inp.get('value')
    if not name:
        continue

    data[name] = value if value else ''

print(data, end="\n\n\n")

```

这将输出以下内容:

```py
{'_wpcmWpid': '',
 'wpcmVal': '',
 'MSOWebPartPage_PostbackSource': '',
 'MSOTlPn_SelectedWpId': '',
 'MSOTlPn_View': '0',
 'MSOTlPn_ShowSettings': 'False',
 'MSOGallery_SelectedLibrary': '',
 'MSOGallery_FilterString': '',
 'MSOTlPn_Button': 'none',
 '__EVENTTARGET': '',
 '__EVENTARGUMENT': '',
[...]

```

接下来，我们将使用收集的表单数据来执行 POST 请求。但是，我们必须确保为下拉框和文本框设置正确的值。我们在脚本中添加了以下几行:

```py
# Set our desired search query
for name in data.keys():
    # Search by
    if 'ddlImLookingFor' in name:
        data[name] = 'ByAirlineName'
    # Airline name
    if 'txtSearchCriteria' in name:
        data[name] = 'Lufthansa'

# Perform a POST
r = session.post(url, data=data)
print(r.text)

```

奇怪的是，与浏览器中发生的情况相反，POST 请求确实返回了一个完整的 HTML 页面，而不是部分结果。这并不太坏，因为我们现在可以使用漂亮的汤来获取结果表。

我们将使用一个流行的数据科学库“pandas”来处理表格数据，而不是手动解析这个表，该库内置了一个有用的“HTML 表到数据框”方法。使用 pip 很容易安装该库:

```py
pip install -U pandas

```

为了解析出 HTML，pandas 默认依赖于“lxml ”,并在无法找到“lxml”的情况下使用“html5lib”来完成。要确保“lxml”可用，请安装它:

```py
pip install -U lxml

```

完整的脚本现在可以组织如下:

```py
import requests
from bs4 import BeautifulSoup
import pandas

url = 'http://www.iata.org/publications/Pages/code-search.aspx'

def get_results(airline_name):
    session = requests.Session()
    # Spoof the user agent as a precaution
    session.headers.update({
        'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ' + ' (KHTML, like Gecko) Chrome/62.0.3202.62
        Safari/537.36'
        })
    r = session.get(url)

    html_soup = BeautifulSoup(r.text, 'html.parser')
    form = html_soup.find(id='aspnetForm')
    data = {}
    for inp in form.find_all(['input', 'select']):
        name = inp.get('name')
        value = inp.get('value')
        if not name:
            continue

        if 'ddlImLookingFor' in name:
            value = 'ByAirlineName'
        if 'txtSearchCriteria' in name:
            value = airline_name
        data[name] = value if value else ''

    r = session.post(url, data=data)
    html_soup = BeautifulSoup(r.text, 'html.parser')
    table = html_soup.find('table', class_="datatable")
    df = pandas.read_html(str(table))
    return df

df = get_results('Lufthansa')
print(df)

```

运行此命令将输出:

```py
[                               0   1     2      3
0           Deutsche Lufthansa AG  LH 220.0  220.0
1              Lufthansa Cargo AG  LH   NaN   20.0
2         Lufthansa CityLine GmbH  CL 683.0  683.0
3 Lufthansa Systems GmbH & Co. KG  S1   NaN    NaN]

```

等效的 Selenium 代码如下所示:

```py
import pandas
from selenium import webdriver
from selenium.webdriver.support.ui import Select

url = 'http://www.iata.org/publications/Pages/code-search.aspx'

driver = webdriver.Chrome()
driver.implicitly_wait(10)

def get_results(airline_name):
    driver.get(url)
    # Make sure to select the right part of the form
    # This will make finding the elements easier
    # as #aspnetForm wraps the whole page, including
    # the search box

    form_div = driver.find_element_by_css_selector('#aspnetForm .iataStandardForm')
    select = Select(form_div.find_element_by_css_selector('select'))
    select.select_by_value('ByAirlineName')
    text = form_div.find_element_by_css_selector('input[type=text]')
    text.send_keys(airline_name)
    submit = form_div.find_element_by_css_selector('input[type=submit]')
    submit.click()
    table = driver.find_element_by_css_selector('table.datatable')
    table_html = table.get_attribute('outerHTML')
    df = pandas.read_html(str(table_html))
    return df

df = get_results('Lufthansa')
print(df)

driver.quit()

```

还有一个我们必须解决的谜团:记住，requests 发出的 POST 请求返回一个完整的 HTML 页面，而不是我们在浏览器中看到的部分结果。服务器是如何区分这两种结果的呢？答案在于提交搜索表单的方式。在请求中，我们用最少的头执行一个简单的 POST 请求。然而，在动态页面上，表单提交是由 JavaScript 处理的，它将执行实际的 POST 请求，并解析出部分结果来显示它们。为了向服务器表明是 JavaScript 发出了请求，请求中包含了两个头，我们也可以在请求中伪造这两个头。如果我们按如下方式修改代码，您也将获得相同的部分结果:

```py
# Include headers to indicate that we want a partial result
session.headers.update({
    'X-MicrosoftAjax'  : 'Delta=true',
    'X-Requested-With' : 'XMLHttpRequest',
    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ' + ' (KHTML, like Gecko) Chrome/62.0.3202.62
    Safari/537.36'
    })

```

## 9.9 抓取和分析网络论坛互动

在这个例子中，我们将搜集在 [`http://bpbasecamp.freeforums.net/board/27/gear-closet`](http://bpbasecamp.freeforums.net/board/27/gear-closet) (一个为背包客和徒步旅行者准备的论坛)的网络论坛帖子，以了解谁是最活跃的用户，谁经常与谁互动。我们将记录交互作用，如下所示:

*   一个“线程”中的第一个帖子不是“回复”任何人，所以我们不会认为这是一个交互，
*   一个帖子中的下一个帖子可以选择性地包含一个或多个引用块，这表明发帖人直接回复了另一个用户，我们将这样看待，
*   如果一个帖子不包含任何引用块，我们就假设这个帖子是对原始帖子的回复。情况可能不一定是这样，用户经常会使用诸如“^^”这样的小文本来表示他们正在回复前面的帖子，但是我们在这个例子中会保持简单(不过，可以根据您对“交互”的定义随意修改脚本)。

让我们开始吧。首先，我们将提取给定论坛 URL 的主题列表:

```py
import requests
import re
from bs4 import BeautifulSoup

def get_forum_threads(url, max_pages=None):
    page = 1
    threads = []
    while not max_pages or page <= max_pages:
        print('Scraping forum page:', page)
        r = requests.get(url, params={'page': page})
        soup = BeautifulSoup(r.text, 'html.parser')
        content = soup.find(class_='content')
        links = content.find_all('a', attrs={'href': re.compile('^\/thread\/')})
        threads_on_page = [a.get('href') for a in links \
                if a.get('href') and not 'page=' in a.get('href')]
        threads += threads_on_page
        page += 1
        next_page = soup.find('li', class_="next")
        if 'state-disabled' in next_page.get('class'):
            break

        return threads

url = 'http://bpbasecamp.freeforums.net/board/27/gear-closet'

threads = get_forum_threads(url, max_pages=5)
print(threads)

```

注意，关于分页，我们在这里必须聪明一点。该论坛将继续返回最后一页，即使当提供大于最大页数的 URL 参数时，这样我们可以检查具有类“next”的项目是否也具有类“state-disabled ”,以确定我们是否已经到达线程列表的末尾。因为我们只想要与第一页相对应的线程链接，所以我们也删除了 URL 中包含“page=”的所有链接。在这个例子中，我们还决定将自己限制在五页以内。运行此命令将输出:

```py
Scraping forum page: 1
Scraping forum page: 2
Scraping forum page: 3
Scraping forum page: 4
Scraping forum page: 5
['/thread/2131/before-asking-which-pack-boot', [...] ]

```

对于每个线程，我们现在想要得到一个帖子列表。我们可以先用一个线程来尝试一下:

```py
import requests
import re
from urllib.parse import urljoin
from bs4 import BeautifulSoup

def get_thread_posts(url, max_pages=None):
    page = 1
    posts = []
    while not max_pages or page <= max_pages:
        print('Scraping thread url/page:', url, page)
        r = requests.get(url, params={'page': page})
        soup = BeautifulSoup(r.text, 'html.parser')
        content = soup.find(class_='content')
        for post in content.find_all('tr', class_="item"):
            user = post.find('a', class_='user-link')
            if not user:
                # User might be deleted, skip...
                continue

            user = user.get_text(strip=True)
            quotes = []
            for quote in post.find_all(class_='quote_header'):
                quoted_user = quote.find('a', class_='user-link')
                if quoted_user:
                    quotes.append(quoted_user.get_text(strip=True))
            posts.append((user, quotes))
        page += 1
        next_page = soup.find('li', class_="next")
        if 'state-disabled' in next_page.get('class'):
            break

    return posts
url = 'http://bpbasecamp.freeforums.net/board/27/gear-closet'
thread = '/thread/2131/before-asking-which-pack-boot'

thread_url = urljoin(url, thread)
posts = get_thread_posts(thread_url)
print(posts)

```

运行它将输出一个列表，其中每个元素都是一个元组，包含发帖者的姓名和帖子中引用的用户列表:

```py
Scraping thread url/page:                                                
    http://bpbasecamp.freeforums.net/thread/2131/before-asking-which-pack-boot 1
Scraping thread url/page:                                                
    http://bpbasecamp.freeforums.net/thread/2131/before-asking-which-pack-boot 2
[('almostthere', []), ('trinity', []), ('paula53', []),                     ('toejam', ['almostthere']), (‘stickman', []), (‘tamtrails', []),    
    ('almostthere', ['tamtrails']), ('kayman', []), (‘almostthere',       [‘kayman']), (‘lanceman', []), (‘trinity', [‘trinity']),             
     (‘Christian', [‘almostthere']), (‘pollock', []), (‘mitsmit', []),   
    ('intothewild', []), (‘Christian', []), (‘softskull', []), (‘argus',    []),(‘lyssa7', []), (‘kevin', []), (‘greenwoodsuncharted', [])]

```

将这两个函数放在一起，我们得到了下面的脚本。我们将使用 Python 的“pickle”模块来存储我们抓取的结果，这样我们就不必一遍又一遍地重新抓取论坛了:

```py
import requests
import re
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import pickle

def get_forum_threads(url, max_pages=None):
    page = 1
    threads = []
    while not max_pages or page <= max_pages:
        print('Scraping forum page:', page)
        r = requests.get(url, params={'page=': page})
        soup = BeautifulSoup(r.text, 'html.parser')
        content = soup.find(class_='content')
        links = content.find_all('a', attrs={'href': re.compile('^\/thread\/')})
        threads_on_page = [a.get('href') for a in links \
                if a.get('href') and not 'page' in a.get('href')]
        threads += threads_on_page
        page += 1
        next_page = soup.find('li', class_="next")
        if 'state-disabled' in next_page.get('class'):
            break

    return threads

def get_thread_posts(url, max_pages=None):
    page = 1
    posts = []
    while not max_pages or page <= max_pages:
        print('Scraping thread url/page:', url, page)
        r = requests.get(url, params={'page': page})
        soup = BeautifulSoup(r.text, 'html.parser')
        content = soup.find(class_='content')
        for post in content.find_all('tr', class_="item"):
            user = post.find('a', class_='user-link')
            if not user:
                # User might be deleted, skip...
                continue

            user = user.get_text(strip=True)
            quotes = []
            for quote in post.find_all(class_='quote_header'):
                quoted_user = quote.find('a', class_='user-link')
                if quoted_user:
                    quotes.append(quoted_user.get_text(strip=True))
            posts.append((user, quotes))
        page += 1
        next_page = soup.find('li', class_="next")
        if 'state-disabled' in next_page.get('class'):
            break

    return posts

url = 'http://bpbasecamp.freeforums.net/board/27/gear-closet'

threads = get_forum_threads(url, max_pages=5)
all_posts = []

for thread in threads:

    thread_url = urljoin(url, thread)
    posts = get_thread_posts(thread_url)
    all_posts.append(posts)

with open('forum_posts.pkl', "wb") as output_file:
    pickle.dump(all_posts, output_file)

```

接下来，我们可以加载结果并在热图中可视化它们。我们将使用“pandas”、“numpy”和“matplotlib”来实现这一目的，所有这些都可以通过 pip 进行安装(如果您已经按照前面的示例安装了 pandas 和 matplotlib，那么您不需要安装其他任何东西):

```py
pip install -U pandas
pip install -U numpy
pip install -U matplotlib

```

让我们从只可视化第一个线程开始(如上面 scraper 的输出片段所示):

```py
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Load our stored results
with open('forum_posts.pkl', "rb") as input_file:
    posts = pickle.load(input_file)

def add_interaction(users, fu, tu):
    if fu not in users:
        users[fu] = {}
    if tu not in users[fu]:
        users[fu][tu] = 0
    users[fu][tu] += 1

# Create interactions dictionary
users = {}
for thread in posts:

    first_one = None
    for post in thread:
        user = post[0]
        quoted = post[1]
        if not first_one:
            first_one = user
        elif not quoted:
            add_interaction(users, user, first_one)
        else:
            for qu in quoted:
                add_interaction(users, user, qu)
    # Stop after the first thread
    break

df = pd.DataFrame.from_dict(users, orient="index").fillna(0)

heatmap = plt.pcolor(df, cmap="Blues")
y_vals = np.arange(0.5, len(df.index), 1)
x_vals = np.arange(0.5, len(df.columns), 1)
plt.yticks(y_vals, df.index)
plt.xticks(x_vals, df.columns, rotation="vertical")
for y in range(len(df.index)):
    for x in range(len(df.columns)):
        if df.iloc[y, x] == 0:
            continue

        plt.text(x + 0.5, y + 0.5, '%.0f' % df.iloc[y, x],
                 horizontalalignment='center',
                 verticalalignment='center')
plt.show()

```

这将为您提供如图 [9-6](#Fig6) 所示的结果。可以看到，各种用户在回复原帖，原帖也在引用其他一些用户的话。

![A463931_1_En_9_Fig6_HTML.jpg](A463931_1_En_9_Fig6_HTML.jpg)

图 9-6

Visualizing user interactions for one forum thread

有多种方法可以实现这种可视化。例如，图 [9-7](#Fig7) 显示了所有论坛主题的用户互动，但只考虑了直接引用。

![A463931_1_En_9_Fig7_HTML.jpg](A463931_1_En_9_Fig7_HTML.jpg)

图 9-7

Visualizing user interactions (direct quotes only) for all scraped forum threads

## 9.10 收集和聚类时尚数据集

在这个例子中，我们将使用 Zalando(一个流行的瑞典网络商店)来获取时尚产品的图像集合，并使用 t-SNE 对它们进行聚类。

Check the API

注意，Zalando 还公开了一个易于使用的 API(文档见 [`https://github.com/zalando/shop-api-documentation/wiki/Api-introduction`](https://github.com/zalando/shop-api-documentation/wiki/Api-introduction) )。在撰写本文时，该 API 不要求身份验证，尽管这在不久的将来会改变，要求用户注册以获得 API 访问令牌。因为我们在这里只获取图像，所以我们不会费心去注册，尽管在一个合适的“应用程序”中，使用 API 选项肯定会被推荐。

我们的第一个脚本下载图像并将它们存储在一个目录中；参见图 [9-8](#Fig8) :

![A463931_1_En_9_Fig8_HTML.jpg](A463931_1_En_9_Fig8_HTML.jpg)

图 9-8

A collection of scraped dress images

```py
import requests
import os, os.path
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

store = 'images'
if not os.path.exists(store):
    os.makedirs(store)

url = 'https://www.zalando.co.uk/womens-clothing-dresses/'
pages_to_crawl = 15

def download(url):

    r = requests.get(url, stream=True)
    filename = urlparse(url).path.split('/')[-1]
    print('Downloading to:', filename)
    with open(os.path.join(store, filename), 'wb') as the_image:
        for byte_chunk in r.iter_content(chunk_size=4096*4):
            the_image.write(byte_chunk)

for p in range(1, pages_to_crawl+1):
    print('Scraping page:', p)
    r = requests.get(url, params={'p' : p})
    html_soup = BeautifulSoup(r.text, 'html.parser')
    for img in html_soup.select('#z-nvg-cognac-root z-grid-item img'):
        img_src = img.get('src')
        if not img_src:
            continue

        img_url = urljoin(url, img_src)
        download(img_url)

```

接下来，我们将使用 t-SNE 聚类算法对照片进行聚类。t-SNE 是一种相对较新的降维技术，特别适合于图像等高维数据集的可视化。你可以在 [`https://lvdmaaten.github.io/tsne/`](https://lvdmaaten.github.io/tsne/) 阅读有关技术。我们将“scikit-learn”与“matplotlib”、“scipy”和“numpy”一起使用，所有这些都是数据科学家熟悉的库，可以通过 pip 安装:

```py
pip install -U matplotlib
pip install -U scikit-learn
pip install -U numpy
pip install -U scipy

```

我们的集群脚本如下所示:

```py
import os.path
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import offsetbox
from sklearn import manifold
from scipy.misc import imread
from glob import iglob

store = 'images'

image_data = []
for filename in iglob(os.path.join(store, '*.jpg')):
     image_data.append(imread(filename))

image_np_orig = np.array(image_data)
image_np = image_np_orig.reshape(image_np_orig.shape[0], -1)

def plot_embedding(X, image_np_orig):
    # Rescale
    x_min, x_max = np.min(X, 0), np.max(X, 0)
    X = (X - x_min) / (x_max - x_min)
    # Plot images according to t-SNE position
    plt.figure()
    ax = plt.subplot(111)
    for i in range(image_np.shape[0]):
        imagebox = offsetbox.AnnotationBbox(
            offsetbox=offsetbox.OffsetImage(image_np_orig[i], zoom=.1),
            xy=X[i],
            frameon=False)
        ax.add_artist(imagebox)

print("Computing t-SNE embedding")

tsne = manifold.TSNE(n_components=2, init="pca")
X_tsne = tsne.fit_transform(image_np)

plot_embedding(X_tsne, image_np_orig)
plt.show()

```

这段代码的工作方式如下。首先，我们加载所有的图像(使用`imread`)并将它们转换成一个 numpy 数组。`reshape`函数确保我们得到一个 n×3m 矩阵，其中 n 是图像的数量，m 是每个图像的像素数量，而不是一个 n×r×g×b 张量，r、g 和 b 分别是红色、绿色和蓝色通道的像素值。在构建了 t-SNE 嵌入后，我们使用 matplotlib 绘制了带有计算出的 x 和 y 坐标的图像，得到了如图 [9-9](#Fig9) 所示的图像(使用了大约一千张刮掉的照片)。可以看出，这里的聚类主要是由图像中颜色的饱和度和强度驱动的。

Image Sizes

幸运的是，我们刮到的所有图像都有相同的宽度和高度。如果不是这种情况，我们首先必须应用调整大小，以确保每个图像都将在数据集中产生一个长度相等的向量。

![A463931_1_En_9_Fig9_HTML.jpg](A463931_1_En_9_Fig9_HTML.jpg)

图 9-9

The result of the t-SNE clustering (applied on about a thousand photos)

## 9.11 刮掉的亚马逊评论的情感分析

我们将收集一个亚马逊评论列表，其中包含对某个特定产品的评级。我们会用一本评论很多的书，比如马克·卢茨的《学习 Python》，可以在 [`https://www.amazon.com/Learning-Python-5th-Mark-Lutz/dp/1449355730/`](https://www.amazon.com/Learning-Python-5th-Mark-Lutz/dp/1449355730/) 找到。如果你点击“查看所有顾客评论”，你会到达 [`https://www.amazon.com/Learning-Python-5th-Mark-Lutz/product-reviews/1449355730/`](https://www.amazon.com/Learning-Python-5th-Mark-Lutz/product-reviews/1449355730/) 。请注意，该产品的 id 为“1449355730”，即使使用没有产品名称的 URL [`https://www.amazon.com/product-reviews/1449355730/`](https://www.amazon.com/product-reviews/1449355730/) ，也可以工作。

Simple URLs

在编写 web scraper 之前，像我们在这里所做的那样使用 URL 总是一个好主意。基于以上所述，我们知道给定的产品标识符足以获取评论页面，而不需要找出确切的 URL，包括产品名称。那么，为什么亚马逊允许这两者，并且默认包含产品名称呢？原因很可能是搜索引擎优化(SEO)。像 Google 这样的搜索引擎更喜欢包含人类可读组件的 URL。

如果您浏览评论页面，您会注意到评论是分页的。通过浏览其他页面并使用浏览器的开发工具，我们看到 POST 请求(通过 JavaScript)被发送到类似于 [`https://www.amazon.com/ss/customer-reviews/ajax/reviews/get/ref=cm_cr_arp_d_paging_btm_2`](https://www.amazon.com/ss/customer-reviews/ajax/reviews/get/ref=cm_cr_arp_d_paging_btm_2) 的 URL，表单数据中包含产品 id，以及其他一些看起来相对容易欺骗的表单字段。让我们看看我们在请求中得到了什么:

```py
import requests
from bs4 import BeautifulSoup

review_url = 'https://www.amazon.com/ss/customer-reviews/ajax/reviews/get/'
product_id = '1449355730'

session = requests.Session()
session.headers.update({
    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ' + ' (KHTML, like Gecko) Chrome/62.0.3202.62
    Safari/537.36'
    })

session.get('https://www.amazon.com/product-reviews/{}/'.format(product_id))

def get_reviews(product_id, page):
    data = {
        'sortBy':'',
        'reviewerType':'all_reviews',
        'formatType':'',
        'mediaType':'',
        'filterByStar':'all_stars',
        'pageNumber':page,
        'filterByKeyword':'',
        'shouldAppend':'undefined',
        'deviceType':'desktop',
        'reftag':'cm_cr_getr_d_paging_btm_{}'.format(page),
        'pageSize':10,
        'asin':product_id,
        'scope':'reviewsAjax1'
        }
    r = session.post(review_url + 'ref=' + data['reftag'], data=data)
    return r.text

print(get_reviews(product_id, 1))

```

注意，我们在这里伪造了“用户代理”头。如果我们不这样做，Amazon 将回复一条消息，要求我们验证我们是否是人类(您可以从浏览器的开发工具中复制这个头的值)。此外，请注意我们设置为“reviewsAjax1”的“scope”表单字段。如果您在浏览器中浏览 reviews 页面，您将会看到该字段的值实际上随着每个请求而增加，即“reviewsAjax1”、“reviewsAjax2”等等。我们也可以决定复制这种行为——如果亚马逊发现了我们的策略，我们就必须这么做，尽管这似乎不是结果正确返回的必要条件。

最后，请注意，POST 请求并不返回完整的 HTML 页面，而是某种手动编码的结果，JavaScript(通常)会对其进行解析:

```py
["script",
 "if(window.ue) { ues('id','reviewsAjax1','FE738GN7GRDZK6Q09S9G');
 ues('t0','reviewsAjax1',new Date());
 ues('ctb','reviewsAjax1','1');
 uet('bb','reviewsAjax1'); }"
]
&&&
["update","#cm_cr-review_list",""]
&&&
["loaded"]
&&&
["append","#cm_cr-review_list","<div id=\"R3JQXR4EMWJ7AD\" data-hook=\"review\"class=\"a-section review\"><div id=\                
     "customer_review-R3JQXR4EMWJ7AD\"class=\"a-section celwidget\">     <div class=\"a-row\"><a class=\"a-link-normal\"title=\"5.0 out      of 5 stars\"
[...]

```

幸运的是，在对回复进行了一番研究之后(可以随意在文本编辑器中复制粘贴完整的回复并通读)，结构似乎很容易理解:

*   回复由几个“指令”组成，格式为 JSON 列表；
*   指令本身由三个&符号“&&&”分隔；
*   包含评论的指令以“附加”字符串开始；
*   评论的实际内容被格式化为 HTML 元素，位于列表的第三个位置。

让我们调整代码，以结构化的格式解析评论。我们将循环所有的指令；使用“json”模块转换它们；检查“附加”条目；然后使用 Beautiful Soup 解析 HTML 片段并获得评论 id、评级、标题和文本。我们还需要一个小的正则表达式来获得评级，它被设置为一个类，值为“a-start-1”到“a-star-5”。我们可以按原样使用这些，但是简单地将“1”转换为“5”可能更容易处理，所以我们已经在这里执行了一些清理:

```py
import requests
import json
import re
from bs4 import BeautifulSoup

review_url = 'https://www.amazon.com/ss/customer-reviews/ajax/reviews/get/'
product_id = '1449355730'

session = requests.Session()
session.headers.update({
    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ' + ' (KHTML, like Gecko) Chrome/62.0.3202.62
    Safari/537.36'
    })

session.get('https://www.amazon.com/product-reviews/{}/'.format(product_id))

def parse_reviews(reply):
    reviews = []
    for fragment in reply.split('&&&'):
        if not fragment.strip():
            continue

        json_fragment = json.loads(fragment)
        if json_fragment[0] != 'append':
            continue

        html_soup = BeautifulSoup(json_fragment[2], 'html.parser')
        div = html_soup.find('div', class_="review")
        if not div:
            continue

        review_id = div.get('id')
        title = html_soup.find(class_='review-title').get_text(strip=True)
        review = html_soup.find(class_='review-text').get_text(strip=True)
        # Find and clean the rating:
        review_cls = ' '.join(html_soup.find(class_='review-rating').get('class'))
        rating = re.search('a-star-(\d+)', review_cls).group(1)
        reviews.append({'review_id': review_id,
                        'rating': rating,
                        'title': title,
                        'review': review})
    return reviews

def get_reviews(product_id, page):
    data = {
        'sortBy':'',
        'reviewerType':'all_reviews',
        'formatType':'',
        'mediaType':'',
        'filterByStar':'all_stars',
        'pageNumber':page,
        'filterByKeyword':'',
        'shouldAppend':'undefined',
        'deviceType':'desktop',
        'reftag':'cm_cr_getr_d_paging_btm_{}'.format(page),
        'pageSize':10,
        'asin':product_id,
        'scope':'reviewsAjax1'
        }
    r = session.post(review_url + 'ref=' + data['reftag'], data=data)
    reviews = parse_reviews(r.text)
    return reviews

print(get_reviews(product_id, 1))

```

这个管用！剩下唯一要做的事情就是遍历所有页面，并使用“数据集”库将评论存储在数据库中。幸运的是，确定何时停止循环很容易:一旦我们没有得到某个页面的任何评论，我们可以停止:

```py
import requests
import json
import re
from bs4 import BeautifulSoup
import dataset

db = dataset.connect('sqlite:///reviews.db')

review_url = 'https://www.amazon.com/ss/customer-reviews/ajax/reviews/get/'
product_id = '1449355730'

session = requests.Session()
session.headers.update({
    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ' + ' (KHTML, like Gecko) Chrome/62.0.3202.62
    Safari/537.36'
    })

session.get('https://www.amazon.com/product-reviews/{}/'.format(product_id))

def parse_reviews(reply):
    reviews = []
    for fragment in reply.split('&&&'):
        if not fragment.strip():
            continue

        json_fragment = json.loads(fragment)
        if json_fragment[0] != 'append':
            continue

        html_soup = BeautifulSoup(json_fragment[2], 'html.parser')
        div = html_soup.find('div', class_="review")
        if not div:
            continue

        review_id = div.get('id')
        review_cls = ' '.join(html_soup.find(class_='review-rating').get('class'))
        rating = re.search('a-star-(\d+)', review_cls).group(1)
        title = html_soup.find(class_='review-title').get_text(strip=True)
        review = html_soup.find(class_='review-text').get_text(strip=True)
        reviews.append({'review_id': review_id,
                        'rating': rating,
                        'title': title,
                        'review': review})
    return reviews

def get_reviews(product_id, page):
    data = {
        'sortBy':'',
        'reviewerType':'all_reviews',
        'formatType':'',
        'mediaType':'',
        'filterByStar':'all_stars',
        'pageNumber':page,
        'filterByKeyword':'',
        'shouldAppend':'undefined',
        'deviceType':'desktop',
        'reftag':'cm_cr_getr_d_paging_btm_{}'.format(page),
        'pageSize':10,
        'asin':product_id,
        'scope':'reviewsAjax1'
        }
    r = session.post(review_url + 'ref=' + data['reftag'], data=data)
    reviews = parse_reviews(r.text)
    return reviews

page = 1
while True:
    print('Scraping page', page)
    reviews = get_reviews(product_id, page)
    if not reviews:
        break

    for review in reviews:
        print(' -', review['rating'], review['title'])
        db['reviews'].upsert(review, ['review_id'])
    page += 1

```

这将输出以下内容:

```py
Scraping page 1
  - 5 let me try to explain why this 1600 page book may actually end     up saving you a lot of time and making you a better Python progra

  - 5 Great start, and written for the novice
  - 5 Best teacher of software development
  - 5 Very thorough
  - 5 If you like big thick books that deal with a lot of ...
  - 5 Great book, even for the experienced python programmer
  - 5 Good Tutorial; you'll learn a lot.
  - 2 Takes too many pages to explain even the most simpliest ...
  - 3 If I had a quarter for each time he says something like "here's    an intro to X
  - 4 it almost seems better suited for a college class
[...]

```

现在我们有了一个包含评论的数据库，让我们来做一些有趣的事情。我们将对评论运行情感分析算法(提供每个评论的情感得分)，然后我们可以对不同的评级进行绘图，以检查评级和文本中的情感之间的相关性。要做到这一点，我们将使用“Vader perspection”库，它可以使用 pip 简单地安装。我们还需要安装“nltk”(自然语言工具包)库:

```py
pip install -U vaderSentiment
pip install -U nltk

```

对于一句话来说，使用 Vader perspective 库相当简单:

```py
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

analyzer = SentimentIntensityAnalyzer()

sentence = "I'm really happy with my purchase"
vs = analyzer.polarity_scores(sentence)

print(vs)
# Shows: {'neg': 0.0, 'neu': 0.556, 'pos': 0.444, 'compound': 0.6115}

```

要获得较长文本的情感，一个简单的方法是计算每个句子的情感得分，并对文本中的所有句子进行平均，如下所示:

```py
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from nltk import tokenize

analyzer = SentimentIntensityAnalyzer()

paragraph = """
    I'm really happy with my purchase.
    I've been using the product for two weeks now.
    It does exactly as described in the product description.
    The only problem is that it takes a long time to charge.
    However, since I recharge during nights, this is something I can live with.
    """

sentence_list = tokenize.sent_tokenize(paragraph)
cumulative_sentiment = 0.0
for sentence in sentence_list:
    vs = analyzer.polarity_scores(sentence)
    cumulative_sentiment += vs["compound"]
    print(sentence, ' : ', vs["compound"])

average_sentiment = cumulative_sentiment / len(sentence_list)
print('Average score:', average_score)

```

如果运行这段代码，ntlk 很可能会抱怨缺少资源:

```py
Resource punkt not found.
  Please use the NLTK Downloader to obtain the resource:

  >>> import nltk
  >>> nltk.download('punkt')
[...]

```

要解决这个问题，请在 Python shell 上执行建议的命令:

```py
>>> import nltk
>>> nltk.download('punkt')

```

下载并安装资源后，上面的代码应该工作正常，并将输出:

```py
I'm really happy with my purchase. : 0.6115
I've been using the product for two weeks now. : 0.0
It does exactly as described in the product description. : 0.0
The only problem is that it takes a long time to charge. : -0.4019
However, since I recharge during nights, this is something I can live with. : 0.0
Average score: 0.04192000000000001

```

让我们将此应用于亚马逊评论列表。我们将计算每个评级的情绪，按评级对它们进行组织，然后使用“matplotlib”库绘制每个评级的情绪分数的 violin 图:

```py
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from nltk import tokenize
import dataset
import matplotlib.pyplot as plt

db = dataset.connect('sqlite:///reviews.db')
reviews = db['reviews'].all()

analyzer = SentimentIntensityAnalyzer()

sentiment_by_stars = [[] for r in range(1,6)]

for review in reviews:
    full_review = review['title'] + '. ' + review['review']
    sentence_list = tokenize.sent_tokenize(full_review)
    cumulative_sentiment = 0.0
    for sentence in sentence_list:
        vs = analyzer.polarity_scores(sentence)
        cumulative_sentiment += vs["compound"]
    average_score = cumulative_sentiment / len(sentence_list)
    sentiment_by_stars[int(review['rating'])-1].append(average_score)

plt.violinplot(sentiment_by_stars,
               range(1,6),
               vert=False, widths=0.9,
               showmeans=False, showextrema=True, showmedians=True,
               bw_method='silverman')
plt.axvline(x=0, linewidth=1, color="black")
plt.show()

```

这将输出一个类似于图 [9-10](#Fig10) 所示的图。在这种情况下，我们确实可以观察到评级和文本情感之间的强烈相关性，尽管有趣的是，即使对于较低的评级(二星和三星)，大多数评论仍然有些积极。当然，利用这个数据集还可以做更多的事情。例如，考虑一个检测虚假评论的预测模型。

![A463931_1_En_9_Fig10_HTML.jpg](A463931_1_En_9_Fig10_HTML.jpg)

图 9-10

Sentiment plots per rating level

## 9.12 搜集和分析新闻文章

我们将使用 Selenium 从谷歌新闻中抓取“头条新闻”，参见 [`https://news.google.com/news/?ned=us` `&` `hl=en`](https://news.google.com/news/%3Fned=us%26hl=en) 。我们的目标是访问每一篇文章，得出文章的标题和主要内容。

Not as Easy as It Looks

从一个页面中获取“主要内容”是一件很棘手的事情，乍看起来似乎如此。您可能会尝试迭代所有最底层的 HTML 元素，并保留其中嵌入了最多文本的元素，但是如果一篇文章中的文本被拆分为多个兄弟元素，例如一个较大的“”中的一系列“

”标记，这种方法就会失效。考虑所有元素并不能解决这个问题，因为您最终会简单地选择页面上的顶部元素(例如“”或“”)，因为这将始终包含最大数量的(即全部)文本。如果您依赖 Selenium 提供的`rect`属性来应用可视化方法(例如，找到在页面上占据最大空间的元素)，情况也是如此。已经编写了大量的库和工具来解决这个问题。看一看，比如[`https://github.com/masukomi/ar90-readability`](https://github.com/masukomi/ar90-readability)[`https://github.com/misja/python-boilerpipe`](https://github.com/misja/python-boilerpipe)[`https://github.com/codelucas/newspaper`](https://github.com/codelucas/newspaper)[`https://github.com/fhamborg/news-please`](https://github.com/fhamborg/news-please)针对新闻提取的一些有趣的库，或者专门的 API 比如[`https://newsapi.org/`](https://newsapi.org/)[`https://webhose.io/news-api`](https://webhose.io/news-api)。在这个例子中，我们将使用 Mozilla 的可读性实现；[见`https://github.com/mozilla/readability`见](https://github.com/mozilla/readability)。这是一个基于 JavaScript 的库，但是我们仍然会想出一种方法在 Python 和 Selenium 中使用它。最后，虽然它在最近几年已经很少使用了，但有趣的是，现在已经有了一种很好的格式，网站可以用它来以结构化的方式提供内容更新:RSS (Rich Site Summary):一种 web 提要，允许用户以标准化的、基于 XML 的格式访问在线内容的更新。注意"< link >"标签，它们的" type "属性设置为" application/rss+xml "。然后“href”属性将宣布可以找到 RSS 提要的 URL。

让我们首先使用 Selenium 从 Google News 中获取一个“头条新闻”链接列表。

我们脚本的第一次迭代如下:

```py
from selenium import webdriver

base_url = 'https://news.google.com/news/?ned=us&hl=en'

driver = webdriver.Chrome()
driver.implicitly_wait(10)
driver.get(base_url)

for link in driver.find_elements_by_css_selector('main a[role="heading"]'):
    news_url = link.get_attribute('href')
    print(news_url)

driver.quit()

```

这将输出以下内容(当然，您的链接可能会有所不同):

```py
http://news.xinhuanet.com/english/2017-10/24/c_136702615.htm
http://www.cnn.com/2017/10/24/asia/china-xi-jinping-thought/index.html
[...]

```

在浏览器中导航至 [`http://edition.cnn.com/2017/10/24/asia/china-xi-jinping-thought/index.html`](http://edition.cnn.com/2017/10/24/asia/china-xi-jinping-thought/index.html) ，并在其开发工具中打开浏览器的控制台。我们现在的目标是从这个页面中提取内容，使用 Mozilla 在 JavaScript 中的可读性实现，这是一个通常用于以可读性更好的格式显示文章的工具。也就是说，我们希望在页面中的 [`https://raw.githubusercontent.com/mozilla/readability/master/Readability.js`](https://raw.githubusercontent.com/mozilla/readability/master/Readability.js) 处“注入”可用的 JavaScript 代码。由于我们能够指示浏览器使用 Selenium 执行 JavaScript，因此我们需要拿出一段适当的 JavaScript 代码来执行这个注入。使用您的浏览器控制台，尝试执行以下代码块:

```py
(function(d, script) {
  script = d.createElement('script');
  script.type = 'text/javascript';
  script.async = true;
  script.onload = function(){
    console.log('The script was successfully injected!');
  };
  script.src = 'https://raw.githubusercontent.com/' +
        'mozilla/readability/master/Readability.js';
  d.getElementsByTagName('head')[0].appendChild(script);
}(document));

```

该脚本的工作方式如下:构造一个新的" 

这并不像我们预期的那样，因为 Chrome 拒绝执行这个脚本:

![A463931_1_En_9_Fig11_HTML.jpg](A463931_1_En_9_Fig11_HTML.jpg)

图 9-11

Trying to inject a “<script>” tag using JavaScript

```py
Refused to execute script from
'https://raw.githubusercontent.com/mozilla/readability/master/Readability.js'
because its MIME type ('text/plain') is not executable, and strict MIME type checking  is enabled.

```

这里的问题是 GitHub 在它的头中指出这个文档的内容类型是“text/plain”，Chrome 阻止我们把它作为脚本使用。为了解决这个问题，我们将在 [`http://www.webscrapingfordatascience.com/readability/Readability.js`](http://www.webscrapingfordatascience.com/readability/Readability.js) 自己托管一份脚本，然后再试一次:

```py
(function(d, script) {
  script = d.createElement('script');
  script.type = 'text/javascript';
  script.async = true;
  script.onload = function(){
    console.log('The script was successfully injected!');
  };
  script.src = 'http://www.webscrapingfordatascience.com/readability/Readability.js';
  d.getElementsByTagName('head')[0].appendChild(script);
}(document));

```

这应该给出正确的结果:

![A463931_1_En_9_Fig12_HTML.jpg](A463931_1_En_9_Fig12_HTML.jpg)

图 9-12

Extracting the article’s information

```py
The script was successfully injected!

```

既然已经注入并执行了“

```py
var documentClone = document.cloneNode(true);
var loc = document.location;
var uri = {
  spec: loc.href,
  host: loc.host,
  prePath: loc.protocol + "//" + loc.host,
  scheme: loc.protocol.substr(0, loc.protocol.indexOf(":")),
  pathBase: loc.protocol + "//" + loc.host +
            loc.pathname.substr(0, loc.pathname.lastIndexOf("/") + 1)
};

var article = new Readability(uri, documentClone).parse();
console.log(article);

```

这将为你提供一个如图 [9-12](#Fig12) 所示的结果，看起来很有希望:“article”对象包含了一个我们可以使用的“title”和“content”属性。

现在的问题是我们如何将这些信息返回给 Selenium。记住，我们可以通过`execute_script`方法从 Selenium 执行 JavaScript 命令。获取我们想要的信息的一种可能方法是使用 JavaScript 将整个页面的内容替换为我们想要的信息，然后使用 Selenium 获取这些信息:

```py
from selenium import webdriver

base_url = 'http://edition.cnn.com/2017/10/24/asia/china-xi-jinping-thought/index.html'

driver = webdriver.Chrome()
driver.implicitly_wait(10)

driver.get(base_url)

js_cmd = '''
(function(d, script) {
  script = d.createElement('script');
  script.type = 'text/javascript';
  script.async = true;
  script.onload = function(){
    var documentClone = document.cloneNode(true);
    var loc = document.location;
    var uri = {
      spec: loc.href,
      host: loc.host,
      prePath: loc.protocol + "//" + loc.host,
      scheme: loc.protocol.substr(0, loc.protocol.indexOf(":")),
      pathBase: loc.protocol + "//" + loc.host +
                loc.pathname.substr(0, loc.pathname.lastIndexOf("/") + 1)
    };
    var article = new Readability(uri, documentClone).parse();
    document.body.innerHTML = '<h1 id="title">' + article.title + '</h1>' +
      '<div id="content">' + article.content + '</div>';
  };
  script.src = 'http://www.webscrapingfordatascience.com/readability/Readability.js';
d.getElementsByTagName('head')[0].appendChild(script);
}(document));
'''

driver.execute_script(js_cmd)

title = driver.find_element_by_id('title').text.strip()
content = driver.find_element_by_id('content').text.strip()

print('Title was:', title)

driver.quit()

```

JavaScript 命令中的“document.body.innerHTML”行将把“”标记的内容替换为一个标题和一个“”标记，这样我们就可以简单地从中检索我们想要的信息。

然而，`execute_script`方法也允许我们将 JavaScript 对象传递回 Python，因此下面的方法也是可行的:

```py
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

base_url = 'http://edition.cnn.com/2017/10/24/asia/china-xi-jinping-thought/index.html'

driver = webdriver.Chrome()
driver.implicitly_wait(10)

driver.get(base_url)

js_cmd = '''
(function(d, script) {
  script = d.createElement('script');
  script.type = 'text/javascript';
  script.async = true;
  script.onload = function() {
      script.id = 'readability-script';
  }
  script.src = 'http://www.webscrapingfordatascience.com/readability/Readability.js';
  d.getElementsByTagName('head')[0].appendChild(script);
}(document));
'''

js_cmd2 = '''
var documentClone = document.cloneNode(true);
var loc = document.location;
var uri = {
  spec: loc.href,
  host: loc.host,
  prePath: loc.protocol + "//" + loc.host,
  scheme: loc.protocol.substr(0, loc.protocol.indexOf(":")),
  pathBase: loc.protocol + "//" + loc.host +
            loc.pathname.substr(0, loc.pathname.lastIndexOf("/") + 1)
};
var article = new Readability(uri, documentClone).parse();
return JSON.stringify(article);
'''

driver.execute_script(js_cmd)

wait = WebDriverWait(driver, 10)
wait.until(EC.presence_of_element_located((By.ID, "readability-script")))

returned_result = driver.execute_script(js_cmd2)

print(returned_result)

driver.quit()

```

这里有几个错综复杂的问题，需要一些额外的信息。首先，注意我们使用了两次`execute_script`方法:一次是注入“<脚本>标签，然后再次取出我们的“文章”对象。然而，由于执行脚本可能需要一些时间，并且 Selenium 的隐式等待在使用`execute_script`时没有考虑到这一点，所以我们使用显式等待来检查是否存在一个“id”为“readability-script”的元素，该元素由“script.onload”函数设置。一旦找到这样的 id，我们就知道脚本已经完成加载，我们可以执行第二个 JavaScript 命令。这里，我们确实需要使用“JSON.stringify”来确保我们返回一个 JSON 格式的字符串，而不是一个原始的 JavaScript 对象给 Python，因为 Python 将不能理解这个返回值并将其转换成一个 None 值的列表(但是，简单类型，如整数和字符串也可以)。

让我们稍微清理一下我们的脚本，并将其与我们的基本框架合并:

```py
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

base_url = 'https://news.google.com/news/?ned=us&hl=en'

inject_readability_cmd = '''
(function(d, script) {
  script = d.createElement('script');
  script.type = 'text/javascript';
  script.async = true;
  script.onload = function() {
      script.id = 'readability-script';
  }
  script.src = 'http://www.webscrapingfordatascience.com/readability/Readability.js';
  d.getElementsByTagName('head')[0].appendChild(script);
}(document));

'''

get_article_cmd = '''
var documentClone = document.cloneNode(true);
var loc = document.location;
var uri = {
  spec: loc.href,
  host: loc.host,
  prePath: loc.protocol + "//" + loc.host,
  scheme: loc.protocol.substr(0, loc.protocol.indexOf(":")),
  pathBase: loc.protocol + "//" + loc.host +
            loc.pathname.substr(0, loc.pathname.lastIndexOf("/") + 1)
};
var article = new Readability(uri, documentClone).parse();
return JSON.stringify(article);
'''
driver = webdriver.Chrome()
driver.implicitly_wait(10)

driver.get(base_url)

news_urls = []
for link in driver.find_elements_by_css_selector('main a[role="heading"]'):
    news_url = link.get_attribute('href')
    news_urls.append(news_url)

for news_url in news_urls:
    print('Now scraping:', news_url)
    driver.get(news_url)

    print('Injecting scripts')
    driver.execute_script(inject_readability_cmd)
    wait = WebDriverWait(driver, 10)
    wait.until(EC.presence_of_element_located((By.ID, "readability-script")))
    returned_result = driver.execute_script(get_article_cmd)

    # Do something with returned_result

driver.quit()

```

注意，我们使用了两个“for”循环:一个是提取我们希望抓取的链接，我们将把它们存储在一个列表中；另一个用来遍历列表。在这种情况下，使用一个循环是行不通的:当我们在循环中导航到其他页面时，Selenium 会在试图用`find_elements_by_css_selector`找到下一个链接时抱怨“陈旧元素”。这基本上是在说:“我正在尝试为您查找下一个元素，但是页面在此期间已经发生了变化，所以我无法确定您想要检索什么。”

如果您尝试执行这个脚本，您会注意到它很快就会失败。这里发生了什么事？要找出问题所在，尝试在浏览器中打开另一个链接，比如 [`https://www.washingtonpost.com/world/chinas-leader-elevated-to-the-level-of-mao-in-communist-pantheon/2017/10/24/ddd911e0-b832-11e7-9b93-b97043e57a22_story.html?utm_term=.720e06a5017d`](https://www.washingtonpost.com/world/chinas-leader-elevated-to-the-level-of-mao-in-communist-pantheon/2017/10/24/ddd911e0-b832-11e7-9b93-b97043e57a22_story.html?utm_term=.720e06a5017d) (一个使用 HTTPS 的网站)，并在浏览器的控制台中手动执行第一个 JavaScript 命令，即通过复制粘贴并执行:

```py
(function(d, script) {
  script = d.createElement('script');
  script.type = 'text/javascript';
  script.async = true;
  script.onload = function() {
      script.id = 'readability-script';
}
script.src = 'http://www.webscrapingfordatascience.com/readability/Readability.js';
  d.getElementsByTagName('head')[0].appendChild(script);
}(document));

```

您可能会得到如下结果:

```py
GET https://www.webscrapingfordatascience.com/readability/Readability.js net::ERR_CONNECTION_CLOSED

```

在其他页面上，您可能会看到:

```py
Mixed Content: The page at [...] was loaded over HTTPS, but requested an insecure script 'http://www.webscrapingfordatascience.com/readability/Readability.js'.
This request has been blocked; the content must be served over HTTPS.

```

这里发生的事情很清楚:如果我们通过 HTTPS 加载一个网站，并试图通过 HTTP 注入一个脚本，Chrome 会阻止这个请求，因为它认为它不安全(这是真的)。其他站点可能应用其他方法来阻止脚本注入，例如使用“内容-安全-策略”头。这将导致如下错误:

```py
Refused to load the script
    'http://www.webscrapingfordatascience.com/readability/Readability.js'  because it violates the following Content Security Policy directive:
    "script-src 'self' 'unsafe-eval' 'unsafe-inline'".

```

Chrome 有一些扩展可以禁用这种检查，但我们在这里将采取不同的方法，这将在大多数页面上工作，除了那些具有最严格内容安全策略的页面:我们将简单地获取 JavaScript 文件的内容，并使用 Selenium 直接执行这些内容，而不是尝试注入一个“

```py
from selenium import webdriver
import requests

base_url = 'https://news.google.com/news/?ned=us&hl=en'
script_url = 'http://www.webscrapingfordatascience.com/readability/Readability.js'

get_article_cmd = requests.get(script_url).text
get_article_cmd += '''

var documentClone = document.cloneNode(true);
var loc = document.location;
var uri = {
  spec: loc.href,
  host: loc.host,
  prePath: loc.protocol + "//" + loc.host,
  scheme: loc.protocol.substr(0, loc.protocol.indexOf(":")),
  pathBase: loc.protocol + "//" + loc.host +
            loc.pathname.substr(0, loc.pathname.lastIndexOf("/") + 1)
};
var article = new Readability(uri, documentClone).parse();
return JSON.stringify(article);
'''

driver = webdriver.Chrome()
driver.implicitly_wait(10)

driver.get(base_url)

news_urls = []
for link in driver.find_elements_by_css_selector('main a[role="heading"]'):
    news_url = link.get_attribute('href')
    news_urls.append(news_url)

for news_url in news_urls:
    print('Now scraping:', news_url)
    driver.get(news_url)

    print('Injecting script')
    returned_result = driver.execute_script(get_article_cmd)

    # Do something with returned_result
driver.quit()

```

这种方法还有一个好处，那就是我们可以一次性执行整个 JavaScript 命令，不再需要依赖显式等待来检查脚本是否已经加载完毕。现在唯一剩下的事情就是将检索到的结果转换为 Python 字典，并将我们的结果存储在数据库中，再次使用“dataset”库:

```py
from selenium import webdriver
import requests
import dataset
from json import loads

db = dataset.connect('sqlite:///news.db')

base_url = 'https://news.google.com/news/?ned=us&hl=en'
script_url = 'http://www.webscrapingfordatascience.com/readability/Readability.js'

get_article_cmd = requests.get(script_url).text
get_article_cmd += '''

var documentClone = document.cloneNode(true);
var loc = document.location;
var uri = {
  spec: loc.href,
  host: loc.host,
  prePath: loc.protocol + "//" + loc.host,
  scheme: loc.protocol.substr(0, loc.protocol.indexOf(":")),
  pathBase: loc.protocol + "//" + loc.host +
            loc.pathname.substr(0, loc.pathname.lastIndexOf("/") + 1)
};
var article = new Readability(uri, documentClone).parse();
return JSON.stringify(article);
'''

driver = webdriver.Chrome()
driver.implicitly_wait(10)

driver.get(base_url)

news_urls = []
for link in driver.find_elements_by_css_selector('main a[role="heading"]'):

    news_url = link.get_attribute('href')
    news_urls.append(news_url)

for news_url in news_urls:
    print('Now scraping:', news_url)
    driver.get(news_url)

    print('Injecting script')
    returned_result = driver.execute_script(get_article_cmd)

    # Convert JSON string to Python dictionary
    article = loads(returned_result)
    if not article:
        # Failed to extract article, just continue
        continue

    # Add in the url
    article['url'] = news_url
    # Remove 'uri' as this is a dictionary on its own
    del article['uri']
    # Add to the database
    db['articles'].upsert(article, ['url'])

    print('Title was:', article['title'])

driver.quit()

```

输出如下所示:

![A463931_1_En_9_Fig13_HTML.jpg](A463931_1_En_9_Fig13_HTML.jpg)

图 9-13

Exploring some scraped articles with DB Browser for SQLite

```py
Now scraping: https://www.usnews.com/news/world/articles/2017-10-24/
              china-southeast-asia-aim-to-build-trust-with-sea-drills-singapore-says Injecting script
Title was: China, Southeast Asia Aim to Build Trust With Sea Drills,      Singapore Says | World News
Now scraping: http://www.philstar.com/headlines/2017/10/24/
              1751999/pentagon-chief-seeks-continued-maritime-cooperation-asean Injecting script
Title was: Pentagon chief seeks continued maritime cooperation with ASEAN | Headlines News, The Philippine Star,
[...]

```

记得用“DB Browser for SQLite”等 SQLite 客户端看一下数据库(“news . DB”)；参见图 [9-13](#Fig13)

我们现在可以使用 Python 来分析我们收集的文章。我们将使用潜在狄利克雷分配(LDA)构建一个主题模型，这将帮助我们按照一些主题对文章进行分类。为此，我们将使用“nltk”、“stop-words”和“gensim”库，可以使用 pip 简单地安装这些库:

```py
pip install -U nltk
pip install -U stop-words
pip install -U gensim

```

首先，我们将遍历所有文章，以便使用一个简单的正则表达式对它们进行标记化(将文本转换为单词元素列表)，删除停用词，并应用词干:

```py
import dataset
from nltk.tokenize import RegexpTokenizer
from nltk.stem.porter import PorterStemmer
from stop_words import get_stop_words

db = dataset.connect('sqlite:///news.db')

articles = []

tokenizer = RegexpTokenizer(r'\w+')
stop_words = get_stop_words('en')
p_stemmer = PorterStemmer()

for article in db['articles'].all():
    text = article['title'].lower().strip()
    text += " " + article['textContent'].lower().strip()
    if not text:
        continue

    # Tokenize
    tokens = tokenizer.tokenize(text)
    # Remove stop words and small words
    clean_tokens = [i for i in tokens if not i in stop_words]
    clean_tokens = [i for i in clean_tokens if len(i) > 2]
    # Stem tokens
    stemmed_tokens = [p_stemmer.stem(i) for i in clean_tokens]
    # Add to list
    articles.append((article['title'], stemmed_tokens))

print(articles[0])

```

我们的第一篇文章现在看起来如下(我们保留标题供以后报告):

```py
('Paul Manafort, former business partner to surrender in Mueller investigation', ['presid', 'trump', 'former', 'campaign', 'chairman', [...]]

```

为了生成 LDA 模型，我们需要计算每个术语在每个文档中出现的频率。为此，我们可以使用 gensim 构建一个文档术语矩阵:

```py
from gensim import corpora

dictionary = corpora.Dictionary([a[1] for a in articles])
corpus = [dictionary.doc2bow(a[1]) for a in articles]

print(corpus[0])

```

`Dictionary`类遍历文本，并为每个惟一的标记分配一个惟一的整数标识符，同时还收集字数和相关的统计数据。接下来，我们的字典被转换成一个单词包语料库，该语料库产生一个向量列表，其数量等于文档的数量。每个文档向量是一系列“(id，count)”元组:

```py
[(0, 10), (1, 17), (2, 7), (3, 11), [...]]

```

我们现在准备构建一个 LDA 模型:

```py
from gensim.models.ldamodel import LdaModel

nr_topics = 30
ldamodel = LdaModel(corpus, num_topics=nr_topics,
                    id2word=dictionary, passes=20)

print(ldamodel.print_topics())

```

这将显示如下内容:

```py
[(0, '0.027*"s" + 0.018*"trump" + 0.018*"manafort" + 0.011*"investig"    + 0.008*"presid" + 0.008*"report" + 0.007*"mueller" + 0.007*"year"  + 0.007*"campaign" + 0.006*"said"'),
 (1, '0.014*"s" + 0.014*"said" + 0.013*"percent" + 0.008*"1" +           0.007*"0" + 0.006*"year" + 0.006*"month" + 0.005*"increas" +        0.005*"3" + 0.005*"spend"'),
 [...]
]

```

该概述显示了每个主题的条目。每个主题由一个可能出现在该主题中的单词列表表示，按照出现的概率排序。请注意，调整模型的“过程”数和数量对于获得好的结果非常重要。一旦结果看起来是可接受的(我们已经增加了刮集的主题数量)，我们可以使用我们的模型为我们的文档分配主题:

```py
from random import shuffle

# Show topics by top-3 terms
for t in range(nr_topics):
    print(ldamodel.print_topic(t, topn=3))

# Show some random articles
idx = list(range(len(articles)))
shuffle(idx)
for a in idx[:3]:
    article = articles[a]
    print('==========================')
    print(article[0])
    prediction = ldamodel[corpus[a]][0]
    print(ldamodel.print_topic(prediction[0], topn=3))
    print('Probability:', prediction[1])

```

这将显示如下内容:

```py
0.014*"new" + 0.013*"power" + 0.013*"storm"
0.030*"rapp" + 0.020*"spacey" + 0.016*"said"
0.024*"catalan" + 0.020*"independ" + 0.019*"govern"
0.025*"manafort" + 0.020*"trump" + 0.015*"investig"
0.007*"quickli" + 0.007*"complex" + 0.007*"deal"
0.018*"earbud" + 0.016*"iconx" + 0.014*"samsung"
0.012*"halloween" + 0.007*"new" + 0.007*"star"
0.021*"octopus" + 0.014*"carver" + 0.013*"vega"
0.000*"rapp" + 0.000*"spacey" + 0.000*"said"
0.025*"said" + 0.017*"appel" + 0.012*"storm"
0.039*"akzo" + 0.018*"axalta" + 0.017*"billion"
0.024*"rapp" + 0.024*"spacey" + 0.017*"said"
0.000*"boehner" + 0.000*"one" + 0.000*"trump"
0.033*"boehner" + 0.010*"say" + 0.009*"hous"
0.000*"approv" + 0.000*"boehner" + 0.000*"quarter"
0.017*"tax" + 0.013*"republican" + 0.011*"week"
0.012*"trump" + 0.008*"plan" + 0.007*"will"
0.005*"ludwig" + 0.005*"underlin" + 0.005*"sensibl"
0.015*"tax" + 0.011*"trump" + 0.011*"look"
0.043*"minist" + 0.032*"prime" + 0.030*"alleg"
0.058*"harri" + 0.040*"polic" + 0.032*"old"
0.040*"musk" + 0.026*"tunnel" + 0.017*"compani"
0.055*"appl" + 0.038*"video" + 0.027*"peterson"
0.011*"serv" + 0.008*"almost" + 0.007*"insid"
0.041*"percent" + 0.011*"year" + 0.010*"trump"
0.036*"univers" + 0.025*"econom" + 0.012*"special"
0.022*"chees" + 0.021*"patti" + 0.019*"lettuc"
0.000*"boehner" + 0.000*"said" + 0.000*"year"
0.000*"boehner" + 0.000*"new" + 0.000*"say"
0.030*"approv" + 0.025*"quarter" + 0.021*"rate"
==========================
Paul Manafort, Who Once Ran Trump Campaign, Indicted on Money Laundering and Tax Charges
0.025*"manafort" + 0.020*"trump" + 0.015*"investig"
Probability: 0.672658189483
==========================
Apple fires employee after daughter's iPhone X video goes viral
0.055*"appl" + 0.038*"video" + 0.027*"peterson"
Probability: 0.990880503145
==========================
Theresa May won't say when she knew about sexual harassment allegations
0.043*"minist" + 0.032*"prime" + 0.030*"alleg"
Probability: 0.774530402797

```

Scraping Topics

这方面还有很大的改进空间，例如，探索其他主题模型映射算法，应用更好的标记化，添加自定义停用词，或者扩展文章集或调整参数。或者，您也可以考虑直接从 Google 新闻页面抓取每篇文章的标签，该页面也将这些标签作为“主题”包含在其页面上。

## 9.13 抓取和分析维基百科图表

在这个例子中，我们将再次使用维基百科(我们已经在关于网络爬行的章节中使用了维基百科)。我们这里的目标是抓取维基百科页面的标题，同时跟踪它们之间的链接，我们将使用这些链接来构建一个图表，并使用 Python 对其进行分析。我们将再次使用“数据集”库作为存储结果的简单方法。以下代码包含完整的爬网设置:

```py
import requests
import dataset
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urldefrag
from joblib import Parallel, delayed

db = dataset.connect('sqlite:///wikipedia.db')
base_url = 'https://en.wikipedia.org/wiki/'

def store_page(url, title):
    print('Visited page:', url)
    print(' title:', title)
    db['pages'].upsert({'url': url, 'title': title}, ['url'])

def store_links(from_url, links):
    db.begin()
    for to_url in links:
        db['links'].upsert({'from_url': from_url, 'to_url': to_url},
                           ['from_url', 'to_url'])
    db.commit()

def get_random_unvisited_pages(amount=10):
    result = db.query('''SELECT * FROM links
        WHERE to_url NOT IN (SELECT url FROM pages)
        ORDER BY RANDOM() LIMIT {}'''.format(amount))
    return [r['to_url'] for r in result]

def should_visit(base_url, url):
    if url is None:

        return None
    full_url = urljoin(base_url, url)
    full_url = urldefrag(full_url)[0]
    if not full_url.startswith(base_url):
        # This is an external URL
        return None
    ignore = ['Wikipedia:', 'Template:', 'File:', 'Talk:', 'Special:',
              'Template talk:', 'Portal:', 'Help:', 'Category:', 'index.php']
    if any([i in full_url for i in ignore]):
        # This is a page to be ignored
        return None
    return full_url

def get_title_and_links(base_url, url):
    html = requests.get(url).text
    html_soup = BeautifulSoup(html, 'html.parser')
    page_title = html_soup.find(id='firstHeading')
    page_title = page_title.text if page_title else ''
    links = []
    for link in html_soup.find_all("a"):
        link_url = should_visit(base_url, link.get('href'))
        if link_url:
            links.append(link_url)
    return url, page_title, links

if __name__ == '__main__':
    urls_to_visit = [base_url]
    while urls_to_visit:
        scraped_results = Parallel(n_jobs=5, backend="threading")(
            delayed(get_title_and_links)(base_url, url) for url in urls_to_visit

        )
        for url, page_title, links in scraped_results:
            store_page(url, page_title)
            store_links(url, links)
        urls_to_visit = get_random_unvisited_pages()

```

这里有很多事情需要额外的解释:

*   数据库的结构如下:一个表“pages”包含一个访问过的 URL 列表和它们的页面标题。方法`store_page`用于存储该表中的条目。另一个表格“links”只包含成对的 URL 来表示页面之间的链接。方法`store_link`用于更新这些，两种方法都使用“数据集”库。对于后者，我们在一个显式数据库事务中执行多个 upsert 操作，以加快速度。
*   方法`get_random_unvisited_` `pages`现在返回一个未访问的 URL 列表，而不仅仅是一个，通过选择一个链接到的 URL 的随机列表，这些 URL 还没有出现在“pages”表中(因此还没有被访问过)。
*   `should_visit`方法用于确定是否应该考虑对某个链接进行爬行。如果它应该被包含，它返回一个正确格式化的 URL，否则不返回。
*   `get_title_and_links`方法执行页面的实际抓取，获取它们的标题和 URL 列表。
*   脚本本身循环，直到不再有未访问的页面(基本上是永远，因为新页面会不断被发现)。它取出我们还没有访问过的随机页面列表，获取它们的标题和链接，并将它们存储在数据库中。
*   注意，我们在这里使用“joblib”库来建立一个并行方法。简单地一个接一个地访问 URL 会有点慢，所以我们使用 joblib 建立一个多线程的方法来同时访问链接，有效地产生多个网络请求。重要的是不要敲打我们自己的连接或维基百科，所以我们将`n_jobs`参数限制在五个。这里使用的参数`back-end`表示我们想要使用多线程而不是多个进程来建立一个并行计算。在 Python 中，这两种方法各有利弊。多进程方法的设置会带来更多的开销，但它可以更快，因为 Python 的内部线程系统由于“全局解释器锁”(GIL)而有点乏味(关于 GIL 的全面讨论不在此处讨论范围之内，但如果您是第一次听说它，请随意在网上查找更多信息)。在我们的例子中，工作本身相对简单:执行一个网络请求并执行一些解析，因此多线程方法就可以了。
*   这也是为什么我们不将结果存储在数据库中的原因，而是等到并行作业完成执行并返回结果。SQLite 不喜欢同时从多个线程或多个进程写入数据，所以我们要等到收集到结果后再将它们写入数据库。另一种方法是使用客户机-服务器数据库系统。注意，我们应该避免用大量的结果使数据库过载。不仅需要将中间结果存储在内存中，而且在写入大量结果时，我们还需要等待一段时间。因为`get_random_unvisited_pages`方法返回一个最多十个 URL 的列表，所以在我们的例子中我们不需要太担心这个。
*   最后，注意脚本的主入口点现在放在“if __name__ == '__main__ ':”下面。在其他示例中，为了简单起见，我们没有这样做，尽管这样做是一种很好的做法。原因如下:当 Python 脚本导入另一个模块时，包含在该模块中的所有代码都会被立即执行。例如，如果我们想在另一个脚本中重用`should_visit`方法，我们可以使用“import myscript”或“from myscript import should_visit”来导入我们的原始脚本在这两种情况下，都将执行“myscript.py”中的完整代码。如果这个脚本包含一个代码块，就像这个例子中的“while”循环，它将开始执行那个代码块，这不是我们在导入脚本时想要的；我们只想加载函数定义。因此，我们希望指示 Python“仅在直接执行脚本时执行这段代码”，这就是“if _ _ name _ _ = = ' _ _ main _ _ ':”检查所做的。如果我们从命令行启动脚本，特殊的“__name__”变量将被设置为“__main__”。如果我们的脚本是从另一个模块导入的，那么“__name__”将被设置为该模块的名称。当我们在这里使用 joblib 时，我们脚本的内容将被发送给所有“工作线程”(线程或进程)，以便它们执行正确的导入并加载正确的函数定义。例如，在我们的例子中，不同的工人应该知道`get_title_and_links`方法。然而，由于工人也将执行脚本中包含的完整代码(就像导入一样)，我们还需要防止他们运行主代码块，这就是为什么我们需要提供“if _ _ name _ _ = = ' _ _ main _ _ ':”检查。

您可以让 crawler 运行任意长的时间，但是请注意，它不太可能完成，在下一步中，一个更小的图也更容易查看。一旦它运行了一段时间，简单地中断它来停止它。因为我们使用了“upsert”，所以稍后可以随意恢复它(它将基于它停止的地方继续爬行)。

我们现在可以使用抓取的结果执行一些有趣的图形分析。在 Python 中，有两个流行的库可以做到这一点，NetworkX(pip 中的“NetworkX”库)和 I graph(pip 中的“python-igraph”)。我们将在这里使用 NetworkX，以及“matplotlib”来可视化图形。

Graph Visualization Is Hard

正如 NetworkX 文档本身所指出的，正确的图形可视化是很难的，库的作者建议人们使用专用于该任务的工具来可视化他们的图形。对于我们的简单用例，内置方法就足够了，尽管我们必须通过 matplotlib 来使事情更有吸引力。如果您对图形可视化感兴趣，可以看看 Cytoscape、Gephi 和 Graphviz 等程序。在下一个例子中，我们将使用 Gephi 来处理可视化工作负载。

下面的代码将图形可视化。我们首先构造一个新的 NetworkX graph 对象，并添加页面作为被访问的节点。接下来，我们添加边缘，虽然只是在访问过的页面之间。作为一个额外的步骤，我们还删除了完全不连接的节点(即使这些节点在这个阶段不应该出现)。然后我们计算一个中心性度量，称为介数，作为节点重要性的度量。此度量是根据从所有节点到所有其他节点的最短路径数计算的，这些节点经过我们正在计算度量的节点。一个节点位于两个其他节点之间的最短路径上的次数越多，根据该度量，它就越重要。我们将根据这个标准给节点涂上不同的蓝色。我们对介数度量应用了一个快速的 sigmoid 函数来“挤压”一个范围内的值，这将导致更有吸引力的可视化。我们还在这里手动为节点添加标签，以便让它们出现在实际节点的上方。这将提供如图 [9-14](#Fig14) 所示的结果。

![A463931_1_En_9_Fig14_HTML.jpg](A463931_1_En_9_Fig14_HTML.jpg)

图 9-14

Visualizing our scraped graph Ignore the Warnings

当运行可视化代码时，您很可能会看到来自 matplotlib 的警告，抱怨 NetworkX 正在使用不推荐使用的函数。这很好，可以安全地忽略，尽管 matplotlib 的未来版本可能不再适合 NetworkX。目前还不清楚 NetworkX 的作者未来是否会继续专注于可视化。正如您将注意到的，可视化中的边的“箭头”看起来也不太漂亮。这是 NetworkX 的一个长期问题。再说一遍:NetworkX 对于分析和图形争论来说很好，尽管对于可视化来说不太好。如果可视化是您的核心关注点，可以看看其他的库。

```py
import
networkx

import matplotlib.pyplot as plt
import dataset

db = dataset.connect('sqlite:///wikipedia.db')
G = networkx.DiGraph()

print('Building graph...')
for page in db['pages'].all():
    G.add_node(page['url'], title=page['title'])

for link in db['links'].all():
    # Only addedge if the endpoints have both been visited
    if G.has_node(link['from_url']) and G.has_node(link['to_url']):
        G.add_edge(link['from_url'], link['to_url'])

# Unclutter by removing unconnected nodes
G.remove_nodes_from(networkx.isolates(G))

# Calculate node betweenness centrality as a measure of importance

print('Calculating betweenness...')
betweenness = networkx.betweenness_centrality(G, endpoints=False)

print('Drawing graph...')

# Sigmoid function to make the colors (a little) more appealing
squash = lambda x : 1 / (1 + 0.5**(20*(x-0.1)))

colors = [(0, 0, squash(betweenness[n])) for n in G.nodes()]
labels = dict((n, d['title']) for n, d in G.nodes(data=True))
positions = networkx.spring_layout(G)

networkx.draw(G, positions, node_color=colors, edge_color='#AEAEAE')

# Draw the labels manually to make them appear above the nodes
for k, v in positions.items():

    plt.text(v[0], v[1]+0.025, s=labels[k],
             horizontalalignment='center', size=8)

plt.show()

```

## 9.14 抓取和可视化董事会成员图形

在这个例子中，我们的目标是构建一个标准普尔 500 公司的社交图，以及它们通过董事会成员的相互联系。我们将从位于 [`https://www.reuters.com/finance/markets/index/.SPX`](https://www.reuters.com/finance/markets/index/.SPX) 的路透社标准普尔 500 页面开始获取股票代码列表:

```py
from bs4 import BeautifulSoup
import requests
import re

session = requests.Session()

sp500 = 'https://www.reuters.com/finance/markets/index/.SPX'

page = 1
regex = re.compile(r'\/finance\/stocks\/overview\/.*')
symbols = []

while True:
    print('Scraping page:', page)
    params = params={'sortBy': '', 'sortDir' :'', 'pn': page}
    html = session.get(sp500, params=params).text
    soup = BeautifulSoup(html, "html.parser")
    pagenav = soup.find(class_='pageNavigation')
    if not pagenav:
        break

    companies = pagenav.find_next('table', class_="dataTable")
    for link in companies.find_all('a', href=regex):
        symbols.append(link.get('href').split('/')[-1])
    page += 1

print(symbols)

```

一旦我们获得了一个符号列表，我们就可以抓取每个符号的董事会成员页面(例如， [`https://www.reuters.com/finance/stocks/company-officers/MMM.N`](https://www.reuters.com/finance/stocks/company-officers/MMM.N) )，获取董事会成员表，并将其存储为 pandas 数据框，我们将使用 pandas 的`to_pickle`方法保存该数据框。如果您还没有安装 pandas，请不要忘记先安装它:

```py
pip install -U pandas

```

将此添加到脚本的底部:

```py
import pandas as pd

officers = 'https://www.reuters.com/finance/stocks/company-officers/{symbol}'

dfs = []

for symbol in symbols:
    print('Scraping symbol:', symbol)
    html = session.get(officers.format(symbol=symbol)).text
    soup = BeautifulSoup(html, "html.parser")
    officer_table = soup.find('table', {"class" : "dataTable"})
    df = pd.read_html(str(officer_table), header=0)[0]
    df.insert(0, 'symbol', symbol)
    dfs.append(df)

# Store the results
df = pd.concat(dfs)
df.to_pickle('sp500.pkl')

```

这种信息可以产生很多有趣的用例，尤其是在图形和社交网络分析领域。我们将再次使用 NetworkX，但只是解析我们收集的信息，并以 Gephi(一种流行的图形可视化工具)可以阅读的格式导出图形，该工具可以从 [`https://gephi.org/users/download/`](https://gephi.org/users/download/) 下载:

```py
import pandas as pd
import networkx as nx
from networkx.readwrite.gexf import write_gexf

df = pd.read_pickle('sp500.pkl')

G = nx.Graph()

for row in df.itertuples():
    G.add_node(row.symbol, type='company')
    G.add_node(row.Name,type='officer')
    G.add_edge(row.symbol, row.Name)

write_gexf(G, 'graph.gexf')

```

在 Gephi 中打开图形文件，应用“ForceAtlas 2”布局技术进行几次迭代。我们也可以显示标签，得到如图 [9-15](#Fig15) 所示的图形。

如果您愿意，可以花些时间探索 Gephi 的可视化和过滤选项。您在 NetworkX 中设置的所有属性(在我们的例子中为“type”)也将在 Gephi 中可用。图 9-16 显示了谷歌、亚马逊和苹果及其董事会成员的过滤图，他们是其他公司的连接器。

![A463931_1_En_9_Fig16_HTML.jpg](A463931_1_En_9_Fig16_HTML.jpg)

图 9-16

Showing connected board members for Google, Amazon, and Apple

![A463931_1_En_9_Fig15_HTML.jpg](A463931_1_En_9_Fig15_HTML.jpg)

图 9-15

Visualizing our scraped graph using Gephi

## 9.15 使用深度学习破解验证码

最后一个例子无疑是最具挑战性的一个，也是最与“数据科学”相关的一个，而不是 web 抓取。事实上，我们不会在这里使用任何网络抓取工具。相反，我们将通过一个相对包含的示例来说明如何在您的 web 抓取管道中加入预测模型，以绕过 CAPTCHA 检查。

我们需要先安装一些工具。我们将使用“OpenCV”，一个用于计算机视觉的非常彻底的库，以及“numpy”用于一些基本的数据争论。最后，我们将使用“captcha”库来生成示例图像。所有这些都可以按如下方式安装:

```py
pip install -U opencv-python
pip install -U numpy
pip install -U captcha

```

接下来，在系统中的某个位置创建一个目录，以包含我们将创建的 Python 脚本。第一个脚本(“constants.py”)将包含一些我们将要使用的常量:

```py
CAPTCHA_FOLDER = 'generated_images'
LETTERS_FOLDER = 'letters'

CHARACTERS = list('QWERTPASDFGHKLZXBNM')
NR_CAPTCHAS = 1000
NR_CHARACTERS = 4

MODEL_FILE = 'model.hdf5'
LABELS_FILE = 'labels.dat'

MODEL_SHAPE = (100, 100)

```

另一个脚本(“generate.py”)将生成一组验证码图像，并将它们保存到“generated_images”目录:

```py
from random import choice
from captcha.image import ImageCaptcha
import os.path
from os import makedirs
from constants import *

makedirs(CAPTCHA_FOLDER)

image = ImageCaptcha()

for i in range(NR_CAPTCHAS):
    captcha = ''.join([choice(CHARACTERS) for c in range(NR_CHARACTERS)])
    filename = os.path.join(CAPTCHA_FOLDER, '{}_{}.png'.format(captcha, i))
    image.write(captcha, filename)
    print('Generated:', captcha)

```

运行这个脚本后，您应该会得到一组 CAPTCHA 图像(文件名中有它们的答案),如图 [9-17](#Fig17) 所示。

![A463931_1_En_9_Fig17_HTML.jpg](A463931_1_En_9_Fig17_HTML.jpg)

图 9-17

A collection of generated CAPTCHA images Isn’t This Cheating?

当然，我们很幸运，我们自己生成验证码，因此也有机会保存答案。然而，在现实世界中，验证码不会暴露他们的答案(这可能会反驳验证码的观点)，因此我们需要找到另一种方法来创建我们的训练集。一种方法是寻找一个特定网站用来生成验证码的库，并用它来收集一组你自己的训练图像，尽可能地复制原始图像。另一种方法是自己手动标记图像，这听起来很可怕，尽管您可能不需要标记成千上万的图像来获得想要的结果。由于人们在填写验证码时也会犯错误，我们有不止一次的机会得到正确的答案，因此不需要达到 100%的准确率。即使我们的预测模型只能得到十分之一的正确图像，这也足以在重试几次后突破验证码。

接下来，我们将编写另一个脚本，将我们的图像分割成单独的部分，每个字符一个。我们可以尝试构建一个模型，一次性预测完整的答案，尽管在许多情况下，逐个字符地进行预测要容易得多。为了分割我们的图像，我们需要调用 OpenCV 来执行一些繁重的工作。关于 OpenCV 和计算机视觉的完整讨论本身就需要一本书，所以我们在这里将坚持一些基础知识。我们将在这里使用的主要概念是阈值，开放和轮廓检测。为了了解这是如何工作的，让我们首先创建一个小的测试脚本来实际展示这些概念:

```py
import cv2
import numpy as np

# Change this to one of your generated images:
image_file = 'generated_images/ABQM_116.png'

image = cv2.imread(image_file)
cv2.imshow('Original image', image)

# Convert to grayscale, followed by thresholding to black and white
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
_, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)
cv2.imshow('Black and white', thresh)

# Apply opening: "erosion" followed by "dilation"
denoised = thresh.copy()
kernel = np.ones((4, 3), np.uint8)
denoised = cv2.erode(denoised, kernel, iterations=1)
kernel = np.ones((6, 3), np.uint8)
denoised = cv2.dilate(denoised, kernel, iterations=1)
cv2.imshow('Denoised', denoised)

# Now find contours and overlay them over our original image
_, cnts, _ = cv2.findContours(denoised.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)

cv2.drawContours(image, cnts, contourIdx=-1, color=(255, 0, 0), thickness=-1)
cv2.imshow('Contours', image)

cv2.waitKey(0)

```

如果您运行这个脚本，您应该会获得一个类似于图 [9-18](#Fig18) 所示的预览窗口列表。在前两步中，我们用 OpenCV 打开我们的图像，并将其转换为简单的纯黑白表示。接下来，我们应用一个“开放的”形态变换，它归结为一个腐蚀，然后是膨胀。侵蚀的基本思想就像土壤侵蚀一样:这种变换通过在图像(可以说是“窗口”)上滑动“内核”来“侵蚀掉”前景对象(假设为白色)的边界，这样，如果周围内核中的所有像素都是白色的，则只有那些白色像素会被保留。否则，它会变成黑色。膨胀的作用正好相反:如果周围的内核中至少有一个像素是白色的，它会通过将像素设置为白色来加宽图像。应用这些步骤是从图像中去除噪声的一种非常常见的策略。上面的脚本中使用的内核大小只是一些试验和错误的结果，你可能想用其他类型的验证码图片来调整这些。请注意，我们允许图像中存在一些噪声。我们不需要获得完美的图像，因为我们相信我们的预测模型将能够“检查这些图像”

![A463931_1_En_9_Fig18_HTML.jpg](A463931_1_En_9_Fig18_HTML.jpg)

图 9-18

Processing an image with OpenCV . From left to right: original image, image after conversion to black and white, image after applying an opening to remove noise, and the extracted contours overlaid in blue over the original image.

接下来，我们使用 OpenCV 的`findContours`方法提取相连的白色像素的“斑点”。OpenCV 提供了各种方法来执行这种提取，并提供了不同的方式来表示结果(例如，是否简化轮廓、是否构建层次等等)。最后，我们使用`drawContours`方法来绘制发现的斑点。这里的`contourIdx`参数表示我们想要绘制所有的顶级轮廓，`thickness`值-1 指示 OpenCV 填充轮廓。

我们现在仍然需要一种方法来使用轮廓创建单独的图像:每个字符一个。我们的方法是使用遮罩。注意，OpenCV 还允许为每个轮廓提取“边界矩形”,这将使“切割”图像更加容易，尽管这可能会给我们带来麻烦，如果字符的部分彼此靠近的话。相反，我们将使用以下代码片段说明的方法:

```py
import cv2
import numpy as np

image_file = 'generated_images/ABQM_116.png'

# Perform thresholding, erosion and contour finding as shown before
image = cv2.imread(image_file)
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
_, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)
denoised = thresh.copy()
kernel = np.ones((4, 3), np.uint8)
denoised = cv2.erode(denoised, kernel, iterations=1)
kernel = np.ones((6, 3), np.uint8)
denoised = cv2.dilate(denoised, kernel, iterations=1)
_, cnts, _ = cv2.findContours(denoised.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)

# Create a fresh 'mask' image
mask = np.ones((image.shape[0], image.shape[1]), dtype="uint8") * 0
# We'll use the first contour as an example
contour = cnts[0]
# Draw this contour over the mask
cv2.drawContours(mask, [contour], -1, (255, 255, 255), -1)

cv2.imshow('Denoised image', denoised)

cv2.imshow('Mask after drawing contour', mask)

result = cv2.bitwise_and(denoised, mask)

cv2.imshow('Result after and operation', result)

retain = result > 0
result = result[np.ix_(retain.any(1), retain.any(0))]

cv2.imshow('Final result', result)

cv2.waitKey(0)

```

如果您运行这个脚本，您将获得如图 [9-19](#Fig19) 所示的结果。首先，我们创建一个新的黑色图像，其大小与初始的去噪图像相同。我们取一个轮廓，用白色画在这个“面具”的上面接下来，去噪后的图像和蒙版在逐位“与”运算中组合，如果两个输入图像中的对应像素都是白色，则保留白色像素，否则将其设置为黑色。接下来，我们应用一些聪明的数字切片来裁剪图像。

这足以让我们开始，尽管还有一个问题需要解决:重叠。在字符重叠的情况下，它们将被发现为一个大轮廓。为了解决这个问题，我们将应用以下操作。首先，从轮廓列表开始，检查两个不同轮廓之间是否有很大程度的重叠，在这种情况下，我们只保留最大的一个。接下来，我们根据轮廓的大小对它们进行排序，取前 n 个轮廓，并在水平轴上从左到右对它们进行排序(n 是 CAPTCHA 中的字符数)。这仍然可能导致比我们需要的更少的轮廓，所以我们迭代每个轮廓，并检查它的宽度是否比期望值高。期望值的一个很好的启发式方法是基于从最左边的白色像素到最右边的白色像素的距离除以我们期望看到的字符数来得到估计的宽度。如果轮廓比我们预期的要宽，我们把它分成 m 等份，m 等于轮廓的宽度除以预期的宽度。这是一种试探，仍然可能导致一些字符没有被完全截断(一些字符比其他字符大)，但这是我们可以接受的。万一我们在这一切结束时没有得到想要的字符数，我们将简单地跳过给定的图像。

![A463931_1_En_9_Fig19_HTML.jpg](A463931_1_En_9_Fig19_HTML.jpg)

图 9-19

Extracting part of an image using a contour mask in OpenCV. On the top left, the starting image is shown. On the right, a new image is created with the contour drawn in white and filled. These two images are combined in a bitwise “and” operation to obtain the image in the second row. The bottom image shows the final result after applying cropping.

我们将所有这些放在一个单独的函数列表中(在文件“functions.py”中) :

```py
import cv2
import numpy as np
from math import ceil, floor
from constants import *

def overlaps(contour1, contour2, threshold=0.8):
    # Check whether two contours' bounding boxes overlap
    area1 = contour1['w'] * contour1['h']
    area2 = contour2['w'] * contour2['h']
    left = max(contour1['x'], contour2['x'])
    right = min(contour1['x'] + contour1['w'], contour2['x'] + contour2['w'])
    top = max(contour1['y'], contour2['y'])
    bottom = min(contour1['y'] + contour1['h'], contour2['y'] + contour2['h'])
    if left <= right and bottom >= top:
        intArea = (right - left) * (bottom - top)
        intRatio = intArea / min(area1, area2)
        if intRatio >= threshold:
            # Return True if the second contour is larger
            return area2 > area1
    # Don't overlap or doesn't exceed threshold
    return None

def remove_overlaps(cnts):
    contours = []
    for c in cnts:
        x, y, w, h = cv2.boundingRect(c)
        new_contour = {'x': x, 'y': y, 'w': w, 'h': h, 'c': c}
        for other_contour in contours:
            overlap = overlaps(other_contour, new_contour)
            if overlap is not None:
                if overlap:
                    # Keep this one...
                    contours.remove(other_contour)
                    contours.append(new_contour)
                # ... otherwise do nothing: keep the original one
                break

        else:
            # We didn't break, so no overlap found, add the contour

            contours.append(new_contour)
    return contours

def process_image(image):
    # Perform basic pre-processing
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)
    denoised = thresh.copy()
    kernel = np.ones((4, 3), np.uint8)
    denoised = cv2.erode(denoised, kernel, iterations=1)
    kernel = np.ones((6, 3), np.uint8)
    denoised = cv2.dilate(denoised, kernel, iterations=1)
    return denoised

def get_contours(image):
    # Retrieve contours
    _, cnts, _ = cv2.findContours(image.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
    # Remove overlapping contours
    contours = remove_overlaps(cnts)
    # Sort by size, keep only the first NR_CHARACTERS
    contours = sorted(contours, key=lambda x: x['w'] * x['h'],
                      reverse=True)[:NR_CHARACTERS]
    # Sort from left to right
    contours = sorted(contours, key=lambda x: x['x'], reverse=False)
    return contours

def extract_contour(image, contour, desired_width, threshold=1.7):

    mask = np.ones((image.shape[0], image.shape[1]), dtype="uint8") * 0
    cv2.drawContours(mask, [contour], -1, (255, 255, 255), -1)
    result = cv2.bitwise_and(image, mask)
    mask = result > 0
    result = result[np.ix_(mask.any(1), mask.any(0))]

    if result.shape[1] > desired_width * threshold:
        # This contour is wider than expected, split it
        amount = ceil(result.shape[1] / desired_width)
        each_width = floor(result.shape[1] / amount)
        # Note: indexing based on im[y1:y2, x1:x2]
        results = [result[0:(result.shape[0] - 1),
                          (i * each_width):((i + 1) * each_width - 1)] \
                   for i in range(amount)]
        return results
    return [result]

def get_letters(image, contours):
    desired_size = (contours[-1]['x'] + contours[-1]['w'] - contours[0]['x']) \
                    / NR_CHARACTERS
    masks = [m for l in [extract_contour(image, contour['c'], desired_size) \
             for contour in contours] for m in l]
    return masks

```

这样，我们终于准备好编写我们的剪切脚本(“cut.py”)

![A463931_1_En_9_Fig20_HTML.jpg](A463931_1_En_9_Fig20_HTML.jpg)

图 9-20

A collection of extracted “S” images

```py
from os import makedirs
import os.path
from glob import glob
from functions import *
from constants import *

image_files = glob(os.path.join(CAPTCHA_FOLDER, '*.png'))

for image_file in image_files:
    print('Now doing file:', image_file)
    answer = os.path.basename(image_file).split('_')[0]
    image = cv2.imread(image_file)
    processed = process_image(image)
    contours = get_contours(processed)
    if not len(contours):
        print('[!] Could not extract contours')
        continue

    letters = get_letters(processed, contours)
    if len(letters) != NR_CHARACTERS:
        print('[!] Could not extract desired amount of characters')
        continue

    if any([l.shape[0] < 10 or l.shape[1] < 10 for l in letters]):

        print('[!] Some of the extracted characters are too small')
        continue

    for i, mask in enumerate(letters):
        letter = answer[i]
        outfile = '{}_{}.png'.format(answer, i)
        outpath = os.path.join(LETTERS_FOLDER, letter)
        if not os.path.exists(outpath):
            makedirs(outpath)
        print('[i] Saving', letter, 'as', outfile)
        cv2.imwrite(os.path.join(outpath, outfile), mask)

```

如果运行这个脚本,“letters”目录现在应该包含每个字母的目录；例如，参见图 [9-20](#Fig20) 。我们现在准备构建我们的深度学习模型。我们将使用一个简单的卷积神经网络架构，使用“Keras”库。

```py
pip install -U keras

```

为了让 Keras 工作，我们还需要安装一个后端(可以说是 Keras 将要使用的“引擎”)。你可以使用相当有限的“theano”库、谷歌的“Tensorflow”或微软的“CNTK”。我们假设您使用的是 Windows，所以 CNTK 是最简单的选择。(否则，请使用 pip 安装“theano”库。)要安装 CNTK，请导航至 [`https://docs.microsoft.com/en-us/cognitive-toolkit/setup-windows-python?tabs=cntkpy231`](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-windows-python?tabs=cntkpy231) 并查找与您的 Python 版本对应的 URL。如果您的计算机中有兼容的 GPU，您可以使用“GPU”选项。如果这不起作用或者你遇到了麻烦，坚持使用“CPU”选项。然后，安装将按如下方式执行(使用 GPU Python 3.6 版本 URL):

```py
pip install -U https://cntk.ai/PythonWheel/GPU/cntk-2.3.1-cp36-cp36m-win_amd64.whl

```

接下来，我们需要创建一个 Keras 配置文件。运行 Python REPL 并导入 Keras，如下所示:

```py
>>> import keras
Using TensorFlow backend.
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "\site-packages\keras\__init__.py", line 3, in <module>
    from . import utils
  File "\site-packages\keras\utils\__init__.py", line 6, in <module>
    from . import conv_utils
  File "\site-packages\keras\utils\conv_utils.py", line 3, in <module>
    from .. import backend as K
  File "\site-packages\keras\backend\__init__.py", line 83, in <module>
    from .tensorflow_backend import *
  File "\site-packages\keras\backend\tensorflow_backend.py", line 1, in <module>
    import tensorflow as tf
ModuleNotFoundError: No module named 'tensorflow'

```

Keras 会抱怨找不到默认后端 Tensorflow。那也行；简单地退出 REPL。接下来，导航到“%用户配置文件%”。keras”在 Windows 的文件资源管理器。那里应该有一个“keras.json”文件。使用记事本或另一个文本编辑器打开该文件，并替换内容，如下所示:

```py
{
    "floatx": "float32",
    "epsilon": 1e-07,
    "backend": "cntk",
    "image_data_format": "channels_last"
}

```

Using Another Back End

如果您使用 Tensorflow，只需将“后端”值设置为“tensorflow”如果您正在使用 theano，请将该值设置为“theano”请注意，在后一种情况下，您可能还需要在您的系统上查找一个“. then orc . txt”文件，并更改其内容，以便在您的系统上正常工作，尤其是“device”条目，您应该将该条目设置为“cpu ”,以防 ano 无法找到您的 GPU。

一旦您完成了这一更改，请尝试将 Keras 再次测试导入到一个新的 REPL 会话中。您现在应该得到以下内容:

```py
>>> import keras
Using CNTK backend
Selected GPU[1] GeForce GTX 980M as the process wide default device.

```

Keras 现在设置好了，正在识别我们的 GPU。如果 CNTK 会抱怨，请记住尝试 CPU 版本，尽管记住在这种情况下训练模型将需要更长的时间(如果您只能使用基于 CPU 的计算，则 theano 和 Tensorflow 也会如此)。

我们现在可以创建另一个 Python 脚本来训练我们的模型(“train.py”):

```py
import cv2
import pickle
from os import listdir
import os.path
import numpy as np
from glob import glob
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.layers.core import Flatten, Dense
from constants import *

data = []
labels = []
nr_labels = len(listdir(LETTERS_FOLDER))

# Convert each image to a data matrix
for label in listdir(LETTERS_FOLDER):
    for image_file in glob(os.path.join(LETTERS_FOLDER, label, '*.png')):
        image = cv2.imread(image_file)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        # Resize the image so all images have the same input shape
        image = cv2.resize(image, MODEL_SHAPE)
        # Expand dimensions to make Keras happy
        image = np.expand_dims(image, axis=2)
        data.append(image)
        labels.append(label)

# Normalize the data so every value lies between zero and one
data = np.array(data, dtype="float") / 255.0
labels = np.array(labels)

# Create a training-test split
(X_train, X_test, Y_train, Y_test) = train_test_split(data, labels,
                                     test_size=0.25, random_state=0)

# Binarize the labels

lb = LabelBinarizer().fit(Y_train)
Y_train = lb.transform(Y_train)
Y_test = lb.transform(Y_test)

# Save the binarization for later
with open(LABELS_FILE, "wb") as f:
    pickle.dump(lb, f)
# Construct the model architecture
model = Sequential()
model.add(Conv2D(20, (5, 5), padding="same",
          input_shape=(MODEL_SHAPE[0], MODEL_SHAPE[1], 1), activation="relu"))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model.add(Conv2D(50, (5, 5), padding="same", activation="relu"))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model.add(Flatten())
model.add(Dense(500, activation="relu"))
model.add(Dense(nr_labels, activation="softmax"))
model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

# Train and save the model
model.fit(X_train, Y_train, validation_data=(X_test, Y_test),
          batch_size=32, epochs=10, verbose=1)
model.save(MODEL_FILE)

```

我们正在做一些事情。首先，我们遍历我们创建的所有图像，调整它们的大小，存储它们的像素矩阵以及它们的答案。接下来，我们将数据标准化，使每个值都位于 0 和 1 之间，这使得神经网络的工作变得更容易。接下来，由于 Keras 不能直接处理“Q”，“W”，…标签，我们需要将这些二进制化:每个标签转换为一个输出顶点，每个索引对应一个可能的字符，其值设置为 1 或 0，这样“Q”就变成了“[1，0，0，0，…]”，“W”就变成了“[0，1，0，0，…]”，以此类推。我们保存这个转换，因为我们还需要它在模型应用期间再次执行转换回字符。接下来，我们构建神经架构(实际上相对简单)，并开始训练模型。如果您运行这个脚本，您将得到如下输出:

```py
Using CNTK backend
Selected GPU[0] GeForce GTX 980M as the process wide default device.

Train on 1665 samples, validate on 555 samples
Epoch 1/10

C:\Users\Seppe\Anaconda3\lib\site-packages\cntk\core.py:361: UserWarning: your data is of type "float64", but your input variable (uid "Input4")   
    expects "<class'numpy.float32'>". Please convert your data           beforehand to speed up training.
  (sample.dtype, var.uid, str(var.dtype)))

  32/1665 [..............................] - ETA: 36s - loss: 3.0294 - acc: 0.0312
  64/1665 [>.............................] - ETA: 22s - loss: 5.1515 - acc: 0.0312

[...]

1600/1665 [===========================>..] - ETA: 0s - loss: 7.6135e-04 - acc: 1.0000
1632/1665 [============================>.] - ETA: 0s - loss: 8.3265e-04 - acc: 1.0000
1664/1665 [============================>.] - ETA: 0s - loss: 8.2343e-04 - acc: 1.0000
1665/1665 [==============================] - 3s 2ms/step - loss: 8.2306e-04 - acc:
     1.0000 - val_loss: 0.3644 - val_acc: 0.9207

```

我们在验证集上获得了 92%的准确率，一点也不差！现在唯一剩下的事情是展示我们如何使用这个网络来预测验证码(“apply.py”):

```py
from keras.models import load_model
import pickle
import os.path
from glob import glob
from random import choice
from functions import *
from constants import *

with open(LABELS_FILE, "rb") as f:
    lb = pickle.load(f)

model = load_model(MODEL_FILE)

# We simply pick a random training image here to illustrate how predictions work. In a real setup, you'd obviously plug this into your web scraping
# pipeline and pass a "live" captcha image
image_files = list(glob(os.path.join(CAPTCHA_FOLDER, '*.png')))
image_file = choice(image_files)

print('Testing:', image_file)

image = cv2.imread(image_file)
image = process_image(image)
contours = get_contours(image)
letters = get_letters(image, contours)

for letter in letters:
    letter = cv2.resize(letter, MODEL_SHAPE)
    letter = np.expand_dims(letter, axis=2)
    letter = np.expand_dims(letter, axis=0)
    prediction = model.predict(letter)
    predicted = lb.inverse_transform(prediction)[0]
    print(predicted)

```

如果您运行这个脚本，您应该会看到如下所示的内容:

```py
Using CNTK backend
Selected GPU[0] GeForce GTX 980M as the process wide default device.

Testing: generated_images\NHXS_322.png
N
H
X
S

```

如您所见，网络正确预测了验证码中的字符序列。这就结束了我们简短的验证码破解之旅。正如我们之前讨论过的，请记住有几种替代方法，比如训练一个 OCR 工具包或者以低成本使用带有“人类破解者”的服务。还要记住，如果您计划将这个想法应用到其他验证码上，您可能需要对 OpenCV 和 Keras 模型进行微调，并且我们在这里使用的验证码生成器仍然相对“简单”然而，最重要的是，验证码的标志是一个警告，基本上明确表示网页抓取器不受欢迎。在你开始破解验证码的左右两边之前，也要记住这一点。

Even a Traditional Model Might Work

正如我们所看到的，建立深度学习管道并不是那么简单。如果您想知道传统的预测建模技术(如随机森林或支持向量机)是否也可行(例如，这两种技术都可以在 scikit-learn 中找到，并且设置和训练起来要快得多)，答案是肯定的，在某些情况下，这些技术可能可行，尽管精度成本很高。这种传统技术很难理解图像的二维结构，而这正是卷积神经网络旨在解决的问题。也就是说，我们已经使用随机森林和大约 100 个手动标记的验证码图像建立了管道，这些图像获得了大约 10%的低准确率，尽管在几次尝试后足以获得正确的答案。