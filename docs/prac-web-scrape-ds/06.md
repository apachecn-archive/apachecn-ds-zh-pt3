# 6.从网络抓取到网络爬行

到目前为止，本书中的例子非常简单，因为我们只(大部分)读到了一页。然而，在编写 web scrapers 时，有很多情况下你会希望抓取多个页面甚至多个网站。在这种情况下，经常会使用“网络爬虫”这个名称，因为它会“爬行”整个站点甚至整个 web。这一章说明了如何从编写网络爬虫过渡到更复杂的网络爬虫，并且强调了在编写爬虫时需要记住的重要方面。您还将学习如何将结果存储在数据库中，以便以后访问和分析。

## 6.1 什么是网络爬行？

“web 抓取”和“web 爬行”之间的区别相对模糊，因为许多作者和程序员会交替使用这两个术语。一般来说,“爬虫”一词指的是程序自己导航网页的能力，甚至可能没有明确定义的最终目标或目的，无休止地探索一个站点或 web 所能提供的内容。像 Google 这样的搜索引擎大量使用网络爬虫来检索 URL 的内容，检查其他链接的页面，检索这些链接的 URL，等等…

编写网络爬虫时，有些微妙的设计选择会改变项目的范围和性质:

*   在许多情况下，爬行将被限制在一组定义良好的页面上，例如，在线商店的产品页面。这些情况相对容易处理，因为您停留在同一个领域，并且对每个产品页面的外观有所期望，或者对您想要提取的数据类型有所期望。
*   在其他情况下，你会把自己限制在一个网站上(一个域名)，但是没有一个明确的信息提取目标。相反，您只是想创建一个站点的副本。在这种情况下，手工编写一个 scraper 并不是一个明智的方法。有许多工具可以帮助你制作一个网站的离线拷贝，包括许多可配置的选项。要找到这些，寻找“网站镜像工具”
*   最后，在更极端的情况下，你会希望保持你的爬行非常开放。例如，您可能希望从一系列关键字开始，Google 每个关键字，搜索每个查询的前 10 个结果，并搜索这些页面，例如图像、表格、文章等等。显然，这是需要处理的最高级的用例。

编写一个健壮的爬虫需要你进行各种检查，并仔细考虑代码的设计。由于爬虫可能会在 web 的任何部分结束，并且经常会运行很长时间，所以您需要仔细考虑停止条件，跟踪您以前访问过的页面(以及是否已经到了再次访问它们的时间)，如何存储结果，以及如何确保崩溃的脚本可以重新启动而不会丢失其当前进度。以下概述提供了一些通用的最佳实践和思考内容:

*   仔细考虑你实际上想要收集哪些数据:你能通过搜集一组预定义的网站来提取你需要的吗，或者你真的需要发现你还不知道的网站吗？就编写和维护而言，第一种选择总是会使代码更容易。
*   使用数据库:最好使用数据库来跟踪要访问的链接、已访问的链接和收集的数据。确保每件事都有时间戳，这样你就知道什么时候被创建和最后一次更新。
*   将抓取和抓取分开:大多数健壮的爬虫将“抓取”部分(访问网站，提取链接，并将它们放入队列，即收集您希望抓取的页面)与实际的“抓取”部分(从页面中提取信息)分开。在同一个程序或循环中做这两件事很容易出错。在某些情况下，让爬虫存储一个页面的 HTML 内容的完整副本可能是一个好主意，这样当您想要抓取信息时就不需要重新访问它。
*   尽早停止:当抓取页面时，立即加入停止标准总是一个好主意。也就是说，如果你在看到一个链接的时候就已经可以确定这个链接不感兴趣，那么就不要把它放在“要抓取”的队列中。这同样适用于从页面中抓取信息。如果你能很快确定内容不有趣，那么就不要再继续那个页面了。
*   重试或中止:请注意，网络是一个动态的地方，链接可能无法工作或页面可能不可用。仔细考虑你想要重试特定链接的次数。
*   爬行队列:也就是说，处理链接队列的方式也很重要。如果您只是应用简单的 FIFO(先进先出)或 LIFO(后进先出)方法，您可能会以快速连续重试失败的链路而告终，这可能不是您想要做的。因此建造冷却期也很重要。
*   并行编程:为了让你的程序更高效，你需要以这样一种方式来编写它，即你可以让多个实例并行工作。因此也需要数据库支持的数据存储。总是假设你的程序可能在任何时候崩溃，并且一个新的实例应该能够立即接管任务。
*   请记住刮擦的法律方面:在下一章中，我们将仔细研究法律问题。一些网站将不会太高兴刮板访问他们。还要确保不要用大量的 HTTP 请求“敲打”网站。尽管许多网站相对来说比较健壮，但是如果您向同一个网站发出数百个 HTTP 请求，一些网站甚至可能会关闭，无法为普通访问者提供服务。

在接下来的内容中，我们不会创造一个 Google 的竞争对手，但是在下一节中，我们会使用两个独立的例子，给你一些关于爬行的提示。

## 6.2 Python 中的 Web 爬行

作为我们的第一个例子，我们将使用位于 [`http://www.webscrapingfordatascience.com/crawler/`](http://www.webscrapingfordatascience.com/crawler/) 的页面。这个页面试图模仿一个简单的数字站，它基本上会吐出一个随机的数字列表和链接，引导你到一个不同的页面。

Numbers Stations

这个例子的灵感来自 2005 年的一个搞笑实验，可以在 [`http://www.drunkmenworkhere.org/218`](http://www.drunkmenworkhere.org/218) 看到。这个想法是为了调查像 Google 和 Yahoo！通过引导他们进入页面迷宫来浏览页面。实验结果仍然可以在 [`http:`](http://www.drunkmenworkhere.org/219) [`//www.drunkmenworkhere.org/219`](http://www.drunkmenworkhere.org/219) 查看，尽管注意这个实验已经很老了——搜索引擎在过去十年里已经发生了很大的变化。

花一些时间来试验一下这个页面，看看它是如何工作的。

Not Easy to Guess

您会注意到这里的页面使用了不可猜测的“r”URL 参数。如果不是这种情况，也就是说，如果 URL 参数的所有值都在一个明确定义的范围内，或者看起来像连续的数字，那么编写一个 scraper 会容易得多，因为您想要获得的 URL 列表是明确定义的。在浏览一个页面时，请记住这一点，看看抓取它是否可行，并确定您必须采取哪种方法。

开始抓取该站点的第一次尝试如下:

```py
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

base_url = 'http://www.webscrapingfordatascience.com/crawler/'
links_seen = set()

def visit(url, links_seen):
    html = requests.get(url).text
    html_soup = BeautifulSoup(html, 'html.parser')
    links_seen.add(url)
    for link in html_soup.find_all("a"):
        link_url = link.get('href')
        if link_url is None:
            continue

        full_url = urljoin(url, link_url)
        if full_url in links_seen:
            continue

        print('Found a new page:', full_url)
        # Normally, we'd store the results here too
        visit(full_url, links_seen)

visit(base_url, links_seen)

```

注意，我们在这里使用了`urljoin`函数。我们这样做的原因是因为页面上链接的“href”属性引用了一个相对 URL，例如，“？r = f 01 e 7 f 02 e 91239 a 2003 BDD 35770 e 1173”，我们需要将其转换为绝对 1。我们可以通过添加基本 URL 来做到这一点，也就是说，`base_url + link_url`，但是一旦我们开始在站点的 URL 树中跟踪更深层次的链接和页面，这种方法就会失效。使用`urljoin`是获取现有绝对 URL、加入相对 URL 并获得格式良好的新绝对 URL 的正确方式。

What About Absolute href Values?

`urljoin`方法甚至可以处理绝对的“href”链接值。使用:

```py
urljoin('http://example.org', 'https://www.other.com/dir/')

```

会返回“ [`https://www.other.com/dir/` 【T2”)，所以这永远是爬行时可以依赖的好函数。](https://www.other.com/dir/)

如果您运行这个脚本，您会看到它将开始访问不同的 URL。但是，如果让它运行一段时间，这个脚本肯定会崩溃，而且不仅仅是因为网络中断。这样做的原因是因为我们使用了递归:`visit`函数一遍又一遍地调用自己，没有机会回到调用树中，因为每个页面都包含到其他页面的链接:

```py
Traceback (most recent call last):
  File "C:\Users\Seppe\Desktop\firstexample.py", line 23, in <module>
    visit(url, links_seen)
[...]
    return wr in self.data
RecursionError: maximum recursion depth exceeded in comparison

```

因此，依赖递归进行 web 爬行通常不是一个健壮的想法。我们可以不使用递归重写代码，如下所示:

```py
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

links_todo = ['http://www.webscrapingfordatascience.com/crawler/']
links_seen = set()
def visit(url, links_seen):
    html = requests.get(url).text
    html_soup = BeautifulSoup(html, 'html.parser')
    new_links = []
    for link in html_soup.find_all("a"):
        link_url = link.get('href')
        if link_url is None:
            continue

        full_url = urljoin(url, link_url)
        if full_url in links_seen:
            continue

        # Normally, we'd store the results here too
        new_links.append(full_url)
    return new_links

while links_todo:
    url_to_visit = links_todo.pop()
    links_seen.add(url_to_visit)
    print('Now visiting:', url_to_visit)
    new_links = visit(url_to_visit, links_seen)
    print(len(new_links), 'new link(s) found')
    links_todo += new_links

```

您可以让这段代码运行一段时间。这个解决方案更好，但是它仍然有几个缺点。如果我们的程序崩溃(例如，当您的互联网连接或网站关闭时)，您将不得不从头重新启动。此外，我们不知道`links_seen`集可能会变得多大。通常情况下，你的电脑会有足够的内存来轻松存储成千上万的网址，虽然我们可能希望求助于数据库来存储中间进度信息以及结果。

## 6.3 将结果存储在数据库中

让我们通过将进度和结果信息存储在数据库中来修改我们的示例，使其对崩溃更加健壮。我们将使用“records”库来管理一个 SQLite 数据库(一个基于文件但功能强大的数据库系统),我们将在其中存储我们的链接队列和从我们抓取的页面中检索到的数字，可以使用 pip 安装该数据库:

```py
pip install -U records

```

修改后的代码如下所示:

```py
import requests
import records
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from sqlalchemy.exc import IntegrityError

db = records.Database('sqlite:///crawler_database.db')

db.query('''CREATE TABLE IF NOT EXISTS links (
            url text PRIMARY KEY,
            created_at datetime,
            visited_at datetime NULL)''')
db.query('''CREATE TABLE IF NOT EXISTS numbers (url text, number integer,
            PRIMARY KEY (url, number))''')

def store_link(url):
    try:
        db.query('''INSERT INTO links (url, created_at)
                    VALUES (:url, CURRENT_TIMESTAMP)''', url=url)
    except IntegrityError as ie:
        # This link already exists, do nothing
        pass

def store_number(url, number):
    try:
        db.query('''INSERT INTO numbers (url, number)
                    VALUES (:url, :number)''', url=url, number=number)
    except IntegrityError as ie:
        # This number already exists, do nothing
        pass

def mark_visited(url):
    db.query('''UPDATE links SET visited_at=CURRENT_TIMESTAMP
                WHERE url=:url''', url=url)

def get_random_unvisited_link():
    link = db.query('''SELECT * FROM links
                        WHERE visited_at IS NULL
                        ORDER BY RANDOM() LIMIT 1''').first()
    return None if link is None else link.url

def visit(url):

    html = requests.get(url).text
    html_soup = BeautifulSoup(html, 'html.parser')
    new_links = []
    for td in html_soup.find_all("td"):
        store_number(url, int(td.text.strip()))
    for link in html_soup.find_all("a"):
        link_url = link.get('href')
        if link_url is None:
            continue

        full_url = urljoin(url, link_url)
        new_links.append(full_url)
    return new_links

store_link('http://www.webscrapingfordatascience.com/crawler/')
url_to_visit = get_random_unvisited_link()
while url_to_visit is not None:
    print('Now visiting:', url_to_visit)
    new_links = visit(url_to_visit)
    print(len(new_links), 'new link(s) found')
    for link in new_links:
        store_link(link)
    mark_visited(url_to_visit)
    url_to_visit = get_random_unvisited_link()

```

From SQL to ORM

我们在这里手动编写 SQL(结构化查询语言)语句来与数据库交互，这对于较小的项目来说很好。然而，对于更复杂的项目，值得研究 Python 中可用的一些 ORM(对象关系映射)库，如 SQLAlchemy 或 Pee-wee，它们允许关系数据库和 Python 对象之间更平滑、更可控的“映射”,因此您可以直接使用后者，而不必编写 SQL 语句。在示例一章中，我们将使用另一个名为“dataset”的库，它也提供了一种便捷的方式来快速将信息转储到数据库中，而无需编写 SQL。还要注意，在 Python 中使用 SQLite 数据库并不一定要使用“records”库。Python 已经内置了一个“sqlite3”模块(正在被 records 使用)，你也可以使用这个模块(见[https://docs.python.org/3/library/sqlite3.html](https://docs.python.org/3/library/sqlite3.html))。我们在这里使用记录的原因是因为它涉及的样板代码少一些。

尝试运行此脚本一段时间，然后停止它(通过关闭 Python 窗口或按 Control+C)。您可以使用 SQLite 客户端查看数据库(“crawler_database.db”)，如“DB Browser for SQLite”，可从 [http://sqli](http://sqlitebrowser.org/) 、tebrowser.org/获得。图 [6-1](#Fig1) 显示了该工具的运行情况。请记住，您也可以使用记录库来获取 Python 脚本中存储的结果。

![A463931_1_En_6_Fig1_HTML.jpg](A463931_1_En_6_Fig1_HTML.jpg)

图 6-1

Taking a look at the crawled results using DB Browser for SQLite SQLite

请注意，我们在这里使用的 SQLite 数据库对于较小的项目来说很好，但是一旦您开始并行化您的程序，可能会带来麻烦。启动上述脚本的多个实例在一定程度上很可能会正常工作，但可能会由于 SQLite 数据库文件被锁定(即被另一个脚本的进程使用了太长时间)而不时崩溃。类似地，在脚本中使用多线程设置时，SQLite 也可能会令人望而生畏。在这种情况下，切换到面向服务器-客户端的数据库，如 MySQL 或 Postgresql，可能是一个不错的选择。

现在让我们尝试使用相同的框架来为维基百科构建一个爬虫。我们的计划是存储页面标题，并从主页开始跟踪每个页面上的“(from，to)”链接。请注意，我们的数据库模式在这里看起来有点不同:

```py
import requests
import records
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urldefrag
from sqlalchemy.exc import IntegrityError

db = records.Database('sqlite:///wikipedia.db')

db.query('''CREATE TABLE IF NOT EXISTS pages (
            url text PRIMARY KEY,
            page_title text NULL,
            created_at datetime,
            visited_at datetime NULL)''')
db.query('''CREATE TABLE IF NOT EXISTS links (
            url text, url_to text,
            PRIMARY KEY (url, url_to))''')

base_url = 'https://en.wikipedia.org/wiki/'

def store_page(url):
    try:
        db.query('''INSERT INTO pages (url, created_at)
                    VALUES (:url, CURRENT_TIMESTAMP)''', url=url)
    except IntegrityError as ie:
        # This page already exists
        pass

def store_link(url, url_to):
    try:
        db.query('''INSERT INTO links (url, url_to)
                    VALUES (:url, :url_to)''', url=url, url_to=url_to)
    except IntegrityError as ie:
        # This link already exists
        pass

def set_visited(url):
    db.query('''UPDATE pages SET visited_at=CURRENT_TIMESTAMP
                WHERE url=:url''', url=url)

def set_title(url, page_title):
    db.query('UPDATE pages SET page_title=:page_title WHERE url=:url',
              url=url, page_title=page_title)

def get_random_unvisited_page():
    link = db.query('''SELECT * FROM pages
                       WHERE visited_at IS NULL
                       ORDER BY RANDOM() LIMIT 1''').first()
    return None if link is None else link.url

def visit(url):

    print('Now visiting:', url)
    html = requests.get(url).text
    html_soup = BeautifulSoup(html, 'html.parser')
    page_title = html_soup.find(id='firstHeading')
    page_title = page_title.text if page_title else ''
    print(' page title:', page_title)
    set_title(url, page_title)
    for link in html_soup.find_all("a"):
        link_url = link.get('href')
        if link_url is None:
            # No href, skip
            continue

        full_url = urljoin(base_url, link_url)
        # Remove the fragment identifier part
        full_url = urldefrag(full_url)[0]
        if not full_url.startswith(base_url):
            # This is an external link, skip
            continue

        store_link(url, full_url)
        store_page(full_url)
    set_visited(url)

store_page(base_url)
url_to_visit = get_random_unvisited_page()
while url_to_visit is not None:
    visit(url_to_visit)
    url_to_visit = get_random_unvisited_page()

```

请注意，我们加入了一些额外的措施来防止访问我们不想包含的链接:我们确保不跟随基本 URL 之外的外部链接。我们还使用`urldefrag`函数删除链接 URL 中的片段标识符(即“#”之后的部分)，因为我们不想再次访问同一页面，即使它附加了片段标识符，因为这相当于访问和解析没有片段标识符的页面。换句话说，我们不想在队列中同时包含“page.html#part1”和“page . html # part 2”；简单地包含“page.html”就足够了。

如果让这个脚本运行一段时间，您可以用收集到的数据做一些有趣的事情。例如，您可以尝试使用 Python 中的“NetworkX”库，使用每个页面上的“(from，to)”链接创建一个图表。在最后一章，我们将进一步探索这类用例。

注意，我们在这里假设我们预先知道我们想要从页面中获取哪些信息(在本例中是页面标题和链接)。如果您事先不知道您想从已爬网的页面中获得什么，您可能想进一步拆分页面的爬网和解析，例如，通过存储可以由第二个脚本解析的 HTML 内容的完整副本。这是最通用的设置，但也带来了额外的复杂性，如以下代码所示:

```py
import requests
import records
import re
import os, os.path
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urldefrag
from sqlalchemy.exc import IntegrityError

db = records.Database('sqlite:///wikipedia.db')

# This table keeps track of crawled and to-crawl pages
db.query('''CREATE TABLE IF NOT EXISTS pages (
            url text PRIMARY KEY,
            created_at datetime,
            html_file text NULL,
            visited_at datetime NULL)''')

# This table keeps track of a-tags
db.query('''CREATE TABLE IF NOT EXISTS links (
            url text, link_url text,
            PRIMARY KEY (url, link_url))''')

# This table keeps track of img-tags
db.query('''CREATE TABLE IF NOT EXISTS images (
            url text, img_url text, img_file text,
            PRIMARY KEY (url, img_url))''')

base_url = 'https://en.wikipedia.org/wiki/'
file_store = './downloads/'

if not os.path.exists(file_store):
    os.makedirs(file_store)

def url_to_file_name(url):
    url = str(url).strip().replace(' ', '_')
    return re.sub(r'(?u)[^-\w.]', '', url)

def download(url, filename):

    r = requests.get(url, stream=True)
    with open(os.path.join(file_store, filename), 'wb') as the_image:
        for byte_chunk in r.iter_content(chunk_size=4096*4):
            the_image.write(byte_chunk)

def store_page(url):
    try:
        db.query('''INSERT INTO pages (url, created_at)
                    VALUES (:url, CURRENT_TIMESTAMP)''',
                 url=url)
    except IntegrityError as ie:
        pass

def store_link(url, link_url):
    try:
        db.query('''INSERT INTO links (url, link_url)
                    VALUES (:url, :link_url)''',
                 url=url, link_url=link_url)
    except IntegrityError as ie:
        pass

def store_image(url, img_url, img_file):
    try:
        db.query('''INSERT INTO images (url, img_url, img_file)
                    VALUES (:url, :img_url, :img_file)''',
                 url=url, img_url=img_url, img_file=img_file)
    except IntegrityError as ie:

pass

def set_visited(url, html_file):
    db.query('''UPDATE pages
                SET visited_at=CURRENT_TIMESTAMP,
                    html_file=:html_file
                WHERE url=:url''',
             url=url, html_file=html_file)

def get_random_unvisited_page():
    link = db.query('''SELECT * FROM pages
                       WHERE visited_at IS NULL
                       ORDER BY RANDOM() LIMIT 1''').first()
    return None if link is None else link.url

def should_visit(link_url):
    link_url = urldefrag(link_url)[0]
    if not link_url.startswith(base_url):
        return None
    return link_url

def visit(url):
    print('Now visiting:', url)
    html = requests.get(url).text
    html_soup = BeautifulSoup(html, 'html.parser')
    # Store a-tag links
    for link in html_soup.find_all("a"):
        link_url = link.get('href')
        if link_url is None:
            continue

        link_url = urljoin(base_url, link_url)
        store_link(url, link_url)
        full_url = should_visit(link_url)
        if full_url:
            # Queue for crawling
            store_page(full_url)
    # Store img-src files
    for img in html_soup.find_all("img"):
        img_url = img.get('src')
        if img_url is None:

continue

        img_url = urljoin(base_url, img_url)
        filename = url_to_file_name(img_url)
        download(img_url, filename)
        store_image(url, img_url, filename)
    # Store the HTML contents
    filename = url_to_file_name(url) + '.html'
    fullname = os.path.join(file_store, filename)
    with open(fullname, 'w', encoding='utf-8') as the_html:
        the_html.write(html)
    set_visited(url, filename)

store_page(base_url)
url_to_visit = get_random_unvisited_page()
while url_to_visit is not None:
    visit(url_to_visit)
    url_to_visit = get_random_unvisited_page()

```

这里发生了很多事情。首先，创建三个表:

*   一个“pages”表来跟踪 URL，不管我们是否已经访问过它们(就像前面的例子一样)，但现在还有一个“html_file”字段引用包含页面 html 内容的文件；
*   一个“链接”表，用于跟踪页面上的链接；
*   一个“图像”表来跟踪下载的图像。

对于我们访问的每个页面，我们提取所有的“ [6-2](#Fig2) 所示。

![A463931_1_En_6_Fig2_HTML.jpg](A463931_1_En_6_Fig2_HTML.jpg)

图 6-2

抓取的图片和网页集合

当然，您还可以对此设置进行许多其他修改。例如，在抓取页面期间，可能没有必要存储链接和图像，因为我们也可以从保存的 HTML 文件中收集这些信息。另一方面，您可能希望采取“尽可能早地保存尽可能多的内容”的方法，并在下载列表中包含其他媒体元素。应该明确的是，网络爬行脚本有多种形式和大小，没有一个“一刀切”的解决每个项目。因此，当在网络爬虫上工作时，要仔细考虑所有的设计角度，以及如何在你的设置中实现它们。

Going Further

提到一些其他仍有可能探索的修改，例如，您可能还希望排除“特殊”或“谈话”页面，或者引用文件的页面不被抓取。还要记住，我们上面的例子没有考虑到从页面收集的信息可能会变得“陈旧”的事实也就是说，一旦一个页面被抓取，我们就不会在脚本中再考虑它。试着想办法改变这一点(提示:例如,“visited_at”时间戳可以用来确定是否该再次访问一个页面)。你如何平衡探索新页面和刷新旧页面？哪组页面将获得优先权，如何获得优先权？

我们的 HTTP、请求、HTML、漂亮的汤、JavaScript、Selenium 和爬行之旅到此结束。本书的最后一章包含了一些完整的例子，展示了一些使用万维网上真实网站的大型项目。然而，首先，我们将从技术上后退一步，讨论网络抓取的管理和法律方面。