# 二、网络的语言是 HTTP

在这一章中，在简要介绍了计算机网络之后，我们将介绍构成 web 的核心构件之一:超文本传输协议(HTTP)。然后我们介绍 Python 请求库，我们将使用它来执行 HTTP 请求，并有效地开始用 Python 检索网站。本章以在 URL 中使用参数的一节结束。

## 2.1 网络的魔力

如今，网络已经如此融入我们的日常活动，以至于我们很少考虑它的复杂性。每当你在网上冲浪时，一系列的网络协议就被启动，与世界各地的计算机建立连接并检索数据，这一切都在几秒钟内完成。例如，考虑以下一系列步骤，当你导航到一个网站，比如说 [`www.google.com`](http://www.google.com) 时，你的网络浏览器会执行这些步骤:

1.  你在网络浏览器中输入“ [`www.google.com`](http://www.google.com) ”，它需要计算出这个网站的 IP 地址。IP 代表“互联网协议”并形成互联网的核心协议，因为它使网络能够在连接的计算机之间路由和重定向通信包，这些计算机都被给予一个 IP 地址。要与谷歌的网络服务器通信，你需要知道它的 IP 地址。由于 IP 地址基本上是一个数字，记住每个网站的所有这些数字有点烦人。因此，就像你如何将电话号码与你手机通讯录中的名字联系起来一样，网络提供了一种机制来将像“ [`www.google.com`](http://www.google.com) ”这样的域名翻译成 IP 地址。
2.  于是，你的浏览器开始找出“ [`www.google.com`](http://www.google.com) ”后面的正确 IP 地址。为此，您的 web 浏览器将使用另一种协议，称为 DNS(代表域名系统)，如下所示:首先，web 浏览器将检查自己的缓存(其“短期内存”)，以查看您最近是否访问过该网站。如果有，浏览器可以重用存储的地址。如果没有，浏览器将询问底层操作系统(例如 Windows)是否知道 [`www.google.com`](http://www.google.com) 的地址。
3.  如果操作系统也不知道这个域，浏览器将向您的路由器发送 DNS 请求，路由器是将您连接到互联网的机器，并且通常还保留自己的 DNS 缓存。如果您的路由器也不知道正确的地址，您的浏览器将开始向已知的 DNS 服务器发送大量数据包，例如，向您的互联网服务提供商(ISP)维护的 DNS 服务器发送数据包，该服务器的 IP 地址是已知的并存储在您的路由器中。然后，DNS 服务器将回复一个响应，基本上表明“ [`www.google.com`](http://www.google.com) ”被映射到 IP 地址“172.217.17.68”。请注意，即使是您的 ISPs DNS 服务器也可能需要询问其他 DNS 服务器(位于 DNS 层次结构的更高层),以防手头没有记录。
4.  所有这些都是为了找出 [`www.google.com`](http://www.google.com) 的 IP 地址。你的浏览器现在可以连接到 172.217.17.68，谷歌的网络服务器。许多协议——一个协议是关于通信方之间的消息应该是什么样子的标准协议——在这里被组合(如果你愿意，可以互相包装)以构造一个复杂的消息。在这个“洋葱”的最外面，我们发现 IEEE 802.3(以太网)协议，它用于与同一网络上的机器进行通信。因为我们不在同一个网络上通信，所以使用互联网协议 IP 来嵌入另一条消息，表明我们希望联系地址为 172.217.17.68 的服务器。在其中，我们发现了另一种协议，称为 TCP(传输控制协议)，它提供了一种通用、可靠的方法来传递网络消息，因为它包括错误检查和将消息拆分为较小数据包的功能，从而确保这些数据包以正确的顺序传递。当数据包在传输过程中丢失时，TCP 也会重新发送数据包。最后，在 TCP 消息中，我们发现另一个消息，它是根据 HTTP 协议(超文本传输协议)格式化的，这是用于请求和接收网页的实际协议。基本上，这里的 HTTP 消息陈述了来自我们的 web 浏览器的一个请求:“请问，我能得到您的索引页面吗？”
5.  Google 的 web 服务器现在发回一个 HTTP 回复，包含我们想要访问的页面的内容。在大多数情况下，这些文本内容是用 HTML 格式化的，这是一种标记语言，我们将在后面详细讨论。从这一大堆文本中，我们的 web 浏览器可以开始呈现实际的页面，也就是说，确保所有内容都按照 HTML 内容的指示整齐地出现在屏幕上。请注意，网页经常会包含一些内容，web 浏览器会在幕后发起新的 HTTP 请求。例如，如果接收到的页面指示浏览器显示一个图像，浏览器将发出另一个 HTTP 请求来获取图像的内容(这样图像看起来就不像 HTML 格式的文本，而只是原始的二进制数据)。因此，只呈现一个 web 页面可能会涉及大量的 HTTP 请求。幸运的是，现代浏览器很聪明，一旦有信息进入，就会开始呈现页面，显示检索到的图像和其他视觉效果。此外，如果可能的话，浏览器会尝试并行发送多个请求来加速这个过程。

有这么多的协议、请求和机器之间的对话在进行，你能在不到一秒钟的时间内浏览一个简单的网页，这简直令人吃惊。为了使构成 web 的大量协议标准化，国际标准化组织(ISO)维护开放系统互连(OSI)模型，该模型将计算机通信组织成七层:

*   第 1 层:物理层:包括以太网协议，也包括 USB、蓝牙和其他无线协议。
*   第 2 层:数据链路层:包括以太网协议。
*   第三层:网络层:包括 IP(互联网协议)。
*   第 4 层:传输层:TCP，但也包括 UDP 等协议，为了提高速度，这些协议不提供 TCP 的高级错误检查和恢复机制。
*   第 5 层:会话层:包括用于打开/关闭和管理会话的协议。
*   第 6 层:表示层:包括格式化和翻译数据的协议。
*   第 7 层:应用层:例如 HTTP 和 DNS。

并非所有网络通信都需要使用所有这些层的协议。例如，要请求一个网页，会涉及到第 1 层(物理层)、第 2 层(以太网层)、第 3 层(IP 层)、第 4 层(TCP 层)和第 7 层(HTTP 层)，但是这些层都是这样构建的，即较高层的每个协议都可以包含在较低层协议的消息中。例如，当您请求一个安全的 web 页面时，HTTP 消息(第 7 层)将被编码在一个加密的消息(第 6 层)中(如果您浏览一个“https”地址，就会发生这种情况)。当编程网络应用程序时，你的目标层越低，你需要处理的功能和复杂性就越多。幸运的是，我们对最顶层感兴趣，也就是 HTTP，用于请求和接收网页的协议。这意味着我们可以将所有关于 TCP、IP、以太网，甚至用 DNS 解析域名的复杂性留给我们使用的 Python 库和底层操作系统。

## 2.2 超文本传输协议:HTTP

我们已经看到了您的 web 浏览器如何与万维网上的服务器通信。消息交换的核心组件包括向 web 服务器发送的超文本传输协议(HTTP)请求消息，随后是 HTTP 响应(有时也称为 HTTP 回复)，可由浏览器呈现。因为我们所有的网络抓取都将建立在 HTTP 之上，所以我们确实需要仔细看看 HTTP 消息，以了解它们是什么样子的。

事实上，HTTP 是一个相当简单的网络协议。它是基于文本的，这至少使它的消息对最终用户来说是可读的(与完全没有文本结构的原始二进制消息相比),并且遵循简单的基于请求-应答的通信模式。也就是说，联系 web 服务器并接收回复只需要两条 HTTP 消息:一条请求和一条回复。如果您的浏览器想要下载或获取额外的资源(比如图像)，这将只需要发送额外的请求-回复消息。

Keep Me Alive

在最简单的情况下，HTTP 中的每个请求-应答周期都包括建立一个全新的底层 TCP 连接。对于繁重的网站，设置许多 TCP 连接并快速连续地拆除它们会产生大量开销，因此 HTTP 1.1 版允许我们保持 TCP 连接“活动”以用于并发的请求-回复 HTTP 消息。HTTP 版甚至允许我们在同一个连接中“复用”(一个“混合消息”的时髦词)，例如，发送多个并发请求。幸运的是，在使用 Python 时，我们不需要太关心这些细节，因为我们将使用的请求库会在后台自动为我们处理这些问题。

现在让我们看看 HTTP 请求和回复是什么样子的。我们记得，客户机(大多数情况下是 web 浏览器)和 web 服务器通过发送纯文本消息进行通信。客户端向服务器发送请求，服务器发送响应或回复。

请求消息由以下内容组成:

*   请求行；
*   许多请求头，每一个都在自己的行上；
*   空行；
*   可选的消息正文，它也可以占用多行。

HTTP 消息中的每一行都必须以<cr><lf>(ASCII 字符 0D 和 0A)结尾。</lf>T3】</cr>

空行就是简单的 <cr><lf>没有其他额外的空白。</lf>T3】</cr>

New Lines

<cr>和<lf>是两个特殊字符，表示应该开始新的一行。您看不到它们是这样出现的，但是当您在记事本中键入一个纯文本文档时，每次按 enter 键，这两个字符都会被放在文档的内容中，以表示“此处出现一个新行”计算的一个恼人的方面是，操作系统并不总是同意使用哪个字符来表示新的一行。Linux 程序倾向于使用<lf>(“换行”字符)，而旧版本的 MacOS 使用<cr>(“回车”字符)。Windows 使用<cr>和<lf>来表示新的一行，这也被 HTTP 标准所采用。不要太担心这个，因为 Python 请求库会为我们正确格式化 HTTP 消息。</lf></cr></cr></lf></lf>T11】</cr>

下面的代码片段显示了由 web 浏览器执行的完整 HTTP 请求消息(除了最后一个空行，我们没有显示每行后面的“ <cr><lf>”):</lf></cr>

```py
GET / HTTP/1.1
Host: example.com
Connection: keep-alive
Cache-Control: max-age=0
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Referer: https://www.google.com/
Accept-Encoding: gzip, deflate
Accept-Language: en-US,en;q=0.8,nl;q=0.6
<CR><LF>

```

让我们仔细看看这条信息。“GET / HTTP/1.1”是请求行。它包含了我们要执行的 HTTP“动词”或“方法”(上面例子中的“GET”)，我们要检索的 URL(“/”)，以及我们理解的 HTTP 版本(“HTTP/1.1”)。不要太担心“得到”这个动词。HTTP 有许多动词(我们将在后面讨论)。现在，重要的是要知道“获取”的意思是:“为我获取这个 URL 的内容。”每当您在地址栏中输入一个 URL 并按 enter 时，您的浏览器将执行一个 GET 请求。

接下来是请求头，每一个都有自己的一行。在这个例子中，我们已经有了很多。请注意，每个头都包括一个名称(例如，“Host”)，后跟一个冒号(“:”)和头的实际值(“example.com”)。浏览器在标题中包含的内容方面非常健谈，Chrome(这里使用的 web 浏览器也不例外)。

HTTP 标准包括一些标准化的标题，这些标题将被适当的 web 浏览器使用，尽管您也可以自由地包括附加的标题。例如，在 HTTP 1.1 和更高版本中，“Host”是一个标准化的强制头。它没有出现在 HTTP 1.0(第一个版本)中的原因很简单:在那些日子里，每个 web 服务器(及其 IP 地址)负责服务一个特定的网站。如果我们将“GET / HTTP/1.1”发送到负责“example.com”的 web 服务器，服务器就知道要获取和返回哪个页面。然而，没过多久，下面这个聪明的想法就出现了:为什么不从同一个服务器上为多个网站提供服务，用同一个 IP 地址呢？例如，负责“example.com”的同一个服务器也可能是提供属于“example.org”的页面的服务器。然而，我们需要一种方法来告诉服务器我们想从哪个域名检索页面。将域名包含在请求行本身，像“GET example.org/ HTTP/1.1”可能是一个可靠的想法，尽管这将破坏与早期 web 服务器的向后兼容性，早期 web 服务器期望在请求行中有一个没有域名的 URL。然后以强制的“主机”头的形式提供了一个解决方案，指示服务器应该从哪个域名检索页面。

Wrong Host

不要试图太聪明地向负责“example.com”的 web 服务器发送请求，并将“Host”头改为“Host:something fully-different . com”。适当的网络服务器会抱怨，并简单地发回一个错误页面说:“嘿，我不是那个域名的服务器。”也就是说，已经在网站上发现了安全问题，在这些网站上，通过欺骗此标题可能会混淆和误导用户。

除了强制的“Host”头之外，我们还看到出现了许多其他头，它们形成了一组“标准化请求头”，这些头不是强制的，尽管所有现代 web 浏览器都包含了它们。例如，“Connection: keep-alive”指示服务器，如果可能的话，应该为后续请求保持连接开放。“用户代理”包含一个很大的文本值，通过它浏览器很高兴地通知服务器它是什么(Chrome)，以及它运行的版本。

The User-Agent Mess

嗯……你会注意到“用户代理”标题包含了“Chrome”，但也包含了许多其他看似不相关的文本，比如“Mozilla”、“AppleWebKit”等等。Chrome 是不是在伪装自己，冒充其他浏览器？从某种程度上来说，它是 it，尽管它不是唯一这样做的浏览器。问题是这样的:当“用户代理”标题出现时，浏览器开始发送它们的名称和版本，一些网站所有者认为检查这个标题并根据询问者用不同版本的页面进行回复是个好主意，例如告诉用户“Netscape 4.0”不被该服务器支持。负责这些检查的例程通常是以一种随意的方式实现的，因此当用户运行某个未知的浏览器时，或者无法正确检查浏览器的版本时，会错误地将他们打发走。因此，这些年来，浏览器厂商别无选择，只能创造性地在这个用户代理头中包含许多其他文本字段。基本上，我们的浏览器在说“我是 Chrome，但我也兼容所有其他浏览器，所以请让我过去。”

“Accept”告诉服务器浏览器更喜欢取回哪种形式的内容，“Accept-Encoding”告诉服务器浏览器也能够取回压缩的内容。“Referer”标题(故意拼写错误)告诉服务器浏览器来自哪个页面(在本例中，点击了“google.com”上的链接，将浏览器发送到“example.com”)。

A Polite Request

即使您的 web 浏览器会尽量表现得有礼貌，例如，告诉 web 服务器它接受哪种形式的内容，也不能保证 web 服务器会真正查看这些标题或跟踪它们。浏览器可能会在它的“Accept”标题中指出它理解“webp”图像，但是 web 服务器可以忽略这个请求并以“jpg”或“png”的形式发回图像。不过，请将这些请求头视为礼貌的请求，仅此而已。

最后，我们的请求消息以空白的 <cr><lf>行结束，没有任何消息体。这些不包括在 GET 请求中，但是我们将在稍后看到 HTTP 消息，在那里这个消息体将发挥作用。</lf></cr>

如果一切顺利，web 服务器将处理我们的请求并发回一个 HTTP 回复。

这些请求看起来非常类似于 HTTP 请求，并且包含:

*   包括状态代码和状态消息的状态行；
*   多个响应头，同样都在同一行上；
*   空行；
*   可选的消息正文。

因此，在我们的上述请求之后，我们可能会得到以下响应:

```py
HTTP/1.1 200 OK
Connection: keep-alive
Content-Encoding: gzip
Content-Type: text/html;charset=utf-8
Date: Mon, 28 Aug 2017 10:57:42 GMT
Server: Apache v1.3
Vary: Accept-Encoding
Transfer-Encoding: chunked
<CR><LF>
<html>
<body>Welcome to My Web Page</body>
</html>

```

还是那句话，我们一行一行来看看 HTTP 回复。第一行表示请求的状态结果。它首先列出服务器理解的 HTTP 版本(“HTTP/1.1”)，然后是状态代码(“200”)和状态消息(“OK”)。如果一切顺利，状态将为 200。我们稍后将仔细研究许多约定的 HTTP 状态代码，但是您可能也熟悉 404 状态消息，它表示请求中列出的 URL 无法检索，也就是说，在服务器上“没有找到”。

接下来是——再一次——来自服务器的一些头。就像 web 浏览器一样，服务器在它们提供的内容方面非常健谈，并且可以包含任意多的标题。这里，服务器在它的头中包含它的当前日期和版本(“Apache v1.3”)。这里另一个重要的标题是“Content-Type”，因为它将为浏览器提供关于回复中包含的内容的信息。这里，它是 HTML 文本，但也可能是二进制图像数据、电影数据等等。

标题后面是一个空白的 <cr><lf>行，以及一个可选的消息体，包含回复的实际内容。在这里，内容是一串包含“欢迎来到我的网页”的 HTML 文本。正是这些 HTML 内容将被您的 web 浏览器解析并显示在屏幕上。同样，消息体是可选的，但是因为我们希望大多数请求实际上都返回一些内容，所以几乎在所有情况下都会出现消息体。</lf></cr>

Message Bodies

例如，即使回复的状态代码是 404，许多网站也会包含一个消息体，为用户提供一个漂亮的页面，表明——对不起——无法找到该页面。如果服务器忽略了它，网络浏览器将只显示默认的“未找到页面”页面。还有一些 HTTP 回复不包含消息体的情况，我们稍后会谈到。

## 2.3 Python 中的 HTTP:请求库

我们已经了解了 HTTP 的基础知识，现在是时候接触一些 Python 代码了。回想一下 web 抓取的主要目的:以自动化的方式从 web 上检索数据。基本上，我们放弃了网络浏览器，转而使用 Python 程序上网冲浪。这意味着我们的 Python 程序需要能够说和理解 HTTP。

毫无疑问，我们可以尝试在 Python(或其他语言)内置的标准网络功能的基础上自己编写这个程序，确保我们整齐地格式化 HTTP 请求消息，并能够解析传入的响应。然而，我们对重新发明轮子不感兴趣，已经有许多 Python 库使这项任务变得更加愉快，因此我们可以专注于我们实际上试图完成的事情。

事实上，Python 生态系统中有相当多的库可以为我们照顾 HTTP。仅举几个例子:

*   Python 3 自带一个名为“urllib”的内置模块，可以处理所有 HTTP 的事情(见[https://docs.python.org/3/library/urllib.html](https://docs.python.org/3/library/urllib.html))。与 Python 2 中的对应模块相比，该模块进行了重大修改，在 Python 2 中，HTTP 功能被分为“urllib”和“urllib2”两部分，使用起来有些麻烦。
*   “httplib2”(见[https://github.com/httplib2/httplib2](https://github.com/httplib2/httplib2)):一个小而快速的 HTTP 客户端库。最初由 Googler Joe Gregorio 开发，现在得到了社区的支持。
*   “urllib3”(见[https://urllib3.readthedocs.io/](https://urllib3.readthedocs.io/)):Python 的一个强大的 HTTP 客户端，由下面的请求库使用。
*   “请求”(见[http://docs.python-requests.org/](http://docs.python-requests.org/)):一个优雅而简单的 Python HTTP 库，为“人类”而建
*   “grequests”(参见[https://pypi.python.org/pypi/grequests](https://pypi.python.org/pypi/grequests)):扩展请求以处理异步、并发的 HTTP 请求。
*   “aiohttp”(参见[http://aiohttp.readthedocs.io/](http://aiohttp.readthedocs.io/)):另一个专注于异步 http 的库。

在本书中，我们将使用“请求”库来处理 HTTP。原因很简单:尽管“urllib”提供了可靠的 HTTP 功能(特别是与 Python 2 中的情况相比)，但使用它经常涉及大量样板代码，使得模块不太容易使用，阅读起来也不太优雅。与“urllib”相比，“urllib3”(不是标准 Python 模块的一部分)用一些高级特性扩展了关于 HTTP 的 Python 生态系统，但它也没有真正关注优雅或简洁。这就是“请求”的由来。这个库建立在“urllib3”之上，但是它允许您用简短、漂亮且易于使用的代码处理大多数 HTTP 用例。“grequests”和“aiohttp”都是更加面向现代的库，旨在使 Python 中的 http 更加异步。这对于非常繁重的应用程序变得尤其重要，在这些应用程序中，您必须尽可能快地发出大量 HTTP 请求。在接下来的内容中，我们将继续讨论“请求”,因为异步编程本身就是一个相当具有挑战性的话题，我们将讨论以健壮的方式加速 web 抓取程序的更传统的方法。如果您以后想这样做，从“requests”转移到“grequests”或“aiohttp”(或其他库)应该不会太难。

通过 pip 可以很容易地安装请求(如果您仍然需要设置 Python 3 和 pip，请参考 1.2.1 节)。在命令行窗口中执行以下命令(如果已经有请求的现有版本，参数“-U”将确保更新该版本):

```py
pip install -U requests

```

接下来，创建一个 Python 文件(“firstexample.py”是个好名字)，并输入以下内容:

```py
import requests

url = 'http://www.webscrapingfordatascience.com/basichttp/'
r = requests.get(url)
print(r.text)

```

如果一切顺利，在执行这个脚本时，您应该会看到下面一行:

```py
Hello from the web!

```

Webscrapingfordatascience Dot Com?

[`www.webscrapingfordatascience.com`](http://www.webscrapingfordatascience.com) 是这本书的配套网站。我们将在本书中使用本网站的页面来展示各种例子。由于 web 是一个快速发展的地方，我们希望确保我们提供的示例尽可能长时间地继续工作。暂时不要对停留在“安全的操场”上感到太沮丧，因为各种现实生活中的例子也包括在最后一章中。

让我们看看这个简短示例中发生了什么:

*   首先，我们导入请求模块。如果您已经在您的系统上正确地安装了请求，那么导入行应该简单地工作，没有任何错误或警告。
*   我们要检索 [`http://www.webscrapingfordatascience.com/basichttp/`](http://www.webscrapingfordatascience.com/basichttp/) 的内容。请尝试在浏览器中打开此网页。你会看到“你好，来自网络！”出现在页面上。这就是我们想要使用 Python 提取的内容。
*   我们使用`requests.get`方法对提供的 URL 执行“HTTP GET”请求。在最简单的情况下，我们只需要提供我们想要检索的页面的 URL。请求将确保根据我们之前看到的内容格式化适当的 HTTP 请求消息。
*   `requests.get`方法返回一个`requests.Response` Python 对象，该对象包含许多关于被检索的 HTTP 回复的信息。同样，requests 负责解析 HTTP 回复，以便您可以立即开始处理它。
*   `r.text`包含文本形式的 HTTP 响应内容体。这里，HTTP 响应体简单地包含了内容“Hello from web！”

A More Generic Request

因为我们将只处理 HTTP GET 请求(目前)，所以`requests.get`方法将构成接下来的例子的基础。稍后，我们还将处理其他类型的 HTTP 请求，比如 POST。每个请求都有相应的方法，例如`requests.post`。还有一个通用的请求方法，看起来像这样:`requests.request('GET', url)`。写起来有点长，但是在您事先不知道要发出哪种类型的 HTTP 请求(GET 或其他)的情况下可能会派上用场。

让我们进一步扩展一下这个例子，看看幕后发生了什么:

```py
import requests

url = 'http://www.webscrapingfordatascience.com/basichttp/'
r = requests.get(url)

# Which HTTP status code did we get back from the server?
print(r.status_code)
# What is the textual status code?
print(r.reason)
# What were the HTTP response headers?
print(r.headers)

# The request information is saved as a Python object in r.request:
print(r.request)
# What were the HTTP request headers?
print(r.request.headers)

# The HTTP response content:
print(r.text)

```

如果运行此代码，您将看到以下结果:

*   回想一下我们之前关于 HTTP 请求和回复的讨论。通过使用`request.Response`对象的`status_code`和`reason`属性，我们可以检索从服务器返回的 HTTP 状态代码和相关的文本消息。这里，状态代码和消息“200 OK”表示一切顺利。
*   `request.Response`对象的`headers`属性返回服务器包含在其 HTTP 回复中的头的字典。再说一遍:服务器可能非常健谈。该服务器报告其数据、服务器版本，并且还提供“内容类型”头。
*   要获得关于被触发的 HTTP 请求的信息，您可以访问一个`request.Response`对象的`request`属性。这个属性本身是一个`request.Request`对象，包含关于准备好的 HTTP 请求的信息。
*   因为 HTTP 请求消息也包含头，所以我们也可以访问这个对象的`headers`属性，以获得一个表示请求所包含的头的字典。请注意，默认情况下，请求礼貌地报告其“用户代理”。此外，请求还可以自动处理压缩页面，因此它还包括一个“Accept-Encoding”头来指示这一点。最后，它包括一个“Accept”头，表示“您拥有的任何格式都可以被发送回来”，并且还可以处理“保持活动”连接。但是，稍后我们会看到需要覆盖请求的默认请求头行为的情况。

```py
200
OK
{'Date': 'Wed, 04 Oct 2017 08:26:03 GMT',
 'Server': 'Apache/2.4.18 (Ubuntu)',
 'Content-Length': '20',
 'Keep-Alive': 'timeout=5, max=99',
 'Connection': 'Keep-Alive',
 'Content-Type': 'text/html; charset=UTF-8'}
<PreparedRequest [GET]>
{'User-Agent': 'python-requests/2.18.4',
 'Accept-Encoding': 'gzip, deflate',
 'Accept': '*/*',
 'Connection': 'keep-alive'}
Hello from the web!

```

## 2.4 查询字符串:带参数的 URL

关于 HTTP: URL 参数的基本工作，我们还需要讨论一件事。尝试修改上面的代码示例，以便抓取 URL [`http://www.webscrapingfordatascience.com/paramhttp/`](http://www.webscrapingfordatascience.com/paramhttp/) 。您应该得到以下内容:

```py
Please provide a "query" parameter

```

尝试在您的 web 浏览器中打开此页面，以验证您是否获得了相同的结果。现在尝试导航到页面 [`http://www.webscrapingfordatascience.com/paramhttp/?query=test`](http://www.webscrapingfordatascience.com/paramhttp/%3Fquery=test) 。你看到了什么？

“可有可无？…”部分被称为“查询字符串”，它意味着包含不适合 URL 的正常层次路径结构的数据。您可能在网上冲浪时多次遇到过这种 URL，例如:

*   [T2`http://www.example.com/product_page.html?product_id=304`](http://www.example.com/product_page.html?product_id=304)
*   [T2`https://www.google.com/search?dcr=0&source=hp&q=test&oq=test`](https://www.google.com/search?dcr=0&source=hp&q=test&oq=test)
*   [T2`http://example.com/path/to/page/?type=animal&location=asia`](http://example.com/path/to/page/?type=animal&location=asia)

网络服务器是智能软件。当服务器接收到对此类 URL 的 HTTP 请求时，它可能会运行一个程序，该程序使用查询字符串中包含的参数(“URL 参数”)来呈现不同的内容。比如比较 [`http://www.webscrapingfordatascience.com/paramhttp/?query=test`](http://www.webscrapingfordatascience.com/paramhttp/%3Fquery=test) 和 [`http://www.webscrapingfordatascience.com/paramhttp/?query=anothertest`](http://www.webscrapingfordatascience.com/paramhttp/%3Fquery=anothertest) 。即使对于这个简单的页面，您也可以看到响应是如何动态合并您在 URL 中提供的参数数据的。

URL 中的查询字符串应遵循以下约定:

*   查询字符串出现在 URL 的末尾，以单个问号“？”开头。
*   参数以键-值对的形式提供，并用&符号分隔。
*   键和值用等号“=”分隔。
*   因为有些字符不能是 URL 的一部分或具有特殊含义(字符“/”、“?、" & "和" = "等)，当在 URL 中使用这些字符时，需要应用 URL "编码"来正确格式化这些字符。使用 URL [`http://www.webscrapingfordatascience.com/paramhttp/?query=another%20test%3F%26`](http://www.webscrapingfordatascience.com/paramhttp/%3Fquery=another%20test%3F%26) 试试看，它会发送“另一个测试？&"作为“查询”参数的值以编码的形式发送给服务器。
*   其他确切的语义没有标准化。一般来说，web 服务器不会考虑 URL 参数的指定顺序，尽管有些服务器可能会考虑。许多 web 服务器也能够处理和使用没有 URL 参数值的页面，例如， [`http://www.example.com/?noparam=` `&` `anotherparam`](http://www.example.com/?noparam=%26anotherparam) 。因为完整的 URL 包含在 HTTP 请求的请求行中，所以 web 服务器可以决定如何解析和处理这些 URL。

URL Rewriting

后一句话还强调了 URL 参数的另一个重要方面:尽管它们有些标准化，但它们并不被视为 URL 的“特殊”部分，它只是作为 HTTP 请求中的纯文本行发送。大多数 web 服务器会注意在它们的终端解析它们，以便在呈现页面时使用它们的信息(或者甚至在它们不被使用时忽略它们——例如，尝试 URL [`http://www.webscrapingfordatascience.com/paramhttp/?query=test` `&` `other=ignored`](http://www.webscrapingfordatascience.com/paramhttp/%3Fquery=test&other=ignored) )，但是近年来，URL 参数的使用在某种程度上被避免了。相反，大多数 web 框架将允许我们定义“好看”的 URL，只在 URL 的路径中包含参数，例如，“/product/302/”而不是“products.html？p=302”。前者看起来更好，搜索引擎优化(SEO)人员也会告诉你搜索引擎也更喜欢这样的 URL。在服务器端，任何传入的 URL 都可以被随意解析，从其中提取片段并“重写”它，因为它是被调用的，所以在准备回复时，一些部分可能最终被用作输入。对于我们这些 web 抓取者来说，这基本上意味着，即使您在 URL 中看不到查询字符串，URL 中仍可能存在服务器可能以不同方式响应的动态部分。

让我们来看看如何处理请求中的 URL 参数。处理这些问题最简单的方法是将它们简单地包含在 URL 本身中:

```py
import requests

url = 'http://www.webscrapingfordatascience.com/paramhttp/?query=test'
r = requests.get(url)

print(r.text)
# Will show: I don't have any information on "test"

```

在某些情况下，请求会尝试帮助您，并为您编码一些字符:

```py
import requests

url = 'http://www.webscrapingfordatascience.com/paramhttp/?query=a query with spaces'
r = requests.get(url)
# Parameter will be encoded as 'a%20query%20with%20spaces'
# You can verify this be looking at the prepared request URL:
print(r.request.url)
# Will show [...]/paramhttp/?query=a%20query%20with%20spaces

print(r.text)
# Will show: I don't have any information on "a query with spaces"

```

但是，有时 URL 过于模糊，请求无法理解它:

```py
import requests

url = 'http://www.webscrapingfordatascience.com/paramhttp/?query=complex?&'

# Parameter will not be encoded
r = requests.get(url)

# You can verify this be looking at the prepared request URL:
print(r.request.url)
# Will show [...]/paramhttp/?query=complex?&

print(r.text)
# Will show: I don't have any information on "complex?"

```

在这种情况下，请求者不能确定你的意思是“什么？& "是属于实际的 URL，还是您想要对其进行编码。因此，请求什么也不做，只是请求 URL。在服务器端，这个特定的 web 服务器能够导出第二个问号(“？)应该是 URL 参数的一部分(应该已经正确编码了，但是不会抱怨)，虽然在这种情况下,“与”号太不明确了。这里，web 服务器假设它是一个普通的分隔符，而不是 URL 参数值的一部分。

那么，我们怎样才能恰当地解决这个问题呢？第一种方法是使用“url-lib.parse”函数`quote`和`quote_plus`。前者意在对 URL 的路径部分中的特殊字符进行编码，并使用百分号“%XX”编码对特殊字符进行编码，包括空格。后者做同样的事情，但是用加号代替空格，它通常用于编码查询字符串:

```py
import requests
from urllib.parse import quote, quote_plus

raw_string = 'a query with /, spaces and?&'
print(quote(raw_string))
print(quote_plus(raw_string))

```

该示例将打印出这两行:

```py
a%20query%20with%20/%2C%20spaces%20and%3F%26
a+query+with+%2F%2C+spaces+and%3F%26

```

`quote`函数应用了百分比编码，但是保留了斜杠(“/”)不变(至少是它的默认设置),因为这个函数打算在 URL 路径上使用。`quote_plus`函数确实应用了类似的编码，但是使用加号(“+”)来编码空格，也将编码斜线。只要我们确保我们的查询参数不使用斜杠，这两种编码方法都可以有效地用于编码查询字符串。如果我们的查询字符串包含斜杠，并且我们确实想使用`quote`，我们可以简单地覆盖它的`safe`参数，如下所示:

```py
import requests
from urllib.parse import quote, quote_plus

raw_string = 'a query with /, spaces and?&'
url = 'http://www.webscrapingfordatascience.com/paramhttp/?query='

print('\nUsing quote:')
# Nothing is safe, not even '/' characters, so encode everything
r = requests.get(url + quote(raw_string, safe=''))
print(r.url)
print(r.text)

print('\nUsing quote_plus:')
r = requests.get(url + quote_plus(raw_string))
print(r.url)
print(r.text)

```

该示例将打印出:

```py
Using quote:
http://[...]/?query=a%20query%20with%20%2F%2C%20spaces%20and%3F%26
I don't have any information on "a query with /, spaces and?&"

Using quote_plus:
http://[...]/?query=a+query+with+%2F%2C+spaces+and%3F%26
I don't have any information on "a query with /, spaces and?&"

```

所有这些编码杂耍会很快导致头痛。请求不是应该让我们的生活变得简单，并为我们处理这些问题吗？不要担心，因为我们可以简单地用请求重写上面的例子，如下所示:

```py
import requests

url = 'http://www.webscrapingfordatascience.com/paramhttp/'

parameters = {
    'query': 'a query with /, spaces and?&'
    }

r = requests.get(url, params=parameters)

print(r.url)
print(r.text)

```

注意`requests.get`方法中`params`参数的用法:您可以简单地传递一个带有未编码的 URL 参数的 Python 字典，请求会自动为您编码。

Empty and Ordered Parameters

例如，“params = { ' query ':“}”中的空参数将在 URL 中以等号结束，即“？查询= "。如果您愿意，您也可以将一个列表传递给`params`，其中每个元素都是一个元组，或者列表本身有两个元素，分别代表每个参数的键和值，在这种情况下，列表的顺序将被遵守。您还可以传递一个`OrderedDict`对象(Python 3 中“collections”模块提供的内置对象),该对象将保留排序。最后，您还可以传递一个表示查询字符串部分的字符串。在这种情况下，请求将在前面加上问号(“？”)为您服务，但是将再次不能提供智能 URL 编码，因此您有责任确保您的查询字符串被正确编码。虽然这并不常用，但在 web 服务器需要一个“？param”结尾没有等号，例如，这在实践中很少发生，但可能会发生。

Silencing requests Completely

即使当向`params`传递一个字符串，或者在`requests.get`方法中包含完整的 url 时，请求仍然会尝试，正如我们已经看到的，提供一点帮助。例如，写:

```py
requests.get('http://www.example.com/?spaces |pipe')

```

会让你以“？spaces%20%7Cpipe "作为请求 URL 中的查询字符串，并为您编码了空格和管道字符(“|”)。在极少数情况下，一个非常挑剔的 web 服务器可能仍然希望 URL 是未编码的。同样，这种情况非常罕见，但我们在野外遇到过这种情况。在这种情况下，您需要按如下方式覆盖请求:

```py
import requests

from urllib.parse import unquote

class NonEncodedSession(requests.Session):

  # Override the default send method

  def send(self, *a, **kw):

    # Revert the encoding which was applied

    a[0].url = unquote(a[0].url)

    return requests.Session.send(self, *a, **kw)

my_requests = NonEncodedSession()

url = 'http://www.example.com/?spaces |pipe'

r = my_requests.get(url)

print(r.url)

# Will show: http://www.example.com/?spaces |pipe

```

作为最后的练习，把头转向 [`http://www.webscrapingfordatascience.com/calchttp/`](http://www.webscrapingfordatascience.com/calchttp/) 。使用“a”、“b”和“op”URL 参数。您应该能够理解下面的代码是做什么的:

```py
import requests

def calc(a, b, op):
    url = 'http://www.webscrapingfordatascience.com/calchttp/'
    params = {'a': a, 'b': b, 'op': op}
    r = requests.get(url, params=params)
    return r.text

print(calc(4, 6, '*'))
print(calc(4, 6, '/'))

```

基于我们上面所看到的，你可能会很想尝试使用现实生活中的网站来学习。然而，在 web 就绪之前，我们还需要克服另一个障碍。例如，当您运行以下命令时，会发生什么情况:

```py
import requests

url = 'https://en.wikipedia.org/w/index.php' + \
      '?title=List_of_Game_of_Thrones_episodes&oldid=802553687'
r = requests.get(url)
print(r.text)

```

Wikipedia Versioning

我们在这里使用“oldid”URL 参数，以便获得“权力的游戏剧集列表”页面的特定版本，以确保我们后续的示例将继续工作。对了，在这里你可以看到“网址重写”在起作用:既有[https://en . Wikipedia . org/wiki/List _ of _ Game _ of _ Thrones _ episodes](https://en.wikipedia.org/wiki/List_of_Game_of_Thrones_episodes)又有[https://en.wikipedia.org/w/index.php?《权力的游戏》剧集](https://en.wikipedia.org/w/index.php?title=List_of_Game_of_Thrones_episodes)也指向同一个页面。不同之处在于，后者使用 URL 参数，而前者不使用，尽管维基百科的网络服务器足够聪明，可以将 URL 路由到正确的“页面”。另外，你可能会注意到我们在这里没有使用`params`参数。我们可以，尽管这里“title”和“oldid”参数都不需要编码，所以我们可以将它们放在 URL 本身中，以使代码的其余部分更短一些。

正如您所看到的，由`r.text`捕获的响应体现在抛出了一系列看起来令人困惑的文本。这是 HTML 格式的文本，尽管我们要寻找的内容隐藏在这个汤的某个地方，但我们需要学习一种适当的方法来从那里获取我们想要的信息。这正是我们在下一章要做的。

The Fragment Identifier

除了查询字符串之外，事实上 URL 还有另一个可选部分，您可能以前遇到过:片段标识符，或者有时称为“散列”。它以一个散列符号(" # ")作为前缀，出现在 URL 的最末尾，甚至在查询字符串之后，例如在" [`http://www.example.org/about.htm?p=8#contact`](http://www.example.org/about.htm?p=8#contact) "中。URL 的这一部分旨在标识对应于该 URL 的文档的一部分。例如，一个网页可以包含一个链接，该链接包含一个片段标识符，如果你点击它，就会立即将你的视图滚动到页面的相应部分。但是，片段标识符的功能与 URL 的其余部分不同，因为它是由 web 浏览器专门处理的，web 服务器根本不参与。事实上，当浏览器从 web 服务器获取资源时，它们甚至不应该在 HTTP 请求中包含片段标识符。相反，浏览器会一直等到 web 服务器发送了它的回复，然后它会使用片段标识符滚动到页面的正确部分。如果您将片段标识符包含在请求 URL 中，大多数 web 服务器会简单地忽略它，尽管有些服务器可能在编程时也会考虑到它们。同样:这是相当罕见的，因为这种服务器提供的内容对于大多数 web 浏览器来说是不可见的，因为它们在请求中遗漏了片段标识符部分，尽管 web 上充满了有趣的边缘案例。

我们现在已经看到了请求库的基础。花些时间浏览一下位于 http://docs.python-requests.org/en/master/的图书馆文档。一旦你开始在你的项目中使用这个库，请求文档的质量是非常高的，并且容易参考。