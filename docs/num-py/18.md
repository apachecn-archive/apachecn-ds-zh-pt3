# 18.数据输入和输出

在几乎所有的科学计算和数据分析应用中，都需要数据输入和输出。这包括加载数据集以及将结果持久存储到磁盘上的文件或数据库中。因此，从程序中获取数据是计算工作流程中的一个关键步骤。有许多存储结构化和非结构化数据的标准化格式。使用标准化格式的好处是显而易见的:您可以使用现有的库来读取和写入数据，从而节省时间和精力。在使用科学和技术计算的过程中，通过与同事和同行的交互，或者从设备和数据库等来源获取数据时，您可能会遇到各种数据格式。作为一名计算从业者，能够高效、无缝地处理数据是非常重要的，不管数据是哪种格式。这就是为什么这一整章都致力于数据输入和输出的原因。

Python 对许多文件格式都有很好的支持。事实上，处理最常见的格式有多种选择。在这一章中，我们将调查数据存储格式及其在计算中的应用，并讨论每种格式适用的典型情况。我们还介绍了 Python 库和工具，用于处理计算中常见的数据格式。

数据可以分为几种类别和类型。重要的类别是结构化和非结构化数据，例如，值可以是分类的(有限的值集)、顺序的(有意义排序的值)或数值的(连续或离散)。值也有类型，比如字符串、整数、浮点数等。存储或传输数据的数据格式应该理想地考虑这些概念，以避免数据或元数据的丢失，并且我们经常需要对如何表示数据进行细粒度的控制。

在计算应用中，大多数时候我们处理的是结构化数据，例如数组和表格数据。非结构化数据集的例子包括自由形式的文本或具有非同类类型的嵌套列表。在本章中，我们将重点关注 CSV 系列格式和用于结构化数据的 HDF5 格式，在本章的最后，我们将讨论 JSON 格式，这是一种轻量级的灵活格式，可用于存储简单和复杂的数据集，并且偏向于存储列表和字典。这种格式非常适合存储非结构化数据。我们还简要讨论了使用 msgpack 格式和 Python 内置的 pickle 格式将对象序列化为可存储数据的方法。

由于数据输入和输出在许多以数据为中心的计算应用中的重要性，出现了几个 Python 库，其目标是简化和帮助处理不同格式的数据以及移动和转换数据。例如，Blaze 库( [`http://blaze.pydata.org/en/latest`](http://blaze.pydata.org/en/latest) )提供了一个高级接口，用于访问不同格式和不同类型来源的数据。在这里，我们主要关注用于读取特定类型文件格式的低级库，这些文件格式对于存储数字数据和非结构化数据集非常有用。然而，我们鼓励有兴趣的读者也去探索更高层次的库，比如 Blaze。

## 导入模块

在这一章中，我们使用了许多不同的库来处理不同类型的数据。特别是，我们需要 NumPy 和 Pandas，像往常一样，我们分别将其作为`np`和`pd`导入:

```py
In [1]: import numpy as np
In [2]: import pandas as pd

```

我们还使用 Python 标准库中的`csv`和`json`模块:

```py
In [3]: import csv
In [4]: import json

```

为了处理 HDF5 格式的数字数据，我们使用了`h5py`和`pytables`库:

```py
In [5]: import h5py
In [6]: import tables

```

最后，在将对象序列化为可存储数据的上下文中，我们探索了`pickle`和`msgpack`库:

```py
In [7]: import pickle
In [8]: import msgpack

```

## 逗号分隔的值

逗号分隔值(CSV)是一种直观且定义宽松的 <sup>[1](#Fn1)</sup> 纯文本文件格式，简单而有效，非常流行于存储表格数据。在这种格式中，每个记录存储为一行，记录的每个字段用分隔符(例如，逗号)分隔。或者，每个字段都可以用引号括起来，以允许字符串值字段包含分隔符字符。此外，第一行有时用于存储列名，注释行也很常见。清单 [18-1](#PC5) 中显示了一个 CSV 文件的例子。

```py
# 2013-2014 / Regular Season / All Skaters / Summary / Points
Rank,Player,Team,Pos,GP,G,A,P,+/-,PIM,PPG,PPP,SHG,SHP,GW,OT,S,S%,TOI/GP,Shift/GP,FO%
1,Sidney Crosby,PIT,C,80,36,68,104,+18,46,11,38,0,0,5,1,259,13.9,21:58,24.0,52.5
2,Ryan Getzlaf,ANA,C,77,31,56,87,+28,31,5,23,0,0,7,1,204,15.2,21:17,25.2,49.0
3,Claude Giroux,PHI,C,82,28,58,86,+7,46,7,37,0,0,7,1,223,12.6,20:26,25.1,52.9
4,Tyler Seguin,DAL,C,80,37,47,84,+16,18,11,25,0,0,8,0,294,12.6,19:20,23.4,41.5
5,Corey Perry,ANA,R,81,43,39,82,+32,65,8,18,0,0,9,1,280,15.4,19:28,23.2,36.0

Listing 18-1Example of a CSV File with a Comment Line, a Header Line, and Mixed Numerical and String-Valued Data Fields (Data source: 
www.nhl.com
)

```

CSV 有时也是字符分隔值的缩写，反映了 CSV 格式通常是指在字段之间使用不同分隔符的一系列格式。例如，通常使用制表符而不是逗号，在这种情况下，格式有时称为 TSV 而不是 CSV。术语“分隔符分隔值(DSV)”有时也指这些类型的格式。

在 Python 中，有几种方法可以读写 CSV 格式的数据，每种方法都有不同的用例及优点。首先，标准 Python 库包含一个名为`csv`的模块，用于读取 CSV 数据。要使用这个模块，我们可以用一个文件句柄作为参数调用`csv.reader`函数。它返回一个类实例，可以用作迭代器，将来自给定 CSV 文件的行解析为 Python 字符串列表。例如，要将文件`playerstats-2013-2014.csv`(如清单 [18-1](#PC5) 所示)读入一个嵌套的字符串列表，我们可以使用

```py
In [9]: with open("playerstats-2013-2014.csv") as f:
   ...:     csvreader = csv.reader(f)
   ...:     rows = [fields for fields in csvreader]
In [10]: rows[1][1:6]
Out[10]: ['Player', 'Team', 'Pos', 'GP', 'G']
In [11]: rows[2][1:6]
Out[11]: ['Sidney Crosby', 'PIT', 'C', '80', '36']

```

请注意，默认情况下，解析行中的每个字段都是字符串值，即使该字段表示一个数值，如前面示例中的 80(比赛次数)或 36(进球)。虽然`csv`模块提供了一种定义定制 CSV 阅读器类的灵活方式，但是这个模块对于阅读带有字符串值字段的 CSV 文件来说是最方便的。

在计算工作中，存储和加载带有数值的数组是很常见的，比如向量和矩阵。NumPy 库为此提供了`np.loadtxt`和`np.savetxt`。这些函数使用几个参数来微调要读取或写入的 CSV 格式的类型:例如，使用`delimiter`参数，我们可以选择使用哪个字符来分隔字段，而`header`和`comments`参数可以分别用于指定标题行和前置到标题的注释行。

例如，考虑使用`np.savetxt`将一个具有随机数和形状(100，3)的数组保存到文件`data.csv`中。为了给数据提供一些上下文，我们还向文件添加了一个标题和一个注释行，并通过参数`delimiter=","`明确请求使用逗号字符作为字段分隔符(默认分隔符是空格字符):

```py
In [12]: data = np.random.randn(100, 3)
In [13]: np.savetxt("data.csv", data, delimiter=",", header="x,y,z",
    ...:            comments="# Random x, y, z coordinates\n")
In [14]: !head -n 5 data.csv
# Random x, y, z coordinates
x,y,z
1.652276634254504772e-01,9.522165919962696234e-01,4.659850998659530452e-01
8.699729536125471174e-01,1.187589118344758443e+00,1.788104702180680405e+00
-8.106725710122602013e-01,2.765616277935758482e-01,4.456864674903074919e-01

```

要将这种格式的数据读回到 NumPy 数组中，我们可以使用`np.loadtxt`函数。它采用与`np.savetxt`相似的参数:特别是，我们再次将`delimiter`参数设置为`","`，以表示由逗号分隔的字段。我们还需要使用`skiprows`参数来跳过文件中的前两行(注释和标题行)，因为它们不包含数字数据:

```py
In [15]: data_load = np.loadtxt("data.csv", skiprows=2, delimiter=",")

```

结果是一个新的 NumPy 数组，它等同于使用`np.savetxt`写入到`data.csv`文件中的原始数组:

```py
In [16]: (data == data_load).all()
Out[16]: True

```

注意，与 Python 标准库中的`csv`模块中的 CSV 阅读器不同，默认情况下，NumPy 中的`loadtxt`函数将所有字段转换为数值，结果是一个带有数值`dtype` ( `float64`)的 NumPy:

```py
In [17]: data_load[1,:]
Out[17]: array([ 0.86997295,  1.18758912,  1.7881047 ])
In [18]: data_load.dtype
Out[18]: dtype('float64')

```

要使用`np.loadtxt`读取包含非数值数据的 CSV 文件——比如我们在前面的文本中使用 Python 标准库读取的`playerstats-2013-2014.csv`文件——我们必须使用`dtype`参数显式设置结果数组的数据类型。如果我们试图在没有设置`dtype`的情况下读取带有非数值的 CSV 文件，我们会得到一个错误:

```py
In [19]: np.loadtxt("playerstats-2013-2014.csv", skiprows=2, delimiter=",")
---------------------------------------------------------------------------
ValueError: could not convert string to float: b'Sidney Crosby'

```

使用`dtype=bytes`(或`str`或`object`，我们得到一个带有未解析值的 NumPy 数组:

```py
In [20]: data = np.loadtxt("playerstats-2013-2014.csv", skiprows=2, delimiter=",", dtype=bytes)
In [21]: data[0][1:6]
Out[21]: array([b'Sidney Crosby', b'PIT', b'C', b'80', b'36'], dtype='|S13') 

```

或者，如果我们只想读取数字类型的列，我们可以使用`usecols`参数选择读取列的子集:

```py
In [22]: np.loadtxt("playerstats-2013-2014.csv", skiprows=2, delimiter=",", usecols=[6,7,8])
Out[22]: array([[  68.,  104.,   18.],
                [  56.,   87.,   28.],
                [  58.,   86.,    7.],
                [  47.,   84.,   16.],
                [  39.,   82.,   32.]])

```

虽然 NumPy `savetxt`和`loadtxt`函数是可配置的、灵活的 CSV 编写器和读取器，但是它们对于全数字数据来说是最方便的。另一方面，Python 标准库模块`csv`对于包含字符串值数据的 CSV 文件来说是最方便的。在 Python 中读取 CSV 文件的第三种方法是使用 Pandas `read_csv`函数。我们已经在第 [12](12.html) 章中看到了这个函数的例子，我们用它从 TSV 格式的数据文件中创建熊猫数据框。Pandas 中的`read_csv`功能在读取包含数值型和字符串型字段的 CSV 文件时非常方便，在大多数情况下，它会自动确定字段的类型并相应地进行转换。例如，当使用`read_csv`读取`playerstats-2013-2014.csv`文件时，我们获得了一个 Pandas 数据帧，其中所有字段都被解析为适当类型的列:

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"> <col class="tcol6 align-left"></colgroup> 
| 

`Rank`

 | 

`Player`

 | 

`GP`

 | 

`G`

 | 

`A`

 | 

`P`

 |
| --- | --- | --- | --- | --- | --- |
| `1` | `Sidney Crosby` | `80` | `36` | `68` | `104` |
| `2` | `Ryan Getzlaf` | `77` | `31` | `56` | `87` |
| `3` | `Claude Giroux` | `82` | `28` | `58` | `86` |
| `4` | `Tyler Seguin` | `80` | `37` | `47` | `84` |
| `5` | `Corey Perry` | `81` | `43` | `39` | `82` |

```py
In [23]: df = pd.read_csv("playerstats-2013-2014.csv", skiprows=1)
In [24]: df = df.set_index("Rank")
In [25]: df[["Player", "GP", "G", "A", "P"]]
Out[25]:

```

使用`DataFrame`实例`df`的`info`方法，我们可以清楚地看到每一列都被转换成了哪种类型(为了简洁，这里的输出被截断了):

```py
In [26]: df.info()
<class 'pandas.core.frame.DataFrame'>
Int64Index: 5 entries, 1 to 5
Data columns (total 20 columns):
Player      5 non-null object
Team        5 non-null object
Pos         5 non-null object
GP          5 non-null int64
G           5 non-null int64
...
S           5 non-null int64
S%          5 non-null float64
TOI/GP      5 non-null object
Shift/GP    5 non-null float64
FO%         5 non-null float64
dtypes: float64(3), int64(13), object(4)
memory usage: 840.0+ bytes

```

也可以使用`DataFrame`对象的`to_csv`方法将数据帧写入 CSV 文件:

```py
In [27]: df[["Player", "GP", "G", "A", "P"]].to_csv("playerstats-2013-2014-subset.csv")
In [28]: !head -n 5 playerstats-2013-2014-subset.csv
Rank,Player,GP,G,A,P
1,Sidney Crosby,80,36,68,104
2,Ryan Getzlaf,77,31,56,87
3,Claude Giroux,82,28,58,86
4,Tyler Seguin,80,37,47,84

```

Python 标准库 NumPy 和 Pandas 的组合为读写各种风格的 CSV 文件提供了一个强大的工具箱。然而，尽管 CSV 文件对于表格数据来说既方便又有效，但是这种格式有明显的缺点。首先，它只能用于存储一维或二维数组，并且不包含可以帮助解释数据的元数据。此外，它在存储或读写方面的效率都不是很高，而且它不能用于为每个文件存储多个数组，需要为多个数组存储多个文件，即使它们密切相关。因此，CSV 的使用应该仅限于简单的数据集。在下一节中，我们将探讨 HDF5 文件格式，该格式旨在高效存储数字数据，并克服 CSV 等简单数据格式及相关格式的所有缺点。

## HDF5

*分层数据格式 5* (HDF5)是用于存储数值数据的格式。它由非营利组织 HDF 集团 <sup>[2](#Fn2)</sup> 开发，在 BSD 开源许可下可用。HDF5 格式于 1998 年发布，旨在高效处理大型数据集，包括支持高性能并行 I/O。因此，HDF5 格式适用于分布式高性能超级计算机和集群，可用于存储和操作 TB 级或更大规模的数据集。然而，HDF5 的优势在于它同样适用于小型数据集。因此，对于计算从业者来说，它是一种真正通用的格式和无价的工具。

该格式的分层方面允许使用类似于文件系统的分层结构在文件内组织数据集。HDF5 文件中实体的术语是*组*和*数据集*，这对应于文件系统类比中的目录和文件。HDF5 文件中的组可以嵌套以创建一个树形结构，因此在格式名称中有*分层*。HDF5 文件中的数据集是特定维度和特定类型元素的同质数组。HDF5 类型系统支持所有标准基本数据类型，也允许定义自定义复合数据类型。HDF5 文件中的组和数据集也可以有*属性*，可以用来存储关于组和数据集的元数据。属性本身可以有不同的类型，如数值或字符串值。

除了文件格式本身，HDF 小组还提供了该格式的库和参考实现。主库是用 C 编写的，它的 C API 的包装器可用于许多编程语言。用于从 HDF5 文件访问数据的 HDF5 库为部分读写操作提供了完善的支持，可用于访问整个数据集的一小部分。这是一个强大的功能，可以在大于计算机内存的数据集上进行计算 <sup>[3](#Fn3)</sup> 。HDF5 格式是一种成熟的文件格式，在不同的平台和计算环境中得到广泛支持。这也使得 HDF5 成为长期存储数据的合适选择。作为一个数据存储平台，HDF5 为许多问题提供了解决方案:跨平台存储、高效的 I/O 和可扩展至超大型数据文件的存储，元数据系统(属性)可用于注释和描述文件中的组和数据集，使数据自描述。总之，这些特性使 HDF5 成为一款出色的计算工具。

对于 Python，有两个使用 HDF5 文件的库:h5py 和 PyTables。这两个库采用不同的方法来使用 HDF5，熟悉这两个库是非常值得的。h5py 库提供了一个相对接近基本 HDF5 概念的 API，侧重于组和数据集。它提供了一个受 NumPy 启发的 API 来访问数据集，这对于熟悉 NumPy 的人来说非常直观。

### h5py

h5py 库为 HDF5 文件格式提供了 Pythonic 接口，并为其数据集提供了类似 NumPy 的接口。有关该项目的更多信息，包括其官方文档，请访问其位于 [`www.h5py.org`](http://www.h5py.org) 的网页。在撰写本文时，该库的最新版本是 2.7.1。

PyTables 库提供了基于 HDF5 格式的高级数据抽象，提供了类似数据库的特性，例如具有可轻松定制的数据类型的表。它还允许将数据集作为数据库进行查询，并允许使用高级索引功能。

### iptables(iptables)

PyTables 库在 HDF5 之上提供了一个类似数据库的数据模型。有关该项目及其文档的更多信息，请参见位于 [`http://pytables.github.io`](http://pytables.github.io) 的网页。在撰写本文时，PyTables 的最新版本是 3.4.3。

在接下来的两节中，我们将更详细地探讨如何使用 h5py 和 PyTables 库来读写 HDF5 文件中的数值数据。

### h5py

我们从参观 h5py 库开始。h5py 的 API 非常简单，使用起来非常愉快，同时功能齐全。这是通过深思熟虑地使用 Pythonic 式的习惯用法来实现的，比如 dictionary 和 NumPy 的数组语义。表 [18-1](#Tab1) 总结了 h5py 库中的基本对象和方法。在下文中，我们将通过一系列示例来探索如何使用这些方法。

表 18-1

h5py API 中主要对象和方法的总结

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
| 

目标

 | 

方法/属性

 | 

描述

 |
| --- | --- | --- |
| `h5py.File` | `__init__(name, mode, ...)` | 打开现有的 HDF5，或创建一个新的 HD F5，文件名为`name`。根据 mode 参数的值，文件可以以只读或读写模式打开(请参见正文)。 |
|   | `flush()` | 将缓冲区写入文件。 |
|   | `close()` | 关闭打开的 HDF5 文件。 |
| `h5py.File, h5py.Group` | `create_group(name)` | 在当前组内创建一个名为`name`(可以是路径)的新组。 |
|   | `create_dataset(name, data=..., shape=..., dtype=..., ...)` | 创建新的数据集。 |
|   | `[] dictionary syntax` | 访问组内的项目(组和数据集)。 |
| `h5py.Dataset` | `dtype` | 数据类型。 |
|   | `shape` | 数据集的形状(维度)。 |
|   | `value` | 数据集的基础数据的完整数组。 |
|   | `[] array syntax` | 访问数据集中的数据元素或子集。 |
| `h5py.File, h5py.Group, h5py.Dataset` | `name` | HDF5 文件层次中对象的名称(路径)。 |
|   | `attrs` | 类似字典的属性访问。 |

#### 文件

我们首先看看如何使用`h5py.File`对象打开现有的 HDF5 文件并创建新的文件。这个对象的初始化器只接受一个文件名作为必需的参数，但是我们通常还需要指定`mode`参数，通过它我们可以选择以只读或读写模式打开一个文件，以及文件在打开时是否应该被截断。mode 参数采用类似于内置 Python 函数`open`的字符串值:`"r"`用于只读(文件必须存在)，`"r+"`用于读写(文件必须存在)，`"w"`用于创建新文件(如果文件存在，则截断)，`"w-"`用于创建新文件(如果文件存在，则出错)，而`"a"`用于读写(如果文件存在，则创建)。为了在读写模式下创建一个新文件，我们可以使用

```py
In [29]: f = h5py.File("data.h5", mode="w")

```

结果是一个文件句柄，这里分配给变量`f`，我们可以用它来访问文件并向文件中添加内容。给定一个文件句柄，我们可以使用`mode`属性查看它是以哪种模式打开的:

```py
In [30]: f.mode
Out[30]: 'r+'

```

注意，即使我们在模式`"w"`下打开文件，一旦文件被打开，它要么是只读的(`"r"`)要么是读写的(`"r+"`)。可使用 HDF5 file 对象执行的其他文件级操作包括使用`flush`方法刷新包含尚未写入文件的数据的缓冲区，以及使用`close`方法关闭文件:

```py
In [31]: f.flush()
In [32]: f.close()

```

#### 组

在表示一个 HDF5 文件句柄的同时，`File`对象也表示 HDF5 组对象，称为*根组*。通过组对象的`name`属性可以访问组的名称。该名称采用路径的形式，类似于文件系统中的路径，它指定了该组在文件分层结构中的存储位置。根组的名称为`"` / `"`:

```py
In [33]: f = h5py.File("data.h5", "w")
In [34]: f.name
Out[34]: '/'

```

group 对象有一个方法`create_group`,用于在现有组中创建新组。用此方法创建的新组成为调用`create_group`方法的组实例的子组:

```py
In [35]: grp1 = f.create_group("experiment1")
In [36]: grp1.name
Out[36]: '/experiment1'

```

这里，组`experiment1`是根组的子组，因此它在分层结构中的名称和路径是`/experiment1`。创建新组时，其直接父组不一定要预先存在。例如，要创建一个新组`/experiment2/measurement`，我们可以直接使用根组*的`create_group`方法，而不需要*首先显式创建`experiment2`组。中间组会自动创建。

```py
In [37]: grp2_meas = f.create_group("experiment2/measurement")
In [38]: grp2_meas.name
Out[38]: '/experiment2/measurement'
In [39]: grp2_sim = f.create_group("experiment2/simulation")
In [40]: grp2_sim.name
Out[40]: '/experiment2/simulation'

```

HDF5 文件的组层次结构可以使用字典式界面进行研究。要检索具有给定路径名的组，我们可以从它的一个祖先组(通常是根节点)执行类似字典的查找:

```py
In [41]: f["/experiment1"]
Out[41]: <HDF5 group "/experiment1" (0 members)>
In [42]: f["/experiment2/simulation"]
Out[42]: <HDF5 group "/experiment2/simulation" (0 members)>

```

相同类型的字典查找也适用于子组(不仅仅是根节点):

```py
In [43]: grp_experiment2 = f["/experiment2"]
In [44]: grp_experiment2['simulation']
Out[44]: <HDF5 group "/experiment2/simulation" (0 members)>

```

`keys`方法返回组中子组和数据集名称的迭代器，而`items`方法返回组中每个实体的`(name, value)`元组的迭代器。这些可用于以编程方式遍历组的层次结构。

```py
In [45]: list(f.keys())
Out[45]: ['experiment1', 'experiment2']
In [46]: list(f.items())
Out[46]: [('experiment1', <HDF5 group "/experiment1" (0 members)>),
          ('experiment2', <HDF5 group "/experiment2" (2 members)>)]

```

要遍历 HDF5 文件中的组层次结构，我们还可以使用方法`visit`，该方法将一个函数作为参数，并使用文件层次结构中每个实体的名称调用该函数:

```py
In [47]: f.visit(lambda x: print(x))
experiment1
experiment2
experiment2/measurement
experiment2/simulation

```

或者是做同样事情的`visititems`方法，除了它调用带有项目名称和项目本身作为参数的函数:

```py
In [48]: f.visititems(lambda name, item: print(name, item))
experiment1 <HDF5 group "/experiment1" (0 members)>
experiment2 <HDF5 group "/experiment2" (2 members)>
experiment2/experiment <HDF5 group "/experiment2/measurement" (0 members)>
experiment2/simulation <HDF5 group "/experiment2/simulation" (0 members)>

```

为了与 Python 字典的语义保持一致，我们还可以使用带有`in` Python 关键字的集合成员测试来操作`Group`对象:

```py
In [49]: "experiment1" in f
Out[49]: True
In [50]: "simulation" in f["experiment2"]
Out[50]: True
In [51]: "experiment3" in f
Out[51]: False

```

使用`visit`和`visititems`方法，以及字典风格的方法`keys`和`items`，我们可以轻松地探索 HDF5 文件的结构和内容，即使我们事先不知道它包含什么以及数据在其中是如何组织的。方便地探索 HDF5 的能力是该格式可用性的一个重要方面。还有一些外部非 Python 工具可用于探索 HDF5 文件的内容，这些工具在处理此类文件时通常很有用。特别是，`h5ls`命令行工具可以方便地快速检查 HDF5 文件的内容:

```py
In [52]: f.flush()
In [53]: !h5ls -r data.h5
/                        Group
/experiment1                 Group
/experiment2                 Group
/experiment2/measurement     Group
/experiment2/simulation      Group

```

这里我们使用了`h5ls`程序的`-r`标志来递归显示文件中的所有项目。`h5ls`程序是名为`hdf5-tools`的软件包提供的一系列 HDF5 实用程序的一部分(参见`h5stat`、`h5copy`、`h5diff`等)。).尽管这些不是 Python 工具，但它们在处理一般的 HDF5 文件时非常有用，在 Python 中也是如此。

#### 资料组

既然我们已经探讨了如何在 HDF5 文件中创建和访问组，那么是时候看看如何存储数据集了。毕竟，存储数字数据是 HDF5 格式的主要目的。使用 h5py 在 HDF5 文件中创建数据集有两种主要方法。创建数据集最简单的方法是使用字典索引语法，将 NumPy 数组分配给 HDF5 组中的一个项目。第二种方法是使用`create_dataset`方法创建一个空数据集，我们将在本节后面看到例子。

例如，为了将两个 NumPy 数组`array1`和`meas1`分别存储到根组和`experiment2/measurement`组中，我们可以使用

```py
In [54]: array1 = np.arange(10)
In [55]: meas1 = np.random.randn(100, 100)
In [56]: f["array1"] = array1
In [57]: f["/experiment2/measurement/meas1"] = meas1

```

为了验证分配的 NumPy 数组的数据集是否被添加到文件中，让我们使用`visititems`方法遍历文件层次结构:

```py
In [58]: f.visititems(lambda name, value: print(name, value))
array1 <HDF5 dataset "array1": shape (10,), type "<i8">
experiment1 <HDF5 group "/experiment1" (0 members)>
experiment2 <HDF5 group "/experiment2" (2 members)>
experiment2/measurement <HDF5 group "/experiment2/measurement" (1 members)>
experiment2/measurement/meas1 <HDF5 dataset "meas1": shape (100, 100), type "<f8">
experiment2/simulation <HDF5 group "/experiment2/simulation" (0 members)>

```

我们看到，`array1`和`meas1`数据集已经添加到文件中。注意，在赋值中用作字典键的路径决定了数据集在文件中的位置。为了检索数据集，我们可以使用与检索组相同的类似字典的语法。例如，为了检索存储在根组中的`array1`数据集，我们可以使用`f["array1"]`:

```py
In [59]: ds = f["array1"]
In [60]: ds
Out[60]: <HDF5 dataset "array1": shape (10,), type "<i8">

```

结果是一个`Dataset`对象，而不是一个 NumPy 数组，就像我们分配给`array1`项的数组一样。`Dataset`对象是 HDF5 中底层数据的代理。像 NumPy 数组一样，`Dataset`对象有几个描述数据集的属性，包括`name`、`dtype`和`shape`。它还有一个返回数据集长度的方法`len`:

```py
In [61]: ds.name
Out[61]: '/array1'
In [62]: ds.dtype
Out[62]: dtype('int64')
In [63]: ds.shape
Out[63]: (10,)
In [64]: ds.len()
Out[64]: 10

```

例如，可以使用 value 属性来访问数据集的实际数据。这将整个数据集作为 NumPy 数组返回，它在这里相当于我们分配给`array1`数据集的数组。

```py
In [65]: ds.value
Out[65]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

```

为了更深入地访问组层次结构中的数据集，我们可以使用类似文件系统的路径名。例如，要检索组`experiment2/measurement`中的`meas1`数据集，我们可以使用

```py
In [66]: ds = f["experiment2/measurement/meas1"]
In [67]: ds
Out[67]: <HDF5 dataset "meas1": shape (100, 100), type "<f8">

```

我们再次得到一个`Dataset`对象，它的基本属性可以使用我们之前介绍的对象属性来检查:

```py
In [68]: ds.dtype
Out[68]: dtype('float64')
In [69]: ds.shape
Out[69]: (100, 100)

```

注意，这个数据集的数据类型是`float64`，而对于数据集 array1，数据类型是`int64`。这种类型信息来自分配给两个数据集的 NumPy 数组。在这里，我们可以再次使用 value 属性将数组检索为 NumPy 数组。相同操作的另一种语法是使用带省略号的括号索引:`ds[...]`。

```py
In [70]: data_full = ds[...]
In [71]: type(data_full)
Out[71]: numpy.ndarray
In [72]: data_full.shape
Out[72]: (100, 100)

```

这是一个类似 NumPy 的数组索引的例子。`Dataset`对象支持 NumPy 中使用的大多数索引和切片类型，这为从文件中部分读取数据提供了一种强大而灵活的方法。例如，为了只从`meas1`数据集中检索第一列，我们可以使用

```py
In [73]: data_col = ds[:, 0]
In [74]: data_col.shape
Out[74]: (100,)

```

结果是对应于数据集中第一列的 100 元素数组。请注意，这种切片是在 HDF5 库中执行的，而不是在 NumPy 中执行的，因此在本例中，只从文件中读取了 100 个元素并存储在生成的 NumPy 数组中，而没有将数据集完全加载到内存中。当处理不适合内存的大型数据集时，这是一个重要的特性。

例如，`Dataset`对象也支持步进索引:

```py
In [75]: ds[10:20:3, 10:20:3] # 3 stride
Out[75]: array([[-0.22321057, -0.61989199,  0.78215645,  0.73774187],
                [-1.03331515,  2.54190817, -0.24812478, -2.49677693],
                [ 0.17010011,  1.88589248,  1.91401249, -0.63430569],
                [ 0.4600099 , -1.3242449 ,  0.41821078,  1.47514922]])

```

以及“花式索引”，其中为数组的一个维度给出索引列表(不适用于多个索引):

```py
In [76]: ds[[1,2,3], :].shape
Out[76]: (3, 100)

```

我们也可以使用布尔索引，其中一个布尔值 NumPy 数组用于索引一个`Dataset`。例如，为了挑选出第一列(`ds[:, 0]`)中的值大于 2 的每一行的前五列(第二轴上的索引`:5`),我们可以使用布尔掩码`ds[:, 0] > 2`来索引数据集:

```py
In [77]: mask = ds[:, 0] > 2
In [78]: mask.shape, mask.dtype
Out[78]: ((100,), dtype('bool'))
In [79]: ds[mask, :5]
Out[79]: array([[ 2.1224865 ,  0.70447132, -1.71659513,  1.43759445, -0.61080907],
                [ 2.11780508, -0.2100993 ,  1.06262836, -0.46637199,  0.02769476],
                [ 2.41192679, -0.30818179, -0.31518842, -1.78274309, -0.80931757],
                [ 2.10030227,  0.14629889,  0.78511191, -0.19338282,  0.28372485]])

```

由于`Dataset`对象使用 NumPy 的索引和切片语法来选择底层数据的子集，因此熟悉 NumPy 的人很容易使用 h5py 在 Python 中处理大型 HDF5 数据集。还要记住，对于大型文件来说，`Dataset`对象上的索引切片与通过`value`属性访问的 NumPy 数组上的索引切片有很大的不同，因为前者避免了将整个数据集加载到内存中。

到目前为止，我们已经了解了如何在 HDF5 文件中创建数据集，方法是将数据显式分配到 group 对象中的一个项目中。我们还可以使用`create_dataset`方法显式地创建数据集。它将新数据集的名称作为第一个参数，我们可以使用`data`参数为新数据集设置数据，也可以通过设置`shape`参数创建一个空数组。例如，代替赋值`f["array2"] = np.random.randint(10, size=10)`，我们也可以使用`create_dataset`方法:

```py
In [80]: ds = f.create_dataset("array2", data=np.random.randint(10, size=10))
In [81]: ds
Out[81]: <HDF5 dataset "array2": shape (10,), type "<i8">
In [82]: ds.value
Out[82]: array([2, 2, 3, 3, 6, 6, 4, 8, 0, 0])

```

当显式调用`create_dataset`方法时，我们可以更好地控制结果数据集的属性。例如，如果我们可以使用`dtype`参数显式设置数据集的数据类型，我们可以使用`compress`参数选择压缩方法，使用`chunks`参数设置块大小，使用`maxsize`参数设置可调整大小的数据集的最大允许数组大小。还有许多其他与`Dataset`对象相关的高级功能。详见`create_dataset`的文档字符串。

当通过指定`shape`参数而不是为初始化数据集提供数组来创建空数组时，我们也可以使用`fillvalue`参数来设置数据集的默认值。例如，要创建一个形状为(5，5)且默认值为-1 的空数据集，我们可以使用

```py
In [83]: ds = f.create_dataset("/experiment2/simulation/data1", shape=(5, 5), fillvalue=-1)
In [84]: ds
Out[84]: <HDF5 dataset "data1": shape (5, 5), type "<f4">
In [85]: ds.value
Out[85]: array([[-1., -1., -1., -1., -1.],
                [-1., -1., -1., -1., -1.],
                [-1., -1., -1., -1., -1.],
                [-1., -1., -1., -1., -1.],
                [-1., -1., -1., -1., -1.]], dtype=float32)

```

HDF5 在空数据集的磁盘使用方面很聪明，不会存储多余的数据，特别是当我们使用`compression`参数选择压缩方法时。有几种压缩方法可用，例如`'gzip'`。使用数据集压缩，我们可以创建一个非常大的数据集，并逐渐用数据填充它们，例如，当测量结果或计算结果变得可用时，而不会在最初浪费大量存储空间。例如，让我们用组`experiment1/simulation`中的`data1`创建一个形状为(5000，5000，5000)的大型数据集:

```py
In [86]: ds = f.create_dataset("/experiment1/simulation/data1", shape=(5000, 5000, 5000), fillvalue=0, compression="gzip")
In [87]: ds
Out[87]: <HDF5 dataset "data1": shape (5000, 5000, 5000), type "<f4">

```

一开始，这个数据集既不使用内存也不使用磁盘空间，直到我们开始用数据填充它。要为数据集赋值，我们可以再次使用类似 NumPy 的索引语法，并为数据集中的特定元素或使用切片语法选择的子集赋值:

```py
In [88]: ds[:, 0, 0] = np.random.rand(5000)
In [89]: ds[1, :, 0] += np.random.rand(5000)
In [90]: ds[:2, :5, 0]
Out[90]: array([[ 0.67240328, 0\.        , 0\.        , 0\.        , 0\.        ],
                [ 0.99613971, 0.48227152, 0.48904559, 0.78807044, 0.62100351]], dtype=float32)

```

注意，没有被赋值的元素被设置为创建数组时指定的值`fillvalue`。如果我们不知道数据集有什么填充值，我们可以通过查看`Dataset`对象的`fillvalue`属性来找出:

```py
In [91]: ds.fillvalue
Out[91]: 0.0

```

为了查看新创建的数据集是否确实存储在我们打算分配给它的组中，我们可以再次使用`visititems`方法来列出`experiment1`组的内容:

```py
In [92]: f["experiment1"].visititems(lambda name, value: print(name, value))
simulation <HDF5 group "/experiment1/simulation" (1 members)>
simulation/data1 <HDF5 dataset "data1": shape (5000, 5000, 5000), type "<f4">

```

虽然数据集`experiment1/simulation/data1`非常大(4 × 5000 <sup>3</sup> 字节~ 465 Gb)，但是由于我们还没有用太多的数据填充它，所以 HDF5 文件仍然不会占用太多的磁盘空间(只有 357 Kb 左右):

```py
In [93]: f.flush()
In [94]: f.filename
Out[94]: 'data.h5'
In [95]: !ls -lh data.h5
-rw-r--r--@ 1 rob  staff   357K Apr  5 18:48 data.h5

```

到目前为止，我们已经了解了如何在 HDF5 文件中创建组和数据集。当然，有时也有必要从文件中删除项目。使用 h5py，我们可以使用 Python `del`关键字从组中删除项目，同样符合 Python 字典的语义:

```py
In [96]: del f["/experiment1/simulation/data1"]
In [97]: f["experiment1"].visititems(lambda name, value: print(name, value))
simulation <HDF5 group "/experiment1/simulation" (0 members)>

```

#### 属性

属性是 HDF5 格式的一个组成部分，这使它成为一种通过使用元数据来注释数据和提供自描述数据的优秀格式。例如，在存储实验数据时，往往会有一些外部参数和条件要与观测数据一起记录。同样，在计算机模拟中，通常需要将附加模型或模拟参数与生成的模拟结果一起存储。在所有这些情况下，最好的解决方案是确保所需的附加参数作为元数据与主数据集一起存储。

HDF5 格式通过使用属性支持这种类型的元数据。HDF5 文件中的每个组和数据集可以附加任意数量的属性。有了 h5py 库，就像组一样，可以使用类似字典的接口来访问属性。`Group`和`Dataset`对象的 Python 属性`attrs`用于访问 HDF5 属性:

```py
In [98]: f.attrs
Out[98]: <Attributes of HDF5 object at 4462179384>

```

要创建一个属性，我们只需为目标对象分配一个`attrs`字典。例如，要为根组创建一个属性`description`，我们可以使用

```py
In [99]: f.attrs["description"] = "Result sets for experiments and simulations"

```

类似地，向`experiment1`和`experiment2`组添加`date`属性:

```py
In [100]: f["experiment1"].attrs["date"] = "2015-1-1"
In [101]: f["experiment2"].attrs["date"] = "2015-1-2"

```

我们还可以将属性直接添加到数据集(不仅仅是组):

```py
In [102]: f["experiment2/simulation/data1"].attrs["k"] = 1.5
In [103]: f["experiment2/simulation/data1"].attrs["T"] = 1000

```

和组一样，我们可以使用`Attribute`对象的`keys`和`items`方法来检索它包含的属性的迭代器:

```py
In [104]: list(f["experiment1"].attrs.keys())
Out[104]: ['date']
In [105]: list(f["experiment2/simulation/data1"].attrs.items())
Out[105]: [('k', 1.5), ('T', 1000)]

```

按照 Python 字典的语义，可以使用 Python `in`操作符来测试属性的存在性:

```py
In [106]: "T" in f["experiment2/simulation/data1"].attrs
Out[106]: True

```

要删除现有的属性，我们可以使用`del`关键字:

```py
In [107]: del f["experiment2/simulation/data1"].attrs["T"]
In [108]: "T" in f["experiment2/simulation"].attrs
Out[108]: False

```

HDF5 组和数据集的属性适合与实际数据集一起存储元数据。大量使用属性有助于为数据提供上下文，这通常是数据有用的前提。

### iptables(iptables)

PyTables 库为 Python 提供了 HDF5 的替代接口。该库的重点是使用 HDF5 格式实现的基于表的高级数据模型，尽管 PyTables 也可用于创建和读取通用 HDF5 组和数据集，如 h5py 库。这里我们关注表数据模型，因为它补充了我们在上一节中讨论的 h5py 库。我们使用本章前面使用的 NHL 播放器统计数据集演示了 PyTables 表对象的使用，其中我们从 Pandas 数据框为该数据集构建了一个 PyTables 表。因此，我们首先使用`read_csv`函数将数据集读入一个`DataFrame`对象:

```py
In [109]: df = pd.read_csv("playerstats-2013-2014.csv", skiprows=1)
     ...: df = df.set_index("Rank")

```

接下来我们通过使用`tables.open_file`函数 <sup>[4](#Fn4)</sup> 来创建一个新的 PyTables HDF5 文件句柄。该函数将文件名作为第一个参数，将文件模式作为可选的第二个参数。结果是一个 PyTables HDF5 文件句柄(这里分配给变量`f`):

```py
In [110]: f = tables.open_file("playerstats-2013-2014.h5", mode="w")

```

与 h5py 库一样，我们可以用文件句柄对象的方法`create_group`创建 HDF5 组。它将父组的路径作为第一个参数，将组名作为第二个参数，还可以选择参数`title`，使用它可以在组上设置描述性的 HDF5 属性。

```py
In [111]: grp = f.create_group("/", "season_2013_2014",
     ...:                      title="NHL player statistics for the 2013/2014 season")
In [112]: grp
Out[112]: /season_2013_2014 (Group) 'NHL player statistics for the 2013/2014 season'
            children := []

```

与 h5py 库不同，PyTables 中的文件句柄对象不代表 HDF5 文件中的根组。要访问根节点，我们必须使用文件句柄对象的`root`属性:

```py
In [113]: f.root
Out[113]: / (RootGroup) "
            children := ['season_2013_2014' (Group)]

```

PyTables 库的一个很好的特性是，使用 HDF5 的类似 struct 的复合数据类型，很容易创建具有混合列类型的表。用 PyTables 定义这种表数据结构的最简单方法是创建一个从`tables.IsDescription`类继承的类。它应该包含由来自`tables`库的数据类型表示组成的字段。例如，要为玩家统计数据集创建表结构的规范，我们可以使用

```py
In [114]: class PlayerStat(tables.IsDescription):
     ...:     player = tables.StringCol(20, dflt="")
     ...:     position = tables.StringCol(1, dflt="C")
     ...:     games_played = tables.UInt8Col(dflt=0)
     ...:     points = tables.UInt16Col(dflt=0)
     ...:     goals = tables.UInt16Col(dflt=0)
     ...:     assists = tables.UInt16Col(dflt=0)
     ...:     shooting_percentage = tables.Float64Col(dflt=0.0)
     ...:     shifts_per_game_played = tables.Float64Col(dflt=0.0)

```

这里的类`PlayerStat`表示一个有八列的表的表结构，其中前两列是固定长度的字符串(`tables.StringCol`)，后面四列是无符号整数(`tables.UInt8Col`和`tables.UInt16Col`，8 位和 16 位大小)，最后两列是浮点类型(`tables.Float64Col`)。数据类型对象的可选参数`dflt`指定了字段的默认值。一旦使用该表单上的类定义了表结构，我们就可以使用`create_table`方法在 HDF5 文件中创建实际的表。它将组对象或父节点的路径作为第一个参数，将表名作为第二个参数，将表规范类作为第三个参数，还可以选择将表标题作为第四个参数(存储为相应数据集的 HDF5 属性):

```py
In [115]: top30_table = f.create_table(grp, 'top30', PlayerStat, "Top 30 point leaders")

```

为了将数据插入到表中，我们可以使用 table 对象的`row`属性来检索一个`Row`访问器类，该类可以用作字典来用值填充行。当 row 对象完全初始化后，我们可以使用`append`方法将行实际插入到表中:

```py
In [116]: playerstat = top30_table.row
In [117]: for index, row_series in df.iterrows():
     ...:     playerstat["player"] = row_series["Player"]
     ...:     playerstat["position"] = row_series["Pos"]
     ...:     playerstat["games_played"] = row_series["GP"]
     ...:     playerstat["points"] = row_series["P"]
     ...:     playerstat["goals"] = row_series["G"]
     ...:     playerstat["assists"] = row_series["A"]
     ...:     playerstat["shooting_percentage"] = row_series["S%"]
     ...:     playerstat["shifts_per_game_played"] = row_series["Shift/GP"]
     ...:     playerstat.append()

```

`flush`方法强制将表数据写入文件:

```py
In [118]: top30_table.flush()

```

为了从表中访问数据，我们可以使用`cols`属性以 NumPy 数组的形式检索列:

```py
In [119]: top30_table.cols.player[:5]
Out[119]: array([b'Sidney Crosby', b'Ryan Getzlaf', b'Claude Giroux',
                 b'Tyler Seguin', b'Corey Perry'], dtype='|S20')
In [120]: top30_table.cols.points[:5]
Out[120]: array([104,  87,  86,  84,  82], dtype=uint16)

```

为了以逐行方式访问数据，我们可以使用`iterrows`方法在表中的所有行上创建一个迭代器。在这里，我们使用这种方法遍历所有行，并将它们打印到标准输出中(为了简洁起见，这里的输出被截断):

```py
In [121]: def print_playerstat(row):
     ...:     print("%20s\t%s\t%s\t%s" %
     ...:           (row["player"].decode('UTF-8'), row["points"],
     ...:            row["goals"], row["assists"]))
In [122]: for row in top30_table.iterrows():
     ...:     print_playerstat(row)
  Sidney Crosby    104    36    68
Ryan Getzlaf        87    31    56
Claude Giroux       86    28    58
Tyler Seguin        84    37    47
...
Jaromir Jagr        67    24    43
John Tavares        66    24    42
Jason Spezza        66    23    43
Jordan Eberle       65    28    37

```

PyTables 表接口最强大的特性之一是能够使用查询从底层 HDF5 中有选择地提取行。例如，`where`方法允许我们将表列的表达式作为字符串传递，PyTables 使用该字符串来过滤行:

```py
In [123]: for row in top30_table.where("(points > 75) & (points <= 80)"):
     ...:     print_playerstat(row)
Phil Kessel        80    37    43
Taylor Hall        80    27    53
Alex Ovechkin      79    51    28
Joe Pavelski       79    41    38
Jamie Benn         79    34    45
Nicklas Backstrom  79    18    61
Patrick Sharp      78    34    44
Joe Thornton       76    11    65

```

使用`where`方法，我们还可以根据多个列来定义条件:

```py
In [124]: for row in top30_table.where("(goals > 40) & (points < 80)"):
     ...:     print_playerstat(row)
Alex Ovechkin     79    51    28
Joe Pavelski      79    41    38

```

这个特性允许我们以类似数据库的方式查询一个表。虽然对于像当前这样的小数据集，我们也可以使用 Pandas 数据框直接在内存中执行这些操作，但请记住，HDF5 文件存储在磁盘上，PyTables 库中 I/O 的有效使用使我们能够处理不适合内存的非常大的数据集，这将阻止我们对整个数据集使用 NumPy 或 Pandas 等。

在结束本节之前，让我们检查一下生成的 HDF5 文件的结构，该文件包含我们刚刚创建的 PyTables 表:

```py
In [125]: f
Out[125]: File(filename=playerstats-2013-2014.h5, title=", mode="w", root_uep='/', filters=Filters(complevel=0, shuffle=False, fletcher32=False, least_significant_digit=None))
          / (RootGroup) " /season_2013_2014 (Group) 'NHL player stats for the 2013/2014 season'
          /season_2013_2014/top30 (Table(30,)) 'Top 30 point leaders'
              description := {
              "assists": UInt16Col(shape=(), dflt=0, pos=0),
              "games_played": UInt8Col(shape=(), dflt=0, pos=1),
              "goals": UInt16Col(shape=(), dflt=0, pos=2),
              "player": StringCol(itemsize=20, shape=(), dflt=b", pos=3),
              "points": UInt16Col(shape=(), dflt=0, pos=4),
              "position": StringCol(itemsize=1, shape=(), dflt=b'C', pos=5),
              "shifts_per_game_played": Float64Col(shape=(), dflt=0.0, pos=6),
              "shooting_percentage": Float64Col(shape=(), dflt=0.0, pos=7)}
           byteorder := 'little'
           chunkshape := (1489,)

```

从 PyTables 文件句柄的字符串表示及其包含的 HDF5 文件层次结构中，我们可以看到 PyTables 库创建了一个数据集`/season_2013_2014/top30`，它使用了一个复杂的复合数据类型，该数据类型是根据我们之前创建的`PlayerStat`对象中的规范创建的。最后，当我们完成修改文件中的数据集时，我们可以使用`flush`方法刷新其缓冲区并强制写入文件，当我们完成对文件的处理时，我们可以使用`close`方法关闭它:

```py
In [126]: f.flush()
In [127]: f.close()

```

虽然我们在这里没有涉及其他类型的数据集，比如常规的同质数组，但是值得一提的是 PyTables 库也支持这些类型的数据结构。例如，我们可以使用`create_array`、`create_carray`和`create_earray`分别构造固定大小的数组、分块数组和可扩展数组。有关如何使用这些数据结构的更多信息，请参见相应的文档字符串。

### 熊猫 HDFStore

使用 Python 在 HDF5 文件中存储数据的第三种方法是在 Pandas 中使用`HDFStore`对象。它可用于在 HDF5 文件中永久存储数据帧和其他 Pandas 对象。要在 Pandas 中使用这个特性，必须安装 PyTables 库。我们可以通过向初始化器传递文件名来创建一个`HDFStore`对象。结果是一个可以用作字典的`HDFStore`对象，我们可以向其分配 Pandas `DataFrame`实例，并将它们存储到 HDF5 文件中:

```py
In [128]: store = pd.HDFStore('store.h5')
In [129]: df = pd.DataFrame(np.random.rand(5,5))
In [130]: store["df1"] = df
In [131]: df = pd.read_csv("playerstats-2013-2014-top30.csv", skiprows=1)
In [132]: store["df2"] = df

```

HDFStore 对象的行为类似于常规的 Python 字典，例如，我们可以通过调用 keys 方法来查看它包含了哪些对象:

```py
In [133]: store.keys()
Out[133]: ['/df1', '/df2']

```

我们可以使用 Python `in`关键字测试给定键的对象是否存在:

```py
In [134]: 'df2' in store
Out[134]: True

```

为了从存储中检索一个对象，我们再次使用类似字典的语义并用相应的键索引该对象:

```py
In [135]: df = store["df1"]

```

从`HDFStore`对象，我们还可以使用`root`属性访问底层的 HDF5 句柄。这实际上只是一个 PyTables 文件句柄:

```py
In [136]: store.root
Out[136]: / (RootGroup) "   children := ['df1' (Group), 'df2' (Group)]

```

一旦我们完成了一个`HDFStore`对象，我们应该使用 close 方法关闭它，以确保所有与它相关的数据都被写入文件。

```py
In [137]: store.close()

```

由于 HDF5 是一种标准文件格式，当然，没有什么可以阻止我们用 Pandas `HDFStore`或 PyTables 以及任何其他 HDF5 兼容软件(如 h5py 库)打开 HDF5 文件。如果我们用 h5py 打开用`HDFStore`生成的文件，我们可以很容易地检查它的内容，并看到`HDFStore`对象如何排列我们分配给它的`DataFrame`对象的数据:

```py
In [138]: f = h5py.File("store.h5")
In [139]: f.visititems(lambda x, y: print(x, "\t" * int(3 - len(str(x))//8), y))
df1                 <HDF5 group "/df1" (4 members)>
df1/axis0           <HDF5 dataset "axis0": shape (5,), type "<i8">
df1/axis1           <HDF5 dataset "axis1": shape (5,), type "<i8">
df1/block0_items    <HDF5 dataset "block0_items": shape (5,), type "<i8">
df1/block0_values   <HDF5 dataset "block0_values": shape (5, 5), type "<f8">
df2                 <HDF5 group "/df2" (8 members)>
df2/axis0           <HDF5 dataset "axis0": shape (21,), type "|S8">
df2/axis1           <HDF5 dataset "axis1": shape (30,), type "<i8">
df2/block0_items    <HDF5 dataset "block0_items": shape (3,), type "|S8">
df2/block0_values   <HDF5 dataset "block0_values": shape (30, 3), type "<f8">
df2/block1_items    <HDF5 dataset "block1_items": shape (14,), type "|S4">
df2/block1_values   <HDF5 dataset "block1_values": shape (30, 14), type "<i8">
df2/block2_items    <HDF5 dataset "block2_items": shape (4,), type "|S6">
df2/block2_values   <HDF5 dataset "block2_values": shape (1,), type "|O8">

```

我们可以看到,`HDFStore`对象将每个`DataFrame`对象存储在其自己的组中，并且它将每个数据帧分割成几个异构的 HDF5 数据集(块),其中的列按其数据类型分组。此外，列名和值存储在单独的 HDF5 数据集中。

```py
In [140]: f["/df2/block0_items"].value
Out[140]: array([b'S%', b'Shift/GP', b'FO%'], dtype='|S8')
In [141]: f["/df2/block0_values"][:3]
Out[141]: array([[ 13.9,  24\. ,  52.5],
                 [ 15.2,  25.2,  49\. ],
                 [ 12.6,  25.1,  52.9]])
In [142]: f["/df2/block1_values"][:3, :5]
Out[142]: array([[  1,  80,  36,  68, 104],
                 [  2,  77,  31,  56,  87],
                 [  3,  82,  28,  58,  86]]) 

```

## 数据

JSON<sup>[5](#Fn5)</sup>(JavaScript 对象符号)是一种人类可读的轻量级纯文本格式，适合存储由列表和字典组成的数据集。这种列表和字典的值本身可以是列表或字典，或者必须是以下基本数据类型:字符串、整数、浮点和布尔，或者值`null`(像 Python 中的`None`值)。这种数据模型允许存储复杂和通用的数据集，没有结构限制，例如 CSV 等格式所需的表格形式。例如，JSON 文档可以用作键值存储，其中不同键值可以有不同的结构和数据类型。

JSON 格式主要被设计为在 web 服务和 JavaScript 应用程序之间传递信息的数据交换格式。事实上，JSON 是 JavaScript 语言的一个子集，因此是一个有效的 JavaScript 代码。然而，JSON 格式本身是一种独立于语言的数据格式，基本上可以从每种语言和环境(包括 Python)中解析和生成。JSON 语法也几乎是有效的 Python 代码，这使得在 Python 中使用它既熟悉又直观。

在第 [10](10.html) 章中，我们已经看到了一个 JSON 数据集的例子，其中我们查看了东京地铁网络的图表。在我们再次访问该数据集之前，我们先简要概述一下 JSON 基础知识以及如何在 Python 中读写 JSON。Python 标准库提供了用于处理 JSON 格式数据的模块`json`。具体来说，这个模块包含从 Python 数据结构(列表或字典)、`json.dump`和`json.dumps`生成 JSON 数据的函数，以及将 JSON 数据解析成 Python 数据结构:`json.load`和`json.loads`的函数。函数`loads`和`dumps`将 Python 字符串作为输入和输出，而`load`和`dump`对文件句柄进行操作，并将数据读写到文件中。

例如，我们可以通过调用`json.dumps`函数来生成 Python 列表的 JSON 字符串。返回值是给定 Python 列表的 JSON 字符串表示，非常类似于可用于创建列表的 Python 代码。然而，一个值得注意的例外是 Python 值`None`，它在 JSON 中被表示为值`null`:

```py
In [143]: data = ["string", 1.0, 2, None]
In [144]: data_json = json.dumps(data)
In [145]: data_json
Out[145]: '["string", 1.0, 2, null]'

```

要将 JSON 字符串转换回 Python 对象，我们可以使用`json.loads`:

```py
In [146]: data = json.loads(data_json)
In [147]: data
Out[147]: ['string', 1.0, 2, None]
In [148]: data[0]
Out[148]: 'string'

```

我们可以使用完全相同的方法将 Python 字典存储为 JSON 字符串。同样，生成的 JSON 字符串本质上与定义字典的 Python 代码相同:

```py
In [149]: data = {"one": 1, "two": 2.0, "three": "three"}
In [150]: data_json = json.dumps(data)
In [151]: data_json
Out[151]: '{"two": 2.0, "three": "three", "one": 1}'

```

为了解析 JSON 字符串并将其转换回 Python 对象，我们再次使用`json.loads`:

```py
In [152]: data = json.loads(data_json)
In [153]: data["two"]
Out[153]: 2.0
In [154]: data["three"]
Out[154]: 'three'

```

列表和字典的结合构成了一个通用的数据结构。例如，我们可以存储元素数量可变的列表或列表字典。这种类型的数据很难直接存储为表格数组，更高级别的嵌套列表和字典会使它变得非常不切实际。当使用`json.dump`和`json.dumps`函数生成 JSON 数据时，我们可以选择给出参数`indent=True`，以获得更易于阅读的缩进 JSON 代码:

```py
In [155]: data = {"one": [1],
     ...:         "two": [1, 2],
     ...:         "three": [1, 2, 3]}
In [156]: data_json = json.dumps(data, indent=True)
In [157]: data_json
Out[157]: {
            "two": [
             1,
             2
             ],
            "three": [
             1,
             2,
             3
            ],
            "one": [
             1
            ]
          }

```

作为更复杂的数据结构的例子，考虑包含列表、字典、元组列表和文本串的字典。我们可以使用与前面文本中相同的方法，使用`json.dumps`生成数据结构的 JSON 表示，但是这里我们使用`json.dump`函数将内容写入文件。与`json.dumps`相比，它额外接受了一个文件句柄作为第二个参数，这是我们需要预先创建的:

```py
In [158]: data = {"one": [1],
     ...:         "two": {"one": 1, "two": 2},
     ...:         "three": [(1,), (1, 2), (1, 2, 3)],
     ...:         "four": "a text string"}
In [159]: with open("data.json", "w") as f:
     ...:     json.dump(data, f)

```

结果是 Python 数据结构的 JSON 表示被写入文件`data.json`:

```py
In [160]: !cat data.json
{"four": "a text string", "two": {"two": 2, "one": 1}, "three": [[1], [1, 2], [1, 2, 3]],
 "one": [1]}

```

要读取 JSON 格式的文件并将其解析为 Python 数据结构，我们可以使用`json.load`，我们需要向它传递一个打开文件的句柄:

```py
In [161]: with open("data.json", "r") as f:
     ...:     data_from_file = json.load(f)
In [162]: data_from_file["two"]
Out[162]: [1, 2]
In [163]: data_from_file["three"]
Out[163]: [[1], [1, 2], [1, 2, 3]]

```

`json.load`返回的数据结构并不总是与`json.dump`存储的数据结构相同。特别是 JSON 是以 Unicode 存储的，所以`json.load`返回的数据结构中的字符串总是 Unicode 字符串。同样，正如我们从前面的例子中看到的，JSON 不区分元组和列表，并且`json.load`总是产生列表而不是元组，并且字典的键的显示顺序没有保证，除非对`dumps`和`dump`函数使用`sorted_keys=True`参数。

既然我们已经看到了如何使用`json`模块将 Python 列表和字典与 JSON 表示相互转换，那么有必要重温一下第 [10 章](10.html)中的东京地铁数据集。这是一个更真实的数据集，也是一个混合了字典、可变长度列表和字符串值的数据结构示例。JSON 文件的前 20 行如下所示:

```py
In [164]: !head -n 20 tokyo-metro.json
{
     "C": {
         "color": "#149848",
          "transfers": [
             [
                 "C3",
                  "F15"
             ],
              [
                 "C4",
                  "Z2"
             ],
             [
                 "C4",
                  "G2"
             ],
              [
                 "C7",
                  "M14"
             ],

```

为了将 JSON 数据加载到 Python 数据结构中，我们以与之前相同的方式使用`json.load`:

```py
In [165]: with open("tokyo-metro.json", "r") as f:
     ...:     data = json.load(f)

```

结果是一个字典，其中包含每条地铁线路的关键字:

```py
In [166]: data.keys()
Out[166]: ['N', 'M', 'Z', 'T', 'H', 'C', 'G', 'F', 'Y']

```

每条地铁线路的字典值也是包含线路颜色、换乘点列表和线路上各站之间的行驶时间的字典:

```py
In [167]: data["C"].keys()
Out[167]: ['color', 'transfers', 'travel_times']
In [168]: data["C"]["color"]
Out[168]: '#149848'
In [169]: data["C"]["transfers"]
Out[169]: [['C3', 'F15'],  ['C4', 'Z2'],  ['C4', 'G2'],  ['C7', 'M14'],  ['C7', 'N6'],
           ['C7', 'G6'],  ['C8', 'M15'],  ['C8', 'H6'],  ['C9', 'H7'],  ['C9', 'Y18'],
           ['C11', 'T9'],  ['C11', 'M18'],  ['C11', 'Z8'],  ['C12', 'M19'],  ['C18', 'H21']]

```

使用作为 Python 字典和列表的嵌套结构加载的数据集，我们可以轻松地迭代和过滤数据结构中的项目，例如，使用 Python 的列表理解语法。以下示例演示了如何选择图表中行驶时间为 1 分钟的 C 线上的连接节点集:

```py
In [170]: [(s, e, tt) for s, e, tt in data["C"]["travel_times"] if tt == 1]
Out[170]: [('C3', 'C4', 1), ('C7', 'C8', 1), ('C9', 'C10', 1)]

```

字典的层次结构和存储在字典中的可变长度的列表使它成为一个没有严格结构的数据集的很好的例子，因此它适合以多种格式存储，比如 JSON。

## 序列化

在上一节中，我们使用 JSON 格式生成内存中 Python 对象的表示，比如列表和字典。这个过程被称为序列化，在本例中，它产生了对象的 JSON 纯文本表示。JSON 格式的一个优点是它是独立于语言的，可以很容易地被其他软件读取。它的缺点是 JSON 文件空间效率不高，只能用来序列化有限类型的对象(列表、字典、基本类型，前面已经讨论过)。有许多替代的序列化技术可以解决这些问题。在这里，我们将简要地看一下解决空间效率问题和可以序列化的对象类型的两种替代方法:msgpack 库和 Python `pickle`模块。

我们从 msgpack 开始，它是一个用于高效存储 JSON 类数据的二进制协议。msgpack 软件可用于多种语言和环境。有关该库及其 Python 绑定的更多信息，请参见位于 [`http://msgpack.org`](http://msgpack.org) 的项目网页。与 JSON 模块类似，`msgpack`库提供了两组函数，分别操作字节列表(`msgpack.packb`和`msgpack.unpackb`)和文件句柄(`msgpack.pack`和`msgpack.unpack`)。`pack`和`packb`函数将 Python 数据结构转换成二进制表示，而`unpack`和`unpackb`函数执行相反的操作。例如，东京地铁数据集的 JSON 文件相对较大，大约需要 27 Kb 的磁盘空间:

```py
In [171]: !ls -lh tokyo-metro.json
-rw-r--r--@ 1 rob  staff    27K Apr  7 23:18 tokyo-metro.json

```

用 msgpack 而不是 JSON 打包数据结构会产生一个相当小的文件，大约 3 Kb:

```py
In [172]: data_pack = msgpack.packb(data)
In [173]: type(data_pack)
Out[173]: bytes
In [174]: len(data_pack)
Out[174]: 3021
In [175]: with open("tokyo-metro.msgpack", "wb") as f:
     ...:     f.write(data_pack)
In [176]: !ls -lh tokyo-metro.msgpack
-rw-r--r--@ 1 rob  staff   3.0K Apr  8 00:40 tokyo-metro.msgpack

```

更准确地说，数据集的字节列表表示只使用了 3021 个字节。在存储空间或带宽至关重要的应用中，这可能是一个显著的改进。然而，我们为提高存储效率付出的代价是，我们必须使用 msgpack 库来解包数据，它使用二进制格式，因此不可读。这是否是一个可接受的折衷将取决于手边的应用。要解包一个二进制`msgpack`字节列表，我们可以使用`msgpack.unpackb`函数，它恢复原始数据结构:

```py
In [177]: del data
In [178]: with open("tokyo-metro.msgpack", "rb") as f:
     ...:     data_msgpack = f.read()
     ...:     data = msgpack.unpackb(data_msgpack)
In [179]: list(data.keys())
Out[179]: ['T', 'M', 'Z', 'H', 'F', 'C', 'G', 'N', 'Y']

```

JSON 序列化的另一个问题是，只有某些类型的 Python 对象可以存储为 JSON。Python `pickle`模块 <sup>[6](#Fn6)</sup> 可以创建几乎任何种类的 Python 对象的二进制表示，包括类实例和函数。使用`pickle`模块遵循与`json`模块完全相同的使用模式:我们有`dump`和`dumps`函数分别将一个对象序列化为一个字节数组和一个文件句柄，还有`load`和`loads`用于反序列化一个 pickled 对象。

```py
In [180]: with open("tokyo-metro.pickle", "wb") as f:
     ...:     pickle.dump(data, f)
In [181]: del data
In [182]: !ls -lh tokyo-metro.pickle
-rw-r--r--@ 1 rob  staff   8.5K Apr  8 00:40 tokyo-metro.pickle

```

pickled 对象的大小比 JSON 序列化小得多，但比 msgpack 产生的序列化大。我们可以使用`pickle.load`函数恢复一个 pickled 对象，该函数需要一个文件句柄作为参数:

```py
In [183]: with open("tokyo-metro.pickle", "rb") as f:
     ...:     data = pickle.load(f)
In [184]: data.keys()
Out[184]: dict_keys(['T', 'M', 'Z', 'H', 'F', 'C', 'G', 'N', 'Y'])

```

pickle 的主要优点是几乎任何类型的 Python 对象都可以被序列化。然而，Python pickles 不能被不是用 Python 编写的软件读取，并且它也不是长期存储的推荐格式，因为 Python 版本之间以及与定义被 pickle 的对象的不同版本的库之间的兼容性不能总是得到保证。如果可能的话，使用 JSON 来序列化基于列表和字典的数据结构通常是一种更好的方法，如果文件大小是一个问题，`msgpack`库提供了一种流行且易于访问的 JSON 替代方法。

## 摘要

在这一章中，我们回顾了在磁盘上读写数字数据的常见数据格式，并介绍了一些可用于处理这些格式的 Python 库。我们首先查看了无处不在的 CSV 文件格式，这是一种简单透明的格式，适用于小而简单的数据集。这种格式的主要优点是它是人类可读的纯文本，这使得它可以直观地理解。然而，它缺少许多在处理数字数据时所需要的特性，比如描述数据的元数据和对多个数据集的支持。当数据的大小和复杂性超出了使用 CSV 格式可以轻松处理的范围时，HDF5 格式自然会成为数字数据的首选格式。HDF5 是一种二进制文件格式，因此它不像 CSV 那样是人类可读的格式，但有一些很好的工具可以用来探索 HDF5 文件中的内容，既可以通过编程方式，也可以使用命令行和基于 GUI 的用户界面。事实上，由于在属性中存储元数据的可能性，HDF5 是自描述数据的一种很好的格式。就 I/O 和存储而言，它也是一种非常有效的数字数据文件格式，甚至可以用作数据模型来计算不适合计算机内存的非常大的数据集。总的来说，HDF5 是一款出色的数值计算工具，任何从事计算工作的人都会因熟悉它而受益匪浅。在本章的最后，我们还简要回顾了 JSON、msgpack 和 Python pickles 将数据序列化为文本和二进制格式。

## 进一步阅读

RFC 4180， [`http://tools.ietf.org/html/rfc4180`](http://tools.ietf.org/html/rfc4180) 中给出了 CSV 文件的非正式规范。它概述了 CSV 格式的许多常用功能，尽管并非所有 CSV 阅读器和编写器都符合本文档的每个方面。h5py 的创建者在 Collette (2013)中对 HDF5 格式和 h5py 库进行了简单易懂、内容丰富的介绍。关于 NetCDF(网络通用数据格式)、 [`www.unidata.ucar.edu/software/netcdf`](http://www.unidata.ucar.edu/software/netcdf) 也值得一读，这是另一种广泛使用的数值数据格式。除了我们在这里讨论的，Pandas 库还提供了 I/O 功能，比如读取 Excel 文件(`pandas.io.excel.read_excel`)和固定宽度格式(`read_fwf`)。关于 JSON 格式，在 [`http://json.org`](http://json.org) 网站上可以找到该格式的简明而完整的规范。随着数据在计算中日益重要的作用，近年来格式和数据存储技术迅速多样化。作为一名计算从业者，从数据库中读取数据，比如 SQL 和 NoSQL 数据库，现在也是一项重要的任务。Python 提供了一个通用的数据库 API，用于标准化来自 Python 应用的数据库访问，如 PEP 249 ( [`www.python.org/dev/peps/pep-0249`](http://www.python.org/dev/peps/pep-0249) )所述。从 Python 读取数据库的另一个值得注意的项目是 SQLAlchemy ( [`www.sqlalchemy.org`](http://www.sqlalchemy.org) )。

## 参考

Collette，A. (2013)。 *Python 和 HDF5。*塞瓦斯托波尔:奥莱利。

<aside class="FootnoteSection" epub:type="footnotes">Footnotes [1](#Fn1_source)

虽然 RFC 4180[http://tools . IETF . org/html/RFC 4180](http://tools.ietf.org/html/rfc4180)有时被视为非官方规范，但实际上存在许多 CSV 的变体和方言。

  [2](#Fn2_source)

[T2`www.hdfgroup.org`](http://www.hdfgroup.org)

  [3](#Fn3_source)

这也称为核外计算。另一个最近的项目也提供了 Python 的核外计算能力，参见 dask 库([http://dask . pydata . org/en/latest](http://dask.pydata.org/en/latest))。

  [4](#Fn4_source)

注意 PyTables 库提供的 Python 模块名为`tables`。因此，`tables.open_file`引用 PyTables 库提供的`tables`模块中的`open_file`函数。

  [5](#Fn5_source)

有关 JSON 的更多信息，请参见 [`http://json.org`](http://json.org) 。

  [6](#Fn6_source)

`pickle`模块的替代是`cPickle`模块，这是一个更有效的重新实现，在 Python 标准库中也有。参见 [`https://pypi.org/project/dill`](https://pypi.org/project/dill) 的`dill`图书馆。

 </aside>