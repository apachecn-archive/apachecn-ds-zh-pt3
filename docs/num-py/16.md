# 十六、贝叶斯统计

在这一章中，我们探索统计学的另一种解释——贝叶斯统计——以及与这种解释相关的方法。与我们在第 [13 章](13.html)和第 [14 章](14.html)中使用的频率统计相反，贝叶斯统计将概率视为一种信任度，而不是观察结果比例的度量。这种不同的观点产生了我们可以用来解决问题的不同的统计方法。虽然统计问题原则上可以使用频率统计或贝叶斯统计来解决，但这两种统计方法适用于不同类型的问题是有实际差异的。

贝叶斯统计基于贝叶斯定理，它将条件概率和无条件概率联系起来。贝叶斯定理是概率论中的一个基本结果，它适用于频率主义者和统计的贝叶斯解释。在贝叶斯推理的背景下，无条件概率用于描述系统的先验知识，贝叶斯定理提供了在进行新的观察后更新这些知识的规则。更新的知识由条件概率描述，条件概率以观察到的数据为条件。系统的初始知识由*先验概率分布*描述，以观测数据为条件的更新知识是*后验概率分布*。在使用贝叶斯统计解决问题时，后验概率分布是我们寻求的未知量，从中我们可以计算期望值和感兴趣的随机变量的其他统计量。虽然贝叶斯定理描述了如何从先验分布计算后验分布，但对于大多数现实问题，计算涉及评估高维积分，这在解析和数值上都非常难以计算。直到最近，这才阻碍了贝叶斯统计在实践中的广泛应用。然而，随着计算统计学的出现，以及允许我们直接从后验分布中采样(而不是直接计算)的有效模拟方法的发展，贝叶斯方法变得越来越受欢迎。使我们能够从后验分布抽样的方法首先是所谓的马尔可夫链蒙特卡罗(MCMC)方法。MCMC 方法的几种替代实现是可用的。例如，传统的 MCMC 方法包括 Gibbs 采样和 Metropolis-Hastings 算法，而最近的方法包括 Hamiltonian 算法和不掉头算法。在这一章中，我们将探讨如何使用这些方法。

用贝叶斯推理方法解决统计问题有时被称为*概率规划*。概率规划的关键步骤如下:(1)建立统计模型。(2)使用 MCMC 方法从感兴趣的量的后验分布中取样。(3)使用所获得的后验分布来计算手头问题的感兴趣的属性，并基于所获得的结果做出推断决定。在本章中，我们将探索如何在 PyMC 库的帮助下，在 Python 环境中执行这些步骤。

### PyMc

PyMC 库，目前称为 PyMC3，提供了一个进行概率编程的框架——也就是说，使用贝叶斯方法的模拟来解决统计问题。在撰写本文时，最新的官方版本是 3.4.1 版。有关该项目的更多信息，请参见 [`http://docs.pymc.io`](http://docs.pymc.io) 的网页。

## 导入模块

在这一章中，我们主要使用`pymc3`库，它以如下方式导入:

```py
In [1]: import pymc3 as mc

```

我们还需要 NumPy、Pandas 和 Matplotlib 分别用于基本的数字、数据分析和绘图。这些库是按照通常的惯例导入的:

```py
In [2]: import numpy as np
In [3]: import pandas as pd
In [4]: import matplotlib.pyplot as plt

```

为了与非贝叶斯统计进行比较，我们还使用 SciPy 的`stats`模块、`statsmodels`库和 Seaborn 库进行可视化:

```py
In [5]: from scipy import stats
In [6]: import statsmodels.api as sm
In [7]: import statsmodels.formula.api as smf
In [8]: import seaborn as sns

```

## 贝叶斯统计简介

贝叶斯统计的基础是贝叶斯定理，它给出了两个事件 *A* 和 *B* 的无条件概率和条件概率之间的关系:

![$$ P\left(A|B\right)P(B)=P\left(B|A\right)P(A), $$](img/332789_2_En_16_Chapter/332789_2_En_16_Chapter_TeX_Equa.png)

其中 *P* ( *A* )和 *P* ( *B* )是事件 *A* 和 *B* 的无条件概率，其中*P*(*A*|*B*)是假设事件 *B* 为真且*P*为事件 *A* 的条件概率上式两边等于 *A* 和 *B* 都为真的概率:*P*(*A*∩*B*)。换句话说，贝叶斯法则规定了在给定 A 为真的情况下， *A* 和 *B* 的概率都等于 *A* 的概率乘以 *B* 的概率，*P*(*A*)*P*(*B*|*A*)，或者等价地，概率为

 *在贝叶斯推理的背景下，贝叶斯规则通常用于这样的情况:当我们对事件 *A* 的概率有一个先验信念，由无条件概率 *P* ( *A* )表示，并且希望在观察到事件 *B* 后更新这个信念。在这种语言中，在给定观察值*B*:*P*(*A*|*B*)的情况下，更新后的信念由条件概率 *A* 表示，我们可以使用贝叶斯法则来计算:

![$$ P\left(A|B\right)=\frac{P\left(B|A\right)P(A)}{P(B)}. $$](img/332789_2_En_16_Chapter/332789_2_En_16_Chapter_TeX_Equb.png)

这个表达式中的每个因子都有不同的解释和名称: *P* ( *A* )是事件 *A* 的*先验*概率，*P*(*A*|*B*)是给定观测值 *B* 的*后验*概率。*P*(*B*|*A*)是假设 *A* 为真，观测到 *B* 的概率不考虑 *A* 、 *P* ( *B* )，称为*模型证据*并能

在统计建模中，我们通常对一组随机变量 *X* 感兴趣，这些变量的特征是具有特定参数 *θ* 的概率分布。在为我们感兴趣的建模过程收集数据之后，我们希望从数据中推断出模型参数的值。在 frequentist 的统计方法中，我们可以在给定观察数据的情况下最大化似然函数，并获得模型参数的估计量。贝叶斯方法是将未知的模型参数 *θ* 视为本身就是随机变量，并使用贝叶斯规则来导出模型参数 *θ* 的概率分布。如果我们将观测数据表示为 *x* ，我们可以使用贝叶斯法则将 *θ* 的概率分布表示为

![$$ p\left(\theta |x\right)=\frac{p\left(x|\theta \right)p\left(\theta \right)}{p(x)}=\frac{p\left(x|\theta \right)p\left(\theta \right)}{\int p\left(x|\theta \right)p\left(\theta \right) d\theta}. $$](img/332789_2_En_16_Chapter/332789_2_En_16_Chapter_TeX_Equc.png)

这个等式中的第二个等式是根据全概率定律得出的，*p*(*x*)=∫*p*(*x*|*θ*)*p*(*θ*)*dθ*。一旦我们计算了模型参数的后验概率分布*p*(*θ*|*x*)，我们就可以，例如，计算模型参数的期望值，并获得类似于我们可以在 frequentist 方法中计算的估计量的结果。此外，当我们有了对*p*(*θ*|*x*)的全概率分布的估计时，我们还可以计算其他的量，比如可信区间，以及在 *θ* 是多元的情况下某些模型参数的边际分布。例如，如果我们有两个模型参数，*θ*=(*θ*<sub>1</sub>， *θ* <sub>2</sub> )，但只对 *θ* <sub>1</sub> 感兴趣，我们可以通过下式得到边际后验概率分布*p*(*θ*<sub>1</sub>|*x*

![$$ p\left({\theta}_1|x\right)=\int p\left({\theta}_1,{\theta}_2|x\right)d{\theta}_2=\frac{\int p\left(x|{\theta}_1,{\theta}_2\right)p\left({\theta}_1,{\theta}_2\right)d{\theta}_2}{\int \int p\left(x|{\theta}_1,{\theta}_2\right)p\left({\theta}_1,{\theta}_2\right)d{\theta}_1d{\theta}_2}. $$](img/332789_2_En_16_Chapter/332789_2_En_16_Chapter_TeX_Equd.png)

这里需要注意的是，最终表达式包含了已知似然函数*p*(*x*|*θ*<sub>1</sub>， *θ* <sub>2</sub> )和先验分布*p*(*θ*<sub>1</sub>， *θ* <sub>2</sub> )的积分，所以我们不需要知道联合概率分布 *p *θ*<sub>2</sub>|*x*计算边际概率分布*p*(*θ*<sub>1</sub>|*x*)。 这种方法为计算模型参数的概率分布并在新数据可用时连续更新分布提供了一种强大而通用的方法。然而，直接计算*p*(*θ*|*x*)，或者它们的边际分布，要求我们能够写出似然函数*p*(*x*|*θ*)和先验分布 *p* ( *θ* )，并且我们能够评估由此产生的积分。对于许多简单但重要的问题，有可能解析地计算这些积分，并找到后验分布的精确闭式表达式。Gelman (2013)等教科书提供了许多可以用这种方法精确解决的问题的例子。然而，对于具有先验分布和似然函数的更复杂的模型，所得到的积分不容易评估，或者对于多元统计模型，所得到的积分可能是高维的，精确和数值评估可能都是不可行的。*

主要是对于无法用精确方法解决的模型，我们可以从使用模拟方法中受益，例如马尔可夫链蒙特卡罗，它允许我们对模型参数的后验概率分布进行采样，从而构建联合或边际后验分布的近似值，或者直接评估积分，例如期望值。基于模拟的方法的另一个重要优势是建模过程可以自动化。在这里，我们专门关注使用蒙特卡罗模拟方法的贝叶斯统计建模。对于理论的全面回顾，和许多解析可解问题的例子，见本章末尾给出的参考文献。在本章的剩余部分，我们将使用 PyMC 库作为概率编程框架，探索统计模型的定义及其后验分布的抽样。

在我们开始计算贝叶斯统计之前，有必要花点时间总结一下贝叶斯方法和我们在前面章节中使用的经典频率主义者方法之间的主要区别:在这两种统计建模方法中，我们都用随机变量来表示模型。定义统计模型的一个关键步骤是对模型中定义的随机变量的概率分布进行假设。在参数方法中，每个概率分布由少量参数表征。在 frequentist 的方法中，那些模型参数具有一些特定的真值，并且观察到的数据被解释为来自真分布的随机样本。换句话说，假设模型参数是固定的，假设数据是随机的。贝叶斯方法采取相反的观点:数据被解释为固定的，模型参数被描述为随机变量。从模型参数的先验分布开始，我们可以更新该分布以说明观察到的数据，并最终根据观察到的数据获得相关模型参数的概率分布。

## 模型定义

统计模型是根据一组随机变量定义的。给定模型中的随机变量可以是独立的，或者更有趣的是，可以是相互依赖的。PyMC 库提供了表示大量概率分布的随机变量的类:例如，`mc.Normal`的一个实例可以用来表示一个正态分布的随机变量。其他的例子有代表离散伯努利分布随机变量的`mc.Bernoulli`、代表均匀分布随机变量的`mc.Uniform`、代表伽马分布随机变量的`mc.Gamma`等等。关于可用发行版的完整列表，参见`dir(mc.distributions)`和每个可用发行版的文档字符串，了解如何使用它们。也可以使用`mc.DensityDist`类来定义定制分布，它采用一个函数来指定随机变量的概率密度函数的对数。

在第 13 章[中，我们看到 SciPy `stats`模块也包含了表示随机变量的类。像 SciPy `stats`中的随机变量类一样，我们可以使用 PyMC 分布来表示具有固定参数的随机变量。然而，PyMC 随机变量的本质特征是分布参数，例如服从正态分布![$$ \mathcal{N}\left(\mu, {\sigma}^2\right) $$](img/332789_2_En_16_Chapter/332789_2_En_16_Chapter_TeX_IEq1.png)的随机变量的均值 *μ* 和方差*σ*T8】2 本身可以是随机变量。这允许我们将模型中的随机变量链接起来，并在模型中出现的随机变量之间的依赖关系中用层次结构来制定模型。](13.html)

让我们从最简单的例子开始。在 PyMC 中，模型由类`mc.Model`的实例表示，随机变量使用 Python 上下文语法添加到模型中:在模型上下文主体中创建的随机变量实例被自动添加到模型中。假设我们对一个由单个随机变量组成的模型感兴趣，该随机变量遵循正态分布，具有固定参数 *μ* = 4 和 *σ* = 2。我们首先定义固定的模型参数，然后创建一个`mc.Model`的实例来表示我们的模型。

```py
In [9]: mu = 4.0
In [10]: sigma = 2.0
In [11]: model = mc.Model()

```

接下来，我们可以通过在模型上下文中创建随机变量来将它们附加到模型中。这里，我们在模型上下文中创建一个随机变量`X`，它是使用一个`with model`语句激活的:

```py
In [12]: with model:
    ...:     mc.Normal('X', mu, tau=1/sigma**2)

```

PyMC 中的所有随机变量类都将变量名作为第一个参数。在`mc.Normal`的情况下，第二个自变量是正态分布的均值，第三个自变量`tau`是精度 *τ* = 1/ *σ* <sup>2</sup> ，其中 *σ* <sup>2</sup> 是方差。或者，我们可以使用`sd`关键字参数来指定标准差，而不是精度:`mc.Normal('X', mu, sd=sigma)`。

我们可以使用`vars`属性检查模型中存在哪些随机变量。这里我们在模型中只有一个随机变量:

```py
In [13]: model.vars
Out[13]: [X]

```

为了对模型中的随机变量进行采样，我们使用了`mc.sample`函数，它实现了 MCMC 算法。`mc.sample`函数接受许多参数，但是至少，我们需要提供样本的数量作为第一个参数，作为第二个参数，一个 step 类实例，它实现了一个 MCMC 步骤。可选地，我们也可以使用`start`关键字参数，提供一个带有参数值的字典作为开始采样的起点。对于 step 方法，这里我们使用了一个`Metropolis`类的实例，它为 MCMC 采样器 <sup>[1](#Fn1)</sup> 实现了 Metropolis-Hastings step 方法。注意，我们在模型上下文中执行所有与模型相关的代码:

```py
In [14]: start = dict(X=2)
In [15]: with model:
    ...:     step = mc.Metropolis()
    ...:     trace = mc.sample(10000, start=start, step=step)
[-----------------100%-----------------] 10000 of 10000 complete in 1.6 sec

```

通过这些步骤，我们从模型中定义的随机变量中抽取了 10000 个值，在这个简单的例子中，它只是一个正态分布的随机变量。为了访问样本，我们可以使用由`mc.sample`函数返回的 trace 对象的`get_values`方法:

```py
In [16]: X = trace.get_values("X")

```

当然，正态分布的概率密度函数(PDF)在分析上是已知的。使用 SciPy `stats`模块，我们可以使用`norm`类实例的`pdf`方法访问 PDF，以便与采样的随机变量进行比较。当前模型的采样值和真实 PDF 如图 [16-1](#Fig1) 所示。

![img/332789_2_En_16_Chapter/332789_2_En_16_Fig1_HTML.png](img/332789_2_En_16_Chapter/332789_2_En_16_Fig1_HTML.png)

图 16-1

正态分布随机变量的概率密度函数(红色/粗线)和 10000 个 MCMC 样本的直方图

```py
In [17]: x = np.linspace(-4, 12, 1000)
In [18]: y = stats.norm(mu, sigma).pdf(x)
In [19]: fig, ax = plt.subplots(figsize=(8, 3))
    ...: ax.plot(x, y, 'r', lw=2)
    ...: sns.distplot(X, ax=ax)
    ...: ax.set_xlim(-4, 12)
    ...: ax.set_xlabel("x")
    ...: ax.set_ylabel("Probability distribution")

```

使用`mc.traceplot`函数，我们还可以可视化生成样本的 MCMC 随机游走，如图 [16-2](#Fig2) 所示。`mc.traceplot`函数自动绘制模型中每个随机变量的核密度估计和采样轨迹。

![img/332789_2_En_16_Chapter/332789_2_En_16_Fig2_HTML.jpg](img/332789_2_En_16_Chapter/332789_2_En_16_Fig2_HTML.jpg)

图 16-2

左图:采样轨迹的密度核估计(蓝色/粗线)和正态概率分布(红色/细线)。右图:MCMC 采样轨迹

```py
In [20]: fig, axes = plt.subplots(1, 2, figsize=(8, 2.5), squeeze=False)
    ...: mc.traceplot(trace, ax=axes)
    ...: axes[0, 0].plot(x, y, 'r', lw=0.5)

```

作为构建更复杂的统计模型的下一步，再次考虑一个具有正态分布随机变量![$$ X\sim \mathcal{N}\left(\mu, {\sigma}^2\right) $$](img/332789_2_En_16_Chapter/332789_2_En_16_Chapter_TeX_IEq2.png)的模型，但是其中参数 *μ* 和 *σ* 本身是随机变量。在 PyMC 中，当创建其他随机变量时，我们可以通过将它们作为参数传递来轻松创建因变量。例如，使用![$$ \mu \sim \mathcal{N}\left(3,1\right) $$](img/332789_2_En_16_Chapter/332789_2_En_16_Chapter_TeX_IEq3.png)和![$$ \sigma \sim \mid \mathcal{N}\left(0,1\right)\mid, $$](img/332789_2_En_16_Chapter/332789_2_En_16_Chapter_TeX_IEq4.png)，我们可以使用以下模型规范创建相关随机变量 *X* :

```py
In [21]: model = mc.Model()
In [22]: with model:
    ...:     mean = mc.Normal('mean', 3.0)
    ...:     sigma = mc.HalfNormal('sigma', sd=1.0)
    ...:     X = mc.Normal('X', mean, sd=sigma)

```

这里我们使用了`mc.HalfNormal`来表示随机变量![$$ \sigma \sim \mid \mathcal{N}\left(0,1\right)\mid, $$](img/332789_2_En_16_Chapter/332789_2_En_16_Chapter_TeX_IEq5.png)，并且对于 *X* 的`mc.Normal`类的均值和标准差参数是随机变量实例，而不是固定的模型参数。像以前一样，我们可以使用`vars`属性来检查一个模型包含哪些随机变量。

```py
In [23]: model.vars
Out[23]: [mean, sigma_log__, X]

```

注意，这里的`pymc3`库用一个经过对数变换的变量`sigma_log__`来表示`sigma`变量，作为处理半正态分布的一种手段。尽管如此，我们仍然可以从模型中直接访问`sigma`变量，如下文所示。

当模型的复杂性增加时，为采样过程明确地选择一个合适的起始点可能不再简单。`mc.find_MAP`函数可用于找到参数空间中对应于后验分布最大值的点，该点可作为采样过程的良好起点。

```py
In [24]: with model:
    ...:     start = mc.find_MAP()
In [25]: start
Out[25]: {'X': array(3.0), 'mean': array(3.0),
          'sigma': array(0.70710674), 'sigma_log__': array(-0.34657365)}

```

如前所述，一旦指定了模型，并且计算了起点，我们就可以使用`mc.sample`函数从模型中的随机变量中进行采样，例如，使用`mc.Metropolis`作为 MCMC 采样步长方法:

```py
In [26]: with model:
    ...:     step = mc.Metropolis()
    ...:     trace = mc.sample(100000, start=start, step=step)
[-----------------100%-----------------] 100000 of 100000 complete in 53.4 sec

```

例如，为了获得变量`sigma`的样本轨迹，我们可以使用`get_values('sigma')`。结果是一个包含样本值的 NumPy 数组，从中我们可以计算进一步的统计数据，例如样本均值和标准差:

```py
In [27]: trace.get_values('sigma').mean()
Out[27]: 0.80054476153369014

```

可以使用相同的方法获得`X`的样本，并从中计算统计数据:

```py
In [28]: X = trace.get_values('X')
In [29]: X.mean()
Out[29]: 2.9993248663922092
In [30]: trace.get_values('X').std()
Out[30]: 1.4065656512676457

```

使用`mc.traceplot`创建的当前模型的轨迹图如图 [16-3](#Fig3) 所示，其中我们使用`varnames`参数给`mc.traceplot`来明确选择要绘制的随机变量。

![img/332789_2_En_16_Chapter/332789_2_En_16_Fig3_HTML.jpg](img/332789_2_En_16_Chapter/332789_2_En_16_Fig3_HTML.jpg)

图 16-3

三个随机变量:`mean`、`sigma`和`X`的核密度估计(左)和 MCMC 随机采样轨迹(右)

```py
In [31]: fig, axes = plt.subplots(3, 2, figsize=(8, 6), squeeze=False)
    ...: mc.traceplot(trace, varnames=['mean', 'sigma', 'X'], ax=axes)

```

### 抽样后验分布

到目前为止，我们已经定义了模型，并从只包含随机变量的模型中取样，而没有参考任何观察到的数据。在贝叶斯模型的上下文中，这些类型的随机变量表示未知模型参数的先验分布。因此，在前面的示例中，我们使用 MCMC 方法从模型的先验分布中进行采样。然而，MCMC 算法的实际应用是从后验分布中进行采样，后验分布表示在更新先验分布以考虑观察结果的影响之后模型变量的概率分布。

为了使模型以观察到的数据为条件，我们需要做的就是在模型中创建相应的随机变量时，使用`observed`关键字参数添加数据:例如，`mc.Normal('X', mean, 1/sigma**2, observed=data)`表示已经观察到随机变量`X`取数组`data`中的值。将观察到的随机变量添加到模型中会自动导致使用`mc.sample`的后续采样对模型的后验分布进行采样，根据贝叶斯规则和为观察数据选择的分布所隐含的似然函数，以观察到的数据为适当条件。例如，考虑我们在前面的文本中使用的模型，具有正态分布随机变量 *X* ，其均值和标准差是随机变量。这里，我们使用 SciPy `stats`模块中的`norm`类，通过从正态分布的随机变量中抽取样本来模拟对 *X* 的观察，其中 *μ* = 2.5， *σ* = 1.5:

```py
In [32]: mu = 2.5
In [33]: s = 1.5
In [34]: data = stats.norm(mu, s).rvs(100)

```

当观察变量被创建并添加到模型中时，通过设置关键字参数`observed=data`将数据输入到模型中:

```py
In [35]: with mc.Model() as model:
    ...:     mean = mc.Normal('mean', 4.0, 1.0) # true 2.5
    ...:     sigma = mc.HalfNormal('sigma', 3.0 * np.sqrt(np.pi/2)) # true 1.5
    ...:     X = mc.Normal('X', mean, 1/sigma**2, observed=data)

```

为 *X* 提供观测数据的结果是，它不再被视为模型中的随机变量。这可以从使用`vars`属性检查模型中看出，其中`X`现在不存在:

```py
In [36]: model.vars
Out[36]: [mean, sigma_log_]

```

相反，在这种情况下，`X`是用于构建似然函数的确定性变量，该函数将由`mean`和`sigma`表示的先验与这些随机变量的后验分布相关联。像以前一样，我们可以使用`mc.find_MAP`功能为采样过程找到一个合适的起点。创建 MCMC 步骤实例后，我们可以使用`mc.sample`对模型的后验分布进行采样:

```py
In [37]: with model:
    ...:     start = mc.find_MAP()
    ...:     step = mc.Metropolis()
    ...:     trace = mc.sample(100000, start=start, step=step)
[-----------------100%-----------------] 100000 of 100000 complete in 36.1 sec

```

使用`mc.find_MAP`计算的起点最大化了给定观测数据的后验概率，并提供了先验分布未知参数的估计值:

```py
In [38]: start
Out[38]: {'mean': array(2.5064940359768246), 'sigma_log': array(0.394681633456101)}

```

然而，为了获得这些参数(本身就是随机变量)分布的估计值，我们需要使用`mc.sample`函数进行 MCMC 采样，如前所述。后验分布抽样的结果如图 [16-4](#Fig4) 所示。请注意，`mean`和`sigma`变量的分布更接近真实的参数值， *μ* = 2.5 和 *σ* = 1.5，而不是之前分别猜测的 4.0 和 3.0，这是由于数据和相应的似然函数的影响。

![img/332789_2_En_16_Chapter/332789_2_En_16_Fig4_HTML.jpg](img/332789_2_En_16_Chapter/332789_2_En_16_Fig4_HTML.jpg)

图 16-4

`mean`和`sigma`后验分布的 MCMC 采样轨迹

```py
In [38]: fig, axes = plt.subplots(2, 2, figsize=(8, 4), squeeze=False)
    ...: mc.traceplot(trace, varnames=['mean', 'sigma'], ax=axes)

```

为了使用来自后验分布的样本计算统计数据和估计数量，我们可以使用`get_values`方法访问包含样本的数组，该方法将随机变量的名称作为参数。例如，在下面的代码中，我们计算模型中两个随机变量的平均值的估计值，并与从中提取数据点的分布的相应真值进行比较:

```py
In [39]: mu, trace.get_values('mean').mean()
Out[39]: (2.5, 2.5290001218008435)
In [40]: s, trace.get_values('sigma').mean()
Out[40]: (1.5, 1.5029047840092264)

```

PyMC 库还提供了分析和总结从`mc.sample`函数获得的边际后验分布统计数据的工具。例如，`mc.forestplot`函数将模型中每个随机变量的均值和可信度区间(即，真实参数值可能在其中的区间)可视化。使用`mc.forestplot`功能将当前示例的样本可视化的结果如图 [16-5](#Fig5) 所示:

![img/332789_2_En_16_Chapter/332789_2_En_16_Fig5_HTML.png](img/332789_2_En_16_Chapter/332789_2_En_16_Fig5_HTML.png)

图 16-5

mean 和 sigma 这两个参数的森林图，显示了它们的可信区间

```py
In [41]: mc.forestplot(trace, varnames=['mean', 'sigma'])

```

类似的信息也可以使用`mc.summary`函数以文本形式呈现，例如，包括平均值、标准差和后验分位数等信息。

```py
In [42]: mc.summary(trace, varnames=['mean', 'sigma'])
mean:

   Mean             SD               MC Error         95% HPD interval
   -------------------------------------------------------------------
   2.472            0.143            0.001            [2.195, 2.757]
   Posterior quantiles:
   2.5            25             50             75             97.5
   |--------------|==============|==============|--------------|
   2.191          2.375          2.470          2.567          2.754

sigma:

   Mean             SD               MC Error         95% HPD interval
   -------------------------------------------------------------------
   1.440            0.097            0.001            [1.256, 1.630]
   Posterior quantiles:
   2.5            25             50             75             97.5
   |--------------|==============|==============|--------------|
   1.265          1.372          1.434          1.501          1.643

```

### 线性回归

回归是统计建模中最基本的工具之一，我们已经在经典统计形式中看到了线性回归的例子，例如，在第 [14](14.html) 和 [15](15.html) 章中。线性回归也可以用贝叶斯方法来处理，并作为一个建模问题来处理，其中我们为未知的模型参数(斜率和截距)分配先验概率分布，并在给定可用观测值的情况下计算后验分布。为了能够比较贝叶斯线性回归和 frequentist 对同一问题的方法之间的异同，例如，使用第 [14 章](14.html)中的方法，这里我们从使用 statsmodels 库对线性回归问题的简短分析开始。接下来，我们继续分析 PyMC 的相同问题。

作为执行线性回归分析的示例数据，这里我们使用一个包含 200 名男性和女性的身高和体重的数据集，我们可以使用 get_rdataset 函数从 statsmodels 库中的 datasets 模块加载该数据集:

```py
In [42]: dataset = sm.datasets.get_rdataset("Davis", "carData")

```

为了简单起见，首先，我们只处理与男性受试者相对应的数据集子集，为了避免处理异常值，我们过滤掉所有体重超过 110 kg 的受试者。这些操作很容易使用 Pandas 方法来执行，这些方法使用布尔掩码来过滤数据帧:

```py
In [43]: data = dataset.data[dataset.data.sex == 'M']
In [44]: data = data[data.weight < 110]

```

生成的 Pandas 数据框对象数据包含几列:

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"> <col class="tcol6 align-left"></colgroup> 
|   | 

`sex`

 | 

`weight`

 | 

`height`

 | 

`repwt`

 | 

`repht`

 |
| --- | --- | --- | --- | --- | --- |
| `0` | `M` | `77` | `182` | `77` | `180` |
| `3` | `M` | `68` | `177` | `70` | `175` |
| `5` | `M` | `76` | `170` | `76` | `165` |

```py
In [45]: data.head(3)
Out[45]:

```

在这里，我们重点关注这个数据集中体重和身高列之间关系的线性回归模型。使用 statsmodels 库及其普通最小二乘回归模型和 Patsy 公式语言，我们在一行代码中为这种关系创建了一个统计模型:

```py
In [46]: model = smf.ols("height ~ weight", data=data)

```

为了实际执行指定模型与观察数据的拟合，我们使用模型实例的 fit 方法:

```py
In [47]: result = model.fit()

```

一旦模型被拟合并且模型结果对象被创建，我们可以使用 predict 方法来计算新观察值的预测值，并绘制身高和体重之间的线性关系，如图 [16-6](#Fig6) 所示。

![img/332789_2_En_16_Chapter/332789_2_En_16_Fig6_HTML.png](img/332789_2_En_16_Chapter/332789_2_En_16_Fig6_HTML.png)

图 16-6

身高对体重，使用普通最小二乘法拟合线性模型

```py
In [48]: x = np.linspace(50, 110, 25)
In [49]: y = result.predict({"weight": x})
In [50]: fig, ax = plt.subplots(1, 1, figsize=(8, 3))
    ...: ax.plot(data.weight, data.height, 'o')
    ...: ax.plot(x, y, color="blue")
    ...: ax.set_xlabel("weight")
    ...: ax.set_ylabel("height")

```

图 [16-6](#Fig6) 所示的线性关系总结了对该数据集进行线性回归的主要结果。它给出了由模型参数(截距和斜率)的特定值描述的最佳拟合线。在 frequentist 的统计方法中，我们还可以计算许多统计数据，例如，*p*-各种假设的值，如模型参数为零(无影响)的假设。

贝叶斯回归分析的最终结果是每个模型参数的边际分布的后验分布。从这样的边际分布，我们可以计算模型参数的平均估计，这大致对应于从一个常客的分析中获得的模型参数。我们还可以计算其他的量，比如可信区间，它描述了估计中的不确定性。为了使用贝叶斯模型对身高与体重进行建模，我们可以使用诸如![$$ \mathrm{height}\sim \mathcal{N}\left(\mathrm{intercept}+\beta\ \mathrm{weight},{\sigma}^2\right) $$](img/332789_2_En_16_Chapter/332789_2_En_16_Chapter_TeX_IEq6.png)的关系，其中截距、 *β* 和 *σ* 是具有未知分布和参数的随机变量。我们还需要给出模型中所有随机变量的先验分布。根据应用的不同，先验的准确选择可能是一个敏感的问题，但当有大量数据需要拟合时，通常使用合理的初始猜测就足够了。这里我们简单地从代表所有模型参数的广泛分布的先验开始。

为了在 PyMC 中对模型进行编程，我们使用了与本章前面相同的方法:首先，我们为模型的随机组件创建随机变量，并将它们分配给具有代表先验分布的特定参数的分布。接下来，我们创建一个确定性变量，它是随机变量的函数，但是使用 observed 关键字参数将观察到的数据附加到它上面，以及高度分布的期望值(height_mu)的表达式中。

```py
In [51]: with mc.Model() as model:
    ...:     sigma = mc.Uniform('sigma', 0, 10)
    ...:     intercept = mc.Normal('intercept', 125, sd=30)
    ...:     beta = mc.Normal('beta', 0, sd=5)
    ...:     height_mu = intercept + beta * data.weight
    ...:     mc.Normal('height', mu=height_mu, sd=sigma, observed=data.height)
    ...:     predict_height = mc.Normal('predict_height', mu=intercept + beta * x, sd=sigma, shape=len(x))

```

如果我们想使用模型来预测特定体重值下的身高，我们还可以在模型中添加一个额外的随机变量。在前面的模型规范中，predict_height 变量就是一个例子。这里 x 是前面创建的 NumPy 数组，值在 50 到 110 之间。因为是数组，所以需要设置 mc 的 shape 属性。普通类初始化为数组的相应长度。如果我们检查模型的 vars 属性，我们现在会看到它包含两个模型参数(截距和 beta)、模型误差的分布(sigma)和用于预测 x 数组中特定权重值的高度的 predict_height 变量:

```py
In [52]: model.vars
Out[52]: [sigma_interval, intercept, beta, predict_height]

```

一旦模型被完全指定，我们可以转向 MCMC 算法，在给定观察数据的情况下，对模型的边际后验分布进行采样。像之前一样，我们可以用 mc.find_MAP 来寻找一个合适的起点。这里我们使用另一个采样器，mc。NUTS (No-U-Turn 采样器)，这是一个新的强大的采样器，已经添加到 PyMC 的版本 3 中。

```py
In [53]: with model:
    ...:     start = mc.find_MAP()
    ...:     step = mc.NUTS()
    ...:     trace = mc.sample(10000, step, start=start)
[-----------------100%-----------------] 10000 of 10000 complete in 43.1 sec

```

采样结果存储在 mc.sample 返回的 trace 对象中。我们可以使用 mc.traceplot 函数可视化概率分布的核密度估计和生成样本的 MCMC 随机行走轨迹。这里，我们再次使用 varnames 参数来明确选择模型中的哪些随机变量要显示在跟踪图中。结果如图 [16-7](#Fig7) 所示。

![img/332789_2_En_16_Chapter/332789_2_En_16_Fig7_HTML.jpg](img/332789_2_En_16_Chapter/332789_2_En_16_Fig7_HTML.jpg)

图 16-7

线性模型截距和贝塔系数的分布和采样轨迹

```py
In [54]: fig, axes = plt.subplots(2, 2, figsize=(8, 4), squeeze=False)
    ...: mc.traceplot(trace, varnames=['intercept', 'beta'], ax=axes)

```

通过计算贝叶斯模型中随机变量轨迹的平均值，可以获得线性模型中截距和系数的值，这些值与前面文本中 statsmodels 分析的结果最接近:

```py
In [55]: intercept = trace.get_values("intercept").mean()
In [56]: intercept
Out[56]: 149.97546241676989
In [57]: beta = trace.get_values("beta").mean()
In [58]: beta
Out[58]: 0.37077795098761318

```

通过访问由 fit 方法返回的结果类中的 params 属性，可以获得 statsmodels 分析的相应结果(请参见前面的内容):

```py
In [59]: result.params
Out[59]: Intercept    152.617348
         weight         0.336477
         dtype: float64

```

通过比较截距和系数的这些值，我们看到这两种方法对于未知模型参数的最大似然估计给出了相似的结果。在 statsmodels 方法中，为了预测给定体重(比如 90 kg)的预期身高，我们可以使用 predict 方法来获得特定的身高:

```py
In [60]: result.predict({"weight": 90}).values
Out[60]: array([ 182.90030002])

```

对于给定的权重，通过计算随机变量 predict_height 的分布的平均值来获得贝叶斯模型中的相应结果:

```py
In [61]: weight_index = np.where(x == 90)[0][0]
In [62]: trace.get_values("predict_height")[:, weight_index].mean()
Out[62]: 183.33943635274935

```

同样，这两种方法的结果是可比较的。然而，在贝叶斯模型中，我们可以获得每个模型体重下身高的全概率分布的估计值。例如，我们可以使用 Seaborn 库中的 distplot 函数绘制体重为 90 kg 的概率分布的直方图和核密度估计，从而得到如图 [16-8](#Fig8) 所示的图表:

![img/332789_2_En_16_Chapter/332789_2_En_16_Fig8_HTML.png](img/332789_2_En_16_Chapter/332789_2_En_16_Fig8_HTML.png)

图 16-8

体重 90 公斤的身高预测概率分布

```py
In [63]: fig, ax = plt.subplots(figsize=(8, 3))
​    ...: sns.distplot(trace.get_values("predict_height")[:, weight_index], ax=ax)
    ...: ax.set_xlim(150, 210)
    ...: ax.set_xlabel("height")
    ...: ax.set_ylabel("Probability distribution")

```

MCMC 轨迹中的每个样本代表我们希望拟合观测数据的线性模型中截距和系数的可能值。为了直观显示平均截距和系数的不确定性(我们可以将其作为最终线性模型参数的估计值),可以绘制对应于每个样本点的线，以及作为散点图的数据和对应于平均截距和斜率的线。这产生了如图 [16-9](#Fig9) 所示的图表。线的分布代表了给定体重的身高估计值的不确定性。越靠近边缘，可用数据点越少，分布越大，在数据点云的中间，分布越紧密。

![img/332789_2_En_16_Chapter/332789_2_En_16_Fig9_HTML.jpg](img/332789_2_En_16_Chapter/332789_2_En_16_Fig9_HTML.jpg)

图 16-9

身高与体重，使用 OLS 和贝叶斯模型进行线性拟合

```py
In [64]: fig, ax = plt.subplots(1, 1, figsize=(8, 3))
    ...: for n in range(500, 2000, 1):
    ...:     intercept = trace.get_values("intercept")[n]
    ...:     beta = trace.get_values("beta")[n]
    ...:     ax.plot(x, intercept + beta * x, color="red", lw=0.25, alpha=0.05)
    ...: intercept = trace.get_values("intercept").mean()
    ...: beta = trace.get_values("beta").mean()
    ...: ax.plot(x, intercept + beta * x, color="k", label="Mean Bayesian prediction")
    ...: ax.plot(data.weight, data.height, 'o')
    ...: ax.plot(x, y, '--', color="blue", label="OLS prediction")
    ...: ax.set_xlabel("weight")
    ...: ax.set_ylabel("height")
    ...: ax.legend(loc=0)

```

在我们这里讨论的线性回归问题中，我们明确定义了统计模型和模型中包含的随机变量。这说明了使用贝叶斯方法和 PyMC 库分析统计模型所需的一般步骤。然而，对于广义线性模型，PyMC 库提供了一个简化的 API 来为我们创建模型和所需的随机变量。通过 mc.glm.GLM.from_formula 函数，我们可以使用 Patsy 公式定义一个广义线性模型(参见第 [14](14.html) 章)并使用 Pandas 数据框提供数据。这将自动负责建立模型。使用 mc.glm.glm 建立模型后，我们可以使用与之前相同的方法从模型的后验分布中进行采样。

```py
In [65]: with mc.Model() as model:
    ...:     mc.glm.GLM.from_formula('height ~ weight', data)
    ...:     step = mc.NUTS()
    ...:     trace = mc.sample(2000, step)
[-----------------100%-----------------] 2000 of 2000 complete in 99.1 sec

```

由 mc.traceplot 函数可视化的 GLM 模型的采样结果如图 [16-10](#Fig10) 所示。在这些轨迹图中，sd 对应于前面文本中使用的显式模型定义中的 sigma 变量，它表示模型和观测数据的残差的标准误差。在轨迹中，注意样本在达到稳定水平之前需要数百个样本。初始过渡时期不会产生具有正确分布的样本，因此当使用样本计算估计值时，我们应该排除初始时期的样本。

![img/332789_2_En_16_Chapter/332789_2_En_16_Fig10_HTML.jpg](img/332789_2_En_16_Chapter/332789_2_En_16_Fig10_HTML.jpg)

图 16-10

使用`mc.glm`模块定义的贝叶斯 GLM 模型的样本轨迹图

```py
In [66]: fig, axes = plt.subplots(3, 2, figsize=(8, 6), squeeze=False)
    ...: mc.traceplot(trace, varnames=['Intercept', 'weight', 'sd'], ax=axes)

```

使用`mc.glm.glm`,我们可以使用贝叶斯统计创建和分析线性模型，就像我们使用 frequentist 的 statsmodels 方法定义和分析模型一样。对于这里研究的简单例子，使用两种统计方法的回归分析给出了相似的结果，没有一种方法比另一种方法更合适。然而，有实际的差异，根据情况可能有利于一个或另一个。例如，使用贝叶斯方法，我们可以获得全部边际后验分布的估计，这对于计算除平均值以外的统计量是有用的。然而，在简单模型上执行 MCMC(如这里考虑的模型)比执行普通的最小二乘拟合需要更多的计算。当分析高维复杂模型(许多未知模型参数)时，贝叶斯方法的真正优势出现了。在这种情况下，定义适当的 frequentist 的模型可能是困难的，解决由此产生的模型具有挑战性。MCMC 算法具有非常吸引人的特性，即它可以很好地扩展到高维问题，因此对于复杂的统计模型具有很强的竞争力。虽然我们在这里考虑的模型都很简单，并且可以使用 frequentist 的方法很容易地解决，但这里使用的一般方法保持不变，创建更复杂的模型只是向模型中添加更多的随机变量。

作为最后一个例子，我们说明当贝叶斯模型的复杂性增加时，也可以使用相同的一般程序。我们返回到身高和体重数据集，但不是只选择男性受试者，这里我们考虑模型中考虑受试者性别的附加级别，以便男性和女性都可以用潜在不同的斜率和截距建模。在 PyMC 中，我们可以通过使用`shape`参数来指定添加到模型中的每个随机变量的维度，从而创建一个多级模型，如下例所示。

我们从准备数据集开始。这里，我们再次将我们的分析限制在体重小于 110 kg 的受试者，以消除异常值，并且我们将`sex`列转换为二元变量，其中 0 表示男性，1 表示女性。

```py
In [67]: data = dataset.data.copy()
In [68]: data = data[data.weight < 110]
In [69]: data["sex"] = data["sex"].apply(lambda x: 1 if x == "F" else 0)

```

接下来，我们定义统计模型，我们这里取为身高~ *N* (截距<sub>T3】IT5】+*β*<sub>T9】I</sub>体重，*<sup>2</sup>，其中 *i* 是男性受试者取值为 0，女性受试者取值为 1 的指标。当创建截距 <sub>*i*</sub> 和 *β* <sub>*i*</sub> 的随机变量时，我们通过指定 shape=2 来指示多级结构(因为在这种情况下，我们有两个级别:男性和女性)。与前面的模型定义相比，唯一的区别是，在定义 height_mu 的表达式时，我们还需要使用一个索引掩码，以便 data.weight 中的每个值都与正确的级别相关联。*</sub>

```py
In [70]: with mc.Model() as model:
    ...:     intercept_mu, intercept_sigma = 125, 30
    ...:     beta_mu, beta_sigma = 0, 5
    ...:
    ...:     intercept = mc.Normal('intercept', intercept_mu, sd=intercept_sigma, shape=2)
    ...:     beta = mc.Normal('beta', beta_mu, sd=beta_sigma, shape=2)
    ...:     error = mc.Uniform('error', 0, 10)
​    ...:
    ...:     sex_idx = data.sex.values
    ...:     height_mu = intercept[sex_idx] + beta[sex_idx] * data.weight
​    ...:
    ...:     mc.Normal('height', mu=height_mu, sd=error, observed=data.height)

```

使用`vars`属性对象检查模型变量显示，我们在模型中又有了三个随机变量:`intercept`、`beta`和`error`。然而，与早期的模型相比，这里的`intercept`和`beta`都有两个级别。

```py
In [71]: model.vars
Out[71]: [intercept, beta, error_interval]

```

我们调用 MCMC 采样算法的方式与本章前面的例子相同。这里我们使用坚果取样器收集 5000 个样本:

```py
In [72]: with model:
    ...:     start = mc.find_MAP()
    ...:     step = mc.NUTS()
    ...:     trace = mc.sample(5000, step, start=start)
[-----------------100%-----------------] 5000 of 5000 complete in 64.2 sec

```

我们也可以像以前一样，使用`mc.traceplot`函数来可视化采样的结果。这使我们能够快速形成模型参数分布的概念，并验证 MCMC 采样产生了合理的结果。当前模型的轨迹图如图 [16-11](#Fig11) 所示，与之前的例子不同，这里我们在面板中为`intercept`和`beta`变量绘制了多条曲线，反映了它们的多级性质:蓝色(深色)线显示男性受试者的结果，绿色(浅色)线显示女性受试者的结果。

![img/332789_2_En_16_Chapter/332789_2_En_16_Fig11_HTML.jpg](img/332789_2_En_16_Chapter/332789_2_En_16_Fig11_HTML.jpg)

图 16-11

身高体重多水平模型中每个变量的模型参数和 MCMC 采样轨迹概率分布的核密度估计

```py
In [73]: mc.traceplot(trace, figsize=(8, 6))

```

使用 trace 对象的`get_values`方法，我们可以提取模型变量的采样数据。这里，`intercept`和`beta`的采样数据是形状为`(5000, 2)`的二维数组:第一维表示每个样本，第二维表示变量的级别。这里我们对每个性别的截距和斜率感兴趣，所以我们取第一个轴上的平均值(所有样本):

```py
In [74]: intercept_m, intercept_f = trace.get_values('intercept').mean(axis=0)
In [75]: beta_m, beta_f = trace.get_values('beta').mean(axis=0)

```

通过对两个维度进行平均，我们还可以获得代表整个数据集的截距和斜率，其中男性和女性受试者被分组在一起:

```py
In [76]: intercept = trace.get_values('intercept').mean()
In [77]: beta = trace.get_values('beta').mean()

```

最后，我们通过将数据绘制成散点图，并绘制与我们获得的男性和女性受试者的截距和斜率相对应的线，以及将所有受试者分组的结果，来可视化结果。结果如图 [16-12](#Fig12) 所示。

![img/332789_2_En_16_Chapter/332789_2_En_16_Fig12_HTML.png](img/332789_2_En_16_Chapter/332789_2_En_16_Fig12_HTML.png)

图 16-12

男性(深蓝色)和女性(浅绿色)受试者的身高和体重

```py
In [78]: fig, ax = plt.subplots(1, 1, figsize=(8, 3))
    ...: mask_m = data.sex == 0
    ...: mask_f = data.sex == 1
    ...: ax.plot(data.weight[mask_m], data.height[mask_m], 'o', color="steelblue",
    ...:         label="male", alpha=0.5)
    ...: ax.plot(data.weight[mask_f], data.height[mask_f], 'o', color="green",
    ...:         label="female", alpha=0.5)
    ...: x = np.linspace(35, 110, 50)
    ...: ax.plot(x, intercept_m + x * beta_m, color="steelblue", label="model male group")
    ...: ax.plot(x, intercept_f + x * beta_f, color="green", label="model female group")
    ...: ax.plot(x, intercept + x * beta, color="black", label="model both groups")
​    ...:
    ...: ax.set_xlabel("weight")
    ...: ax.set_ylabel("height")
    ...: ax.legend(loc=0)

```

回归线如图 [16-12](#Fig12) 所示，图 [16-11](#Fig11) 所示的分布图表明，考虑到男女受试者不同的截距和斜率，模型得到了改进。在具有 PyMC 的贝叶斯模型中，改变分析中使用的基础模型仅仅是向模型中添加随机变量、定义它们如何相互关联以及为每个随机变量分配先验分布的问题。实际求解模型所需的 MCMC 采样与模型细节无关。这是贝叶斯统计建模最吸引人的方面之一。例如，在前面文本中考虑的多级模型中，不是将截距和斜率变量的先验指定为独立的概率分布，而是我们可以将先验的分布参数与另一个随机变量相关联，从而获得分层贝叶斯模型，其中描述每个级别的截距和斜率的分布的模型参数从公共分布中提取。层次模型有许多用途，是贝叶斯统计擅长的许多应用之一。

## 摘要

在这一章中，我们已经使用 PyMC 库提供的计算方法探索了贝叶斯统计。贝叶斯统计方法在几个基本观点上不同于经典的频率统计。从实际计算的角度来看，贝叶斯方法通常很难精确求解。事实上，精确计算贝叶斯模型的后验分布通常非常昂贵。然而，我们经常可以做的是应用强大而有效的采样方法，允许我们使用模拟来找到近似的后验分布。贝叶斯统计框架的关键作用是允许我们定义统计模型，然后应用抽样方法找到模型的近似后验分布。这里，我们使用 PyMC 库作为 Python 中的贝叶斯建模框架。我们简要地探讨了根据具有给定分布的随机变量定义统计模型，以及使用 PyMC 库中实现的 MCMC 方法对这些模型的后验分布进行模拟和采样。

## 进一步阅读

关于贝叶斯统计理论的通俗易懂的介绍，请参见 Kruschke (2014)和 Downey (2013)。Gelman (2013)给出了更具技术性的讨论。在*概率编程和黑客的贝叶斯方法*中给出了一个面向计算的关于使用 Python 的贝叶斯方法的介绍，在 [`http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers`](http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers) 可以免费在线获得。VanderPlas (2014)对贝叶斯和频率主义者的统计方法之间的差异进行了有趣的讨论，并使用 Python 编写了示例，该讨论也可在 [`http://arxiv.org/pdf/1411.5018.pdf`](http://arxiv.org/pdf/1411.5018.pdf) 获得。

## 参考

唐尼(2013)。*想想贝氏。*塞瓦斯托波尔:奥莱利。

盖尔曼公司(2013 年)。*贝叶斯数据分析*(第 3 版。).纽约:CRC 出版社。

Kruschke，J. (2014 年)。*做贝叶斯数据分析。*阿姆斯特丹:学术出版社。

VanderPlas，J. (2014)。频繁主义和贝叶斯主义:Python 驱动的初级读本。继续进行。CONF 科学的第 13 条蟒蛇。奥斯汀:SCIPY。

<aside class="FootnoteSection" epub:type="footnotes">Footnotes [1](#Fn1_source)

参见`Slice`、`HamiltonianMC`和`NUTS`采样器，它们或多或少可以互换使用。

 </aside>**