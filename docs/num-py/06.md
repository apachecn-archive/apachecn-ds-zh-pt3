# 6.最佳化

在本章中，我们将在第 [5](05.html) 章关于方程求解的基础上，探索解决最优化问题的相关主题。一般来说，优化是从一组可行的候选元素中寻找和选择最佳元素的过程。在数学优化中，这个问题通常被公式化为确定一个函数在给定区域上的极值。极值或最优值可以指函数的最小值或最大值，这取决于应用和具体问题。在这一章中，我们关心的是一个或几个变量的实值函数的优化，它可以有选择地服从一组限制函数定义域的约束。

数学优化的应用是多种多样的，解决优化问题必须采用的方法和算法也是如此。由于最优化是一种普遍重要的数学工具，它已经被开发并适用于科学和工程的许多领域，并且用于描述最优化问题的术语在不同的领域之间是不同的。例如，被优化的数学函数可以被称为成本函数、损失函数、能量函数或目标函数等等。这里我们使用通用术语目标函数。

优化与方程求解密切相关，因为在函数的最佳值处，其导数(或多元情况下的梯度)为零。然而，反过来不一定是正确的，但是解决最优化问题的一种方法是求解导数或梯度的零点，并测试结果候选的最优性。然而，这种方法并不总是可行的，通常需要采用其他数值方法，其中许多与第 [5](05.html) 章中涉及的求根数值方法密切相关。

在本章中，我们将讨论如何使用 SciPy 的优化模块`optimize`来解决非线性优化问题，并且我们将简要探讨如何使用凸优化库`cvxopt`来解决带有线性约束的线性优化问题。这个库也有强大的二次规划问题求解器。

### cvxopt

凸优化库 cvxopt 提供了线性和二次优化问题的求解器。在撰写本文时，最新版本是 1.1.9。有关更多信息，请参见该项目的网站 [`http://cvxopt.org`](http://cvxopt.org) 。这里我们使用这个库进行约束线性优化。

## 导入模块

和上一章一样，这里我们使用了 SciPy 库中的`optimize`模块。这里我们假设这个模块是以如下方式导入的:

```
In [1]: from scipy import optimize

```

在本章的后半部分，我们还将使用`cvxopt`库来研究线性编程，我们假设它是完整导入的，没有任何别名:

```
In [2]: import cvxopt

```

对于基本的数字、符号和绘图，我们还使用 NumPy、SymPy 和 Matplotlib 库，这些库是使用前面章节中介绍的约定导入和初始化的:

```
In [3]: import matplotlib.pyplot as plt
In [4]: import numpy as np
In [5]: import sympy
In [6]: sympy.init_printing()

```

## 最优化问题的分类

这里，我们将注意力限制在具有一个或多个因变量的实值函数的数学优化上。许多数学优化问题都可以用这种方式表述，但一个值得注意的例外是离散变量上的函数优化，例如整数，这超出了本书的范围。

这里所考虑的这种类型的一般优化问题可以被公式化为最小化问题![$$ {\min}_xf(x) $$](../images/332789_2_En_6_Chapter/332789_2_En_6_Chapter_TeX_IEq1.png)，服从以下集合:等式约束*m*g(*x*)= 0，不等式约束*p*h(*x*)≤0。这里的 *f* ( *x* )是 *x* 的实值函数，可以是标量也可以是向量*x*=(*x*T23】0，*x*T27】1，…，*x*<sub>*n*</sub>)<sup>*T 而 *g* ( *x* )和 *h* ( *x* )可以是向量值函数:*f*:ℝ<sup>*n*</sup>⟶ℝ、*g*:ℝ<sup>*n*</sup>⟶ℝ<sup>*m*</sup>和 *注意最大化 *f* ( *x* )等价于最小化-*f*(*x*)，所以不失一般性，只考虑最小化问题就足够了。**</sup>

根据目标函数 *f* ( *x* )以及等式和不等式约束 *g* ( *x* )和 *h* ( *x* )的性质，这个公式包含了丰富多样的问题。这种形式的一般数学优化很难解决，并且没有有效的方法来解决完全通用的优化问题。然而，对于许多重要的特殊情况有有效的方法，因此在最优化中，为了能够解决问题，尽可能多地了解目标函数和约束条件是很重要的。

优化问题根据函数 *f* ( *x* )、 *g* ( *x* )、 *h* ( *x* )的性质进行分类。首先，问题是*单变量*或*一维*如果 *x* 是标量， *x* ∈ ℝ，而*多变量*或*多维*如果 *x* 是向量，*x*∈ℝ<sup>*n*</sup>。对于高维目标函数，随着更大的 *n* ，优化问题更难解决并且计算要求更高。如果目标函数和约束都是线性的，这个问题就是一个线性优化问题，或者说*线性规划*问题。 <sup>[1](#Fn1)</sup> 如果目标函数或约束条件中有一个是非线性的，则是非线性优化问题，或者是*非线性规划*问题。关于约束，最优化的重要子类是无约束问题，以及具有线性和非线性约束的问题。最后，处理等式和不等式约束需要不同的方法。

通常，非线性问题比线性问题更难解决，因为它们有更多种可能的行为。一个一般的非线性问题可能同时存在局部和全局最小值，这使得寻找全局最小值变得非常困难:迭代求解器可能经常收敛到局部最小值而不是全局最小值，或者如果同时存在局部和全局最小值，甚至可能不能完全收敛。但是，非线性问题的一个重要子类可以高效求解的是*凸问题*，它与严格局部极小值的不存在和唯一全局极小值的存在直接相关。根据定义，一个函数在区间[ *a* ， *b* 上是凸的，如果这个区间上的函数值位于通过端点( *a* ， *f* ( *a* )和( *b* ， *f* ( *b* )的直线之下。这个条件可以很容易地推广到多元情况，它包含了许多重要的性质，如区间上存在唯一的最小值。由于像这样的强性质，即使凸问题是非线性的，它们也可以被有效地解决。局部和全局最小值、凸函数和非凸函数的概念如图 [6-1](#Fig1) 所示。

![../images/332789_2_En_6_Chapter/332789_2_En_6_Fig1_HTML.png](../images/332789_2_En_6_Chapter/332789_2_En_6_Fig1_HTML.png)

图 6-1

具有一个全局最小值和两个局部最小值的凸函数(左)和非凸函数(右)的图示

目标函数 *f* ( *x* )和约束条件 *g* ( *x* )和 *h* ( *x* )是否连续和光滑是对可用于解决优化问题的方法和技术具有非常重要意义的性质。这些函数或它们的导数或梯度的不连续性给许多可用的解决最优化问题的方法带来困难，在下文中，我们假设这些函数确实是连续和光滑的。与此相关的是，如果函数本身不是确切已知的，但是由于测量或其他原因而包含噪声，那么下面讨论的许多方法可能都不适合。

连续光滑函数的优化与非线性方程求解密切相关，因为函数 *f* ( *x* )的极值对应于其导数或梯度为零的点。因此，寻找最优值 *f* ( *x* )的候选值等价于求解(一般非线性)方程组∇ *f* ( *x* ) = 0。但是∇ *f* ( *x* ) = 0 的一个解，也就是所谓的驻点，不一定对应 *f* ( *x* )的最小值；它也可以是最大值或鞍点；见图 [6-2](#Fig2) 。因此，通过求解∇ *f* ( *x* ) = 0 获得的候选值应该进行最优性测试。对于无约束目标函数，高阶导数或海森矩阵

![$$ {\left\{{H}_f(x)\right\}}_{ij}=\frac{\partial^2f(x)}{\partial {x}_i\partial {x}_j}, $$](../images/332789_2_En_6_Chapter/332789_2_En_6_Chapter_TeX_Equa.png)

对于多元情况，可用于确定驻点是否为局部最小值。具体来说，如果二阶导数是正的，或者是 Hessian 正定的，当在驻点*x*<sup>∫</sup>求值时，那么*x*<sup>∫</sup>是局部最小值。负二阶导数或负定 Hessian 对应于局部最大值，零二阶导数或不定 Hessian 对应于鞍点。

![../images/332789_2_En_6_Chapter/332789_2_En_6_Fig2_HTML.png](../images/332789_2_En_6_Chapter/332789_2_En_6_Fig2_HTML.png)

图 6-2

一维函数的不同驻点的图示

因此，代数求解方程系统∇ *f* ( *x* ) = 0 并测试候选解的最优性是解决优化问题的一种可能策略。然而，这并不总是可行的方法。特别是，我们可能没有一个用于计算导数的解析表达式来表示 *f* ( *x* ),由此产生的非线性方程组可能不容易求解，尤其是不容易找到它的所有根。对于这种情况，有替代的数值优化方法，其中一些类似于第 [5](05.html) 章中讨论的求根方法。在本章的剩余部分，我们将探讨各种类型的优化问题，以及如何在实践中使用 Python 可用的优化库来解决这些问题。

## 单变量优化

仅依赖于单个变量的函数的优化相对容易。除了寻找函数导数的根的分析方法之外，我们还可以采用与一元函数的求根方法相似的技术，即括号法和牛顿法。与单变量求根的二分法一样，可以单独使用函数求值来使用括号和迭代细化区间。通过在两个内点*x*T8】1 和*x*T12】2、*x*1<*x*T20】2 处对函数求值，并选择[*x*T24】1，可以得到包含最小值的区间[ *a* 、 *b* 如果*f*(*x*<sub>1</sub>)>*f*(*x*<sub>2</sub>)，则 *a* ， *x* <sub>2</sub> ，否则为新区间。 这个思想被用在*黄金分割搜索*方法中，该方法另外使用了选择 *x* <sub>1</sub> 和 *x* <sub>2</sub> 的技巧，使得它们在[ *a* ， *b* ]区间中的相对位置满足黄金分割比例。这具有允许重新使用来自先前迭代的一个函数评估的优点，并且因此在每次迭代中仅需要一个新的函数评估，但是仍然在每次迭代中以常数因子减小间隔。对于在给定区间上具有唯一最小值的函数，这种方法保证收敛到一个最优点，但不幸的是，对于更复杂的函数，这是不保证的。因此，仔细选择初始间隔是很重要的，最好是相对接近一个最佳点。在 SciPy `optimize`模块中，函数`golden`实现了黄金搜索方法。

作为求根的二分法，黄金搜索法是一种(相对)安全但收敛缓慢的方法。如果使用函数评估的值，而不是仅将这些值相互比较(这类似于在二分法中仅使用函数的符号)，则可以构建具有更好收敛性的方法。函数值可用于拟合多项式，例如二次多项式，可对其进行插值以找到最小值的新近似，从而给出新函数评估的候选，之后可迭代该过程。这种方法可以更快地收敛，但是比包围更危险，并且可能根本不收敛或者可能收敛到给定包围区间之外的局部最小值。

牛顿的求根方法是二次近似方法的一个例子，通过将该方法应用于导数而不是函数本身，可以应用于求函数最小值。这就产生了迭代公式*x*<sub>*k*+1</sub>=*x*<sub>*k*</sub>—*f*<sup>’</sup>(*x*<sub>*k*</sub>/*f*<sup>’</sup>(*x*<sub>*k*该公式还要求在每次迭代中计算导数和二阶导数。如果这些导数的解析表达式可用，这可能是一个好方法。如果只有函数评估可用，导数可以用求根的割线法的模拟来近似。**</sub>

前面两种方法的组合通常用于单变量优化例程的实际实现中，提供稳定性和快速收敛。在 SciPy 的`optimize`模块中，`brent`函数就是这样一种混合方法，它通常是用 SciPy 优化一元函数的首选方法。该方法是黄金分割搜索方法的变体，其使用逆抛物线插值来获得更快的收敛。

不用直接调用`optimize.golden`和`optimize.brent`函数，使用统一接口函数`optimize.minimize_scalar`很方便，它根据`method`关键字参数的值调度到`optimize.golden`和`optimize.brent`函数，其中当前允许的选项有`'Golden'`、`'Brent'`或`'Bounded'`。最后一个选项分派给`optimize.fminbound`，在一个有界区间上进行优化，对应一个不等式约束的优化问题，限制目标函数 *f* ( *x* )的定义域。请注意，`optimize.golden`和`optimize.brent`函数可能会在初始括号间隔之外收敛到局部最小值，但`optimize.fminbound`会在这种情况下返回允许范围末端的值。

作为说明这些技术的例子，考虑下面的经典优化问题:最小化单位体积的圆柱体的面积。这里，合适的变量为圆柱体的半径 *r* 和高度 *h* ，目标函数为 *f* ([ *r* ，*h*)= 2*πr*<sup>2</sup>+2*πRH*，满足等式约束*g*(*r*， *h 由于这个问题在这里被公式化，它是一个带有等式约束的二维优化问题。但我们可以对其中一个因变量代数求解约束方程，例如*h*= 1/*πr*<sup>2</sup>，并将其代入目标函数，得到一个无约束的一维优化问题:*f*(*r*)= 2*πr*<sup>2</sup>+2/*r*。首先，我们可以使用 SymPy 象征性地解决这个问题，使用将 *f* ( *r* )的导数等于零的方法:*

![$$ \frac{2^{2/3}}{2\sqrt[3]{\pi }} $$](../images/332789_2_En_6_Chapter/332789_2_En_6_Chapter_TeX_Equb.png)

```
In [7]: r, h = sympy.symbols("r, h")
In [8]: Area = 2 * sympy.pi * r**2 + 2 * sympy.pi * r * h
In [9]: Volume = sympy.pi * r**2 * h
In [10]: h_r = sympy.solve(Volume - 1)[0]
In [11]: Area_r = Area.subs(h_r)
In [12]: rsol = sympy.solve(Area_r.diff(r))[0]
In [13]: rsol
Out[13]:

```

```
In [14]: _.evalf()
Out[14]: 0.541926070139289

```

现在验证二阶导数为正并且`rsol`对应于最小值:

![$$ 3\sqrt[3]{2\pi } $$](../images/332789_2_En_6_Chapter/332789_2_En_6_Chapter_TeX_Equc.png)

```
In [15]: Area_r.diff(r, 2).subs(r, rsol)
Out[15]: 12π
In [16]: Area_r.subs(r, rsol)
Out[16]:

```

```
In [17]: _.evalf()
Out[17]: 5.53581044593209

```

对于简单的问题，这种方法通常是可行的，但是对于更现实的问题，我们通常需要求助于数值技术。为了使用 SciPy 的数值优化函数解决这个问题，我们首先定义一个实现目标函数的 Python 函数`f`。为了解决优化问题，我们将这个函数传递给，例如，`optimize.brent`。可选地，我们可以使用`brack`关键字参数来指定算法的开始间隔:

```
In [18]: def f(r):
    ...:     return 2 * np.pi * r**2 + 2 / r
In [19]: r_min = optimize.brent(f, brack=(0.1, 4))
In [20]: r_min
Out[20]: 0.541926077256
In [21]: f(r_min)
Out[21]: 5.53581044593

```

我们可以使用标量最小化问题的通用接口`optimize.minimize_scalar`，而不是直接调用`optimize.brent`。注意，在这种情况下要指定一个开始间隔，我们必须使用`bracket`关键字参数:

```
In [22]: optimize.minimize_scalar(f, bracket=(0.1, 4))
Out[22]:  nit: 13
          fun: 5.5358104459320856
            x: 0.54192606489766715
         nfev: 14

```

所有这些方法都给出使圆柱体面积最小的半径约为 0.54(符号计算的确切结果是![$$ {2}^{2/3}/2\sqrt[3]{\pi } $$](../images/332789_2_En_6_Chapter/332789_2_En_6_Chapter_TeX_IEq2.png) `)`，最小面积约为 5.54(确切结果是![$$ 3\sqrt[3]{2\pi } $$](../images/332789_2_En_6_Chapter/332789_2_En_6_Chapter_TeX_IEq3.png))。我们在本例中最小化的目标函数绘制在图 [6-3](#Fig3) 中，其中最小值标有红星。如果可能的话，在尝试数值优化之前可视化目标函数是一个好主意，因为它可以帮助确定合适的初始区间或数值优化例程的起始点。

![../images/332789_2_En_6_Chapter/332789_2_En_6_Fig3_HTML.png](../images/332789_2_En_6_Chapter/332789_2_En_6_Fig3_HTML.png)

图 6-3

*单位体积圆柱体的表面积作为半径 r 的函数*

## 无约束多元优化

多变量优化比前面讨论的单变量优化要困难得多。特别地，求解梯度根的非线性方程的解析方法在多变量情况下很少是可行的，并且在黄金搜索方法中使用的包围方案也不能直接应用。相反，我们必须求助于从坐标空间中的某个点开始的技术，并使用不同的策略来更好地逼近最小点。这种类型最基本的方法是考虑目标函数 *f* ( *x* )在给定点 *x* 的梯度∇ *f* ( *x* )。一般来说，梯度的负值，−∇ *f* ( *x* )，总是指向函数 *f* ( *x* )减少最多的方向。作为最小化策略，因此沿着这个方向移动一段距离 *α* <sub>*k*</sub> 然后在新的点迭代这个方案是明智的。这个方法被称为*最速下降法*，它给出了迭代公式*x*<sub>*k*+1</sub>=*x*<sub>*k*</sub>—*α*<sub>*k*</sub>∇f(*x**k*，其中适当的 *α* <sub>*k*</sub> 可以例如通过求解一维优化问题![$$ {\min}_{ak}f\left({x}_k-{\alpha}_k\nabla f\left({x}_k\right)\right) $$](../images/332789_2_En_6_Chapter/332789_2_En_6_Chapter_TeX_IEq4.png)来选择。这种方法保证取得进展并最终收敛到函数的最小值，但是收敛可能相当慢，因为这种方法倾向于沿着梯度的方向过冲，给出了到最小值的曲折逼近。然而，最速下降法是许多多变量优化算法的概念基础，通过适当的修改，收敛速度可以加快。

多变量优化的牛顿法是对最速下降法的改进，可以提高收敛性。在单变量的情况下，牛顿法可以看作是函数的局部二次近似，当最小化时，给出一个迭代方案。在多变量的情况下，迭代公式为![$$ {x}_{k+1}={x}_k-{H}_f^{-1}\left({x}_k\right)\nabla f\left({x}_k\right) $$](../images/332789_2_En_6_Chapter/332789_2_En_6_Chapter_TeX_IEq5.png)，与最速下降法相比，梯度已被替换为从左边乘以函数的 Hessian 矩阵的逆矩阵的梯度。 <sup>[ 2 ](#Fn2)</sup> 一般来说，这改变了步长的方向和长度，所以这种方法严格来说不是最速下降法，如果从最小值开始太远，可能不会收敛。然而，当接近最小值时，它很快收敛。像往常一样，在收敛速度和稳定性之间有一个折衷。正如这里所表述的，牛顿法需要函数的梯度和 Hessian。

在 SciPy 中，牛顿法是在函数`optimize.fmin_ncg`中实现的。此函数采用以下参数:一个用于目标函数的 Python 函数、一个起点、一个用于计算梯度的 Python 函数和一个(可选)用于计算 Hessian 的 Python 函数。为了了解如何使用该方法来解决优化问题，我们考虑以下问题:![$$ {\min}_xf(x) $$](../images/332789_2_En_6_Chapter/332789_2_En_6_Chapter_TeX_IEq6.png)其中目标函数是*f*(*x*)=(*x*<sub>1</sub>—1)<sup>4</sup>+5(*x*<sub>2</sub>—1)<sup>2</sup>—2*x*<sub>1</sub>*为了应用牛顿的方法，我们需要计算梯度和海森。对于这种特殊情况，这可以很容易地手工完成。然而，为了通用性，在下面我们使用 SymPy 来计算梯度和 Hessian 的符号表达式。为此，我们首先为目标函数定义符号和符号表达式，然后为每个变量使用`sympy.diff`函数，以符号形式获得梯度和 Hessian:*

![$$ \left[\begin{array}{c}-2{x}_2+4{\left({x}_1-1\right)}^3\\ {}-2{x}_1+10{x}_2-10\end{array}\right] $$](../images/332789_2_En_6_Chapter/332789_2_En_6_Chapter_TeX_Equd.png)

```
In [23]: x1, x2 = sympy.symbols("x_1, x_2")
In [24]: f_sym = (x1-1)**4 + 5 * (x2-1)**2 - 2*x1*x2
In [25]: fprime_sym = [f_sym.diff(x_) for x_ in (x1, x2)]
In [26]: # Gradient
    ...: sympy.Matrix(fprime_sym)
Out[26]:

```

![$$ \left[\begin{array}{cc}12{\left({x}_1-1\right)}^2&amp; -2\\ {}-2&amp; 10\end{array}\right] $$](../images/332789_2_En_6_Chapter/332789_2_En_6_Chapter_TeX_Eque.png)

```
In [27]: fhess_sym = [[f_sym.diff(x1_, x2_) for x1_ in (x1, x2)] for x2_ in (x1, x2)]
In [28]: # Hessian
    ...: sympy.Matrix(fhess_sym)
Out[28]:

```

现在我们有了梯度和 Hessian 的符号表达式，我们可以使用`sympy.lambdify`为这些表达式创建矢量化函数。

```
In [29]: f_lmbda = sympy.lambdify((x1, x2), f_sym, 'numpy')
In [30]: fprime_lmbda = sympy.lambdify((x1, x2), fprime_sym, 'numpy')
In [31]: fhess_lmbda = sympy.lambdify((x1, x2), fhess_sym, 'numpy')

```

然而，`sympy.lambdify`生成的函数为相应表达式中的每个变量取一个参数，SciPy 优化函数期望一个矢量化函数，其中所有坐标都打包到一个数组中。为了获得与 SciPy 优化例程兼容的函数，我们用一个重新排列参数的 Python 函数包装了由`sympy.lambdify`生成的每个函数:

```
In [32]: def func_XY_to_X_Y(f):
    ...:     """
    ...:     Wrapper for f(X) -> f(X[0], X[1])
    ...:     """
    ...:     return lambda X: np.array(f(X[0], X[1]))
In [33]: f = func_XY_to_X_Y(f_lmbda)
In [34]: fprime = func_XY_to_X_Y(fprime_lmbda)
In [35]: fhess = func_XY_to_X_Y(fhess_lmbda)

```

现在，函数`f`、`fprime`和`fhess`是形式上的矢量化 Python 函数，例如`optimize.fmin_ncg`所期望的，我们可以通过调用这个函数来对手头的问题进行数值优化。除了我们已经从 SymPy 表达式中准备好的函数之外，我们还需要给牛顿法一个起点。这里我们用(0，0)作为起点。

```
In [36]: x_opt = optimize.fmin_ncg(f, (0, 0), fprime=fprime, fhess=fhess)
         Optimization terminated successfully.
              Current function value: -3.867223
              Iterations: 8
              Function evaluations: 10
              Gradient evaluations: 17
              Hessian evaluations: 8
In [37]: x_opt
Out[37]: array([ 1.88292613,  1.37658523])

```

该例程在( *x* <sub>1</sub> ， *x* <sub>2</sub> ) = (1.88292613，1.37658523)处找到一个最小点，并且关于该解决方案的诊断信息也被打印到标准输出，包括迭代次数以及达到该解决方案所需的函数、梯度和 Hessian 评估的次数。像往常一样，直观显示目标函数和解决方案(见图 [6-4](#Fig4) ):

![../images/332789_2_En_6_Chapter/332789_2_En_6_Fig4_HTML.png](../images/332789_2_En_6_Chapter/332789_2_En_6_Fig4_HTML.png)

图 6-4

**目标函数**f*(*x*)=(*x*<sub>1</sub>-1)<sup>4</sup>+5(*x*+5<sub>2</sub>-1)<sup>2</sup>-2*x*<sub>1</sub>*x*<sub>2<sub>最小点用一颗红星标出。</sub></sub>*

 *```
In [38]: fig, ax = plt.subplots(figsize=(6, 4))
    ...: x_ = y_ = np.linspace(-1, 4, 100)
    ...: X, Y = np.meshgrid(x_, y_)
    ...: c = ax.contour(X, Y, f_lmbda(X, Y), 50)
    ...: ax.plot(x_opt[0], x_opt[1], 'r*', markersize=15)
    ...: ax.set_xlabel(r"$x_1$", fontsize=18)
    ...: ax.set_ylabel(r"$x_2$", fontsize=18)
    ...: plt.colorbar(c, ax=ax)

```

在实践中，可能不总是能够提供用于评估目标函数的梯度和 Hessian 的函数，并且通常使用仅需要函数评估的求解器是方便的。对于这种情况，有几种方法可以对梯度或 Hessian 或两者进行数值估计。近似 Hessian 的方法称为拟牛顿法，也有完全避免使用 Hessian 的替代迭代方法。两种流行的方法是 Broyden-Fletcher-gold farb-Shanno(BFGS)和共轭梯度法，它们在 SciPy 中作为函数`optimize.fmin_bfgs`和`optimize.fmin_cg`实现。BFGS 方法是一种准牛顿方法，可以逐渐建立 Hessian 的数值估计，如果需要，还可以建立梯度的数值估计。共轭梯度法是最速下降法的一种变型，不使用 Hessian 法，并且它可以用于仅从函数评估中获得的梯度的数值估计。使用这些方法，解决一个问题所需的函数求值次数要比牛顿法多得多，另一方面，牛顿法也计算梯度和 Hessian。`optimize.fmin_bfgs`和`optimize.fmin_cg`都可以选择接受一个函数来评估梯度，但是如果没有提供，梯度是从函数评估中估计出来的。

前面给出的用牛顿法解决的问题也可以用`optimize.fmin_bfgs`和`optimize.fmin_cg`来解决，不需要为黑森`:`提供函数

```
In [39]: x_opt = optimize.fmin_bfgs(f, (0, 0), fprime=fprime)
         Optimization terminated successfully.
             Current function value: -3.867223
             Iterations: 10
             Function evaluations: 14
             Gradient evaluations: 14
In [40]: x_opt
Out[40]: array([ 1.88292605,  1.37658523])

In [41]: x_opt = optimize.fmin_cg(f, (0, 0), fprime=fprime)
         Optimization terminated successfully.
             Current function value: -3.867223
             Iterations: 7
             Function evaluations: 17
             Gradient evaluations: 17
In [42]: x_top
Out[42]: array([ 1.88292613,  1.37658522])

```

请注意，在这里，如前一节中优化求解器的诊断输出所示，函数和梯度求值的次数比牛顿法多。如前所述，这两种方法都可以在不提供梯度函数的情况下使用，如下例所示，使用`optimize.fmin_bfgs`解算器:

```
In [43]: x_opt = optimize.fmin_bfgs(f, (0, 0))
         Optimization terminated successfully.
             Current function value: -3.867223
             Iterations: 10
             Function evaluations: 56
             Gradient evaluations: 14
In [44]: x_opt
Out[44]: array([ 1.88292604,  1.37658522])

```

在这种情况下，函数求值的数量甚至更大，但是显然不必为梯度和 Hessian 实现函数是方便的。

一般来说，BFGS 方法通常是一个很好的第一种尝试方法，特别是在梯度和 Hessian 都不知道的情况下。如果只有梯度是已知的，那么 BFGS 方法仍然是普遍推荐使用的方法，尽管共轭梯度法通常是 BFGS 方法的一个有竞争力的替代方法。如果梯度和 Hessian 都已知，那么牛顿法是一般情况下收敛最快的方法。然而，应该注意的是，尽管 BFGS 和共轭梯度法理论上比牛顿法收敛得慢，但它们有时能提供更好的稳定性，因此是优选的。与拟牛顿法和共轭梯度法相比，牛顿法的每次迭代在计算上要求更高，特别是对于大型问题，尽管需要更多的迭代，这些方法可能更快。

到目前为止，我们讨论的多变量优化方法一般都收敛于局部最小值。对于具有许多局部最小值的问题，这很容易导致求解器很容易陷入局部最小值的情况，即使存在全局最小值。虽然对于这个问题没有完整和通用的解决方案，但是可以部分缓解这个问题的实用方法是在坐标网格上使用强力搜索来为迭代求解器找到合适的起始点。至少这给出了在给定坐标范围内寻找全局最小值的系统方法。在 SciPy 中，函数`optimize.brute`可以进行这样的系统搜索。为了说明这种方法，考虑极小化函数 4 sin*xπ*+6 sin*yπ*+(*x*1)<sup>2</sup>+(*y*1)<sup>2</sup>的问题，该函数有大量的局部极小值。这使得为迭代求解器选择合适的初始点变得棘手。为了用 SciPy 解决这个优化问题，我们首先为目标函数定义一个 Python 函数:

```
In [45]: def f(X):
   ...:      x, y = X
   ...:      return (4 * np.sin(np.pi * x) + 6 * np.sin(np.pi * y)) + (x - 1)**2 + (y - 1)**2

```

为了系统地搜索坐标网格上的最小值，我们调用`optimize.brute`，将目标函数`f`作为第一个参数，将一组`slice`对象作为第二个参数，每个对象对应一个坐标。`slice`对象指定搜索最小值的坐标网格。这里我们还设置了关键字参数`finish=None`，它防止`optimize.brute`自动提炼最佳候选。

```
In [46]: x_start = optimize.brute(f, (slice(-3, 5, 0.5), slice(-3, 5, 0.5)), finish=None)
In [47]: x_start
Out[47]: array([ 1.5,  1.5])
In [48]: f(x_start)
Out[48]: −9.5

```

在给定的`slice`对象元组指定的坐标网格上，最优点为( *x* <sub>1</sub> ， *x* <sub>2</sub> ) = (1.5，1.5)，对应的目标函数最小值为 9.5。对于更复杂的迭代求解器来说，这是一个很好的起点，比如`optimize.fmin_bfgs`:

```
In [49]: x_opt = optimize.fmin_bfgs(f, x_start)
         Optimization terminated successfully.
              Current function value: -9.520229
              Iterations: 4
              Function evaluations: 28
              Gradient evaluations: 7
In [50]: x_opt
Out[50]: array([ 1.47586906,  1.48365788])
In [51]: f(x_opt)
Out[51]: −9.52022927306

```

这里 BFGS 方法给出了最终的最小点(*x*T3】1，*x*T7】2)=(1.47586906，1.48365788)，目标函数的最小值为 9.52022927306。对于这种类型的问题，猜测初始起点很容易导致迭代求解器收敛到局部最小值，而`optimize.brute`提供的系统方法通常是有用的。

一如既往，尽可能将目标函数和解决方案可视化是非常重要的。接下来的两个代码单元绘制了当前目标函数的等高线图，并用红星标记了所获得的解(见图 [6-5](#Fig5) )。和前面的例子一样，我们需要一个包装器函数来重组目标函数的参数，因为坐标向量传递给函数的方式不同(分别是分开的数组和打包到一个数组中)。

![../images/332789_2_En_6_Chapter/332789_2_En_6_Fig5_HTML.png](../images/332789_2_En_6_Chapter/332789_2_En_6_Fig5_HTML.png)

图 6-5

**目标函数**f*(*x*)= 4*sin xπ*+6*sin yπ*+(*x*-1)<sup>2</sup>+(*y*-1)<sup>2</sup>*。最小值标有红星。**

 *```
In [52]: def func_X_Y_to_XY(f, X, Y):
    ...:     """
    ...:     Wrapper for f(X, Y) -> f([X, Y])
    ...:     """
    ...:     s = np.shape(X)
    ...:     return f(np.vstack([X.ravel(), Y.ravel()])).reshape(*s)
In [53]: fig, ax = plt.subplots(figsize=(6, 4))
    ...: x_ = y_ = np.linspace(-3, 5, 100)
    ...: X, Y = np.meshgrid(x_, y_)
    ...: c = ax.contour(X, Y, func_X_Y_to_XY(f, X, Y), 25)
    ...: ax.plot(x_opt[0], x_opt[1], 'r*', markersize=15)
    ...: ax.set_xlabel(r"$x_1$", fontsize=18)
    ...: ax.set_ylabel(r"$x_2$", fontsize=18)
    ...: plt.colorbar(c, ax=ax)

```

在本节中，我们已经为特定的解算器显式调用了函数，例如`optimize.fmin_bfgs`。然而，与标量优化一样，SciPy 也为所有多变量优化求解器提供了一个带有函数`optimize.minimize`的统一接口，该函数根据`method`关键字参数的值分派给求解器特定的函数(记住，提供统一接口的单变量最小化函数是`optimize.scalar_minimize`)。为了清楚起见，这里我们倾向于为特定的解算器显式调用函数，但一般来说，使用`optimize.minimize`是个好主意，因为这样可以更容易地在不同的解算器之间切换。例如，在前面的例子中，我们以下面的方式使用了`optimize.fmin_bfgs`,

```
In [54]: x_opt = optimize.fmin_bfgs(f, x_start)

```

我们也可以用

```
In [55]: result = optimize.minimize(f, x_start, method= 'BFGS')
In [56]: x_opt = result.x

```

`optimize.minimize`函数返回一个代表优化结果的`optimize.OptimizeResult`实例。特别是，该解决方案可通过该类的`x`属性获得。

## 非线性最小二乘问题

在第 [5](05.html) 章中，我们遇到了线性最小二乘问题，并探讨了如何用线性代数方法解决这些问题。一般来说，最小二乘问题可以看作是一个目标函数为![$$ g\left(\beta \right)={\sum}_{i=0}^m{r}_i{\left(\beta \right)}^2={\left|\left|r\left(\beta \right)\right|\right|}^2 $$](../images/332789_2_En_6_Chapter/332789_2_En_6_Chapter_TeX_IEq7.png)的优化问题，其中 *r* ( *β* )是一个带有残差的向量*r*<sub>*I*</sub>(*β*)=*y*<sub>*I*</sub>-*f*(*x* <sub>**β* 为一组 *m* 观测值(*x*<sub>I</sub>*</sub>， *y* <sub>* i *</sub> )。 这里 *β* 是一个未知参数的向量，指定函数 *f* ( *x* ， *β* )。如果这个问题在参数 *β* 中是非线性的，它被称为非线性最小二乘问题，并且由于它是非线性的，它不能用第 [5](05.html) 章中讨论的线性代数技术来解决。相反，我们可以使用前一节中描述的多元优化技术，如牛顿法或拟牛顿法。然而，这种非线性最小二乘优化问题具有特定的结构，并且已经开发了几种适合于解决这种特定优化问题的方法。一个例子是 Levenberg-Marquardt 方法，它基于在每次迭代中连续线性化问题的思想。

在 SciPy 中，函数`optimize.leastsq`提供了一个使用 Levenberg-Marquardt 方法的非线性最小二乘解算器。为了说明如何使用这个函数，考虑一个形式为 *f* ( *x* ，*β*)=*β*<sub>0</sub>+*β*<sub>1</sub>exp(*β*<sub>2</sub>*x*<sup>2</sup>)的非线性模型和一组观察值( *x* 在下面的例子中，我们用添加到真实值的随机噪声来模拟观察值，并且我们解决了最小化问题，给出了参数 *β* 的最佳最小二乘估计。首先，我们用参数向量 *β* 的真值定义一个元组，并为模型函数定义一个 Python 函数。该函数应返回与给定的 *x* 值相对应的 *y* 值，以变量 *x* 作为第一个参数，以下参数为未知函数参数:

```
In [57]: beta = (0.25, 0.75, 0.5)
In [58]: def f(x, b0, b1, b2):
    ...:    return b0 + b1 * np.exp(-b2 * x**2)

```

一旦定义了模型函数，我们就生成模拟观察结果的随机数据点。

```
In [59]: xdata = np.linspace(0, 5, 50)
In [60]: y = f(xdata, *beta)
In [61]: ydata = y + 0.05 * np.random.randn(len(xdata))

```

准备好模型函数和观测数据后，我们就可以开始求解非线性最小二乘问题了。第一步是在给定数据和模型函数的情况下为残差定义一个函数，该函数根据尚未确定的模型参数 *β* 来指定。

```
In [62]: def g(beta):
    ...:     return ydata - f(xdata, *beta)

```

接下来，我们定义参数向量的初始猜测，并让`optimize.leastsq`函数求解参数向量的最佳最小二乘拟合:

```
In [63]: beta_start = (1, 1, 1)
In [64]: beta_opt, beta_cov = optimize.leastsq(g, beta_start)
In [65]: beta_opt
Out[65]: array([ 0.25733353,  0.76867338,  0.54478761])

```

这里，最佳拟合非常接近真实的参数值(0.25，0.75，0.5)，如前所述。通过绘制真实和拟合函数参数的观察数据和模型函数，我们可以直观地确认拟合模型似乎很好地描述了数据(见图 [6-6](#Fig6) )。

![../images/332789_2_En_6_Chapter/332789_2_En_6_Fig6_HTML.png](../images/332789_2_En_6_Chapter/332789_2_En_6_Fig6_HTML.png)

图 6-6

*非线性最小二乘拟合函数 f* ( *x* 、*β*)=*β*<sub>0</sub>+*β*<sub>1</sub>*exp*(-*β*<sub>2</sub>*x*<sup>2</sup>)*与β* = (0.25，0

```
In [66]: fig, ax = plt.subplots()
    ...: ax.scatter(xdata, ydata, label="samples")
    ...: ax.plot(xdata, y, 'r', lw=2, label='true model')
    ...: ax.plot(xdata, f(xdata, *beta_opt), 'b', lw=2, label='fitted model')
    ...: ax.set_xlim(0, 5)
    ...: ax.set_xlabel(r"$x$", fontsize=18)
    ...: ax.set_ylabel(r"$f(x, \beta)$", fontsize=18)
    ...: ax.legend()

```

SciPy `optimize`模块还通过函数`optimize.curve_fit`提供了非线性最小二乘拟合的替代接口。这是一个围绕`optimize.leastsq`的方便包装器，它消除了为最小二乘问题显式定义残差函数的需要。因此，前一个问题可以使用以下方法更简洁地解决:

```
In [67]: beta_opt, beta_cov = optimize.curve_fit(f, xdata, ydata)
In [68]: beta_opt
Out[68]: array([ 0.25733353,  0.76867338,  0.54478761])

```

## 约束优化

约束给优化问题增加了另一层复杂性，它们需要自己的分类。约束优化的一种简单形式是坐标变量受到某些限制的优化。比如:![$$ {\min}_xf(x) $$](../images/332789_2_En_6_Chapter/332789_2_En_6_Chapter_TeX_IEq8.png)服从 0 ≤ * x * ≤ 1。约束 0 ≤ * x * ≤ 1 很简单，因为它只限制坐标的范围，而不依赖于其他变量。这种类型的问题可以使用 SciPy 中的 L-BFGS-B 方法来解决，这是我们前面使用的 BFGS 方法的变体。该解算器可通过函数`optimize.fmin_l_bgfs_b`或`method`参数设置为`'L-BFGS-B'`的`optimize.minimize`获得。要定义坐标边界，必须使用`bound`关键字参数，其值应该是包含每个约束变量的最小值和最大值的元组列表。如果最小值或最大值被设置为`None`，则被解释为无界。

作为用 L-BFGS-B 解算器求解有界优化问题的例子，考虑最小化目标函数*f*(*x*)=(*x*<sub>1</sub>1】<sup>2</sup>-(*x*T12】21)<sup>2</sup>服从约束 2≤*x*T18】1≤3 和 0 为了解决这个问题，我们首先为目标函数定义一个 Python 函数，并根据给定的约束，为这个问题中的两个变量中的每一个定义边界元组。作为比较，在下面的代码中，我们也用相同的目标函数求解无约束优化问题，我们绘制目标函数的等高线图，其中无约束和有约束的最小值分别用蓝色和红色的星星标记(见图 [6-7](#Fig7) )。

![../images/332789_2_En_6_Chapter/332789_2_En_6_Fig7_HTML.png](../images/332789_2_En_6_Chapter/332789_2_En_6_Fig7_HTML.png)

图 6-7

目标函数 f(x)的轮廓，带有无约束(蓝色星形)和约束最小值(红色星形)。约束问题的可行域用灰色阴影表示。

```
In [69]: def f(X):
    ...:     x, y = X
    ...:     return (x - 1)**2 + (y - 1)**2
In [70]: x_opt = optimize.minimize(f, [1, 1], method="BFGS").x
In [71]: bnd_x1, bnd_x2 = (2, 3), (0, 2)
In [72]: x_cons_opt = optimize.minimize(f, [1, 1], method='L-BFGS-B',
    ...:                                 bounds=[bnd_x1, bnd_x2]).x
In [73]: fig, ax = plt.subplots(figsize=(6, 4))
    ...: x_ = y_ = np.linspace(-1, 3, 100)
    ...: X, Y = np.meshgrid(x_, y_)
    ...: c = ax.contour(X, Y, func_X_Y_to_XY(f, X, Y), 50)
    ...: ax.plot(x_opt[0], x_opt[1], 'b*', markersize=15)
    ...: ax.plot(x_cons_opt[0], x_cons_opt[1], 'r*', markersize=15)
    ...: bound_rect = plt.Rectangle((bnd_x1[0], bnd_x2[0]),
    ...:                            bnd_x1[1] - bnd_x1[0], bnd_x2[1] - bnd_x2[0], facecolor="grey")
    ...: ax.add_patch(bound_rect)
    ...: ax.set_xlabel(r"$x_1$", fontsize=18)
    ...: ax.set_ylabel(r"$x_2$", fontsize=18)
    ...: plt.colorbar(c, ax=ax)

```

由包含一个以上变量的等式或不等式定义的约束处理起来有些复杂。然而，对于这种类型的问题也有通用的技术。例如，使用拉格朗日乘子，可以通过引入附加变量将约束优化问题转化为无约束问题。例如，考虑等式约束 *g* ( *x* ) = 0 的优化问题![$$ {\min}_xf(x) $$](../images/332789_2_En_6_Chapter/332789_2_En_6_Chapter_TeX_IEq9.png)。在无约束优化问题中， *f* ( *x* )的梯度在最优点处消失，∇ * f * ( * x * ) = 0。可以看出，约束问题的对应条件是负梯度位于约束法线支持的空间中，即![$$ -\nabla f(x)=\lambda {\mathrm{J}}_g^T(x) $$](../images/332789_2_En_6_Chapter/332789_2_En_6_Chapter_TeX_IEq10.png)。这里 J<sub>*g*</sub>(*x*)是约束函数 *g* ( *x* )的雅可比矩阵， *λ* 是拉格朗日乘子(新变量)的向量。这个条件源于将函数λ(x，λ)=*f*(*x*)+*λ*<sup>*T*</sup>*g*(*x*)的梯度等于零，这就是所谓的拉格朗日函数。因此，如果 *f* ( *x* )和 *g* ( *x* )都是连续光滑的，那么函数λ(x，λ)的一个驻点( *x* <sub>0</sub> ， *λ* <sub>0</sub> )对应的一个 *x* <sub>0</sub> 就是原约束的一个最优注意，如果 *g* ( *x* )是一个标量函数(即只有一个约束)，那么雅可比 j<sub>*g*</sub>(*x*)简化为梯度∇ * g * ( *x* )。

为了说明这种技术，考虑边长为 *x* <sub>1</sub> 、 *x* <sub>2</sub> 和 *x* <sub>3</sub> 的矩形的体积最大化问题， 受总表面积应为 1 的约束:*g*(*x*)= 2*x*<sub>1</sub>*x*<sub>2</sub>+2*x*<sub>0</sub>*x*<sub>2</sub>+2*x*<sub>1</sub>*x *为了使用拉格朗日乘子来解决这个优化问题，我们形成拉格朗日乘子λ(x)=*f*(*x*)+*λg*(*x*)并且寻找∇λ(x = 0 的驻点。使用 SymPy，我们可以通过首先为问题中的变量定义符号，然后为 *f* ( *x* )、 *g* ( *x* )和λ(x)构造表达式来执行这项任务，**

```
In [74]: x = x0, x1, x2, l = sympy.symbols("x_0, x_1, x_2, lambda")
In [75]: f = x0 * x1 * x2
In [76]: g = 2 * (x0 * x1 + x1 * x2 + x2 * x0) - 1
In [77]: L = f + l * g

```

最后使用`sympy.diff`计算∇λ(x，并使用`sympy.solve`求解方程∇λ(x)= 0:

![$$ \left[\left\{\lambda :-\frac{\sqrt{6}}{24},{x}_0:\frac{\sqrt{6}}{6},{x}_1:\frac{\sqrt{6}}{6},{x}_2:\frac{\sqrt{6}}{6}\right\},\left\{\lambda :\frac{\sqrt{6}}{24},{x}_0:-\frac{\sqrt{6}}{6},{x}_1:-\frac{\sqrt{6}}{6},{x}_2:-\frac{\sqrt{6}}{6}\right\}\right] $$](../images/332789_2_En_6_Chapter/332789_2_En_6_Chapter_TeX_Equf.png)

```
In [78]: grad_L = [sympy.diff(L, x_) for x_ in x]
In [79]: sols = sympy.solve(grad_L)
In [80]: sols
Out[80]:

```

这个过程给出了两个驻点。我们可以通过评估每种情况下的目标函数来确定哪一个对应于最优解。但是，这里只有一个驻点对应一个物理上可以接受的解:既然这个问题中 *x* <sub>*i*</sub> 是一个矩形边的长度，那么它一定是正的。因此，我们可以立即识别出有趣的解决方案，它对应于直观的结果![$$ {x}_0={x}_1={x}_2=\frac{\sqrt{6}}{6} $$](../images/332789_2_En_6_Chapter/332789_2_En_6_Chapter_TeX_IEq11.png)(一个立方体)。作为最后的验证，我们使用获得的解来评估约束函数和目标函数:

![$$ \frac{\sqrt{6}}{36} $$](../images/332789_2_En_6_Chapter/332789_2_En_6_Chapter_TeX_Equg.png)

```
In [81]: g.subs(sols[0])
Out[81]: 0
In [82]: f.subs(sols[0])
Out[82]:

```

这种方法也可以扩展到处理不等式约束，并且存在各种应用这种方法的数值方法。一个例子是被称为序列最小二乘编程的方法，缩写为 SLSQP，它在 SciPy 中作为`optimize.slsqp`函数可用，并通过`optimize.minimize`和`method='SLSQP'`可用。`optimize.minimize`函数接受关键字参数`constraints`，它应该是一个字典列表，每个字典指定一个约束。该字典中允许的键(值)有`type` ( `'eq'`或`'ineq'`)、`fun`(约束函数)、`jac`(约束函数的雅可比)，以及`args`(约束函数的附加参数和计算其雅可比的函数)。例如，描述前一个问题中的约束的约束字典将是`dict(type='eq', fun=g)`。

为了使用 SciPy 的 SLSQP 解算器数值求解整个问题，我们需要为目标函数和约束函数定义 Python 函数:

```
In [83]: def f(X):
    ...:     return -X[0] * X[1] * X[2]
In [84]: def g(X):
    ...:     return 2 * (X[0]*X[1] + X[1] * X[2] + X[2] * X[0]) - 1

```

注意，由于 SciPy 优化函数解决最小化问题，并且这里我们对最大化感兴趣，函数`f`在这里是原始目标函数的负数。接下来我们为 *g* ( *x* ) = 0 定义约束字典，最后调用`optimize.minimize`函数

```
In [85]: constraint = dict(type='eq', fun=g)
In [86]: result = optimize.minimize(f, [0.5, 1, 1.5], method="SLSQP", constraints=[constraint])
In [87]: result
Out[87]:  status: 0
         success: True
            njev: 18
            nfev: 95
             fun: -0.068041368623352985
               x: array([ 0.40824187,  0.40825127,  0.40825165])
         message: 'Optimization terminated successfully.'
             jac: array([-0.16666925, -0.16666542, -0.16666527,  0.])
             nit: 18
In [88]: result.x
Out[88]: array([ 0.40824187,  0.40825127,  0.40825165])

```

正如所预料的那样，这个解与用拉格朗日乘子进行符号计算得到的分析结果吻合得很好。

解决有不等式约束的问题，我们需要做的就是在约束字典中设置`type='ineq'`，并提供相应的不等式函数。为了演示具有非线性不等式约束的非线性目标函数的最小化，我们返回到之前考虑的二次问题，但是在这种情况下，不等式约束*g*(*x*)=*x*<sub>1</sub>-1.75(*x*<sub>0</sub>-0.75)<sup>4</sup>≥0。像往常一样，我们首先定义目标函数和约束函数，以及约束字典:

```
In [89]: def f(X):
    ...:     return (X[0] - 1)**2 + (X[1] - 1)**2
In [90]: def g(X):
    ...:     return X[1] - 1.75 - (X[0] - 0.75)**4
In [91]: constraints = [dict(type='ineq', fun=g)]

```

接下来，我们准备通过调用`optimize.minimize`函数来解决优化问题。为了比较，这里我们也求解相应的无约束问题。

```
In [92]: x_opt = optimize.minimize(f, (0, 0), method="BFGS").x
In [93]: x_cons_opt = optimize.minimize(f, (0, 0), method="SLSQP", constraints=constraints).x

```

为了验证所获得的解的可靠性，我们绘制了目标函数的轮廓以及表示可行区域的阴影区域(其中满足不等式约束)。受约束和不受约束的解分别标有红色和蓝色的星号(见图 [6-8](#Fig8) )。

![../images/332789_2_En_6_Chapter/332789_2_En_6_Fig8_HTML.png](../images/332789_2_En_6_Chapter/332789_2_En_6_Fig8_HTML.png)

图 6-8

目标函数的等高线图，约束问题的可行域为灰色阴影。红色和蓝色的星星分别是约束和非约束问题中的最优点。

```
In [94]: fig, ax = plt.subplots(figsize=(6, 4))
In [95]: x_ = y_ = np.linspace(-1, 3, 100)
    ...: X, Y = np.meshgrid(x_, y_)
    ...: c = ax.contour(X, Y, func_X_Y_to_XY(f, X, Y), 50)
    ...: ax.plot(x_opt[0], x_opt[1], 'b*', markersize=15)
    ...: ax.plot(x_, 1.75 + (x_-0.75)**4, 'k-', markersize=15)
    ...: ax.fill_between(x_, 1.75 + (x_-0.75)**4, 3, color="grey")
    ...: ax.plot(x_cons_opt[0], x_cons_opt[1], 'r*', markersize=15)
    ...:
    ...: ax.set_ylim(-1, 3)
    ...: ax.set_xlabel(r"$x_0$", fontsize=18)
    ...: ax.set_ylabel(r"$x_1$", fontsize=18)
    ...: plt.colorbar(c, ax=ax)

```

对于只有*不等式约束的优化问题，SciPy 使用线性逼近约束优化(COBYLA)方法提供了一个替代的求解器。该解算器可通过`optimize.fmin_cobyla`或`optimize.minimize`和`method='COBYLA'`访问。通过用`method='COBYLA'`替换`method='SLSQP'`，前一个例子也可以用这个求解器解决。*

### 线性规划

在前一节中，我们考虑了非常一般的优化问题的方法，其中目标函数和约束函数都可以是非线性的。然而，在这一点上，值得后退一步，考虑一种更受限制的优化问题，即*线性规划*，其中目标函数是线性的，所有约束都是线性等式或不等式约束。这类问题显然没有那么普遍，但事实证明，线性规划有许多重要的应用，并且它们可以比一般的非线性问题更有效地解决。这是因为线性问题的性质使得可以使用完全不同的方法。特别地，线性优化问题的解必须位于约束边界上，因此搜索线性约束函数的交点就足够了。这在实践中可以有效地完成。这类问题的一种流行算法被称为*单纯形*，它系统地从一个顶点移动到另一个顶点，直到到达最佳顶点。也有更多最近的内点方法有效地解决线性规划问题。使用这些方法，具有数千个变量和约束的线性规划问题很容易解决。

线性规划问题一般写成所谓的标准形式:![$$ {\min}_x{c}^Tx $$](../images/332789_2_En_6_Chapter/332789_2_En_6_Chapter_TeX_IEq12.png)其中 *Ax* ≤ *b* 和 *x* ≥ 0。这里 *c* 和 *x* 是长度为 *n* 的向量，而 *A* 是 a * m * × *n* 矩阵和*b*A*m*-向量。例如，考虑最小化函数*f*(*x*)=*x*<sub>0</sub>+2*x*<sub>1</sub>3*x*<sub>2</sub>的问题，受三个不等式约束*x*<sub>0</sub>+*x*<sub>1<sub>在标准形式上，我们有*c*=(1，2，3)， *b* = (1，2，3)，以及</sub></sub>

![$$ A=\left(\begin{array}{ccc}1&amp; 1&amp; 0\\ {}-1&amp; 3&amp; 0\\ {}0&amp; -1&amp; 1\end{array}\right). $$](../images/332789_2_En_6_Chapter/332789_2_En_6_Chapter_TeX_Equh.png)

为了解决这个问题，这里我们使用了`cvxopt`库，它为线性规划求解器提供了`cvxopt.solvers.lp`函数。该求解器期望将前面文本中介绍的标准形式中使用的 *c* 、 *A* 和 *b* 向量和矩阵作为参数，按照给定的顺序。`cvxopt`库使用自己的类来表示矩阵和向量，但幸运的是，它们可以通过数组接口 <sup>[3](#Fn3)</sup> 与 NumPy 数组互操作，因此可以使用`cvxopt.matrix`和`np.array`函数从一种形式转换为另一种形式。由于 NumPy 数组是科学 Python 环境中事实上的标准数组格式，因此尽可能使用 NumPy 数组是明智的，只有在必要时才转换为 cvxopt 矩阵，即在调用`cvxopt.solvers`中的一个解算器之前。

为了使用`cvxopt`库解决上述示例问题，我们首先为 *A* 矩阵和 *c* 和 *b* 向量创建 NumPy 数组，并使用`cvxpot.matrix`函数将它们转换为`cvxopt`矩阵:

```
In [96]: c = np.array([-1.0, 2.0, -3.0])
In [97]: A = np.array([[ 1.0, 1.0, 0.0],
                       [-1.0, 3.0, 0.0],
                       [ 0.0, -1.0, 1.0]])
In [98]: b = np.array([1.0, 2.0, 3.0])
In [99]: A_ = cvxopt.matrix(A)
In [100]: b_ = cvxopt.matrix(b)
In [101]: c_ = cvxopt.matrix(c)

```

`cvxopt`兼容的矩阵和向量`c_`、`A_`和`b_`现在可以传递给线性规划求解器`cvxopt.solvers.lp`:

```
In [102]: sol = cvxopt.solvers.lp(c_, A_, b_)
         Optimal solution found.
In [103]: sol
Out[103]: {'dual infeasibility': 1.4835979218054372e-16,
           'dual objective': -10.0,
           'dual slack': 0.0,
           'gap': 0.0,
           'iterations': 0,
           'primal infeasibility': 0.0,
           'primal objective': -10.0,
           'primal slack': -0.0,
           'relative gap': 0.0,
           'residual as dual infeasibility certificate': None,
           'residual as primal infeasibility certificate': None,
           's': <3x1 matrix, tc="d">,
           'status': 'optimal',
           'x': <3x1 matrix, tc="d">,
           'y': <0x1 matrix, tc="d">,
           'z': <3x1 matrix, tc="d">}
In [104]: x = np.array(sol['x'])
In [105]: x
Out[105]: array([[ 0.25],
                [ 0.75],
                [ 3.75]])
In [106]: sol['primal objective']
Out[106]: -10.0

```

优化问题的解决方案以向量 *x* 的形式给出，在这个特定的例子中是 *x* = (0.25，0.75，3.75)，对应于 *f* ( *x* )值 10。使用这种方法和`cvxopt.solvers.lp`解算器，可以很容易地解决数百或数千个变量的线性规划问题。所有需要做的就是将优化问题写在标准表单上，并创建 *c* 、 *A* 和 *b* 数组。

## 摘要

优化——从一组备选方案中选择最佳方案——是科学和工程中许多应用的基础。数学最优化为系统地处理最优化问题提供了一个严格的框架，如果它们可以被公式化为一个数学问题的话。最优化的计算方法是在实践中解决这种最优化问题的工具。因此，在科学计算环境中，优化扮演着非常重要的角色。对于使用 Python 的科学计算，SciPy 库提供了解决许多标准优化问题的高效例程，可用于解决各种各样的计算优化问题。然而，优化是一个很大的数学领域，需要不同的方法来解决不同类型的问题，Python 有几个优化库，为特定类型的优化问题提供了专门的解决方案。一般来说，SciPy `optimize`模块为各种各样的优化问题提供了良好而灵活的通用解算器，但对于特定类型的优化问题，也有许多专门的库提供了更好的性能或更多的功能。这种库的一个例子是`cvxopt`，它用线性和二次问题的高效解算器补充了 SciPy 中的通用优化例程。

## 进一步阅读

关于最优化的通俗易懂的介绍，以及本章介绍的几种方法的数值性质的更详细的讨论，例如，参见 Heath (2002)。有关优化的更严格和深入的介绍，请参见 E.K.P. Chong (2013)。出色的书(S. Boyd，2004)中的`cvxopt`库的创建者给出了对凸优化的彻底处理，该书也可在 [`http://stanford.edu/~boyd/cvxbook`](http://stanford.edu/%257Eboyd/cvxbook) 在线获得。

## 参考

E.K.P. Chong，S. Z. (2013 年)。*优化入门*(第 4 版。).纽约:威利。

希思，M. (2002 年)。科学计算:介绍性调查。).波士顿:麦格劳希尔公司。

南博伊德，L. V. (2004 年)。*凸优化。*剑桥:剑桥大学出版社。

<aside class="FootnoteSection" epub:type="footnotes">Footnotes [1](#Fn1_source)

由于历史原因，优化问题通常被称为*编程*问题，与计算机编程无关。

  [2](#Fn2_source)

在实际中，不需要计算 Hessian 的逆，取而代之我们可以求解线性方程组*h*<sub>*f*</sub>(*x*<sub>*k*</sub>)*y*<sub>*k*</sub>=—∇f(*x*<sub>*k*</sub>)并使用积分公式*x*

*  [3](#Fn3_source)

详见 [`http://docs.scipy.org/doc/numpy/reference/arrays.interface.html`](http://docs.scipy.org/doc/numpy/reference/arrays.interface.html) 。

 *</aside>**