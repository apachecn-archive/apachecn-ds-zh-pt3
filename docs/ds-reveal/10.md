# 10.聚类分析

本章以结构化的形式简要介绍了聚类分析的概念。在前面的章节中，我们已经充分讨论了监督学习。在监督学习中，我们提出一个具有一组正确答案的模型，然后我们允许它预测看不见的数据。现在，让我们把注意力稍微转移一下。假设我们有一组变量的数据，但没有需要关注的独立变量。在这种情况下，我们不会对一个现象做出任何合理的假设。

本章介绍无监督学习。我们没有给出一个有一系列正确答案的模型；相反，我们允许它做出明智的猜测。无监督学习包括降维和聚类分析。在本章中，我们首先让您熟悉一种被广泛称为主成分分析(PCA)的降维方法，然后让您熟悉聚类分析。此后，我们开发和评估模型，如 K-均值模型，凝聚模型，和 DBSCAN 模型。在对数据进行预处理和建模之前，我们先来讨论一下聚类分析。

## 什么是聚类分析？

聚类分析是一种确定地找到一组具有相似性和差异性的数据点的方法。在聚类分析中，我们关注解释数据变化的不同聚类。没有真正的因变量。我们平等对待所有变量。流行的聚类技术包括*质心聚类*，它是随机的，将数据点的质心选择到一组指定的聚类中，即 K-means；*密度聚类*，根据密度总体对数据点进行分组，即 DBSCAN 以及*分布聚类*，基于某种分布，即高斯混合模型等，识别数据点属于一个聚类的概率。

## 实践中的聚类分析

表 [10-1](#Tab1) 显示了该用例的数据。数据中有三个变量。没有实际的自变量。所有的变量都同等重要。我们关心的是数据中的相似性和差异性。同样，我们从 Kaggle 获得了示例数据。 <sup>[1](#Fn1)</sup>

表 10-1

资料组

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"></colgroup> 
|   | 

年龄

 | 

年收入(千美元)

 | 

支出得分(1-100 分)

 |
| --- | --- | --- | --- |
| **0** | Nineteen | Fifteen | Thirty-nine |
| **1** | Twenty-one | Fifteen | Eighty-one |
| **2** | Twenty | Sixteen | six |
| **3** | Twenty-three | Sixteen | Seventy-seven |
| **4** | Thirty-one | Seventeen | Forty |

## 相关矩阵

我们从计算数据中变量之间的关联开始。当数据中有一组连续变量时，使用皮尔逊方法来衡量相关性。列表 [10-1](#PC1) 产生皮尔逊相关矩阵(见表 [10-2](#Tab2) )。

表 10-2

皮尔逊相关矩阵

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"></colgroup> 
|   | 

年龄

 | 

年收入(千美元)

 | 

支出得分(1-100 分)

 |
| --- | --- | --- | --- |
| **年龄** | 1.000000 | -0.012398 | -0.327227 |
| **年收入(千美元)** | -0.012398 | 1.000000 | 0.009903 |
| **消费得分(1-100)** | -0.327227 | 0.009903 | 1.000000 |

```py
dfcorr = df.corr()
dfcorr

Listing 10-1Pearson Correlation Matrix

```

表 [10-2](#Tab2) 表达了年龄和年收入之间脆弱的负相关，以及年龄和支出分数之间坚固的负相关。

## 协方差矩阵

计算协方差的主要目的是得到特征矩阵。列表 [10-2](#PC2) 和表格 [10-3](#Tab3) 总结了变量之间的可变性。

表 10-3

协方差矩阵

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"></colgroup> 
|   | 

年龄

 | 

年收入(千美元)

 | 

支出得分(1-100 分)

 |
| --- | --- | --- | --- |
| **年龄** | 195.133166 | -4.548744 | -118.040201 |
| **年收入(千美元)** | -4.548744 | 689.835578 | 6.716583 |
| **消费得分(1-100)** | -118.040201 | 6.716583 | 666.854271 |

```py
dfcov = df.cov()
dfcov

Listing 10-2Covariance Matrix

```

至多，我们对变量之间的协方差不感兴趣；相反，我们感兴趣的是它们之间的相关性以及相关性的严重程度。

## 特征矩阵

为了可靠地找出变量之间相关性的严重程度，使用特征值。特征值小于 0 表示多重共线性，介于 10 和 100 之间表示轻微多重共线性，大于 100 表示严重多重共线性。清单 [10-3](#PC3) 计算并列出特征值和特征向量(见表 [10-4](#Tab4) )。

表 10-4

自定义矩阵

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"></colgroup> 
|   | 

特征值

 | 

年龄

 | 

年收入(千美元)

 | 

支出得分(1-100 分)

 |
| --- | --- | --- | --- | --- |
| **年龄** | 167.228524 | 0.973210 | 0.188974 | -0.130965 |
| **年收入(千美元)** | 700.264355 | 0.005517 | -0.588641 | -0.808376 |
| **消费分数(1**–**100)** | 684.330136 | 0.229854 | -0.785997 | 0.573914 |

```py
eigenvalues, eigenvectors = np.linalg.eig(dfcov)
eigenvalues = pd.DataFrame(eigenvalues)
eigenvectors = pd.DataFrame(eigenvectors)
eigen = pd.concat([eigenvalues,eigenvectors],axis=1)
eigen.index = df.columns
eigen.columns = ("Eigen values","Age","Annual Income (k$)","Spending Score (1-100)")
eigen

Listing 10-3Eigen Matrix

```

表 [10-4](#Tab4) 显示了特征向量和特征值。存在严重的多重共线性。正如我们在前面的章节中突出提到的，多重共线性会对结论产生不利影响。

清单 [10-4](#PC4) 应用`StandardScaler()`方法来缩放数据，使得平均值为 0，标准偏差为 1。

```py
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df = scaler.fit_transform(df)

Listing 10-4Normalize Data

```

接下来，我们执行 PCA。

## 主成分分析

在主成分分析中，我们使用特征值来寻找数据中变化的来源。PCA 计算累积比例。请记住，我们不会将 PCA 建立在模型的基础上。它找到解释变异的数据点。和因子分析一样，我们用 PCA <sup>[2](#Fn2)</sup> 来精简一组变量的结构。它将主成分计算为变量的线性组合。该方法的主要目的是解释总方差。它还将数据压缩成更少的组件，以便使用的组件表示模型估计。最后，它消除了具有大特征值(极端可变性)的因素。不仅如此，它还使用凯泽标准来发现要保留的因子的数量。图 [10-1](#Fig1) 简化了一组变量的结构。

![../images/506148_1_En_10_Chapter/506148_1_En_10_Fig1_HTML.png](../images/506148_1_En_10_Chapter/506148_1_En_10_Fig1_HTML.png)

图 10-1

主成分分析

在这里，变量(M1、M2 和 M3)是基于一些因素或组成部分，C1 和 C2。回想一下，我们对发现支持主成分的变量感兴趣。清单 [10-5](#PC5) 返回 PCA 加载并创建一个加载矩阵。

```py
pca = PCA()
pca.fit_transform(df)
pca_variance = pca.explained_variance_
plt.figure(figsize=(8, 6))
plt.bar(range(3), pca_variance, align="center", label="Individual Variance")
plt.legend()
plt.ylabel("Variance Ratio")
plt.xlabel("Principal Components")
plt.show()

Listing 10-5Explained Variance

```

图 [10-2](#Fig2) 显示了组件对预测变量相对重要性的排序。它证实了第一个因素解释了数据中的大部分变化。见清单 [10-6](#PC6) 。

![../images/506148_1_En_10_Chapter/506148_1_En_10_Fig2_HTML.jpg](../images/506148_1_En_10_Chapter/506148_1_En_10_Fig2_HTML.jpg)

图 10-2

解释方差

```py
pca = PCA(n_components=3).fit(df)
pca_components = pca.components_.T
pca_components = pd.DataFrame(pca_components)
pca_components.index = df.columns
pca_components.columns = df.columns
pca_components

Listing 10-6Principal Components Matrix

```

清单 [10-6](#PC6) 返回一个条件索引，描述每个组件的 PCA 加载(见表 [10-5](#Tab5) )。

表 10-5

主成分

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"></colgroup> 
|   | 

年龄

 | 

年收入(千美元)

 | 

支出得分(1-100 分)

 |
| --- | --- | --- | --- |
| **年龄** | -0.188974 | 0.130965 | 0.973210 |
| **年收入(千美元)** | 0.588641 | 0.808376 | 0.005517 |
| **消费分数(1**–**100)** | 0.785997 | -0.573914 | 0.229854 |

年龄具有最高的 PCA 负荷(它是第一成分)，其次是年收入(`k$`)等等。清单 [10-7](#PC7) 转换数据。记住，我们将组件的数量指定为 3。

```py
pca2 = PCA(n_components=3)
pca2.fit(df)
x_3d = pca2.transform(df)
plt.figure(figsize=(8,6))
plt.scatter(x_3d[:,0], x_3d[:,2], c=old_df['Annual Income (k$)'])
plt.xlabel("y")
plt.show()

Listing 10-7Finalize PCA

```

执行降维后，我们使用散点图绘制数据点。图 [10-3](#Fig3) 显示了数据点的分布。见清单 [10-8](#PC8) 。

![../images/506148_1_En_10_Chapter/506148_1_En_10_Fig3_HTML.jpg](../images/506148_1_En_10_Chapter/506148_1_En_10_Fig3_HTML.jpg)

图 10-3

主成分

```py
df = pca.transform(df)

Listing 10-8Reduce Data Dimension

```

## 肘形曲线

肘部曲线被广泛认为是曲线曲线的*拐点。我们用它来发现数据中的聚类数。它在 x 轴上显示聚类的数量，在 y 轴上显示方差的百分比。我们使用一个分界点来选择聚类的数量。截止点是平滑折弯结束和急剧折弯开始的点。见清单 [10-9](#PC9) 。*

```py
Nc = range(1,20)
kmeans = [KMeans(n_clusters=i) for i in Nc]
scores = [kmeans[i].fit(df).score(df) for i in range(len(kmeans))]
fig, ax = plt.subplots()
plt.plot(Nc, scores)
plt.xlabel("No. of clusters")
plt.show()

Listing 10-9Elbow Curve

```

图 [10-4](#Fig4) 显示了从群集 1 到群集 3 的平滑弯曲。然而，从集群 3 开始，曲线突然弯曲。我们用 3 作为分界点。

![../images/506148_1_En_10_Chapter/506148_1_En_10_Fig4_HTML.jpg](../images/506148_1_En_10_Chapter/506148_1_En_10_Fig4_HTML.jpg)

无花果

ure 10-4

剥猫皮有各种各样的方法。您也可以使用 scree 图来认可截止点。请记住，我们通常使用 PCA 中的 scree 图来查找要保留的元件数量。清单 [10-10](#PC10) 产生一个 scree 图，我们也可以用它来确定用于集群模型的集群数量(见图 [10-5](#Fig5) )。

![../images/506148_1_En_10_Chapter/506148_1_En_10_Fig5_HTML.jpg](../images/506148_1_En_10_Chapter/506148_1_En_10_Fig5_HTML.jpg)

图 10-5

碎石图

```py
ks = range(1,20)
ds = []
for k in ks:
    cls = KMeans(n_clusters=k)
    cls.fit(df)
    ds.append(cls.inertia_)
fig, ax = plt.subplots()
plt.plot(ks, ds)
plt.xlabel("Value of k")
plt.ylabel("Distortion")
plt.show()

Listing 10-10Scree Plot

```

图 [10-5](#Fig5) 批准碎石绘图点。

## k 均值聚类

K-means 模型是最流行的聚类模型。它将数据分割成具有最近平均值*质心*的 *k* 个聚类，然后找到子集之间的距离以形成一个聚类。它还减少了集群内的距离，提高了集群间的距离。我们用公式 10-1 表示。

![$$ Dis\left({x}_1,{x}_2\right)=\sqrt{\sum_{i=0}^n{\left({x}_{1i}-{y}_{2i}\right)}^2} $$](../images/506148_1_En_10_Chapter/506148_1_En_10_Chapter_TeX_Equ1.png)

(Equation 10-1)

基本上，模型找到初始的 *k* (类的数量)并计算类之间的距离。此后，它将每个数据点分配到最近的质心。该模型最适合大样本。见清单 [10-11](#PC11) 。

```py
kmeans = KMeans(n_clusters=3)
kmeans_output = kmeans.fit(df)
kmeans_output

Listing 10-11K-Means Model Using Default Hyperparameters

```

### k-均值超参数优化

至多，我们不想用初始超参数来拟合模型，除非它产生最佳性能。对于每个用例，我们必须找到最佳的超参数(假设一个模型有可调的超参数)。表 [10-6](#Tab6) 概述了关键可调 K 均值超参数。

表 10-6

可调超参数

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

参数

 | 

描述

 |
| --- | --- |
| `copy_X` | 确定是否必须复制自变量。 |
| `tol` | 找到精度。默认值为 0.003。 |
| `max_iter` | 查找最大迭代次数。 |
| `tol` | 找到优化的公差 |

清单 [10-12](#PC12) 找到 K 均值模型的最佳超参数。

```py
param_gridkmeans = {"copy_x":[False,True],
                    "max_iter":[1,10,100,1000],
                    "n_init":[5,10,15,20],
                    "tol":[0.0001,0.001,0.01,1.0]}
grid_modelkmeans = GridSearchCV(estimator=kmeans_output, param_grid=param_gridkmeans)
grid_modelkmeans.fit(df)
print("Best score: ", grid_modelkmeans.best_score_, "Best hyper-parameters: ", grid_modelkmeans.best_params_)

Listing 10-12Hyperparameter Optimization

```

最佳得分:-49470.220198874384 最佳超参数:{'copy_x': False，' max_iter': 1，' n_init': 15，' tol': 0.01}

清单 [10-13](#PC13) 总结了具有最优超参数的 K 均值模型。

```py
kmeans = KMeans(n_clusters=3,
                copy_x=False,
                max_iter= 1, n_init= 15,
                tol= 0.01)
kmeans_output = kmeans.fit(df)
kmeans_output

Listing 10-13Finalize the K-Means Model

```

清单 [10-14](#PC14) 列出了预测的标签(表格 [10-7](#Tab7) )。

表 10-7

预测标签

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
|   | 

预测

 |
| --- | --- |
| **0** | one |
| **1** | one |
| **2** | one |
| **3** | one |
| **4** | one |
| **...** | ... |
| **195** | Zero |
| **196** | Two |
| **197** | Zero |
| **198** | Two |
| **199** | Zero |

```py
y_predkmeans = pd.DataFrame(kmeans_output.labels_, columns = ["Predicted"])
y_predkmeans

Listing 10-14Predicted Labels

```

表 [10-7](#Tab7) 没有告诉我们太多。我们基本上是在观察 K 均值模型产生的标签。

### 质心

数据的集中趋势表明了关于数据的许多信息。我们必须找到星团的中心，也就是质心。清单 [10-15](#PC15) 应用`cluster_centers_`寻找质心(见表 [10-8](#Tab8) )。

表 10-8

质心

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"></colgroup> 
|   | 

群组 1

 | 

群组 2

 | 

第 3 组

 |
| --- | --- | --- | --- |
| **0** | -8.879001 | -11.467679 | 0.891796 |
| **1** | -10.435900 | 42.928296 | -5.127039 |
| **2** | 43.326081 | 2.774284 | 1.470721 |

```py
kmean_centroids = pd.DataFrame(kmeans_output.cluster_centers_, columns = ("Cluster 1","Cluster 2","Cluster 3"))
kmean_centroids

Listing 10-15Centroids

```

为了真正理解这个模型，清单 [10-16](#PC16) 创建了一个散点图，显示集群和集群中心(见图 [10-6](#Fig6) )。

![../images/506148_1_En_10_Chapter/506148_1_En_10_Fig6_HTML.jpg](../images/506148_1_En_10_Chapter/506148_1_En_10_Fig6_HTML.jpg)

图 10-6

k 均值模型

```py
fig, ax = plt.subplots()
plt.scatter(df[:,0],df[:,1],c=kmeans_output.labels_,cmap="viridis",s=20)
plt.scatter(kmean_centroids[:,0], kmean_centroids[:,1], color="red")
plt.xlabel("y")
plt.show()

Listing 10-16Visualize the K-Means Model

```

K-means 模型进行智能猜测，直到数据点被分配到最近的质心，并找到质心的平均值。图 [10-6](#Fig6) 表明数据中有三个明显的聚类。

## 剪影配乐

我们使用剪影法来评估 K-means 模型。该方法比较了内聚和分离。其值的范围为-1 到 1，其中-1 表示严重的错误标注，0 表示聚类中存在重叠，1 表示样本远离相邻聚类。参见清单 [10-17](#PC17) 。

```py
metrics.silhouette_score(df, y_predkmeans)
0.3839349967742105

Listing 10-17Silhouette Score

```

剪影评分 0.38，说明模特离完美还很远。集群中存在重叠。

## 凝聚聚类

凝聚模型是一种分层模型，使用自下而上的方法来生成树状图。它开发了 *n* 个集群(每个集群一个)，然后计算邻近度。此后，它合并彼此靠近的聚类，同时更新邻近度矩阵。最终结果是一个单一的集群。有几种方法可以计算数据点之间的距离。表 [10-9](#Tab9) 强调了关键的距离计算技术。

表 10-9

距离技巧

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

技术

 | 

描述

 |
| --- | --- |
| **单链聚类** | 寻找聚类之间的最小距离 |
| **完全连锁聚类** | 寻找聚类之间的最大距离 |
| **平均连锁聚类** | 寻找聚类之间的平均距离 |
| **质心连锁聚类** | 寻找聚类质心之间的距离 |

与随机初始化质心的 K-means 模型不同，agglomerative 基于分辨率生成多个分区。清单 [10-18](#PC18) 完成了凝聚模型。

```py
agglo = AgglomerativeClustering(n_clusters=None,distance_threshold=0)
agglo_output = agglo.fit(pca_df)
agglo_output
Listing 10-19.

Listing 10-18Finalize the Agglomerative Model

```

最终确定凝聚模型后，列表 [10-19](#PC19) 查找并列出预测标签(表 [10-10](#Tab10) )。清单 [10-20](#PC20) 显示了烧结模型。

表 10-10

预测标签

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
|   | 

预测

 |
| --- | --- |
| **0** | -1 |
| **1** | -1 |
| **2** | -1 |
| **3** | -1 |
| **4** | -1 |
| **...** | ... |
| **195** | -1 |
| **196** | -1 |
| **197** | -1 |
| **198** | -1 |
| **199** | -1 |

```py
y_pred = pd.DataFrame(agglo_output.labels_,columns = ["Predicted"])
y_pred

Listing 10-19Predicted Labels

```

```py
fig, ax = plt.subplots()
plt.scatter(df[:,0],df[:,1],c=agglo_output.labels_,cmap="viridis",s=20)
plt.xlabel("y")
plt.show()

Listing 10-20Agglomerative Model

```

图 [10-7](#Fig7) 指出了三个集群。大集群分解成两个小集群。我们还发现集群 1 和集群 2 之间有少量重叠。

![../images/506148_1_En_10_Chapter/506148_1_En_10_Fig7_HTML.jpg](../images/506148_1_En_10_Chapter/506148_1_En_10_Fig7_HTML.jpg)

图 10-7

凝聚模型

我们使用树状图来描述聚类树的层次结构。清单 [10-21](#PC21) 创建一个连锁矩阵，创建每个节点下的样本数，并创建相应的树状图(见图 [10-8](#Fig8) )。

![../images/506148_1_En_10_Chapter/506148_1_En_10_Fig8_HTML.jpg](../images/506148_1_En_10_Chapter/506148_1_En_10_Fig8_HTML.jpg)

图 10-8

凝聚树状图

```py
def plot_dendrogram(model, **kwargs):
    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack([model.children_, model.distances_,
                                      counts]).astype(float)
    dendrogram(linkage_matrix, **kwargs)
plot_dendrogram(agglo_output, truncate_mode="level", p=3)

Listing 10-21Visualize the Dendrogram

```

图 [10-8](#Fig8) 显示了聚类连接的数据点(一个父母带两个孩子)。

## 带噪声的基于密度的空间聚类模型

我们使用基于密度的带噪声的空间聚类模型(DBSCAN)来发现任意空间的聚类。它可以识别包含多个数据点的半径区域。任何数据点都可以是核心点、边界点或异常点。基于这些点将点分组为簇。表 [10-11](#Tab11) 强调了这些要点。

表 10-11

DBSCAN 点

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

点

 | 

描述

 |
| --- | --- |
| **核心点** | 由最少数量的邻居包围的点。核心点被边界点包围。 |
| **边界点** | 没有被最小数量的邻居包围并且可以从核心点到达的点。 |
| **异常点** | 既不是核心点也不是边界点的点。这是一个远离核心点所及的点。 |

它会找到相邻的核心点，并将它们放在同一个群集中。因此，一个集群包含至少一个核心点(可到达点)或边界点。此外，它还发现了被不同簇包围的簇。参见清单 [10-22](#PC22) 。

```py
dbscan = DBSCAN()
dbscan_output = dbscan.fit(df)
dbscan_output

Listing 10-22Finalize the DBSCAN Model

```

列表 [10-23](#PC23) 和表格 [10-12](#Tab12) 突出显示预测标签。

表 10-12

预测类别

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
|   | 

预测

 |
| --- | --- |
| **0** | -1 |
| **1** | -1 |
| **2** | -1 |
| **3** | -1 |
| **4** | -1 |
| **...** | ... |
| **195** | -1 |
| **196** | -1 |
| **197** | -1 |
| **198** | -1 |
| **199** | -1 |

```py
y_pred = pd.DataFrame(dbscan_output.labels_,columns = ["Predicted"])
y_pred

Listing 10-23Predicted Labels

```

清单 [10-24](#PC24) 描述了 DBSCAN 模型的特征(见图 [10-9](#Fig9) )。

![../images/506148_1_En_10_Chapter/506148_1_En_10_Fig9_HTML.jpg](../images/506148_1_En_10_Chapter/506148_1_En_10_Fig9_HTML.jpg)

图 10-9

DBSCAN 模型

```py
fig, ax = plt.subplots()
plt.scatter(df[:,0],df[:,1],c=dbscan_output.labels_,cmap="viridis",s=20)
plt.xlabel("y")
plt.show()

Listing 10-24Visualize the DBSCAN Model

```

图 [10-9](#Fig9) 显示 DBSCAN 模型并没有做出充分的智能猜测。这并不意味着该模型在所有情况下都表现不佳。当满足基于密度的标准时，该模型是合适的。很明显，高斯分布表现出优越的性能。

## 结论

本章介绍了降维和聚类分析。我们开发和评估了三种聚类模型，即 K-means、DBSCAN 模型和凝聚模型。我们开始使用主成分分析法来降低数据的维度。此后，我们采用肘曲线来发现在 K-均值模型和凝聚模型上指定的聚类数。在开发 DBSCAN 模型时，我们不假设集群的数量。

集群模型是宽松的；他们没有强有力的假设。聚类分析令人生畏的部分是模型评估。它们缺乏稳健的评估指标，我们依靠剪影方法进行模型评估。

<aside aria-label="Footnotes" class="FootnoteSection" epub:type="footnotes">Footnotes [1](#Fn1_source)

[T2`https://www.kaggle.com/kandij/mall-customers`](https://www.kaggle.com/kandij/mall-customers)

  [2](#Fn2_source)

[T2`https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)

 </aside>