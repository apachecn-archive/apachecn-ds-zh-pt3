# 七、使用支持向量寻找超平面

前一章介绍了一种称为线性判别分析(LDA)的线性分类模型，当协方差矩阵相等时，它会平均分配组。虽然分类器是最佳线性分类模型之一，但它有其局限性。首先，我们不能用分类变量来估计因变量。第二，我们在严格的正态假设下训练和测试模型。本章汇集了一个替代的线性分类模型，称为支持向量机(SVM)。它是合奏大家庭的一部分；它估计连续变量或分类变量。它应用一个核函数来变换数据，使超平面最适合数据。与 LDA 不同，SVM 对数据的基本结构不做任何假设。

## 支持向量机

SVM 通常假设数据点在低维是线性不可区分的，而在高维是线性可区分的。它找到一个最优超平面，以充分利用边缘和简化非线性问题。首先，它有效地捕获数据，并开发了一个新的维度向量空间。此后，它将新向量空间的线性边界发展成两类。它将数据点定位在超平面的上方或下方，从而导致分类。它使用一个核函数来估计一个具有极端边缘的超平面。主要的核函数是线性函数、多项式函数和 sigmoid 函数。除非你是在分析高维数据，否则完全没有必要去深入研究这些函数。您可以随时使用超参数优化来找到最佳函数。

### 支持向量

支持向量是超平面旁边的数据点。它们决定了超平面的位置。此外，当我们消除这些数据点时，超平面的位置会改变。

### 超平面

超平面是分成两类的线(每类落在两边)。它是双方的分界线。当使用真实世界的数据时，定位最佳超平面是具有挑战性的。SVM 通过一个叫做*内核化*的程序将数据转换到更高维度。转换完数据后，它展开的不是一条直线，而是一个平面。它应用一个核函数来转换数据，因此它在适当的位置定位一个超平面。图 [7-1](#Fig1) 描绘了一个超平面。

![img/506148_1_En_7_Chapter/506148_1_En_7_Fig1_HTML.png](img/506148_1_En_7_Chapter/506148_1_En_7_Fig1_HTML.png)

图 7-1

超平面

图 [7-1](#Fig1) 显示了等式 7-1 中表示的直线公式。

![$$ WT\ x+b=0 $$](img/506148_1_En_7_Chapter/506148_1_En_7_Chapter_TeX_Equ1.png)

(Equation 7-1)

这里， *W* 表示直线的斜率， *x* 表示输入向量， *b* 表示偏置。两条线(以橙色突出显示)穿过支持向量并支持最佳平面。一个体面的超平面对于支持向量有一个极端的余量。

它通过使用扩大分离的优化过程，计算出如何在训练期间定位超平面。我们不手动添加超平面。它应用一种被认为是内核技巧的技术来转换数据。该技术使其能够以较小的计算能力扩大维向量空间。

### 支持向量分类器

支持向量分类器(SVC)，也称为*边缘分类器*，允许某些数据点位于超平面的不适当一侧。如果数据不可直接区分，它包括松弛变量，通过允许某些数据点落在边界内来允许违反约束；此外，它还惩罚他们。它将时差变量限制为非负值。如果松弛变量超过 0，它会通过在数据点旁边放置一个平面来设置约束，而不是通过选择一个λ来设置边距。图 [7-2](#Fig2) 显示了 SVC 是如何工作的。

![img/506148_1_En_7_Chapter/506148_1_En_7_Fig2_HTML.png](img/506148_1_En_7_Chapter/506148_1_En_7_Fig2_HTML.png)

图 7-2

LinearSVC 工作流程

LinearSVC 分类器执行三个基本任务。

*   *转换*:将输入空间转换为高维向量空间

*   *直线分离或直线分割*:实现最大余量的超平面

*   *分类*:将数据点分配到一个类别

## 开发 LinearSVC 分类器

我们从 Kaggle 获得了示例数据。 <sup>[1](#Fn1)</sup> 清单 [7-1](#PC1) 完成分类器。

```py
from sklearn.svm import LinearSVC
lsvc = LinearSVC()
lsvc.fit(x_train,y_train)

Listing 7-1Finalize the LinearSVC Classifier

```

### 线性超参数优化

表 [7-1](#Tab1) 突出显示了关键的超参数。

表 7-1

可调超参数

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

参数

 | 

描述

 |
| --- | --- |
| `Dual` | 确定模型是否必须解决双重问题 |
| `fit_intercept` | 确定模型是否必须计算截距。 |
| `max_iter` | 确定最大迭代次数。 |
| `Penalty` | 确定要使用的正则化方法。可用的罚项包括 l1 罚(套索)和 l2 罚(岭)。 |
| `Tol` | 确定优化的公差。 |
| `Loss` | 确定函数的类型。可用的损失函数包括铰链和方形铰链。 |

清单 [7-2](#PC2) 创建了一个字典，其中包含我们期望从中找到最佳交叉验证分数的组参数和特定范围的值，然后应用`GridSearchCV()`方法来开发网格模型，并找到具有最佳分数的超参数。

```py
param_gridlsvc = {"dual":[False,True],
                  "fit_intercept":[False,True],
                  "max_iter":[1,10,100,100],
                  "penalty":("l1","l2"),
                  "tol":[0.0001,0.001,0.01,1.0],
                  "loss":("hinge", "squared_hinge")}
grid_modellsvc = GridSearchCV(estimator=lsvc, param_grid=param_gridlsvc)
grid_modellsvc.fit(x_train,y_train)
print("Best score: ", grid_modellsvc.best_score_, "Best parameters: ", grid_modellsvc.best_params_)

Listing 7-2Hyperparameter Optimization

```

最佳得分:0.7671331467413035 最佳参数:{'dual': False，' fit_intercept': True，' loss': 'squared_hinge '，' max_iter': 100，' penalty': 'l1 '，' tol': 0.001}

### 完成 LinearSVC 分类器

先前的发现表明，我们使用平方铰链函数，并使用 L1 罚项控制偏差和方差。清单 [7-3](#PC3) 完成了 LinearSVC 分类器。

```py
lsvc = LinearSVC(dual= False,
                 fit_intercept= True,
                 max_iter= 100,
                 penalty= 'l1',
                 tol= 0.001,
                 loss="squared_hinge")
lsvc.fit(x_train,y_train)

Listing 7-3Finalize the LinearSVC Classifier

```

### 评估 LinearSVC 分类器

清单 [7-4](#PC4) 并排列出了实际类别和预测类别。参见表 [7-2](#Tab2) 。

表 7-2

实际值和预测值

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
|   | 

实际的

 | 

预测

 |
| --- | --- | --- |
| **661** | one | one |
| **122** | Zero | Zero |
| **113** | Zero | Zero |
| **14** | one | one |
| **529** | Zero | Zero |
| **...** | ... | ... |
| **第 476 章** | one | Zero |
| **第 482 章** | Zero | Zero |
| **230** | one | one |
| **527** | Zero | Zero |
| **380** | Zero | Zero |

```py
y_predlsvc = lsvc.predict(x_test)
pd.DataFrame({"Actual":y_test,"Predicted":y_predlsvc})

Listing 7-4Actual Values and Predicted Values

```

#### 混淆矩阵

列表 [7-5](#PC5) 和表格 [7-3](#Tab3) 给了我们一份关于分类器性能的摘要报告。

表 7-3

混淆矩阵

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
|   | 

预测:否

 | 

预测:是

 |
| --- | --- | --- |
| **实际:否** | Ninety-eight | nine |
| **实际:是** | Eighteen | Twenty-nine |

```py
cmatlsvc = pd.DataFrame(metrics.confusion_matrix(y_test,y_predlsvc),
                        index=["Actual: No","Actual: Yes"],
                        columns=("Predicted: No","Predicted: Yes"))
cmatlsvc

Listing 7-5Confusion Matrix

```

#### 分类报告

列表 [7-6](#PC6) 和表格 [7-4](#Tab4) 显示了 LinearSVC 分类器的可靠性。它提供了模型的准确度分数、精确度分数、召回率和其他关键评估指标。

表 7-4

分类报告

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"></colgroup> 
|   | 

精确

 | 

召回

 | 

f1-分数

 | 

支持

 |
| --- | --- | --- | --- | --- |
| **0** | 0.844828 | 0.915888 | 0.878924 | 107.000000 |
| **1** | 0.763158 | 0.617021 | 0.682353 | 47.000000 |
| **精度** | 0.824675 | 0.824675 | 0.824675 | 0.824675 |
| **宏平均值** | 0.803993 | 0.766455 | 0.780638 | 154.000000 |
| **加权平均值** | 0.819902 | 0.824675 | 0.818931 | 154.000000 |

```py
creportlsvc = pd.DataFrame(metrics.classification_report(y_test,y_predlsvc, output_dict=True)).transpose()
creportlsvc

Listing 7-6Classification Report

```

LinearSVC 分类器的准确率为 82%。当预测类 0 时，它的准确率为 84%,当预测类 1 时，它的准确率为 76%。

#### 学习曲线

列表 [7-7](#PC7) 生成一条曲线，描述了 LinearSVC 分类器在学习训练数据时的精度变化(见图 [7-3](#Fig3) )。

![img/506148_1_En_7_Chapter/506148_1_En_7_Fig3_HTML.jpg](img/506148_1_En_7_Chapter/506148_1_En_7_Fig3_HTML.jpg)

图 7-3

学习曲线

```py
trainsizelsvc, trainscorelsvc, testscorelsvc = learning_curve(lsvc, x, y, cv=5, n_jobs=5, train_sizes=np.linspace(0.1,1.0,50))
trainscorelsvc_mean = np.mean(trainscorelsvc,axis=1)
testscorelsvc_mean = np.mean(testscorelsvc,axis=1)
plt.plot(trainsizelsvc,trainscorelsvc_mean,color="red", label="Training Score")
plt.plot(trainsizelsvc,testscorelsvc_mean,color="navy", label="Cross Validation Score")
plt.xlabel("Training Set Size")
plt.ylabel("Accuracy")
plt.legend(loc=4)
plt.show()

Listing 7-7Learning Curve

```

在学习过程的第一阶段，训练准确度分数急剧下降到约 70%的历史低点，随后出现向上反弹。当分类器达到第 200 个<sup>个</sup>数据点时，训练准确度得分提高。最多，训练准确度分数低于交叉验证准确度分数。最后，当分类器即将到达第 600 个<sup>第</sup>个数据点时，训练准确度分数超过交叉验证准确度分数。

## 结论

本章介绍了 SVM 如何将输入空间转换为一个更高维的向量空间，并创建一个超平面和分类变量。我们用平方铰链损失函数和 L1 惩罚项配置了 LinearSVC 分类器。记住，我们不开发 ROC 曲线和精确召回曲线，因为分类器缺乏概率重要性。我们依靠分类报告和学习曲线来理解分类器的基本性能。

<aside aria-label="Footnotes" class="FootnoteSection" epub:type="footnotes">Footnotes [1](#Fn1_source)

访问 [`https://www.kaggle.com/uciml/pima-indians-diabetes-database`](https://www.kaggle.com/uciml/pima-indians-diabetes-database) 下载数据。

 </aside>