# 1.简单线性回归简介

这本书向你介绍了数据科学的世界。它揭示了进行数据科学的正确方法。它涵盖了基本的统计和编程技术，以帮助您从广泛的角度理解数据科学。不仅如此，它还为使用数据科学技术解决问题提供了理论、技术和数学基础。

本章涵盖了*参数法*，也称为*线性法*。了解如何在违反回归假设的情况下测试回归变量，将使您能够轻松处理后续章节中的问题。阅读时，记住示例数据有一个因变量是很重要的。本章不涉及具有方差膨胀因子(VIF)的多重共线性。

简单线性回归估计自变量和因变量之间关系的性质，其中*自变量*是连续变量或分类变量，而*因变量*必然是连续变量。它研究自变量的变化如何影响因变量的变化。我们用公式 1-1 表示标准公式。

![$$ y=a+ bx $$](../images/506148_1_En_1_Chapter/506148_1_En_1_Chapter_TeX_Equ1.png)

(Equation 1-1)

这里， *y* 代表因变量， *a* 代表截距(假设我们持有自变量常数，因变量的均值)， *b* 代表斜率或梯度(直线的方向)，而 *x* 代表自变量。

该模型将直线拟合到数据点。如果它完美地将直线拟合到数据点，那么它就是一个示例性的模型。相反，如果数据点严重偏离直线，那么它就违反了回归假设。

![../images/506148_1_En_1_Chapter/506148_1_En_1_Fig1_HTML.png](../images/506148_1_En_1_Chapter/506148_1_En_1_Fig1_HTML.png)

图 1-1

完美线性回归模型的成对散点图 x

图 [1-1](#Fig1) 展示了一个最优模型(由于可变性，它在现实世界中几乎不会发生)。数据点最多分散在直线附近。为了解决这个问题，回归器(或回归模型)在建模期间引入误差项，使得数据点的平方偏差很小。它估计截距和斜率以减少误差项。最常见的回归变量是最小二乘模型。等式 1-2 表示最小二乘模型的公式。

![$$ \hat{y}={\beta}_0+{\beta}_1{X}_1+{\varepsilon}_i $$](../images/506148_1_En_1_Chapter/506148_1_En_1_Chapter_TeX_Equ2.png)

(Equation 1-2)

这里，![$$ \hat{y} $$](../images/506148_1_En_1_Chapter/506148_1_En_1_Chapter_TeX_IEq1.png)代表期望因变量，*β*T3】0 代表截距，*β*T7】1 代表斜率，*X*T11】1 代表自变量， *ε* <sub>* i *</sub> 代表误差项(残差为 *n 的 *i* <sup> th</sup>*

![$$ {e}_i={y}_i-\hat{y_i} $$](../images/506148_1_En_1_Chapter/506148_1_En_1_Chapter_TeX_Equ3.png)

(Equation 1-3)

最小二乘回归器确保残差平方和很小。等式 1-4 通过使用下面的性质来估计残差的平方和。

![$$ {e}_1^2+{e}_2^2\dots {e}_i^2 $$](../images/506148_1_En_1_Chapter/506148_1_En_1_Chapter_TeX_Equ4.png)

(Equation 1-4)

等式 1-4 假设残差总是等于零，并且估计是无偏的。

## 回归假设

如果回归变量满足表 [1-1](#Tab1) 中规定的假设，那么它是可靠的。

表 1-1

线性回归假设

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

假定

 | 

描述

 |
| --- | --- |
| **线性度** | 自变量和因变量之间必须有线性关系。我们使用成对散点图验证了这一假设。 |
| **常态** | 数据必须来自正态分布。当变量的值围绕真实平均值对称分布时，存在正态分布。我们使用正态概率图(也称为*正态 Q-Q 图*)验证了这一假设。对于可靠的正态性检验，使用一种称为 *Kolmogorov-Smirnov 检验*的检验统计量。如果不存在正态性，则对数据进行变换。 |
| **多重共线性** | 数据中必须很少或没有多重共线性。如果自变量相关性高，则存在多重共线性。我们通过使用相关矩阵、容差和 VIF 来验证这一假设。如果存在多重共线性，我们必须将数据居中。示例数据只有一个因变量，因此不包括 VIF 的多重共线性。 |
| **无自相关** | 残差中不能有自相关。当残差不是相互独立时，就存在自相关。我们使用滞后图、自相关函数图和部分自相关图来验证这一假设。对于可靠的自相关测试，使用都柏林-沃森测试。 |

相反，如果一个回归变量违反了基本的回归假设，那么就很难推广一个模型。

## 实践中的简单线性回归

示例数据 <sup>[1](#Fn1)</sup> 有两列(每列代表一个变量)。第一列包含员工的工作年限值(自变量)，第二列包含员工的工资值(因变量)。

## 检查数据质量

最小二乘模型是敏感的(数据质量影响其性能)。为了获得最佳模型性能，请确保没有缺失值或异常值，并在训练回归变量之前缩放数据。

### 检测缺失值

数据中明显存在的缺失值会导致模型性能不佳。为了避免这种困境，在加载数据后，检查数据中是否有丢失的值。

#### 缺失值热图

清单 [1-1](#PC1) 绘制了显示缺失值的热图(见图 [1-2](#Fig2) )。

![../images/506148_1_En_1_Chapter/506148_1_En_1_Fig2_HTML.jpg](../images/506148_1_En_1_Chapter/506148_1_En_1_Fig2_HTML.jpg)

图 1-2

缺失值热图

```
import seaborn as sns
sns.heatmap(df.isnull(),cmap="Blues")
plt.show()

Listing 1-1Missing Values Heatmap

```

图 [1-2](#Fig2) 显示数据中没有检测到缺失值。如果数据中有缺失值，热图将如图 [1-3](#Fig3) 所示。

![../images/506148_1_En_1_Chapter/506148_1_En_1_Fig3_HTML.jpg](../images/506148_1_En_1_Chapter/506148_1_En_1_Fig3_HTML.jpg)

图 1-3

缺失值热图

如果数据中有缺失值，用变量的平均值或中值替换缺失值。列表 [1-2](#PC2) 用平均值替换缺失值。

```
df["YearsExperience"].fillna(df["YearsExperience"].mean())
df["Salary"].fillna(df["YearsExperience"].mean())

Listing 1-2Replace Missing Values with the Mean Value

```

列表 [1-3](#PC3) 用中间值替换缺失值。

```
df["YearsExperience"].fillna(df["YearsExperience"].median())
df["Salary"].fillna(df["YearsExperience"].median())

Listing 1-3Replace Missing Values with the Median Value

```

### 检测常态

回归假设数据遵循正态分布(数据点围绕真实平均值对称分布)。如果数据不正常，执行数据转换以减少其偏斜度。负偏差数据需要幂变换或指数变换。相反，正偏差数据需要对数变换或平方根变换。具有不同均值和方差的标准化正态分布用数学方法写成分布的形式。

![$$ N\left(0,1\right) $$](../images/506148_1_En_1_Chapter/506148_1_En_1_Chapter_TeX_Equ5.png)

(Equation 1-5)

这里，平均值等于 0，标准差等于 1。

#### 柱状图

直方图在 x 轴上显示变量的间隔，在 y 轴上显示值的频率。列表 [1-4](#PC4) 描绘了员工的工作年限，图 [1-4](#Fig4) 展示了员工的工作年限分布。

![../images/506148_1_En_1_Chapter/506148_1_En_1_Fig4_HTML.jpg](../images/506148_1_En_1_Chapter/506148_1_En_1_Fig4_HTML.jpg)

图 1-4

柱状图

```
sns.distplot(df["YearsExperience"])
plt.ylabel("Frequency")
plt.xlabel("Experience")
plt.show()

Listing 1-4Employees’ Years of Work Experience Histogram

```

图 [1-4](#Fig4) 显示员工的工作年限值略微遵循正态分布。我们不能用平均值来概括自变量。

清单 [1-5](#PC5) 绘制了员工工资的直方图(见图 [1-5](#Fig5) )。

![../images/506148_1_En_1_Chapter/506148_1_En_1_Fig5_HTML.jpg](../images/506148_1_En_1_Chapter/506148_1_En_1_Fig5_HTML.jpg)

图 1-5

柱状图

```
sns.distplot(df["Salary"])
plt.ylabel("Frequency")
plt.xlabel("Salary")
plt.show()

Listing 1-5Employees’ Salaries Histogram

```

图 [1-4](#Fig4) 和图 [1-15](#Fig15) 均显示非极值正态分布。当数据中存在异常值时，偏态经常发生。

### 检测异常值

异常值代表太小或太大的数据点。它可以通过夸大错误率来影响模型。如果数据中有异常值，则删除它们，或者用平均值或中值替换它们。

#### 箱形图

检测异常值的最简单方法是使用箱线图。箱形图简明扼要地总结了有关数据的独特形状和离散度的信息。它具有以下属性:代表数据中中间值的方框，代表 50%的数据点所在点的中线，以及代表 25%的数据点位于直线之上而 75%的数据点位于直线之下的点的四分位数。当 Q3 表示 75%的数据结束而 25%的数据未结束的点时，直线和其他线表示将 Q1 或 Q3 与除异常值之外的最远数据点连接起来的触须。

列表 [1-6](#PC6) 描绘了员工的工作年限。

```
sns.boxplot(df["YearsExperience"])
plt.xlabel("Experience")
plt.show()

Listing 1-6Employees’ Years of Work Experience Box Plot

```

![../images/506148_1_En_1_Chapter/506148_1_En_1_Fig6_HTML.jpg](../images/506148_1_En_1_Chapter/506148_1_En_1_Fig6_HTML.jpg)

图 1-6

箱形图

图 [1-6](#Fig6) 显示，员工的工作年限值稍微偏左，数据中没有异常值。

清单 [1-7](#PC7) 为因变量的值构建了一个方框图(见图 [1-7](#Fig7) )。

![../images/506148_1_En_1_Chapter/506148_1_En_1_Fig7_HTML.jpg](../images/506148_1_En_1_Chapter/506148_1_En_1_Fig7_HTML.jpg)

图 1-7

箱形图

```
sns.boxplot(df["Salary"])
plt.xlabel("Salary")
plt.show()

Listing 1-7Employees’ Salaries Box Plot

```

图 [1-7](#Fig7) 表示员工的工资值向左倾斜，数据中没有检测到异常值。图 [1-6](#Fig6) 和图 [1-7](#Fig7) 都有点像正态分布。

列表 [1-8](#PC8) 绘制了员工的工作年限和工资。图 [1-8](#Fig8) 展示了用直线连接的随时间的变化。它有两个轴:x 轴(水平)和 y 轴(垂直)。这些轴用图形表示为( *x* ， *y* )。自变量在 x 轴上，因变量在 y 轴上。

![../images/506148_1_En_1_Chapter/506148_1_En_1_Fig8_HTML.png](../images/506148_1_En_1_Chapter/506148_1_En_1_Fig8_HTML.png)

图 1-8

线形图

```
df.plot(kind="line")
plt.xlabel("Experience")
plt.ylabel("Salary")
plt.show()

Listing 1-8Employees’ Years of Work Experience and Employees’ Salaries Line Plot

```

图 [1-8](#Fig8) 没有显示理想线。数据点总是遵循一个长期的上升趋势。这条直线向右上方倾斜。员工的工资随着工作年限的增加而增加。

清单 [1-9](#PC9) 返回一个使用核密度估计显示概率密度函数的图(见图 [1-9](#Fig9) )。它捕捉连续随机变量的概率。

![../images/506148_1_En_1_Chapter/506148_1_En_1_Fig9_HTML.jpg](../images/506148_1_En_1_Chapter/506148_1_En_1_Fig9_HTML.jpg)

图 1-9

密度图

```
sns.kdeplot(df["YearsExperience"],df["Salary"])
plt.title("Experience and salary density plot")
plt.xlabel("Experience")
plt.ylabel("Salary")
plt.show()

Listing 1-9Density Plot

```

图 [1-9](#Fig9) 表示二维概率密度；但是，分布不够正态。它有聚集的数据点，使得无意中很难解释数据。

清单 [1-10](#PC10) 返回描述变量之间关联的散点图(见图 [1-10](#Fig10) )。

![../images/506148_1_En_1_Chapter/506148_1_En_1_Fig10_HTML.jpg](../images/506148_1_En_1_Chapter/506148_1_En_1_Fig10_HTML.jpg)

图 1-10

成对散点图

```
sns.jointplot(x="YearsExperience",y="Salary",data=df,kind="reg")
plt.xlabel("Experience")
plt.ylabel("Salary")
plt.show()

Listing 1-10Employees’ Years of Work Experience and Employees’ Salaries Joint Plot

```

图 [1-10](#Fig10) 表示直线与数据点完全吻合。员工的工作年限与员工的工资之间存在很强的正相关关系。

列表 [1-11](#PC11) 使用集中趋势总结了数据(见表 [1-2](#Tab2) )。

表 1-2

描述统计学

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"> <col class="tcol6 align-left"> <col class="tcol7 align-left"> <col class="tcol8 align-left"> <col class="tcol9 align-left"></colgroup> 
|   | 

数数

 | 

意思是

 | 

标准

 | 

部

 | 

25%

 | 

50%

 | 

75%

 | 

最大

 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| **年经验** | Thirty | 5.313333 | 2.837888 | One point one | Three point two | Four point seven | Seven point seven | Ten point five |
| **工资** | Thirty | 76003.000000 | 27414.429785 | Thirty-seven thousand seven hundred and thirty-one | Fifty-six thousand seven hundred and twenty point seven five | Sixty-five thousand two hundred and thirty-seven | One hundred thousand five hundred and forty-four point seven five | One hundred and twenty-two thousand three hundred and ninety-one |

```
df.describe().transpose()

Listing 1-11Summarize the Central Tendency

```

表 [1-2](#Tab2) 强调了员工工作经验的算术平均值为 5 年，员工工资的算术平均值为每年 76，003 美元。雇员工作经验年数的值偏离平均值 3，雇员工资的值偏离 27，414 美元。工作年经验最少的是 1 年，最低工资是 37731 美元。工资最高的员工年薪为 122，391 美元，有大约 11 年的工作经验。四分之一的员工年薪约为 56，721 美元。年薪 100，544 美元及以上的雇员属于上层。

## 理解相关性

线性回归就是检查相关系数。相关性估计自变量和因变量之间线性关系的表面强度。有几种方法可以确定两个变量之间的相关性，如皮尔逊相关法、斯皮尔曼相关法和肯德尔相关法。

### 皮尔逊相关法

清单 [1-12](#PC12) 应用皮尔逊相关法(因变量为连续变量)。方程 1-6 表达了这种方法。

![$$ {r}_{xy}=\frac{s_{xy}}{s_x{s}_y} $$](../images/506148_1_En_1_Chapter/506148_1_En_1_Chapter_TeX_Equ6.png)

(Equation 1-6)

这里， *s* <sub>*x*</sub> 代表自变量的标准差， *s* <sub>*y*</sub> 代表因变量的标准差，*s*<sub>*xy*</sub>代表协方差。该方法生成范围从-1 到 1 的值，其中-1 表示强负相关关系，0 表示无相关关系，1 表示强正相关关系。

#### 相关矩阵

图 [1-11](#Fig11) 显示了员工工作年限与员工工资之间的相关性。

![../images/506148_1_En_1_Chapter/506148_1_En_1_Fig11_HTML.jpg](../images/506148_1_En_1_Chapter/506148_1_En_1_Fig11_HTML.jpg)

图 1-11

相关矩阵热图

```
dfcorr = df.corr(method="pearson")
sns.heatmap(dfcorr, annot=True,annot_kws={"size":12}, cmap="Blues")
plt.show()

Listing 1-12Correlation Matrix

```

图 [1-11](#Fig11) 显示了一条从左上角到右下角的 1 线(每个变量都与自身完美关联)。员工的工作年限和员工的工资之间有很强的正相关关系。

### 协方差方法

协方差是两个变量之间的联合可变性(它估计两个变量如何一起变化)。等式 1-7 表示协方差方法。

![$$ {S}_{xy}=\frac{\sum \left({x}_i-\underset{\_}{x}\right)\left({y}_i-\underset{\_}{y}\right)\ }{n-1} $$](../images/506148_1_En_1_Chapter/506148_1_En_1_Chapter_TeX_Equ7.png)

(Equation 1-7)

这里， *n* 是样本数，*y*<sub>T5】I</sub>表示标量随机变量，![$$ \underset{\_}{x} $$](../images/506148_1_En_1_Chapter/506148_1_En_1_Chapter_TeX_IEq2.png)和![$$ \underset{\_}{y} $$](../images/506148_1_En_1_Chapter/506148_1_En_1_Chapter_TeX_IEq3.png)是标量随机变量的平均值。

### 协方差矩阵

列表 [1-13](#PC13) 产生协方差矩阵(见图 [1-12](#Fig12) )。

![../images/506148_1_En_1_Chapter/506148_1_En_1_Fig12_HTML.png](../images/506148_1_En_1_Chapter/506148_1_En_1_Fig12_HTML.png)

图 1-12

协方差矩阵热图

```
dfcov = df.cov()
sns.heatmap(dfcov, annot=True,annot_kws={"size":12}, cmap="Blues")
plt.show()

Listing 1-13Covariance Matrix

```

图 [1-12](#Fig12) 揭示了协方差约为 0.0076(接近 1)。它证实了变量之间的正相关关系。

## 分配和整形数组

清单 [1-14](#PC14) 使用 NumPy 分配数组 *x* 和 *y* 。数组保存一个或多个变量。

```
x = np.array(df["YearsExperience"])
y = np.array(df["Salary"])

Listing 1-14Assign x and y arrays

```

清单 [1-15](#PC15) 整形 *x* 和 *y* (将一维数据转换成二维数据)。

```
x = x.reshape(-1,1)
y = y.reshape(-1,1)

Listing 1-15Reshape x and y arrays

```

## 将数据分为训练数据和测试数据

监督模型需要数据分区。清单 [1-16](#PC16) 通过调用`train_test_split()`方法将数据分成训练和测试数据。

```
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=0)

Listing 1-16Split Data into Training and Test Data

```

注意，80%的数据是用于训练的，20%的数据是用于测试的。

## 标准化数据

广泛使用的缩放方法有`StandardScaler()`，以均值为 0，标准差为 1，2 的方式缩放数据；`MinMaxScaler()`，如果数据中有负值，则在 0 和 1 之间或-1 和 1 之间缩放数据；以及`RobustScaler()`，去除异常值并使用标度数据(使用之前的任一标度)。清单 [1-17](#PC17) 应用了`StandardScaler()`方法来标准化数据。

```
from sklearn.model_selection import StandardScaler
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

Listing 1-17Normalize Data

```

## 使用 Statsmodels 开发最小二乘模型

Statsmodels 是一个 Python 科学包，包含一组不同的类和函数，用于正确估计统计模型和执行测试统计。它补充了统计计算的 SciPy 包。默认情况下，它不包括截距。清单 [1-18](#PC18) 手动给模型添加一个截距。

```
x_constant = sm.add_constant(x_train)
x_test = sm.add_constant(x_test)

Listing 1-18Add a Constant

```

清单 [1-19](#PC19) 完成回归器。

```
model = sm.OLS(y_train, x_constant).fit()

Listing 1-19Develop the Least Squares Model

```

### 预言

列表 [1-20](#PC20) 列出了预测值(见表 [1-3](#Tab3) )。

表 1-3

预测值

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
|   | 

预计工资

 |
| --- | --- |
| **0** | 40748.961841 |
| **1** | 122699.622956 |
| **2** | 64961.657170 |
| **3** | 63099.142145 |
| **4** | 115249.562855 |
| **5** | 107799.502753 |

```
y_pred = model.predict(x_test)
pd.DataFrame(y_pred, columns = ["Predicted salary"])

Listing 1-20Predicted Values

```

### 评估模型

清单 [1-21](#PC21) 返回回归器的性能摘要(见表 [1-4](#Tab4) )。

表 1-4

结果

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"></colgroup> 
| 

离开变量:

 | 

y

 | 

r 平方:

 | 

Zero point nine four one

 |
| --- | --- | --- | --- |
| **型号:** | 内源性阿片样物质 | **可调 R 平方:** | Zero point nine three nine |
| **方法:** | 最小平方 | **F-统计量:** | Three hundred and fifty-two point one |
| **日期:** | 2020 年 10 月 16 日，Fri | **概率(F-统计量):** | 5.03e-15 |
| **时间:** | 14:42:15 | **对数似然:** | -242.89 |
| **观察次数:** | Twenty-four | AIC:t1] | Four hundred and eighty-nine point eight |
| **测向残差:** | Twenty-two | **BIC:** | Four hundred and ninety-two point one |
| **测向模型:** | one |   |   |
| **协方差类型:** | 不稳健 |   |   |

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"> <col class="tcol6 align-left"> <col class="tcol7 align-left"></colgroup> 
|   | 

**coef**

 | 

**标准误差**

 | 

**T**

 | 

**P > &#124;t&#124;**

 | 

**【0.025**

 | 

**0.975】**

 |
| --- | --- | --- | --- | --- | --- | --- |
| **常数** | 7.389e+04 | One thousand two hundred and eighty-one point eight six one | Fifty-seven point six four | Zero | 7.12e+04 | 7.65e+04 |
| **x1** | 2.405e+04 | One thousand two hundred and eighty-one point eight six one | Eighteen point seven six five | Zero | 2.14e+04 | 2.67e+04 |

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"></colgroup> 
| 

**综合:**

 | 

Three point one zero five

 | 

德宾-沃森:

 | 

Two point six zero eight

 |
| --- | --- | --- | --- |
| **Prob(综合):** | Zero point two one two | **花园-贝拉(JB):** | One point five six seven |
| **偏斜:** | Zero point two nine seven | **Prob(JB):** | Zero point four five seven |
| **峰度:** | One point eight nine eight | **伯爵。否〔t1〕** | One |

```
summary = model.summary()
summary

Listing 1-21Profile

```

r 平方表示模型解释的数据变化。另一方面，调整后的 R 平方表示影响因变量解释的自变量的变化。该模型解释了数据中 94.1%的变异，自变量(YearsExperience)解释了数据中 93.9%的变异。模型最好地解释了数据。p 值仅为 0.05。我们拒绝零假设，支持另一个假设。雇员的工作年限和工资之间有很大差异。

## 使用 SciKit-Learn 开发最小二乘模型

SciKit-Learn 包在培训和测试模型方面很受欢迎。它具有支持数据预处理和模型选择、训练和评估的特性。

### 最小二乘模型超参数优化

超参数是在训练模型之前设置的值。它控制着模型的学习过程。超参数优化包括指定一个值列表，并找到产生最佳模型性能的值。

#### 步骤 1:用默认超参数拟合最小二乘模型

清单 [1-22](#PC22) 用默认超参数拟合最小二乘模型。

```
from sklearn.linear_model import LinearRegression
lm = LinearRegression()
lm.fit(x_train,y_train)

Listing 1-22Develo the Least Squares Model

```

#### 步骤 2:确定交叉验证分数的平均值和标准差

交叉验证包括检查模型概括数据的程度。清单 [1-24](#PC24) 应用 R <sup>2</sup> 分数作为寻找交叉验证分数的标准。还有其他分数可以用作标准，如均方误差(在考虑回归关系后由模型解释的关于数据的可变性)和均方根误差(在不考虑回归关系的情况下解释的可变性)。清单 [1-23](#PC23) 找到回归变量的默认参数。

```
lm.get_params()

Listing 1-23Default Parameters

```

密集搜索返回估计量、参数空间、用于搜索候选项的方法、交叉验证方案和得分函数。交叉验证方法包括 RandomizedSearchCV，它考虑特定数量的候选人；GridSearchCV，考虑所有参数组合；以及 BayesSearchCV，其通过将高斯过程处理为先验 for 函数，使用先前的损失来确定对损失进行采样的最佳点。下面，我们将向您展示如何使用 GridSearchCV 和 BayesSearchCV 执行交叉验证。表 [1-5](#Tab5) 突出显示了可优化的超参数。

表 1-5

可优化超参数

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

参数

 | 

描述

 |
| --- | --- |
| **拟合 _ 截距** | 确定模型是否必须估计截距 |
| **正常化** | 决定我们是否必须标准化独立变量 |
| **副本 _X** | 决定我们是否必须复制自变量 |

清单 [1-24](#PC24) 创建网格模型。

```
from sklearn.model_selection import GridSearchCV
param_grid = {'fit_intercept':[True,False],
              'normalize':[True,False],
              'copy_X':[True, False]}
grid_model  = GridSearchCV(estimator=lm,
                           param_grid=param_grid,
                           n_jobs=-1)
grid_model.fit(x_train,y_train)

Listing 1-24Develop the Grid Model Using GridSearchCV

```

清单 [1-25](#PC25) 返回最佳交叉验证和最佳参数。

```
print("Best score: ", grid_model.best_score_, "Best parameters: ", grid_model.best_params_)

Listing 1-25Find Best Hyperparameters Using GridSearhCV

```

最佳得分:0.9272138118711238 最佳参数:{'copy_X': True，' fit_intercept': True，' normalize': True}

找到正确超参数的另一种方法是使用贝叶斯优化。清单 [1-26](#PC26) 使用 Skopt 执行贝叶斯优化。要在 Python 环境中安装它，使用`pip install scikit-optimize`。

```
from skopt import BayesSearchCV
param_grid = {'fit_intercept':[True,False],
              'normalize':[True,False],
              'copy_X':[True, False]}
grid_model = BayesSearchCV(lm,param_grid,n_iter=30,random_state=1234,verbose=0)
grid_model.fit(x_train,y_train)

Listing 1-26Develop the Grid Model Using BayesSearchCV

```

清单 [1-27](#PC27) 找到最佳交叉验证分数和最佳参数。

```
print("Best score: ", grid_model.best_score_, "Best parameters: ", grid_model.best_params_)

Listing 1-27Find Best Hyperparameters Using BayesSearchCV

```

最佳得分:0.9266258295344271 最佳参数:OrderedDict([('copy_X '，False)，(' fit_intercept '，True)，(' normalize '，False)])

#### 最终确定最小二乘模型

清单 [1-28](#PC28) 使用清单 [1-27](#PC27) 中估计的超参数完成最小二乘模型。

```
lm = LinearRegression(copy_X= True,
                      fit_intercept= True,
                      normalize= True)
lm.fit(x_train,y_train)

Listing 1-28Finalize the Least Squares Model

```

### 找到截击点

清单 [1-29](#PC29) 估算截距。截距是自变量的平均值，假设我们持有因变量常数。

```
lm.intercept_
   array([73886.20833333])

Listing 1-29Intercept

```

### 求估计系数

清单 [1-30](#PC30) 估算系数。

```
lm.coef_
array([[24053.85556857]])

Listing 1-30Coefficient

```

直线的公式表示如下:

![$$ \hat{y}=73\ 886.21+24\ 053.851{X}_1+{\varepsilon}_i $$](../images/506148_1_En_1_Chapter/506148_1_En_1_Chapter_TeX_Equ8.png)

(Equation 1-8)

工作经验每增加一年，工资增加 24，054 英镑。

### 使用 SciKit-Learn 测试最小二乘模型性能

清单 [1-31](#PC31) 应用`predict()`方法返回预测值，然后它将数组传递给 dataframe(参见表 [1-6](#Tab6) )。

表 1-6

实际值

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
|   | 

预计工资

 |
| --- | --- |
| **0** | 40748.961841 |
| **1** | 122699.622956 |
| **2** | 64961.657170 |
| **3** | 63099.142145 |
| **4** | 115249.562855 |
| **5** | 107799.502753 |

```
y_pred = lm.predict(x_test)
pd.DataFrame(y_pred, columns = ["Predicted salary"])

Listing 1-31Tabulate Predicted Values

```

列表 [1-32](#PC32) 列出了实际值(见表 [1-7](#Tab7) )。

表 1-7

工资的实际值

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
|   | 

实际工资

 |
| --- | --- |
| **0** | Thirty-seven thousand seven hundred and thirty-one |
| **1** | One hundred and twenty-two thousand three hundred and ninety-one |
| **2** | Fifty-seven thousand and eighty-one |
| **3** | Sixty-three thousand two hundred and eighteen |
| **4** | One hundred and sixteen thousand nine hundred and sixty-nine |
| **5** | One hundred and nine thousand four hundred and thirty-one |

```
pd.DataFrame(y_test, columns = ["Actual salary"])

Listing 1-32Develop an Actual Values Table

```

表 [1-7](#Tab7) 表明回归变量存在微小误差。

#### 绝对平均误差

绝对误差是估计值和真实值之间的差异。例如，如果模型预测工资为 40，749 美元，而实际值为 37，731 美元，则模型的绝对误差为 40，749 美元–37，731 美元= 3，018 美元。平均绝对误差(MAE)代表在不考虑方向的情况下估计误差的平均大小。

![$$ MAE=\frac{1}{n}{\sum}_{i=1}^n\mid {x}_i-x\mid $$](../images/506148_1_En_1_Chapter/506148_1_En_1_Chapter_TeX_Equ9.png)

(Equation 1-9)

这里， *n* 表示误差的数量，∣*x*T4】t5】IT7】t8 x∣表示绝对误差。

#### 均方误差

均方差(MSE)表示在考虑回归关系后，模型对数据解释的可变性。

![$$ MSE=\frac{1}{n}{\sum}_{i=1}^n\mid {y}_i-\hat{y}\mid $$](../images/506148_1_En_1_Chapter/506148_1_En_1_Chapter_TeX_Equ10.png)

(Equation 1-10)

#### 均方根误差

均方根误差(RMSE)代表在不考虑回归关系的情况下解释的可变性。要得到 RMSE 分数，使用均方差的平方根。

![$$ RMSE=\sqrt{MSE} $$](../images/506148_1_En_1_Chapter/506148_1_En_1_Chapter_TeX_Equ11.png)

(Equation 1-11)

#### r 平方

R-squared (R <sup>2</sup> )表示由模型解释的关于数据的可变性。该指标包含范围从 0 到 1 的值，其中 0 表示回归模型不能很好地解释数据的可变性，1 表示回归模型能最好地解释数据。

![$$ {R}^2=1-\frac{\sum {\left({y}_i-\hat{y}\right)}^2}{\sum {\left({y}_i-\underset{\_}{y}\right)}^2} $$](../images/506148_1_En_1_Chapter/506148_1_En_1_Chapter_TeX_Equ12.png)

(Equation 1-12)

这里，![$$ \hat{y} $$](../images/506148_1_En_1_Chapter/506148_1_En_1_Chapter_TeX_IEq4.png)代表 y 的预测值，![$$ \underset{\_}{y} $$](../images/506148_1_En_1_Chapter/506148_1_En_1_Chapter_TeX_IEq5.png)代表 y 的平均值，R <sup>2</sup> 的分数必须乘以一百。例如，如果分数是 0.9，那么模型解释了数据中 90%的可变性。

还有其他模型评估指标，如下所示:

*   *解释的差异分数*:一个度量标准，它捕捉模型和实际数据之间的差异。如果分数接近 1，那么有很强的相关性。

*   *平均伽马偏差*:捕捉因变量标度并估计相对误差的度量。这是一个 Tweedie 偏差，幂参数为 2。

*   *平均泊松偏差*:捕捉因变量标度并估计相对误差的度量。它是一个 Tweedie 偏差，幂参数为 1。

清单 [1-33](#PC33) 列出了评估指标(见表 [1-8](#Tab8) )。

表 1-8

绩效矩阵

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
|   | 

价值观念

 |
| --- | --- |
|  | 2.446172e+03 |
| 姆塞 | 1.282341e+07 |
| RMSE | 3.580979e+03 |
| **R**<sup>T3】2T5】</sup> | 9.881695e-01 |
| **解释方差得分** | 9.897038e-01 |
| **平均伽马偏差** | 3.709334e-03 |
| **平均泊松偏差** | 2.129260e+02 |

```
MAE = metrics.mean_absolute_error(y_test,y_pred)
MSE = metrics.mean_squared_error(y_test,y_pred)
RMSE = np.sqrt(MSE)
R2 = metrics.r2_score(y_test,y_pred)
EV = metrics.explained_variance_score(y_test,y_pred)
MGD = metrics.mean_gamma_deviance(y_test,y_pred)
MPD = metrics.mean_poisson_deviance(y_test,y_pred)
lmmodelevaluation = [[MAE,MSE,RMSE,R2,EV,MGD,MPD]]
lmmodelevaluationdata = pd.DataFrame(lmmodelevaluation,
                                     index = ["Values"],
                                     columns = ["MAE",
                                              "MSE",
                                              "RMSE",
                                              "R2",
                                              "Explained variance score",
                                              "Mean gamma deviance",
                         "Mean Poisson deviance"]).transpose()
lmmodelevaluationdata

Listing 1-33Evaluation Metrics

```

表 [1-8](#Tab8) 表明该模型解释了 98%的数据可变性。员工的工作年限和员工的工资有很强的相关性。平均而言，不考虑方向的误差幅度为 2446，均方误差为 1。

### 培训用数据

列表 [1-34](#PC34) 通过绘制一条穿过数据点的线来验证变量之间是否存在线性关系(见图 [1-13](#Fig13) )。

![../images/506148_1_En_1_Chapter/506148_1_En_1_Fig13_HTML.jpg](../images/506148_1_En_1_Chapter/506148_1_En_1_Fig13_HTML.jpg)

图 1-13

培训用数据

```
plt.scatter(x_train,y_train,s=200)
plt.plot(x_test,y_pred,color="red")
plt.xlabel("Actual Experience")
plt.ylabel("Actual Salary")
plt.show()

Listing 1-34Training Data

```

图 [1-13](#Fig13) 证实了数据点接近一条直线，但接近程度并不极端。员工的工作年限和员工的工资是线性关系。

### 测试数据

列表 [1-35](#PC35) 返回图 [1-14](#Fig14) ，显示员工工作年限的实际值和员工工作年限的预测值。

![../images/506148_1_En_1_Chapter/506148_1_En_1_Fig14_HTML.jpg](../images/506148_1_En_1_Chapter/506148_1_En_1_Fig14_HTML.jpg)

图 1-14

测试数据

```
plt.scatter(y_test,y_pred,s=200)
plt.axhline(color="red")
plt.xlabel("Actual Salary")
plt.ylabel("Predicted Salary")
plt.show()

Listing 1-35Test Data

```

图 [1-14](#Fig14) 肯定了完美贴合的模型。如前所述，由于可变性，完美的模型在现实世界中很少见。

### 实际值和预测值

模型评估包括比较因变量的实际值和预测值。列表 [1-36](#PC36) 返回员工工资实际值与员工工资预测值的差值(见图 [1-15](#Fig15) )。

![../images/506148_1_En_1_Chapter/506148_1_En_1_Fig15_HTML.jpg](../images/506148_1_En_1_Chapter/506148_1_En_1_Fig15_HTML.jpg)

图 1-15

实际值和预测值

```
plt.scatter(y_test,y_pred,s=200)
plt.axhline(color="red")
plt.xlabel("Actual Salary")
plt.ylabel("Predicted Salary")
plt.show()

Listing 1-36Actual Values and Predicted Values

```

员工工资的实际值与员工工资的预测值之间存在显著差异，但这些差异并不大。

## 诊断残差

残差表示实际值和预测值之间的差异。残差诊断包括发现模型所犯错误的行为模式。我们可以使用几个工具来检查模型是否满足假设。分析残差最简单的方法是查看它们的平均值。如果残差的平均值接近于 0，则该模型非常适合。

为了用图形表示残差并理解错误的行为模式，我们可以使用诸如自相关函数图、偏自相关函数图、拟合值和残差值图、杠杆值和残差值图、拟合值和学生化残差值、杠杆和学生化残差值(也称为 Cook 的 D 影响图)以及正态概率(或正态 Q-Q)图。这些图对于检测残差中的异常是有用的。

如果残差中有一个模式或形状，则模型违反了回归假设。有三种主要模式，即*扇形模式*，当预测值的可变性增加时发生(误差项不受自变量的值影响)；*抛物线型*，当预测值相对或存在乘法误差时出现；以及*双弓形*，当自变量为比例或百分比时发生。还有其他因素会影响残差的行为模式，例如数据的分布和异常值的可见存在。确保我们为模型提供高质量的数据是很重要的。

### 使用统计模型评估残差

清单 [1-37](#PC37) 显示了残差诊断指标，清单 [1-38](#PC38) 显示了箱线图。

```
model_residual = model.resid
model_fitted = model.fittedvalues
model_leverage = model.get_influence().hat_matrix_diag
model_norm_residual = model.get_influence().resid_studentized_internal
model_norm_residual_ab_sqrt = np.sqrt(np.abs(model_norm_residual))

Listing 1-37Residuals Diagnosis Metrics

```

图 [1-16](#Fig16) 显示了残差的离差。

![../images/506148_1_En_1_Chapter/506148_1_En_1_Fig16_HTML.jpg](../images/506148_1_En_1_Chapter/506148_1_En_1_Fig16_HTML.jpg)

图 1-16

残差箱线图

```
model_residual = pd.DataFrame(model_residual)
model_residual.columns = ["Residuals"]
sns.boxplot(model_residual["Residuals"])
plt.xlabel("Residuals")
plt.show()

Listing 1-38Residuals Box Plot

```

图 [1-16](#Fig16) 显示残差的均值接近于 0，但接近程度并不极端。残差围绕真实值对称分布(它们表现良好)。

#### 正常 Q-Q

列表 [1-39](#PC39) 绘制了正态 Q-Q。图 [1-17](#Fig17) 确定数据是否来自正态理论分布。它通过比较两个概率分布(x 轴上的理论分位数和 y 轴上的样本分位数值)来验证正态假设。

![../images/506148_1_En_1_Chapter/506148_1_En_1_Fig17_HTML.jpg](../images/506148_1_En_1_Chapter/506148_1_En_1_Fig17_HTML.jpg)

图 1-17

正常 Q-Q

```
fig, ax = plt.subplots()
fig = sm.graphics.qqplot(model_residual,ax=ax,line="45",fit=True,dist=stats.norm)
plt.show()

Listing 1-39Normal Q-Q

```

图 [1-17](#Fig17) 显示数据点没有形成一条直线。它们偏离了直线。它还显示了数据中的异常值。

#### 库克的影响

列表 [1-40](#PC40) 描绘了厨师的影响力。图 [1-18](#Fig18) 检测将回归函数拉向自身的异常值。它使用临界值来确定异常值。临界值代表一个值，在这个值上我们可以决定一个观察值是否是异常值。我们使用 4/n-k-1 来估计截止值，其中 *n* 是样本大小， *k* 是用于评估性能的异常值的数量。

![../images/506148_1_En_1_Chapter/506148_1_En_1_Fig18_HTML.jpg](../images/506148_1_En_1_Chapter/506148_1_En_1_Fig18_HTML.jpg)

图 1-18

库克的影响

```
fig, ax = plt.subplots()
fig = sm.graphics.influence_plot(model,ax=ax,criterion="cooks")
plt.show()

Listing 1-40Cook’s D Influence

```

图 [1-18](#Fig18) 显示了有影响的数据点(观察值 0 处的一个小点和观察值 11 处的一个极值点)。移除它们可以显著提高模型的预测能力。

#### 拟合值和残值

列表 [1-41](#PC41) 绘制拟合值和残值。图 [1-19](#Fig19) 比较拟合值(预测响应)和残差(实际值和预测值之间的差异)。它检查不相等的方差、非线性和异常值。

![../images/506148_1_En_1_Chapter/506148_1_En_1_Fig19_HTML.jpg](../images/506148_1_En_1_Chapter/506148_1_En_1_Fig19_HTML.jpg)

图 1-19

拟合值和残值

```
plt.scatter(model_fitted,model_residual,s=200)
plt.xlabel("Fitted Salary")
plt.ylabel("Residual Salary")
plt.show()

Listing 1-41Fitted Values and Residual Values

```

图 [1-19](#Fig19) 显示了良好拟合工资和剩余工资图的特征。没有大的残差。此外，残差在零附近随机反弹。该模型满足线性假设。

#### 利用价值和剩余价值

列表 [1-42](#PC42) 绘制杠杆值(距离 y 的平均值 *x* 多远)和残差(见图 [1-20](#Fig20) )。

![../images/506148_1_En_1_Chapter/506148_1_En_1_Fig20_HTML.jpg](../images/506148_1_En_1_Chapter/506148_1_En_1_Fig20_HTML.jpg)

图 1-20

利用价值和剩余价值

```
plt.scatter(model_leverage,model_residual,s=200)
plt.xlabel("Leverage Salary")
plt.ylabel("Residual Salary")
plt.show()

Listing 1-42Leverage Values and Residual Values

```

图 [1-2](#Fig2) 表明有一个高杠杆数据点导致预测不准确。

#### 拟合值和学生化残值

清单 [1-43](#PC43) 返回预测响应和标准化删除残差(见图 [1-21](#Fig21) )。等于或大于 3 的学生化残差是影响点。

![../images/506148_1_En_1_Chapter/506148_1_En_1_Fig21_HTML.jpg](../images/506148_1_En_1_Chapter/506148_1_En_1_Fig21_HTML.jpg)

图 1-21

拟合值和学生化残值

```
plt.scatter(model_leverage,model_norm_residual,s=200)
plt.xlabel("Leverage Salary")
plt.ylabel("Studentized Residual Salary")
plt.show()

Listing 1-43Fitted Values and Studentized Residual Values

```

图 [1-21](#Fig21) 证实了随机分布(残差不遵循趋势或模式)。

#### 杠杆价值和学生化剩余价值

识别有影响的数据点的另一种方法包括将独立数据点与其他数据点的距离与标准化删除残差进行比较。参见清单 [1-44](#PC44) 。

```
plt.scatter(model_leverage,model_norm_residual,s=200)
plt.xlabel("Leverage Salary")
plt.ylabel("Studentized Residual Salary")
plt.show()

Listing 1-44Fitted Values and Studentized Residual Values

```

图 [1-22](#Fig22) 显示了良好残差的特征，但是有一个极端异常值。

![../images/506148_1_En_1_Chapter/506148_1_En_1_Fig22_HTML.jpg](../images/506148_1_En_1_Chapter/506148_1_En_1_Fig22_HTML.jpg)

图 1-22

拟合值和学生化残值

### 使用 SciKit-Learn 评估残差

下一节使用自相关函数和偏自相关函数研究残差之间的相关程度。

#### 自相关函数

自相关表示数据与自身相关的程度，与其他一些数据相反(参见清单 [1-45](#PC45) )。图 [1-23](#Fig23) 确定前一个滞后的残差之间是否存在自相关。它有两个轴，滞后在 x 轴，残差的自相关函数在 y 轴。

![../images/506148_1_En_1_Chapter/506148_1_En_1_Fig23_HTML.jpg](../images/506148_1_En_1_Chapter/506148_1_En_1_Fig23_HTML.jpg)

图 1-23

自相关函数图

```
from statsmodels.graphics.tsaplots import plot_acf
residuallm = y_test - y_pred
plot_acf(residuallm)
plt.xlabel("Lag")
plt.ylabel("ACF")
plt.show()

Listing 1-45Autocorrelation Function

```

图 [1-23](#Fig23) 显示在滞后 0 时，相关性为 1(数据与其自身相关)。在滞后 1、2、4 和 5 处有轻微的负相关，在滞后 3 处有中度的负相关。

#### 部分自相关函数

列表 [1-46](#PC46) 绘制了在低电平滞后时未解释的系数的部分相关性(见图 [1-24](#Fig24) )。

![../images/506148_1_En_1_Chapter/506148_1_En_1_Fig24_HTML.jpg](../images/506148_1_En_1_Chapter/506148_1_En_1_Fig24_HTML.jpg)

图 1-24

部分自相关函数

```
from statsmodels.graphics.tsaplots import plot_pacf
plot_pacf(residualslm)
plt.xlabel("Lag")
plt.ylabel("PACF")
plt.show()

Listing 1-46Partial Autocorrelation Function

```

滞后 0 可以解释所有高阶自相关。

## 结论

本章讲述了简单线性回归方法和最小二乘模型及其应用。它使用两个回归变量(同时应用 Statsmodels 和 SciKit-Learn)检验了员工的工作经验和员工的工资之间是否存在显著差异。调查结果表明，员工的工作经验和工资之间存在显著差异。存在线性，但回归变量违反了某些回归假设。由于数据中的微小异常，他们会产生边际误差(这在小样本中很常见)。随后的一章涵盖了处理错误的技术；它介绍了由偏差和方差引起的误差的概念，以及如何使用高级参数方法(如岭回归和套索回归)来解决误差。

<aside aria-label="Footnotes" class="FootnoteSection" epub:type="footnotes">Footnotes [1](#Fn1_source)

[T2`https://www.kaggle.com/rohankayan/years-of-experience-and-salary-dataset`](https://www.kaggle.com/rohankayan/years-of-experience-and-salary-dataset)

 </aside>