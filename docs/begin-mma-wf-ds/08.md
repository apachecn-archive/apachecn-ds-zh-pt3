# 8.用 Wolfram 语言进行机器学习

本节将介绍作为线性回归优化方法的梯度下降算法；将显示相应的计算以及模型学习曲线的概念。稍后，我们将看到如何使用 Wolfram 语言的专门功能进行机器学习，例如预测、分类和聚类分类，在线性回归的情况下，用于逻辑回归和聚类搜索。除此之外，这些函数生成的不同对象和结果以及度量模型的指标将会为这些函数显示出来。在每种情况下，我们将解释模型的哪些部分是使用 Wolfram 语言正确构建的基础。对于这本书的这一部分，我们将使用已知数据集的例子，如费舍尔的虹膜数据集，波士顿住房数据集，和泰坦尼克号数据集。

## 梯度下降算法

梯度下降是一种优化算法，其在于通过迭代过程找到函数的最小值。为了构建该过程，在点 X <sub>j</sub> 周围，利用形状 f(xl) = θ0+θ1*X <sub>j</sub> 的线性模型假设来最小化平方误差损失函数。损失函数由下面的表达式给出。

![$$ J\left(\theta \right)=\frac{1}{2\ast N}\ast \sum \limits_{j=1}^N{\left(f\left({x}_j\right)-{y}_j\right)}^2 $$](../images/500903_1_En_8_Chapter/500903_1_En_8_Chapter_TeX_Equa.png)

该算法的迭代过程包括系数的计算，直到获得收敛。系数由以下表达式给出。

![$$ {\displaystyle \begin{array}{l}{\theta_0}^{i+1}={\theta_0}^i-\frac{\alpha }{N}\ast \sum \limits_{j=1}^N\left({\theta_0}^i+{\theta_1}^i\ast {x}_j-{y}_j\right)\\ {}{\theta_1}^{i+1}={\theta_1}^i-\frac{\alpha }{N}\ast \sum \limits_{j=1}^N\left({\theta_0}^i+{\theta_1}^i\ast {x}_j-{y}_j\right)\ast {x}_j\end{array}}, $$](../images/500903_1_En_8_Chapter/500903_1_En_8_Chapter_TeX_Equb.png)T2】

其中求和是从关于θ0 和θ1 的偏导数获得的。项α对应于学习速率，这是一个在构建学习过程时使误差最小化的参数。有关该方法和演示的更多数学深度，请参见 Stuart Russell 和 Peter Norvig 的书*人工智能:一种现代方法*(2010 年，新泽西州上马鞍河:Prentice Hall)。

### 获取数据

首先，我们用 RandomReal 函数定义数据，并建立一个种子。这是为了保持数据的再现性，以防实践相同的例子。

```
In[1]:=
SeedRandom[888]
x=RandomReal[{0,1},50];
y=-1-x+0.6*RandomReal[{0,1},50];

```

因此，让我们用 2D 散点图图 [8-1](#Fig1) 来观察数据。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig1_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig1_HTML.jpg)

图 8-1

随机生成数据的 2D 散点图

```
In[2]:= ListPlot[Transpose[{x,y}],AxesLabel→{"X axis","Y axis"},PlotStyle→Red]
Out[2]=

```

### 算法实现

现在让我们开始用 Wolfram 语言实现该算法。该算法将包括定义常数、迭代次数和学习速率。然后，我们将创建两个包含初始值零的列表，其中将存储每次迭代的系数值。稍后，我们将通过一个带表的循环来执行系数的计算，直到达到迭代次数才会结束。在我们的例子中，我们将建立 250 次迭代，学习率为 1。

`In[3]:= itt=250;(*Number of iterations*)`

a `=1;(*Learning rate*)`

`0=Range@@{0,itt};(* Array for values of Theta_0*)`

`1=Range@@{0,itt};(* Array for values of Theta_1*)`

`Table[{`

ζ〔t1〕〔t5〕〔T2〕ζ〔T3〕ζ〔T4〕

ζ〔t1〕〔t5〕〔T2〕ζ〔T3〕ζ〔T4〕

由于我们已经确定了系数的计算，我们将通过构造一个函数并使用最后一次迭代的系数值来建立线性平差方程，这些系数值位于列表θ0 y θ1 的最后一个位置。

```
In[4]:= F[X_]:= θ0[[Length@ θ0]]+ θ1[[Length@ θ1]]*X

```

为了知道最佳拟合的形状，我们添加 X 变量作为参数。这将给出我们的形式:F(X) = θ0+θ1*X。

```
In[5]:= F[X]
Out[5]= -0.707789-0.923729 X

```

让我们看看这条线是如何拟合图 [8-2](#Fig2) 中的数据的。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig2_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig2_HTML.jpg)

图 8-2

根据数据调整行

```
In[6]:= Show[{Plot[F[X],{X,0,1},PlotStyle→Blue,AxesLabel→{"X axis","Y axis"}],ListPlot[Transpose[{x,y}],PlotStyle→Red]}]
Out[6]=

```

既然我们已经建立了线性模型，我们可以对学习率随迭代次数的变化和函数 j 给出的损失值进行图形比较。

但首先我们必须声明损失函数 j。对于求和，我们可以使用 sigma 的特殊符号，或者编写![$$ {\sum}_{i=1}^{\mathrm{imax}} expr $$](../images/500903_1_En_8_Chapter/500903_1_En_8_Chapter_TeX_IEq3.png)或 Sum [expr，{i，i <sub> max </sub> }]。

```
In[7]:=J[Theta0_,Theta1_]:=1/(2*Length[x])*Sum[(Theta0 + (Theta1*x[[i]]) - y[[i]])^2,{i,1,Length@x}]

```

下面是当重复过程图 [8-3](#Fig3) 时，对于α1=1、α2=0.1、α3=0.01、α4=0.001 和α5=0.001 的学习率值，损失与每个交互的关系图。

### 多个阿尔法

看到前面构建的过程后，我们可以对不同的 alphas 重复这个过程。下面是当重复该过程时，对于α1=1、α2=0.1、α3=0.01、α4=0.001 和α5=0.001 的学习率值，损失对每个交互的图。

```
In[8]:=
α1=Transpose[{Range[0,itt], J[θ0,θ1]}];
α2=Transpose[{Range[0,itt], J[θ0,θ1]}];
α3=Transpose[{Range[0,itt], J[θ0,θ1]}];
α4=Transpose[{Range[0,itt], J[θ0,θ1]}];
α5=Transpose[{Range[0,itt], J[θ0,θ1]}];

```

用列表线图绘制并可视化不同 alphas 的学习曲线(图 [8-3](#Fig3) )。当改变 alpha 值时，试着检查调整后的线是如何变化的。

```
In[9]:= ListLinePlot[{α1,α2,α3,α4,α5},FrameLabel→{"Number of Iterations","Loss Function"},Frame→True,PlotLabel→"Learning Curve",PlotLegends→SwatchLegend[{Style["α=1",#],Style["α=0.1",#],Style["α=0.01",#],Style["α=0.001",#],Style["α=0.0001",#]},LegendLabel→Style["Learning rate",White],LegendFunction→(Framed[#,RoundingRadius→5,Background→Gray]&)]]&[White]
Out[9]=

```

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig3_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig3_HTML.jpg)

图 8-3

梯度下降算法的学习曲线

在上图(图 [8-3](#Fig3) )中，我们可以看到迭代的规模与成本的关系，以及它是如何随着 alpha 值的变化而变化的。有了高学习率，我们可以在每一步覆盖更多的领域，但我们有超过最低点的风险。为了知道算法是否有效，我们必须看到损失函数在每次新的迭代中是递减的。相反的情况将是算法没有正常工作的指示；这可归因于各种因素，例如代码错误或学习率的不正确值。正如我们在图表中看到的，适当的α值对应于 1 到 10 <sup>-4</sup> 之间的小值。没有必要必须使用这些相同的值；您可以使用此范围内的值。根据数据的形式，对于迭代步骤来说，算法可能收敛或不收敛于相同的不同α值。如果我们选择非常小的 alpha 值，该算法可能需要很长时间才能收敛，正如我们可以看到的 alpha 值 10 <sup>-3</sup> 或 10 <sup>-4</sup> 。

## 线性回归

尽管能够构建算法来执行线性回归，但 Wolfram 语言具有专门的机器学习功能。在线性回归问题的情况下，有预测函数。预测函数还可以与不同的算法一起工作，而不仅仅是回归任务算法。

### 预测功能

预测函数通过使用训练数据创建预测函数来帮助我们预测值。它还允许我们选择不同的学习算法，其目的是能够预测数值、视觉、分类值或组合。可供选择的方法有决策树、梯度提升树、线性回归、神经网络、最近邻、随机森林和高斯过程。对于每种方法，其中都有选项；这些选项根据训练预测函数所选择的算法而有所不同。让我们看看线性回归法。预测的输入数据可以是规则列表、关联或数据集的形式。

### 波士顿数据集

让我们来看第一个例子，从 Wolfram 数据库加载 Boston Homes 数据(图 [8-4](#Fig4) )。波士顿数据集包含波士顿马萨诸塞州地区的住房信息。要寻找更深入的信息，请访问大卫·哈里森和丹尼尔·鲁宾菲尔德的文章“享乐房价和对清洁空气的需求”。*《环境经济与管理杂志》*(1978；*5*【1】，81-102。 [`https://doi.org/10.1016/0095-0696(78)90006-2)`](https://doi.org/10.1016/0095-0696(78)90006-2)) 或《回归诊断:识别共线性的有影响的数据和来源:546 》一书，作者:David Belsley、Edwin Kuh 和 Roy Welsch，(2013；Wiley-Interscience)。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig4_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig4_HTML.jpg)

图 8-4

波士顿房价数据集

```
In[1]:= Bstn=ResourceData[ResourceObject["Sample Data: Boston Homes"]]
Out[1]=

```

尝试使用滚动条查看数据集的完整视图。让我们看看列的描述，并以表格的形式显示出来。

```
In[2]:= ResourceData[ResourceObject["Sample Data: Boston Homes"],"ColumnDescriptions"]//TableForm
Out[2]//TableForm=
Per capita crime rate by town
Proportion of residential land zoned for lots over 25000 square feet
Proportion of non-retail business acres per town
Charles River dummy variable (1 if tract bounds river, 0 otherwise)
Nitrogen oxide concentration (parts per 10 million)
Average number of rooms per dwelling
Proportion of owner-occupied units built prior to 1940
Weighted mean of distances to five Boston employment centers
Index of accessibility to radial highways
Full-value property-tax rater per $10000
Pupil-teacher ratio by town
1000(Bk-0.63)^2 where Bk is the proportion of Black or African-American residents by town
Lower status of the population (percent)
Median value of owner-occupied homes in $1000s

```

### 模型创建

我们将尝试创建一个模型，该模型能够通过住宅中房间的数量来预测波士顿地区的房价。为了实现这一点，感兴趣的列对应于 RM(每所住宅的平均房间数)和 MEDV(自有住房的中值)，因为我们想要找出房间数和房价之间是否存在线性关系。运用一点常识，房间数量最多的房子更大，因此有能力容纳更多的人，使价格上升。

首先看一下 MEDV 和 RM 散点图 [8-5](#Fig5) 。

```
In[3]:= MEDVvsRM=Transpose[{Normal[Bstn[All,"RM"]],Normal[Bstn[All,"MEDV"]]}];
ListPlot[MEDVvsRM,PlotMarkers→"OpenMarkers",Frame→True,FrameLabel→{Style["RM",Red],Style["MEDV",Red]},GridLines→All, PlotStyle→Black,ImageSize→500]
Out[3]=

```

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig5_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig5_HTML.jpg)

图 8-5

2D 散点图

如图 [8-5](#Fig5) 所示，随着平均房间数的增加，房价也随之增加。这表明这两个变量之间可能存在直接的比例关系。根据图中所示，让我们看看这些变量之间的相关值。我们将通过相关性矩阵来展示这一点，首先计算值的相关性，指定记号名称，并用 MatrixPlot 绘制(图 [8-6](#Fig6) )。

```
In[4]:= CorreLat=SetPrecision[Correlation[Transpose[{Normal[Bstn[All,"RM"]],Normal[Bstn[All,"MEDV"]]}]],2];
XTicks={{1,"RM"},{2,"MEDV"},{1,"RM"},{2,"MEDV"}};
YTicks={{1,"RM"},{2,"MEDV"},{1,"RM"},{2,"MEDV"}};
PostionsValues={Text[#1,{0.5,1.5}],Text[#1,{1.5,0.5}],Text[#2,{1.5,1.5}],Text[#2,{0.5,0.5}]}&[CorreLat[[1,1]],CorreLat[[1,2]]];
MatrixPlot[CorreLat,ColorFunction→"DarkRainbow",FrameTicks→{ XTicks,YTicks,XTicks,YTicks},Epilog→{White,PostionsValues},PlotLegends→BarLegend[{"DarkRainbow",{0,1}},4],ImageSize→180]
Out[4]=

```

通过观察矩阵图(图 [8-6](#Fig6) ，可以得出 RM 和 MEDV 之间存在良好的线性关系。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig6_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig6_HTML.jpg)

图 8-6

结合相关矩阵的矩阵图

现在让我们随机打乱数据集，用 Thread 建立一个规则列表；这是因为要在预测函数中输入的数据必须如下:{ x→y }-换句话说，输入和目标值。

```
In[5]:=
NewData=RandomSample[Thread[Normal[Bstn[All,"RM"]]→Normal[Bstn[All,ssss"MEDV"]]]];

```

一旦随机抽样，我们将选择前 354 个元素(70%)，这将是训练集，其余 152 个(30%)将是测试集。

```
In[6]:={training,test}={NewData[[;;354]],NewData[[355;;]]};

```

我们继续训练模型，作为目标的自有住房(MEDV)平均值的预测器。作为一种方法，我们选择线性回归。当训练模型时，训练报告选项的规格包括 Panel(面板的动态更新)、Print(包括时间、训练示例、最佳方法、当前损失的周期性信息)、ProgressIndicator(简单进度条)、SimplePanel(没有图的动态更新面板)和 None。面板是默认选项(图 [8-7](#Fig7) )。

```
In[7]:=
PF=Predict[training,Method→"LinearRegression",TrainingProgressReporting→"Panel"]
Out[7]=

```

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig7_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig7_HTML.jpg)

图 8-7

已定型模型的预测函数对象

输入代码时，根据添加到 TrainingProgressReporting 的选项，应出现进度条和面板报告(图 [8-8](#Fig8) )。面板显示的时间取决于模型的训练时间。要为训练时间设置一个特定的时间，请添加一个选项 TimeGoal，它指定模型的训练应该持续多长时间。时间值是 CPU 时间的秒数，也就是没有单位的数字。对于时间单位(秒、分、小时)，需要使用 Quantity 命令，比如 TimeGoal → Quantity ["时间量级"，#] & / @ { "秒"、"分"、"小时" }。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig8_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig8_HTML.jpg)

图 8-8

预测函数的进度报告

回到模型:如图 [8-7](#Fig7) 所示，返回对象是一个预测函数(试着用 Head 验证一下)。当已经给预测函数指定了名称时，可以为此获得关于模型的附加信息；使用命令信息(图 [8-9](#Fig9) )。信息适用于所有其他表达方式，而不仅仅是机器学习。

```
In[8]:= Information[PF]
Out[8]=

```

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig9_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig9_HTML.jpg)

图 8-9

训练模型的信息报告

信息面板(图 [8-9](#Fig9) )包括数据类型、均方根(标准差)、方法、批量评估速度、损耗、模型记忆、训练样本数和训练时间。面板底部的图形是标准偏差、模型学习曲线和其他算法的学习曲线。如果将光标悬停在数值参数上，将显示置信区间和单位。如果是通过方法名完成的，它会显示线性回归方法的参数。由于我们没有在线性回归方法中选择特定的优化算法，Mathematica 试图在算法中搜索最佳算法(这可以在所有算法的学习曲线中看到)。我们将在后面看到如何访问这些选项。

Note

可以在 Predict 函数中使用的每个方法都有选项和子选项；要查看完全定制，请使用 Wolfram 语言文档中心。

表 [8-1](#Tab1) 显示了可用于模型训练的不同通用选项，以及它们的定义和预测函数训练过程的可能值。

表 8-1

预测函数的最常见选项

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

[计]选项

 | 

定义

 |
| --- | --- |
| 方法 | 算法可能的值:决策树、梯度增强树、线性回归、最近邻。神经网络、随机森林和高斯过程。 |
| 绩效目标 | 性能优化可能的值:直接训练、内存、质量、速度、训练速度、自动。支持值的组合(P PerformanceGoal→ {val1，val2})。 |
| 随机播种 | 伪随机数生成器的种子可能值:自动。“自定义种子”，继承(先前计算中使用的随机种子)。 |
| 目标设备 | 指定执行训练或测试过程的设备可能的值:CPU 或 GPU。如果安装了 GPU，自动目标设备将是 GPU: |
| 时间目标 | 培训过程花费的时间 |
| 培训进展指示器 | 进度报告可能的值:面板、打印、ProgressIndicator、简单面板、无。 |

### 模型测量

一旦建立了模型，我们必须观察和分析测试集中预测函数的性能。为此，我们必须在 PredictorMeasurments 命令中完成。预测函数放在自变量中(图 [8-10](#Fig10) )，接着是测试集，接着是要添加的一个或多个属性。

```
In[9]:= PRM=PredictorMeasurements[PF,test]
Out[9]=

```

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig10_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig10_HTML.jpg)

图 8-10

测试模型的预测器测量对象

返回的对象称为 PredictorMeasurementsObject(图 [8-10](#Fig10) )。我们可以从 PredictorMeasurements 命令添加属性。我们可以给对象分配一个变量来更简单地访问它。让我们用测试集来看看模型报告(图 [8-11](#Fig11) )。

```
In[10]:= PRM["Report"]
Out[10]=

```

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig11_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig11_HTML.jpg)

图 8-11

测试模型报告

该报告(图 [8-11](#Fig11) )显示了不同的参数，如均方根(标准差)、平均交叉熵等。它向我们展示了模型的拟合图以及当前值和预测值。我们看到这个模型在大多数情况下都是好的，除了仍然有一些影响性能的异常值。

为了更好地理解模型的精度，让我们看看图 [8-12](#Fig12) 中所示的均方根误差(RMSE)和 RSquared(决定系数)。要显示相关的不确定性，请使用选项“用真值计算不确定性”。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig12_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig12_HTML.jpg)

图 8-12

标准差和 r 平方值

```
In[11]:= Dataset[AssociationMap[PRM[#,ComputeUncertainty→True]&,{"StandardDeviation","RSquared"}]]
Out[11]=

```

这给了我们一个稍高的 RMSE 值，而不是一个好的 r 平方值。请记住，r 平方的值表明了该模型在进行预测方面有多好。这两个值表明，尽管房间数量和价格之间可能存在线性关系，但这不一定用线性回归来解释。这些观察结果也是一致的，记住我们获得了 0.7 的相关值。

### 模型评估

模型中的图形是模型图和目标变量(比较图)。若要检查方差的分布，请使用残差直方图函数，若要检查残差图，请使用残差图。这些如图 [8-13](#Fig13) 所示。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig13_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig13_HTML.jpg)

图 8-13

残差直方图、残差图和比较图

```
In[12]:= PRM[#]&/@{"ResidualHistogram","ResidualPlot","ComparisonPlot"}
Out[12]=

```

为了找出预测值测量对象的所有属性，我们将属性写成一个参数。这些属性会因方法而异。

```
In[13]:= PRM["Properties"]
Out[13]= {BatchEvaluationTime,BestPredictedExamples,ComparisonPlot,EvaluationTime,Examples,FractionVarianceUnexplained,GeometricMeanProbabilityDensity,LeastCertainExamples,Likelihood,LogLikelihood,MeanCrossEntropy,MeanDeviation,MeanSquare,MostCertainExamples,Perplexity,PredictorFunction,ProbabilityDensities,ProbabilityDensityHistogram,Properties,RejectionRate,Report,ResidualHistogram,ResidualPlot,Residuals,RSquared,StandardDeviation,StandardDeviationBaseline,TotalSquare,WorstPredictedExamples}

```

如果我们对选择的方法或超参数不满意，可以通过为超参数配置新值来重新训练模型。我们在信息命令的帮助下访问当前方法的值，并添加 method(向我们显示用于训练模型的方法)、MethodDescription(所用方法的描述)和 MethodOption(方法选项)的属性。

```
In[14]:= Information[PF,"MethodOption"]
Out[14]= Method→{LinearRegression,L1Regularization→0,L2Regularization→0.00001,OptimizationMethod→NormalEquation}

```

我们可以看到，有 L1 正则化、L2 正则化和 OptimizationMethod 等术语。前两个术语与正则化方法相关，L1 指的是套索回归名称，L2 指的是岭回归名称。除了减少变化之外，正则化用于最小化模型的复杂性；它还提高了模型的精度，解决了过拟合问题。这是通过向损失函数添加惩罚来实现的；该罚值被加到系数绝对值的总和上![$$ {\lambda}_1\ast {\sum}_{i=0}^{\mathrm{N}}\left|{\theta}_i\right| $$](../images/500903_1_En_8_Chapter/500903_1_En_8_Chapter_TeX_IEq4.png)，而对于 L2，它由表达式![$$ \left({\lambda}_2/2\right)\ast {\sum}_{i=0}^{\mathrm{N}}{\theta_i}^2 $$](../images/500903_1_En_8_Chapter/500903_1_En_8_Chapter_TeX_IEq5.png)给出，其中最小化的函数是损失函数![$$ \left(1/2\right)\ast {\sum}_{i=0}^{\mathrm{N}}{\left({y}_i-f\left(\theta, {x}_i\right)\right)}^2 $$](../images/500903_1_En_8_Chapter/500903_1_En_8_Chapter_TeX_IEq6.png)。更多数学深度，请访问*人工智能:现代方法*。斯图尔特·拉塞尔和彼得·诺维格(2010 年，新泽西州上萨德尔河:普伦蒂斯霍尔)和*加雷斯·詹姆斯、特雷弗·哈斯蒂、罗伯特·蒂布拉尼和丹妮拉·威滕(2017 年；第一版。2013 年，第 7 版印刷，2017 年版。:施普林格)。第三项是我们要选择哪种优化方法的选项；现有的方法有法线方程、随机梯度下降法和正交拟牛顿法。也就是说，必须强调的是，当使用 L1 和 L2 标准的系数向量时，这就是所谓的弹性净回归模型。在参数相关的情况下，可以使用弹性网。要了解更多理论，请参考下一篇参考文献，Trevor Hastie、Robert Tibshirani 和 Jerome Friedman 编写的《统计学习的要素:数据挖掘、推理和预测》第二版*(2009 年第 2 版，第 9 版，2017 年第 3 版)。:施普林格)。

### 再训练模型超参数

正如后面所讨论的，让我们重新训练该模型，但是使用 L1 → 12、L2 → 100 的值和优化算法 optimization method→stochasticgradientdesinc，TrainingProgressReporting → None，performance goal→“Quality”，RandomSeeding → 10000，target device→“CPU”。

```
In[15]:= PF2=Predict[training,Method→{"LinearRegression","L1Regularization"→ 12,"L2Regularization"→100,"OptimizationMethod"→ Automatic},TrainingProgressReporting→None,PerformanceGoal→"Quality",RandomSeeding→10000,TargetDevice→"CPU"];

```

若要查看与示例相关的属性，请在 Predictorfunction 的输入数据后键入 properties，例如 PF2["example "，" Properties"]。

现在，让我们像之前一样通过显示图表和指标来比较新模型的性能(图 [8-14](#Fig14) 和图 [8-15](#Fig15) )。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig15_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig15_HTML.jpg)

图 8-15

RMSE 和 r 平方的新值

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig14_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig14_HTML.jpg)

图 8-14

约束模型的图

```
In[16]:= PRM2=PredictorMeasurements[PF2,test];
PRM[#]&/@{"ResidualHistogram","ResidualPlot","ComparisonPlot"}
Dataset[AssociationMap[PRM2[#,ComputeUncertainty→True]&,{"StandardDeviation","RSquared"}]]
Out[16]=

```

```
Out[16]=

```

在图中观察，我们看到模型只是下降到一定程度；这与 r 平方的新值一致，该值减小到 0.51。然而，在预测未来时，它仍然是一个糟糕的模型。这可以归因于优化选择，L1 和 L2 参数的选择。

## 逻辑回归

逻辑回归是统计学中常用的技术，但它也用于机器学习中。逻辑回归考虑到响应变量的值只取两个值，0 和 1；这也可以解释为假或真的条件。它是一个二元分类器，根据模型的构建方式，使用一个函数来预测是否满足某个条件的概率。通常，这种类型的模型用于分类，因为它能够为我们提供概率和分类，因为逻辑回归的值在两个值之间振荡。在逻辑回归中，目标变量是包含编码数据的二元变量。欲了解更多信息，请访问 Laura Igual、Santi Seguí、Jordi Vitrià、Eloi Puertas、Petia Radeva、Oriol Pujol、Sergio Escalera、Francesc Dantí和 Lluis 加里多(2017 版)撰写的*数据科学导论:概念、技术和应用的 Python 方法。:施普林格)。*

### 泰坦尼克号数据集

在下面的例子中，我们将使用泰坦尼克号数据集，这是一个描述乘客生存状态的数据集。使用的变量是阶级、年龄、性别和生存条件。我们将从 ExampleData 中直接加载数据作为数据集(图 [8-16](#Fig16) )并枚举数据集的行。

Note

本节将完全使用查询语言来构建，以便读者可以理解如何在数据集内更深入地使用它。

```
In[1]:= Titanic=Query[AssociationThread[Range[Length@#]→Range[Length@#]]][ExampleData[{"Dataset","Titanic"}]]&[ExampleData[{"Dataset","Titanic"}]]
Out[1]=

```

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig16_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig16_HTML.jpg)

图 8-16

泰坦尼克号数据集

让我们使用 dimensions 命令来看看数据的维度。

```
In[2]:= Dimensions@Titanic
Out[2]= {1309,4}

```

解释结果，我们看到数据集由 4 列 1309 行组成。查看数据集，有四列按阶级、年龄、性别和幸存状态分类。如果我们使用空格键，我们会看到有些元素没有注册数据输入。若要查看哪些列包含丢失的数据，请通过计算与每列中丢失的模式相对应的元素数来执行以下代码。

```
In[3]:= Query[Count[_Missing],#]@Titanic&/@{"class","age","sex","survived"}
Out[3]= {0,263,0,0}

```

这给了我们一个结果，年龄列中有 263 个缺失值，其他的值为零。让我们删除包含这些缺失数据的行，但是首先我们将从缺失数据中提取行号，方法是从 age 列中选择等于缺失的元素，然后提取行 id。

```
In[4]:= Query[Select[#age==Missing[]&]][Titanic];
Normal@Keys@%
Out[5]= {16,38,41,47,60,70,71,75,81,107,108,109,119,122,126,135,148,153,158,167,177,180,185,197,205,220,224,236,238,242,255,257,270,278,284,294,298,319,321,36,4,383,385,411,470,474,478,484,492,496,525,529,532,582,596,598,673,681,682,683,706,707,757,758,768,769,776,790,796,799,801,802,803,805,806,809,813,814,816,817,820,836,843,844,853,855,857,859,866,872,873,875,877,880,883,887,888,901,902,903,904,919,921,922,923,924,927,928,929,930,931,932,941,943,945,946,947,949,955,956,957,958,959,962,963,972,974,977,983,984,985,988,989,990,992,994,995,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1010,1013,1014,1015,1017,1019,1023,1024,1028,1029,1030,1031,1033,1034,1035,1036,1037,1038,1039,1040,1042,1043,1044,1045,1053,1054,1055,1056,1070,1071,1072,1073,1074,1075,1077,1078,1079,1081,1082,1086,1096,1110,1115,1116,1117,1122,1123,1124,1125,1129,1133,1136,1137,1138,1139,1150,1151,1152,1155,1156,1160,1163,1164,1165,1167,1168,1169,1171,1173,1174,1175,1176,1177,1178,1179,1180,1181,1185,1186,1187,1194,1195,1196,1198,1199,1200,1201,1203,1213,1214,1215,1216,1217,1220,1222,1242,1243,1244,1246,1247,1248,1250,1251,1254,1256,1263,1269,1283,1284,1285,1292,1293,1294,1298,1303,1304,1306}

```

这些数字表示包含“年龄”列缺失数据的行。为了消除它们，我们使用 DeleteMissing 命令，考虑到在级别 1 上有丢失的数据。最终数据集见(图 [8-17](#Fig17) )

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig17_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig17_HTML.jpg)

图 8-17

没有缺失值的泰坦尼克号数据集

```
In[5]:= Titanic=DeleteMissing[Titanic,1,1]
Out[5]=

```

例如，为了证实不再有任何丢失的数据，您可以对计数应用相同的代码，或者通过查看被删除的行的键。

```
In[6]:= Titanic[Key[16]]
Out[6]= Missing[KeyAbsent,16]

```

这意味着没有与键 16 相关联的内容。如果要检查所有键，请使用缺失数据的行列表。

### 数据探索

一旦我们移除了缺失的数据，我们就可以计算出由每个阶级、性别和生存状态组成的元素的数量(图 [8-18](#Fig18) )。

```
In[7]:= Dataset@
<|
"Class"→Query[Counts,"class"]@Titanic,"Sex"→ Query[Counts,"sex"]@Titanic,
"Survival status"→Query[Counts,"survived"]@Titanic
|>
Out[7]=

```

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig18_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig18_HTML.jpg)

图 8-18

基本要素包括阶级、性别和生存状态

在排除了缺少元素的行之后，我们看到数据集由第一类中的 284 个元素、第二类中的 261 个元素和第三类中的 501 个元素组成(图 [8-19](#Fig19) )。还要注意的是，超过一半的登记乘客是男性，死亡人数多于幸存者。可以通过显示百分比来验证这一点(图 [8-19](#Fig19) )。同样的方法也适用于列 class 和 sex。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig19_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig19_HTML.jpg)

图 8-19

阶级、性别和生存状态的饼状图

```
In[8]:= Row[{PieChart[{N@(#[[1]]/Total@#),N@(#[[2]]/Total@#)}&[Counts[Query[All,"survived"][Titanic]]], PlotLabel→Style["Percentage of survival",#3,#4], ChartLegends→ {"Survived", "Died"}, ImageSize→#1,ChartStyle→#2,LabelingFunction→(Placed[Row[{SetPrecision[100#,3],"%"}],"RadialCallout"]&)],
PieChart[{N@(#[[1]]/Total@#),N@(#[[2]]/Total@#)}&[Counts[Query[All,"sex"][Titanic]]], PlotLabel→Style["Percentage by sex",#3,#4], ChartLegends→{"Female", "Male"}, ImageSize→#1,ChartStyle→#2,LabelingFunction→(Placed[Row[{SetPrecision[100#,3],"%"}],"RadialCallout"]&)],
PieChart[{N@(#[[1]]/Total@#),N@(#[[2]]/Total@#),N@(#[[3]]/Total@#)}&[Counts[Query[All,"class"][Titanic]]], PlotLabel→Style["Percentage by class",#3,#4], ChartLegends→{"1st", "2nd","3rd"}, ImageSize→#1,ChartStyle→#2,LabelingFunction→(Placed[Row[{SetPrecision[100#,3],"%"}],"RadialCallout"]&)]},"----"]&[200,{ColorData[97,20],ColorData[97,13],ColorData[97,32]},Black,20]
Out[8]=

```

在这个案例中，我们将预测泰坦尼克号乘客的存活率。我们将建立一个模型，该模型将对给定的阶级、年龄和性别是否会幸存下来进行分类。特征将是阶级、年龄和性别，目标将是生存状态。我们将使用这些变量作为特征，然后模型将使用这些特征来分类他们的阶级、年龄和性别是否幸存，这是我们的目标变量。为此，我们将数据集分为 80%的训练(837 个元素)和 20%的测试(209 个元素)。为了分割数据集，首先我们将进行随机抽样；之后，我们将提取 IDs 的键，并创建按训练和测试集划分的新数据集(图 [8-20](#Fig20) )。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig20_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig20_HTML.jpg)

图 8-20

按训练和测试集划分的 Titanic 数据集

```
In[9]:= BlockRandom[SeedRandom[8888];
RandomSample[Titanic]];
Keys@Normal@Query[All][%];
{train,test}={%[[1;;837]],%[[838;;1046]]};
dataset=Query[<|"Train"→{Map[Key,train]},"Test"→{Map[Key,test]} |> ][Titanic]
Out[9]=

```

### 分类功能

Classify 命令是 Wolfram 语言机器学习方案中使用的另一个超级功能。该函数可用于解决分类问题的任务中。该函数接受的数据是数字、文本、声音和图像数据。此函数的输入数据可以采用与预测函数{x → y}相同的方式。但是，也可以以元素列表、元素关联或数据集的形式输入数据。在这种情况下，我们将把它作为数据集来介绍。

在这种情况下，我们将通过指定指向目标(存活)的列输入(类、年龄、性别)来从数据集格式中提取数据。现在我们来构建分类器函数(图 [8-21](#Fig21) )，选项如下，方法→ {LogisticRegression，L1 →自动，L2 →自动}。当选择自动时，我们让 Mathematica 选择 L1 和 L2 参数的最佳组合。对于 OptimizationMethod，请设置 StochasticGradientDescent 方法。和绩效目标设定质量。最后，选择一个值为 100，000 的种子和 CPU 单元作为目标设备。

逻辑回归的优化方法是有限记忆的 Broyden-Fletcher-gold farb-Shanno 算法(LBFGS)、StochasticGradientDescent(随机梯度法)和 Newton(牛顿法)。这些用于估计逻辑函数的参数。将使用查询语言从数据集中的数据构建规则。

```
In[10]:= CF=Classify[Flatten[Values[Normal[Query["Train",All,All,{#class,#age,#sex}→ #survived&][dataset]]]],Method→{"LogisticRegression","L1Regularization"→ Automatic,"L2Regularization"→ Automatic,"OptimizationMethod"→"StochasticGradientDescent"},PerformanceGoal→"Quality",RandomSeeding→100000,TargetDevice→"CPU",TrainingProgressReporting→None]
Out[10]=

```

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig21_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig21_HTML.jpg)

图 8-21

分类功能对象

训练后，与预测函数一样，分类函数返回分类函数对象(图 [8-21](#Fig21) )而不是预测函数。检查分类器函数，我们可以看到两种输入数据类型——名义数据和数值数据。类，即生存状态—假或真。使用的方法(逻辑回归)；以及示例的数量(837)。要获得有关模型的信息，请使用信息命令。让我们看看模型报告(图 [8-22](#Fig22) )。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig22_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig22_HTML.jpg)

图 8-22

关于已训练分类器函数的信息

```
In[11]:= Information[CF]
Out[11]=

```

Note

如果您单击图表上方的箭头，将显示三个图:学习曲线、准确度和所有算法的学习曲线。如果将指针悬停在最后一行上，会出现一个工具提示，显示相应的参数以及所用的方法，如图 [8-23](#Fig23) 所示。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig23_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig23_HTML.jpg)

图 8-23

逻辑回归方法的算法规范工具提示

我们看到，该模型的准确性约为 78%。我们还通过点击图中的箭头观察到，从 500 个或更多的例子来看，学习曲线和准确度没有显示出改进的迹象。若要访问已定型模型的所有属性，请将属性作为选项添加到信息中。

```
In[12]:= Information[CF,"Properties"]
Out[12]= {Accuracy,BatchEvaluationSpeed,BatchEvaluationTime,Classes,ClassNumber,ClassPriors,EvaluationTime,ExampleNumber,FeatureNames,FeatureNumber,FeatureTypes,FunctionMemory,FunctionProperties,IndeterminateThreshold,L1Regularization,L2Regularization,LearningCurve,MaxTrainingMemory,MeanCrossEntropy,Method,MethodDescription,MethodOption,OptimizationMethod,PerformanceGoal,Properties,TrainingClassPriors,TrainingTime,UtilityFunction}

```

Note

根据使用的方法，属性可能会有所不同。

让我们看看数据的概率是多少:阶级=第三，年龄= 23，性别=男性。概率→类别或概率的名称或数量→最可能类别的数量。

```
In[13]:= CF[{"3rd",23,"male"},{"Probability"→ False,"TopProbabilities"→ 2}]
Out[13]= {0.839494,{False→0.839494,True→0.160506}}

```

后一个例子的概率表明乘客的生存状态可能更倾向于错误状态。

要查看新分类的完整属性，请键入示例，后跟属性。包含的属性是决策(根据概率及其效用函数选择最佳类别)和分布(分类分布对象)。每个类别的概率显示为关联、预期效用(预期概率)、对数概率(自然对数概率)、概率(所有类别的概率)和顶概率(最可能的类别)。这显示在以下数据集中(图 [8-24](#Fig24) )。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig24_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig24_HTML.jpg)

图 8-24

定型模型的分类器函数的属性

```
In[14]:= Dataset@
AssociationMap[CF[{"3rd",23,"male"},#] &,{"Decision","Distribution","ExpectedUtilities","LogProbabilities","Probabilities","TopProbabilities"}]
Out[14]=

```

Note

要检查对数结果，请使用 Log 命令 Log["base "，" number"]。

### 测试模型

我们现在将使用 ClassifierMeasurements(图 [8-25](#Fig25) )命令测试测试数据上的模型，添加函数和测试集作为参数，并计算不确定性。

```
In[15]:= CM=ClassifierMeasurements[CF,Flatten[Values[Normal[Query["Test",All,All,{#class,#age,#sex}→ #survived&][dataset]]]],ComputeUncertainty→True,RandomSeeding→8888]
Out[15]=

```

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig25_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig25_HTML.jpg)

图 8-25

分类函数的 ClassifierMeasurements 对象

返回的对象称为 ClassifierMeasurementsObject(图 [8-25](#Fig25) )，用于在测试测试集后寻找 ClassifierFunction 的属性。现在让我们来看看报告(图 [8-26](#Fig26) )。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig26_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig26_HTML.jpg)

图 8-26

测试模型的报告

```
In[16]:= CM["Report"]
Out[16]=

```

图中的报告显示了诸如测试例子的数量、准确性和准确性基线等信息。它还向我们显示了混淆矩阵，它向我们显示了分类模型的预测结果，显示了正确和错误预测的数量；在这种情况下，这些被类分解，返回假或真，这给我们一个模型正在犯的错误和它正在犯的错误类型的概念。基本上，它显示了每个类的真阳性和真阴性以及假阳性和假阴性。

我们来具体看一下图(混淆矩阵)(图 [8-27](#Fig27) )。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig27_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig27_HTML.jpg)

图 8-27

测试模型的混淆矩阵图

```
In[17]:= CM["ConfusionMatrixPlot"]
Out[17]=

```

要获得混淆矩阵的值，请使用 CM["ConfusionMatrix"]或类 CM["ConfusionFunction"]。

查看该图，我们看到该模型进行了分类，从顶部的左到右开始，106 个错误示例被正确分类，21 个错误示例被正确分类，34 个正确示例被正确分类，48 个正确示例被正确分类。为了更好地可视化性能，让我们来看看每一类的 ROC 曲线(图 [8-28](#Fig28) ),它们各自的值，以及 Matthews 相关系数和 AUC 值。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig28_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig28_HTML.jpg)

图 8-28

每个类别的 ROC 曲线以及 AUC 和 MCC 值

```
In[18]:= {CM["ROCCurve"],Dataset@<|{"AUC"→CM["AreaUnderROCCurve"]},{"MCC"→CM["MatthewsCorrelationCoefficient"]}|>}

```

显然，这两个类具有相同的值，但是与 ROC 曲线相比，我们可以看到，假类比真类具有更好的分类；让我们看看最不确定的例子，这样我们就可以看到真实类比虚假类有更糟糕的确定例子。这样，我们可以显示模型的不太精确的结果，这些结果具有最高的熵分布和每个类别的平均交叉熵。

```
In[19]:= CM[{"LeastCertainExamples","ClassMeanCrossEntropy"}]
Out[19]= {{{3rd,39,female}→False,{3rd,38,female}→True,{3rd,37,female}→False,{3rd,37,female}→False,{3rd,36,female}→True,{3rd,32,female}→False,{1st,4,male}→True,{3rd,30,female}→False,{3rd,28,female}→False,{3rd,27,female}→True},<|False→0.363541,True→0.85931|>}

```

要获取 MCC 系数的值，请使用以下属性:FalseDiscoveryRate、FalsePositiveRate、FalseNegativeRate(每个类的假阳性和假阴性发现率)、FalseNegativeExamples、FalseNegativeNumber(真阴性)、FalsePositiveExamples 和 FalsePositiveNumber(真阳性)。这些以简短的形式显示在这里。

```
In[20]:= CM[#]&/@{"FalseDiscoveryRate","FalseNegativeRate","FalsePositiveRate"}
Out[20]= {<|False→0.242857,True→0.304348|>,<|False→0.165354,True→0.414634|>,<|False→0.414634,True→0.165354|>}

```

查看模型在预测中是否表现一致的另一种方法是查看关键指标值，如准确性、召回率、f1 得分精度和准确性拒绝图(图 [8-29](#Fig29) )。让我们来看看模型的这些指标。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig29_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig29_HTML.jpg)

图 8-29

准确度、召回率、f1 得分、精确度和准确度喷射图的值的表格

```
In[21]:= CM[{"Accuracy","Recall","F1Score","Precision","AccuracyRejectionPlot"}]//TableForm
Out[21]//TableForm=

```

要查看准确性的相关度量，请键入以下属性:准确性(正确分类的示例数)、准确性基线(预测公共类的准确性)和准确性剔除图(弧形图，准确性剔除曲线)。但是，若要查找有关概率和测试集的预测类的信息，请使用下列属性:DecisionUtilities(测试集中每个示例的效用函数值)、Probabilities(测试集中每个示例的概率)和 ProbabilityHistogram(类概率的直方图)。

让我们通过绘制乘客生存状态的概率来看看概率是如何表现的(图 [8-30](#Fig30) )。记住错误的状态意味着一名乘客没有生还，而正确的状态意味着一名乘客幸存。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig30_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig30_HTML.jpg)

图 8-30

每个类别的概率，取决于类别、年龄和性别

```
In[22]:= TruPlot=
{Plot[{CF[{#1,age,#4},"Probability"→ #6 ],CF[{#2,age,#4},"Probability"→ #6 ],CF[{#3,age,#4},"Probability"→ #6 ]}, {age,0,90},PlotLegends→{"Male in 1st class", "Male in 2nd class ", "Male in 3rd class"},FrameLabel→ {Style["Age in years",Bold,15], Style["Probability",Bold,15]}, Frame→#6,FrameTicks→#7,GridLines→ {{20,40,60,80}},ImageSize→#8],Plot[{CF[{#1,age,#5},"Probability"→ #6 ],CF[{#2,age,#5},"Probability"→ #6 ],CF[{#3,age,#5},"Probability"→ #6 ]}, {age,0,90},PlotLegends→{"Female in 1st class", "Female in 2nd class ", "Female in 3rd class"},FrameLabel→ {Style["Age in years",Bold,15], Style["Probability",Bold,15]}, Frame→#6,FrameTicks→#7,GridLines→ {{20,40,60,80}},ImageSize→#8]}&["1st","2nd","3rd","male","female",True,All,250];
FalsPlot={Plot[{CF[{#1,age,#4},"Probability"→ #6 ],CF[{#2,age,#4},"Probability"→ #6],CF[{#3,age,#4},"Probability"→ #6]}, {age,0,90},PlotLegends→{"Male in 1st class", "Male in 2nd class ", "Male in 3rd class"},FrameLabel→ {Style["Age in years",Bold,15], Style["Probability",Bold,15]}, Frame→True,FrameTicks→#7,GridLines→ {{20,40,60,80}},ImageSize→#8],Plot[{CF[{#1,age,#5},"Probability"→ #6 ],CF[{#2,age,#5},"Probability"→ #6 ],CF[{#3,age,#5},"Probability"→ #6]}, {age,0,90},PlotLegends→{"Female in 1st class", "Female in 2nd class ", "Female in 3rd class"},FrameLabel→ {Style["Age in years",Bold,15], Style["Probability",Bold,15]}, Frame→True,FrameTicks→#7,GridLines→ {{20,40,60,80}},ImageSize→#8]}&["1st","2nd","3rd","male","female",False,All,250];
Headings={Style["True class",Black,20,FontFamily→"Arial Rounded MT"],Style["False class",Black,20,FontFamily→"Arial Rounded MT"]};
Grid[{{Headings[[1]],Headings[[2]]},{TruPlot[[1]],FalsPlot[[2]]},{TruPlot[[2]],FalsPlot[[1]]}},Alignment→{{Center,Center},{None,None}},Dividers→{False,1}]
Out[22]=

```

在图 [8-30](#Fig30) 所示的图表中，可以清楚地看到，随着年龄的增长，男性的生存概率降低，甚至达到低于 20%的概率值，无论是 1 级、2 级<sup>和 3 级</sup>。这与女性的生存概率相反，女性的生存概率从 60%以上的几率值开始，随着年龄的增长而下降，一等女性的生存概率达到 50%以上。

## 数据分组

数据聚类方法是一种无监督学习，如 m .埃姆雷·雪拉比和凯末尔·艾登在*无监督学习算法*(2018；2016 年第一版原版的软皮再版。由…编辑:施普林格)。它通常用于查找数据聚类的结构和特征，其中要观察的点被分成不同的组，通过这些组，基于独特的特征对它们进行比较。

在下面的例子中，我们将创建一个二元数据序列，并绘制出数据点列表(图 [8-31](#Fig31) )。要查找集群，有 find clusters 命令；该命令根据相似性对点进行划分。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig31_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig31_HTML.jpg)

图 8-31

随机数据的 2D 散点图

```
In[1]:= BlockRandom[
SeedRandom[321];
RndPts=Table[{i,RandomReal[{0,1}]},{i,1,450}];]
ListPlot[RndPts,PlotRange→All,PlotStyle→Directive[Thick,Blue],Frame→True,FrameTicks→All]
Out[1]=

```

### 聚类识别

FindClusters 函数用于检测一组数据中具有相似特征的分区。该函数将聚类元素收集到该函数找到的子组中。当您没有向 Find Clusters 命令添加选项时，Mathematica 将自动设置聚类识别参数。用于其他机器学习方法的一些选项也可以用于此命令。例如，PerformanceGoal、Method 和 RandomSeeding 等。

```
In[2]:= Clusters=FindClusters[RndPts,PerformanceGoal→"Speed",Method→Automatic,DistanceFunction→Automatic,RandomSeeding→1234];
Short[Clusters,4]
Out[2]//Short= {{{1,0.924416},{2,0.695055},{5,0.715785},{8,0.951038},<<137>>,{372,0.895003},{395,0.917268},{410,0.974659},{422,0.962478}},{<<1>>},{{236,<<19>>},<<166>>}}

```

让我们看看识别了多少个集群。我们将使用 Length 命令；这样我们将获得列表的一般形式。

```
In[3]:= Length[Clusters]
Out[3]= 3

```

我们看到结果是三。这可以解释为:列表包含三个元素(即三个子列表)，每个列表代表一个簇，每个簇内有一个子列表，包含每个已识别簇的点。为了找出每个集群中包含多少元素，我们使用 Map 命令，并在规范级别应用 Dimension 命令。

```
In[4]:= Map[Dimensions,Clusters,1]
Out[4]= {{145,2},{138,2},{167,2}}

```

这告诉我们，第一簇包含 145 个元素，第二簇包含 138 个元素，第三簇包含 167 个元素；这些是我们之前创建的相同数量的点，等于 450。每个簇由一个两点坐标系组成。FindClusters 命令返回识别分类的点。让我们看看生成的集群的图；这如图 [8-32](#Fig32) 所示。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig32_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig32_HTML.jpg)

图 8-32

确定的三个集群的 2D 散点图

```
In[5]:= ListPlot[Clusters,PlotStyle→{Red,Blue,Green},PlotLegends→Automatic,Frame→True,FrameTicks→All,PlotLabel→Style["Cluster Plot",Italic,20,Black],Prolog→{LightYellow,Rectangle[Scaled[{0,0}],Scaled[{1,1}]]}]
Out[5]=

```

正如我们所看到的，Find Clusters 自动找到了聚类并给它们着色。为了显式地建立要搜索的聚类数，我们添加所需的数字作为第二个参数，即以 FindCluster ["points "，" number of clusters"]的形式。在前面的例子中，我们将方法选项设置为自动。这里显示了查找集群的不同方法。Agglomerate(这是单链接聚类的算法)、基于密度的带噪声应用空间聚类(DBSCAN)、NeighborhoodContraction(最近邻链算法)、Jarvis Patrick(Jarvis \[Dash]Patrick 聚类算法)、KMeans (k-means 聚类)、MeanShift (mean-shift 聚类)、KMedoids (k-medoids 划分)、SpanningTree(最小生成树聚类)、Spectral(频谱聚类)和 GaussianMixture(高斯混合模型)。

### 选择距离函数

除了方法选项之外，还有 DistanceFunction，其值为 Automatic。该选项用于定义如何计算点之间的距离。一般情况下当我们选择自动时，使用的是平方欧氏距离(∑(*y*<sub>*I*</sub>—*x*<sub>*I*</sub>)<sup>2</sup>，SquaredEuclideanDistance)。距离函数还有其他值，euclideandindation![$$ \left(\sum \sqrt{{\left({y}_i-{x}_i\right)}^2}\right) $$](../images/500903_1_En_8_Chapter/500903_1_En_8_Chapter_TeX_IEq7.png)、Manhattan distance(∑|*x*_ {*I*}-*y*_ {*I*} |、ChessboardDistance 或 Chebyshev distance(*max*(|*x*_ {*I*}-*y【T30*

现在我们知道了如何识别聚类，我们想知道每个聚类的质心。为此，有必要计算聚类点的平均值。一系列点的质心从下面的表达式![$$ \mu =\sum \frac{x_i}{n} $$](../images/500903_1_En_8_Chapter/500903_1_En_8_Chapter_TeX_IEq8.png)中获得，可以解释为点的平均值。对于计算，我们从每个聚类中提取数据，并计算其算术平均值。

```
In[6]:= {Cluster1Centroid,Cluster2Centroid,Cluster3Centroid}={N@Mean@Clusters[[1,All]],N@Mean@Clusters[[2,All]],N@Mean@Clusters[[3,All]]}
Out[6]= {{182.807,0.815713},{115.935,0.300888},{353.108,0.39227}}

```

让我们将聚类和它们的质心绘制在一起，以形象化地显示这些点是如何相对于每个质心进行分类的(图 [8-33](#Fig33) )。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig33_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig33_HTML.jpg)

图 8-33

三个星团的 2D 散点图用它们各自的质心来标识

```
In[7]:= ClusterPlot=ListPlot[Clusters,PlotStyle→{Red,Blue,Green},PlotLegends→{"Cluster 1","Cluster 2","Cluster 3"}];
CentroidPlot=ListPlot[{Cluster1Centroid,Cluster2Centroid,Cluster3Centroid},PlotStyle→Black];
Show[{ClusterPlot,CentroidPlot},Prolog→{LightYellow,Rectangle[Scaled[{0,0}],Scaled[{1,1}]]},Frame→ True,FrameTicks→ All,PlotLabel→Style["Cluster Plot",Italic,20,Black]]
Out[7]=

```

要确保第一个聚类对应于红色点，请尝试使用 ListPlot 来绘制聚类[[1，All]]中包含的点，以及第二个聚类(蓝色)和第三个聚类(绿色)中包含的点。

作为一种替代方法，我们可以通过在绘图中添加 Epilog 选项来突出显示质心区域。Epilog 是另一个类似 Prolog 的图形选项，但我们将使用它来突出显示质心点的区域(图 [8-34](#Fig34) )。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig34_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig34_HTML.jpg)

图 8-34

三个星团的 2D 散点图用它们各自的质心来标识

```
In[8]:= Show[{ClusterPlot,CentroidPlot},Prolog→{LightYellow,Rectangle[Scaled[{0,0}],Scaled[{1,1}]]},Frame→ True,FrameTicks→ All,Epilog→{Opacity[0.2],PointSize[0.1],Point[Cluster1Centroid],Point[Cluster2Centroid],Point[Cluster3Centroid]}]
Out[8]=

```

### 识别类别

一旦我们用命令 FindClusters 标识了我们的类，我们就可以使用 ClusteringComponents 命令来标记或标识所找到的不同类。我们必须在 ClusteringComponents 命令中指定集群的数量和查找集群的位置，因为有多种方法可以使用 ClusteringComponents。

```
In[9]:= Classes=ClusteringComponents[Clusters,3,2,Method→Automatic,DistanceFunction→Automatic,RandomSeeding→ 1234,PerformanceGoal→"Speed"]
Out[9]= {{1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,1,1,2,1,2,1,2,2,1,1,1,1,2,1,1,2,1,1,2,1,2,2,2,2,1,1,2,2,1,2,2,2,2,2,2,1,2,2,2,2,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2},{1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,3,1,1,3,1,3,1,1,1,1,1,1,1,1,1,3,1,3,3,1,3,1,1,3,1,1,1,1,3,1,3,1,1,1,3,3,1,1,3,3,3,3,3,3},{2,3,2,3,3,3,3,3,3,3,3,3,3,2,3,3,3,3,3,2,3,3,3,3,3,3,3,2,3,3,3,3,3,3,3,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,2,3,3,3,3,3,3,3,3,3,2,2,2,3,2,2,3,3,2,3,3,3,2,3,3,3,3,3,2,3,2,3,3,3,3,2,3,3,2,3,3,2,2,3,3,3,3,3,3,2,2,2,3,3,3,3,3,3,3,3,3,3,3,2,3,3,3,2,3,3,2,2,3,2,3,3,3,3,3,2,3,3,3,3,3,2,3,3,3,2,3,3,3,2,3,2,2,3,2,3,3,2,2,3,3,2,2,2,3,3,3,3,2,3,3}}

```

这样，对应于这三个类别的数字就会出现。该命令仅标识有三种类型的类；它没有提到每个类意味着什么。这是因为聚类方法通常是对未标记的数据执行的，所以解释是作为分析的一部分执行的。让我们数一数每个类有多少个元素。

```
In[10]:= Flatten[Classes]//Counts
Out[10]= <|1→174,2→132,3→144|>

```

该命令返回给我们，一类包含 174，二类包含 132，三类包含 144。需要澄清的一点是，为什么用 FindClusters 和 ClusteringCompnents 标识的分类会延迟。这是因为通过在距离函数中设置自动选项，我们告诉 Mathematica 找到最佳距离函数。根据数据的不同，一个函数可能以不同的形式收集元素，我们将在后面看到。

### k 均值聚类

目前，我们已经看到了如何以通用的方式搜索集群。在这一部分，我们将重点介绍 K-means 方法。

K-means 是一种查找和分类数据组(K)的技术，以便共享相似特征的元素被分组在一起，并且对于相反的情况(非相似特征)以相同的方式分组。为了区分数据是否包含相似性，该方法计算数据之间相对于质心的距离。它们之间具有较小距离的元素将是那些共享相似性的元素。这种技术是作为一个迭代过程来执行的，在这个过程中，对各组进行调整，直到它们达到收敛。基本上，K-means 方法是一种简单的算法，它包括通过特定的划分在不同的组中进行分类，其中每个点或观察值属于该组。聚类是通过最小化每个对象与其组的质心之间的距离之和来完成的。k-means 聚类技术试图构建聚类，以使它们在一个组内具有最小的变化。这是通过最小化表达式![$$ J\left({C}_i\right)={\sum}_{x\;j\in {C}_i}^N{\left|\left|{x}_j-{\mu}_i\right|\right|}^2 $$](../images/500903_1_En_8_Chapter/500903_1_En_8_Chapter_TeX_IEq9.png)来完成的，其中 C <sub>i</sub> 表示第 I 个聚类，x <sub>j</sub> 表示点，𝜇 <sub>𝑖</sub> 表示每个聚类 C <sub>i</sub> 的质心。函数的平方项是距离函数；最常用的是平方欧几里德距离，如本例所示。要了解更多关于这项技术背后的数学基础，请参考参考资料*统计学习介绍:应用 R* 作者 Gareth James、Daniela Witten、Trevor Hastie 和 Robert Tibshirani。(第 1 版。2013 年，第 7 版印刷，2017 年版。:施普林格)。

在下面的例子中，我们将使用 ExampleData 中的 Fisher 's Irises 数据集。记住这个数据集的特性，执行下面的代码。

```
In[11]:= ExampleData[{"Statistics","FisherIris"},"ColumnDescriptions"]
Out[11]= {Sepal length in cm.,Sepal width in cm.,Petal length in cm.,Petal width in cm.,Species of iris}

```

让我们提取数据集并将变量 iris 赋给它。

```
In[12]:= iris=ExampleData[{"Statistics","FisherIris"}];

```

看一下数据集。

```
In[13]:= Short[iris,6]
Out[13]//Short= {{5.1,3.5,1.4,0.2,setosa},{4.9,3.,1.4,0.2,setosa},{4.7,3.2,1.3,0.2,setosa},{4.6,3.1,1.5,0.2,setosa},{5.,3.6,1.4,0.2,setosa},{5.4,3.9,1.7,0.4,setosa},{4.6,3.4,1.4,0.3,setosa},{5.,3.4,1.5,0.2,setosa},{4.4,2.9,1.4,0.2,setosa},{4.9,3.1,1.5,0.1,setosa},{5.4,3.7,1.5,0.2,setosa},{4.8,3.4,1.6,0.2,setosa},<<126>>,{6.,3.,4.8,1.8,virginica},{6.9,3.1,5.4,2.1,virginica},{6.7,3.1,5.6,2.4,virginica},{6.9,3.1,5.1,2.3,virginica},{5.8,2.7,5.1,1.9,virginica},{6.8,3.2,5.9,2.3,virginica},{6.7,3.3,5.7,2.5,virginica},{6.7,3.,5.2,2.3,virginica},{6.3,2.5,5.,1.9,virginica},{6.5,3.,5.2,2.,virginica},{6.2,3.4,5.4,2.3,virginica},{5.9,3.,5.1,1.8,virginica}}

```

### 降维

由于 iris 数据集由被分类为三种类型的物种的四个特征组成，我们将使用 PCA 方法，因为该方法用于减少高维度问题。在这种情况下，我们想要的是通过两个主要组件来表示这些特性。为此，我们继续对数据进行标准化，也就是说，它们具有零均值和标准差 1，因为方差较大的变量更有可能影响 PCA。

```
In[14]:= ST=Standardize[iris[[All,{1,2,3,4}]]];(*Showing only the first 4 terms*)
%[[1;;4]]//TableForm
Out[14]//TableForm=
-0.897674    1.0156       -1.33575      -1.31105
-1.1392      -0.131539    -1.33575      -1.31105
-1.38073     0.327318     -1.3924       -1.31105
-1.50149     0.0978893    -1.2791       -1.31105

```

有两种方法可以完成这一过程，要么使用 DimensionReduce 命令，要么使用 DimensionReduction 命令，这两种方法用于减少数据的维数。两者的区别在于，第一个以列表的形式返回值。第二个函数返回一个 DimensionReducerFunction(图 [8-35](#Fig35) )作为预测和分类的输出。两者都属于 Wolfram 语言的机器学习专用函数。在这种情况下，我们将使用 DimensionReduction 命令。因为我们有了数据，所以我们引入标准化的数据作为参数，然后是指定的目标维度(2)，使用 as“principal component analysis”方法。这将为我们提供 DimensionReducerFunction，它将为我们指定 DR。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig35_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig35_HTML.jpg)

图 8-35

降级函数对象

```
In[15]:= DR=DimensionReduction[ST,2,Method→"PrincipalComponentsAnalysis"]
Out[15]=

```

该函数的属性是“ReducedVectors”(简化向量列表)、“OriginalData”(给定简化向量，从原始数据列表中推导)、“ReconstructedData”(通过简化和反演进行数据重构)、“ImputedData”(用估算值替换缺失值)。我们调用标准化数据值的函数，显示前五个值。坐标 x 和 y 将分别对应于主分量 1 和 2。

```
In[16]:= PCA=DR[ST,"ReducedVectors"];
TableForm[%[[1;;5]],TableHeadings→{None, {"First Principal Component","Second Principal Component"}},TableAlignments→Center]
Out[16]//TableForm=
First Principal Component        Second Principal Component
             -2.2647                      0.480027
            -2.08096                     -0.674134
            -2.36423                     -0.341908
            -2.29938                     -0.597395
            -2.38984                      0.646835

```

这将计算每个组成部分的方差，然后计算总和，以找到解释方差的比例。观察到 PC1 似乎代表了 76%的数据分散，PC2 似乎代表了 23%。为了获得累计百分比，我们将每个组成部分的变化相加。要查看更多关于变异比例的深度信息，请参考*统计学习介绍:在 R* 中的应用(詹姆斯，g .、威滕，d .、哈斯蒂，t .、&蒂布拉尼，R；第一版。2013 年，第 7 版印刷，2017 年版。:施普林格)。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Figa_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Figa_HTML.jpg)

我们看前面工艺做出的主要部件的图(图 [8-36](#Fig36) )。如果您查看示例数据中的完整 iris 数据，前 50 个元素对应于 setosa 物种，接下来的 50 个元素对应于 versicolor，最后的 50 个元素对应于 virginica。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig36_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig36_HTML.jpg)

图 8-36

两个主成分的散点图

```
In[18]:= Labels={Style["First principal component",Black,Bold],Style["Second Principal component",Black,Bold]};
ListPlot[{PCA[[1;;50]],PCA[[51;;100]],PCA[[100;;150]]},PlotLegends→Placed[{Placeholder["setosa"],Placeholder["versicolor"],Placeholder["virginica"]},Right],PlotMarkers→"OpenMarkers",GridLines→All,Frame→True,Axes→False,FrameTicks→All,FrameLabel→Labels]
Out[18]=

```

### 应用 K-均值

现在让我们使用曼哈顿距离，用 K-means 来寻找聚类。通过指定寻找三个集群；我们假设数据可以分为三类。这是因为我们知道原始数据属于三个物种(setosa、versicolor 和 virginica)。这里显示了星团的图(图 [8-37](#Fig37) )，以及它们各自的质心。当选择 k-means 方法时，可以添加子选项，如 InitialCentroids。服装开始质心(质心坐标列表)可以键入，或者我们可以离开自动选项。要输入质心坐标，我们使用以下形式的方法→ {"KMeans "，" InitialCentroids" → {{x1，y1}，{x2，y2}，{x3，y3}...}}，其中 x1，y1 表示 C1(聚类 1)的质心。初始质心不会给命令 FindClusters，以保持某种随机性。

```
In[19]:= Clstr=FindClusters[PCA,3,Method→"KMeans",DistanceFunction→SquaredEuclideanDistance,RandomSeeding→8888];
ListPlot[Clstr,PlotRange→All,Frame→True,AspectRatio→0.8,Axes→False,PlotStyle→{ColorData[97,1],ColorData[97,2],ColorData[97,3]},PlotLabel→Style["K-means clustering for K=3",FontFamily→"Times",Black,20,Italic],FrameTicks→All,PlotLegends→Placed[{Placeholder[Style["Cluster 1",Bold,Black,10]],Placeholder[Style["Cluster 2",Bold,Black,10]],Placeholder[Style["Cluster 3",Bold,Black,10]]},Right],PlotMarkers→ "OpenMarkers",FrameLabel→Labels,GridLines→All,Epilog→{Opacity[1],PointSize[0.01],Point[Mean@Clstr[[1,All]]],Point[Mean@Clstr[[2,All]]],Point[Mean@Clstr[[3,All]]]}]
Out[19]=

```

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig37_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig37_HTML.jpg)

图 8-37

由两个主成分确定的 3 组

在图 [8-37](#Fig37) 中，似乎该方法清楚地将左侧点识别为单个聚类(setosa 物种)，而聚类 2 和 3 之间的一些点可能被错误分类。

### 链接距离函数

更改 DistanceFunction 可以修改分类的排列方式，下一段代码显示了 k = 3 并选择不同距离函数的情况。在下一个代码块中，使用不同的距离函数对相同的 k (3)进行聚类计算，并存储到各自的变量中。然后为每个不同的距离函数绘制聚类图(图 [8-38](#Fig38) ),最后在图形网格中显示。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig38_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig38_HTML.jpg)

图 8-38

对于不同的距离函数，K = 3 时的 K-均值聚类

```
In[20]:= {ED,MhD,ChD,CosD}={FindClusters[PCA,3,PerformanceGoal→#1,Method→#2,DistanceFunction→EuclideanDistance,RandomSeeding→#3],FindClusters[PCA,3,PerformanceGoal→#1,Method→#2,DistanceFunction→ManhattanDistance,RandomSeeding→#3],FindClusters[PCA,3,PerformanceGoal→#1,Method→#2,DistanceFunction→ChessboardDistance,RandomSeeding→#3],FindClusters[PCA,3,PerformanceGoal→#1,Method→#2,DistanceFunction→CosineDistance,RandomSeeding→#3]}&["Quality","KMeans",8888];
{EDplt,MhDplt,ChDplt,CosDplt}={
ListPlot[ED,Frame→#1,AspectRatio→#2,PlotMarkers→#3,PlotStyle→#4,GridLines→#5,PlotRange→#6,ImageSize→#7,FrameLabel→#8,Axes→#9,FrameTicks→#10,
Epilog→{Opacity@#11,PointSize@#12,Point[Mean@ED[[1,All]]],Point[Mean@ED[[2,All]]],Point[Mean@ED[[3,All]]]},PlotLabel→ Style["Euclidean Distance",Black]],
ListPlot[MhD,Frame→#1,AspectRatio→#2,PlotMarkers→#3,PlotStyle→#4,GridLines→#5,PlotRange→#6,ImageSize→#7,FrameLabel→#8,Axes→#9,FrameTicks→#10,
Epilog→{Opacity@#11,PointSize@#12,Point[Mean@MhD[[1,All]]],Point[Mean@MhD[[2,All]]],Point[Mean@MhD[[3,All]]]},PlotLabel→ Style["Manhattan Distance",Black]],
ListPlot[ChD,Frame→#1,AspectRatio→#2,PlotMarkers→#3,PlotStyle→#4,GridLines→#5,PlotRange→#6,ImageSize→#7,FrameLabel→#8,Axes→#9,FrameTicks→#10,
Epilog→{Opacity@#11,PointSize@#12,Point[Mean@ChD[[1,All]]],Point[Mean@ChD[[2,All]]],Point[Mean@ChD[[3,All]]]},PlotLabel→ Style["Chessborad Distance",Black]],
ListPlot[CosD,Frame→#1,AspectRatio→#2,PlotMarkers→#3,PlotStyle→#4,GridLines→#5,PlotRange→#6,ImageSize→#7,FrameLabel→#8,Axes→#9,FrameTicks→#10,
Epilog→{Opacity@#11,PointSize@#12,Point[Mean@CosD[[1,All]]],Point[Mean@CosD[[2,All]]],Point[Mean@CosD[[3,All]]]},PlotLabel→ Style["Cosine Distance",Black]]
}&[True,0.8,"OpenMarkers",{ColorData[97,1],ColorData[97,2],ColorData[97,3]},All,Automatic,300,Labels,False,All,1,0.03];
LegendsText={Placeholder[Style["Cluster 1",Bold,Black,10]],Placeholder[Style["Cluster 2",Bold,Black,10]],Placeholder[Style["Cluster 3",Bold,Black,10]]};
Labeled[Legended[GraphicsGrid[{{EDplt,MhDplt},{ChDplt,CosDplt}},Frame→All,Background→White,Spacings→1],PointLegend[{ColorData[97,1],ColorData[97,2],ColorData[97,3]},LegendsText,LegendMarkers→"OpenMarkers"]],Style["K-means clustering for K=3",FontFamily→"Times",Black,20,Italic],Top]
Out[20]=

```

如图 [8-38](#Fig38) 所示，聚类可以具有不同的排列和不同的距离函数；还需要注意的一点是，每个子图中的簇形心都是变化的(图 [8-38](#Fig38) )。

### 不同的 K

已经看到，对于不同的距离函数，聚类会有所不同，现在让我们用不同的 K 来构建该过程，即 k= 2、3、4 和 5，如图 [8-39](#Fig39) 所示。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig39_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig39_HTML.jpg)

图 8-39

K-表示 K 从 2 到 5

```
In[21]:= {K2,K3,K4,K5}={FindClusters[PCA,2,PerformanceGoal→#1,Method→#2,DistanceFunction→#3,RandomSeeding→#4],FindClusters[PCA,3,PerformanceGoal→#1,Method→#2,DistanceFunction→#3,RandomSeeding→#4],FindClusters[PCA,4,PerformanceGoal→#1,Method→#2,DistanceFunction→#3,RandomSeeding→#4],FindClusters[PCA,5,PerformanceGoal→#1,Method→#2,DistanceFunction→#3,RandomSeeding→#4]}&["Speed","KMeans",SquaredEuclideanDistance,8888];
{PK2,PK3,PK4,PK5}={
ListPlot[K2,Frame→#1,AspectRatio→#2,PlotMarkers→#3,PlotStyle→#4,GridLines→#5,PlotRange→#6,ImageSize→#7,FrameLabel→#8,Axes→#9,FrameTicks→#10,
Epilog→{Opacity@#11,PointSize@#12,Point[Mean@K2[[1,All]]],Point[Mean@K2[[2,All]]]},PlotLabel→ Style["K=2",Black]],
ListPlot[K3,Frame→#1,AspectRatio→#2,PlotMarkers→#3,PlotStyle→#4,GridLines→#5,PlotRange→#6,ImageSize→#7,FrameLabel→#8,Axes→#9,FrameTicks→#10,
Epilog→{Opacity@#11,PointSize@#12,Point[Mean@K3[[1,All]]],Point[Mean@K3[[2,All]]],Point[Mean@K3[[3,All]]]},PlotLabel→ Style["K=3",Black]],
ListPlot[K4,Frame→#1,AspectRatio→#2,PlotMarkers→#3,PlotStyle→#4,GridLines→#5,PlotRange→#6,ImageSize→#7,FrameLabel→#8,Axes→#9,FrameTicks→#10,
Epilog→{Opacity@#11,PointSize@#12,Point[Mean@K4[[1,All]]],Point[Mean@K4[[2,All]]],Point[Mean@K4[[3,All]]],Point[Mean@K4[[4,All]]]},PlotLabel→ Style["K=4",Black]],
ListPlot[K5,Frame→#1,AspectRatio→#2,PlotMarkers→#3,PlotStyle→#4,GridLines→#5,PlotRange→#6,ImageSize→#7,FrameLabel→#8,Axes→#9,FrameTicks→#10,
Epilog→{Opacity@#11,PointSize@#12,Point[Mean@K5[[1,All]]],Point[Mean@K5[[2,All]]],Point[Mean@K5[[3,All]]],Point[Mean@K5[[4,All]]],Point[Mean@K5[[5,All]]]},PlotLabel→ Style["K=5",Black]]
}&[True,0.8,"OpenMarkers",{ColorData[97,1],ColorData[97,2],ColorData[97,3],ColorData[97,4],ColorData[97,5]},All,Automatic,260,Labels,False,All,1,0.015];
LegendsText2={Placeholder[Style["Cluster 1",Bold,Black,10]],Placeholder[Style["Cluster 2",Bold,Black,10]],Placeholder[Style["Cluster 3",Bold,Black,10]],Placeholder[Style["Cluster 4",Bold,Black,10]],Placeholder[Style["Cluster 5",Bold,Black,10]]};
Labeled[Legended[GraphicsGrid[{{PK2,PK3},{PK4,PK5}},Frame→All,Background→White,Spacings→1],PointLegend[{ColorData[97,1],ColorData[97,2],ColorData[97,3],ColorData[97,4],ColorData[97,5]},LegendsText2,LegendMarkers→"OpenMarkers"]],Style["K-means clustering for K=2,3,4,5",FontFamily→"Times",Black,20,Italic],Top]
Out[21]=

```

如图 [8-39](#Fig39) 所示，簇的排列也取决于 k 的数量。作为 ClusteringComponents 的补充，我们可以计算 k = 3 时记录的标签数。

```
In[22]:=ClusteringComponents[Clstr,3,2,Method→"KMeans",DistanceFunction→SquaredEuclideanDistance,RandomSeeding→8888]
Out[22]={{1,1,1,1,1,1,1,1,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,1,1,1,2,1,1,1,1},{3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3},{2,3,2,2,2,2,3,2,3,2,3,2,3,2,3,3,3,3,3,2,2,2,2,3,2,3,2,2,2,3,2,2,2,2,2,3,2,2,3,2,3,3,3,3,3,3,3,3,3}}

In[23]:= Counts[Flatten[%]]
Out[23]= <|1→46,2→29,3→75|>

```

本质上，给定一个聚类问题，k-means 技术旨在用于未标记的数据，即没有定义类别的数据。可能改变该方法操作的一些因素包括如下。

*   扩散，或者说点与点之间的距离。如果数据包含异常值，当视觉上观察到相反的情况时，就会反映出这一点，异常值可能被错误地分类为聚类的一部分。

*   数据的维度。考虑到更多的信息和特性经常被添加到模型中，维度的数量会增加。这种类型的问题可以使用数据转换方法来解决，如 PCA 中的示例所示，但有一些限制，因为 PCA 方法可能会丢失要素的敏感信息。

*   k 的值是手动确定的，但是当成本函数值高时，可以解释为聚类的方差高，而成本函数值低时，聚类的方差低。最后两个假设也可以归因于这样的事实，对于较低的 k 值，许多观察值可以被分组为大的单个聚类，而对于高 k 值的观察值，它们可以是适当的组。

### 聚类分类

另一个属于集群功能的命令叫做集群分类(图 [8-40](#Fig40) )。该命令的工作方式与 Classify 相同。在下一个示例中，我们将使用该命令来查看 k-means 聚类如何基于两个特征对物种进行分类:萼片长度和萼片宽度。当我们随机抽样时，我们将把数据分成两半。

```
In[24]:=
 BlockRandom[
SeedRandom[88888];
RandomSample[iris[[All,{1,2}]]];
]
TrS=%[[1;;75]];
TsT=%%[[76;;150]];
In[25]:= CC=ClusterClassify[TrS,3,Method→"KMeans",DistanceFunction→Automatic,PerformanceGoal→"Speed",RandomSeeding→8888 ]
Out[25]=

```

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig40_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig40_HTML.jpg)

图 8-40

聚类分类模型的分类函数

获取分类器函数(图 [8-40](#Fig40) )，我们可以看到分类器的细节，可以看到输入向量是一个数值向量，类别数(三个)，方法，训练样本数。

Note

要正确使用-means 方法，需要指定聚类数；否则，该命令将无法正确执行。

要查看有关分类器功能的信息，请使用信息(图 [8-41](#Fig41) )。

```
In[26]:= Information[CC]
Out[26]=

```

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig41_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig41_HTML.jpg)

图 8-41

K-均值的分类器信息

关于分类器功能的更多详细信息如图 [8-41](#Fig41) 所示。要获得属性的完整列表，请键入“properties”作为第二个参数。许多指标，如 BatchEvaluationSpeed、BatchEvaluationTime 和 TrainingTime，可用于比较不同方法的时间。

```
In[27]:= Information[CC,"Properties"]
Out[27]= {BatchEvaluationSpeed,BatchEvaluationTime,Classes,ClassNumber,ClassPriors,DistanceFunction,EvaluationTime,ExampleNumber,FeatureNames,FeatureNumber,FeatureTypes,FunctionMemory,FunctionProperties,IndeterminateThreshold,LearningCurve,MaxTrainingMemory,Method,MethodDescription,MethodOption,PerformanceGoal,Properties,TrainingClassPriors,TrainingTime,UtilityFunction}

```

现在，让我们获得关于从聚类分类器识别的类的信息、类的数量、距离函数、特征名称和训练类概率。

```
In[28]:= Information[CC,#]&/@{"Classes","ClassNumber","DistanceFunction","FeatureNames","TrainingClassPriors"}
Out[28]= {{1,2,3},3,EuclideanDistance,{f1},<|1→0.373333,2→0.293333,3→0.333333|>}

```

我们可以看到有三个班:一班、二班和三班。所使用的距离函数是 EuclideanDistance，数字矢量特征通过名称 f1 引用。使用了一个简单的例子，选择萼片长度为 1，萼片宽度为 2，以显示在测试数据时可以使用的不同属性；这显示在数据集表格中(图 [8-42](#Fig42) )。首先编写示例，然后是属性决策(属于该示例的分类)、分布(直方图的分类分布对象)、预期效用(预期概率和不确定阈值)、对数概率(对数概率)、概率(基于类的测试数据的概率)和顶概率(测试数据的最佳概率)。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig42_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig42_HTML.jpg)

图 8-42

简单示例的数据集

```
In[29]:= Dataset[AssociationMap[CC[{1,2},#]&,{"Decision","Distribution","ExpectedUtilities","LogProbabilities","Probabilities","TopProbabilities"}]]
Out[29]=

```

我们可以看到，该示例属于第三类，相关概率为 1 → 0.976148。让我们看看其余的数据，并绘制聚类分类。

分类数据图如图 [8-43](#Fig43) 所示。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig43_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig43_HTML.jpg)

图 8-43

以虹膜数据为例，对前两个特征进行聚类分类

```
In[30]:= ListPlot[Pick[TsT,CC[TsT],#]&/@{1,2,3},PlotMarkers→"OpenMarkers",GridLines→Automatic,PlotLegends→{Placeholder[Style["Cluster 1",Bold,Black,10]],Placeholder[Style["Cluster 2",Bold,Black,10]],Placeholder[Style["Cluster 3",Bold,Black,10]]},Frame→True,FrameTicks→All,FrameLabel→{"Sepal Lenght","Sepal Width"}]
Out[30]=

```

作为补充，可以添加低于既定概率值的概率限制，并设置不确定阈值，如图 [8-44](#Fig44) 所示。

![../images/500903_1_En_8_Chapter/500903_1_En_8_Fig44_HTML.jpg](../images/500903_1_En_8_Chapter/500903_1_En_8_Fig44_HTML.jpg)

图 8-44

具有概率限制的前两个特征的虹膜数据的例子上的聚类分类

```
In[31]:= ListPlot[Pick[TsT,CC[TsT,IndeterminateThreshold→ 0.6],#]&/@{1,2,3,Indeterminate},PlotMarkers→ "OpenMarkers",PlotLegends→{Placeholder[Style["Cluster 1",Bold,Black,10]],Placeholder[Style["Cluster 2",Bold,Black,10]],Placeholder[Style["Cluster 3",Bold,Black,10]],Placeholder[Style["Indeterminate",Bold,Black,10]]},Frame→True,FrameTicks→All,FrameLabel→{"Sepal Lenght","Sepal Width"},GridLines→Automatic]
Out[31]=

```