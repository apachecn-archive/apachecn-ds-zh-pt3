

# 一、表格格式

> 整洁的数据集都是相似的，但每一个杂乱的数据集都有自己的杂乱之处。
> 
> –哈德利·韦翰(参见列夫·托尔斯泰)

大量的数据确实并且应该以表格的形式存在；简单地说，这意味着具有[行](Glossary.xhtml#_idTextAnchor116)和[列](Glossary.xhtml#_idTextAnchor025)的格式。在理论意义上，如果我们也有*关系*的概念，就可以用多个“平面”或“表格”集合来表示[结构化数据](Glossary.xhtml#_idTextAnchor135)的每个集合的*。[关系数据库管理系统(RDBMS)](Glossary.xhtml#_idTextAnchor110)自 1970 年以来取得了巨大的成功，世界上很大一部分数据都存储在 RDBMS 中。另一大部分以非关系的格式存在，但仍然是表格形式，其中关系可能以一种特别的但不可数的方式被估计。*

正如序言中提到的，数据摄取章节将主要关注使数据变脏的结构或机械问题。在本书的后面，我们将更多地关注数据中的内容或数字问题。

本章讨论表格格式，包括 CSV、电子表格、SQL 数据库和科学阵列存储格式。最后几节介绍了关于数据框的一些一般概念，这通常是数据科学家处理表格数据的方式。本章的大部分内容都是关于使用几种不同的工具和编程语言接收和处理各种数据格式的实际机制。前言讨论了为什么我希望在我的选择中保持语言不可知论——或者多语言。如果每种格式都容易出现特定类型的数据完整性问题，那么就要特别注意这一点。实际上*补救*那些特有的问题很大程度上是留给后面的章节；检测它们是我们这里关注的焦点。

《银河系漫游指南》幽默地写道:“不要惊慌！”。我们将更详细地解释这里提到的概念。

***

我们运行本书中标准的设置代码。正如序言中提到的，假设已经利用了可用的配置文件，每章都可以完整运行。虽然在 Python 中使用`import *`通常不是最佳实践，但我们在这里这样做是为了引入许多名字，而不需要很长的导入块:

```py
from src.setup import *

%load_ext rpy2.ipython

%%R

library(tidyverse) 
```

现在有了各种 Python 和 R 库，让我们利用它们开始清理数据。

# 整理

> 每次战争结束后，总有人要收拾残局。
> 
> 玛丽亚·维萨瓦·安娜·辛博尔斯卡

**概念**:

*   整洁和数据库规范化
*   行与列
*   标签与值

Hadley Wickham 和 Garrett Grolemund 在他们出色的免费书籍 [*R for Data Science*](https://r4ds.had.co.nz/) 中，推广了的“整理数据”概念 R 包的集合试图在具体的库中实现这个概念。Wickham 和 Grolemund 的整齐数据的思想与数据库规范化的概念有着非常密切的知识渊源，这是一个他们和本书都没有深入探讨的大主题。关于数据库规范化的权威参考是 C. J. Date 的*数据库系统介绍*(Addison Wesley；1975 年和许多后续版本)。

简而言之，整齐的数据仔细地将变量(表格的列，也称为特征或字段)与[观察值](Glossary.xhtml#_idTextAnchor089)(表格的行，也称为[样本](Glossary.xhtml#_idTextAnchor118))分开。在这两者的交叉点上，我们找到值，每个单元格中有一个数据项(数据)。不幸的是，我们遇到的数据往往不是以这种有用的方式排列的，它需要*规范化*。特别是，真正的值通常表示为列或行。为了说明这意味着什么，让我们考虑一个例子。

回到我们在前言中提到的小学小班，我们可能会遇到类似这样的数据:

```py
students = pd.read_csv('data/students-scores.csv')

students 
```

```py
 Last Name   First Name   4th Grade   5th Grade   6th Grade

0     Johnson          Mia           A          B+          A-

1       Lopez         Liam           B           B          A+

2         Lee     Isabella           C          C-          B-

3      Fisher        Mason           B          B-          C+

4       Gupta       Olivia           B          A+           A

5    Robinson       Sophia          A+          B-           A 
```

这种数据视图对于人类来说很容易阅读。我们可以看到每个学生在几年的教育中得到的分数的趋势。此外，这种格式可以很容易地进行有用的可视化:

```py
# Generic conversion of letter grades to numbers

def num_score(x):

    to_num = {'A+': 4.3, 'A': 4, 'A-': 3.7,

              'B+': 3.3, 'B': 3, 'B-': 2.7,

              'C+': 2.3, 'C': 2, 'C-': 1.7}

    return x.map(lambda x: to_num.get(x, x)) 
```

下一个单元使用了一种“流畅”的编程风格，这种风格对于一些 Python 程序员来说可能并不熟悉。我将在下一节的数据框中讨论这种风格。流畅风格在许多数据科学工具和语言中使用。

例如，这是典型的[熊猫](Glossary.xhtml#_idTextAnchor094)代码，它按年份绘制学生的分数:

```py
(students

     .set_index('Last Name')

     .drop('First Name', axis=1)

     .apply(num_score)

     .T

     .plot(title="Student score by year")

     .legend(bbox_to_anchor=(1, .75))

); 
```

![](img/B17126_01_01.png)

图 1.1:按年份分列的学生成绩

一旦班级发展到第 7 ^个年级，或者如果我们要获得第 3 ^个年级的信息，这种数据布局就会暴露出它的局限性。为了容纳这样的额外数据，我们需要改变列的数量和位置，而不是简单地添加额外的行。进行新的观察或识别新的样本(行)是很自然的事情，但是改变潜在的变量(列)通常很困难。

字母等级所属的特定班级级别(例如第 4 ^级级)本质上是一个值，而不是变量。另一种思考方式是根据自变量对因变量，或者根据机器学习术语，特征对目标。在某些方面，班级水平可能与最终的字母等级相关或影响它；例如，不同级别的老师可能有不同的偏差，或者某个年龄的孩子对学校作业失去或获得兴趣。

对于大多数分析目的来说，如果我们在进一步处理之前使数据变得整洁(标准化),这些数据会更有用。在熊猫中，`DataFrame.melt()`方法可以完成这种整理。我们将一些列固定为`id_vars`，并将组合列的名称设置为变量，将字母 grade 设置为一个单独的新列。熊猫的方法有点神奇，需要一些练习来适应。关键是它保存数据，只需在列标签和数据值之间移动数据:

```py
students.melt(

    id_vars=["Last Name", "First Name"], 

    var_name="Level",

    value_name="Score"

).set_index(['First Name', 'Last Name', 'Level']) 
```

```py
First Name      Last Name      Level      Score

       Mia        Johnson  4th Grade          A

      Liam          Lopez  4th Grade          B

  Isabella            Lee  4th Grade          C

     Mason         Fisher  4th Grade          B

       ...            ...        ...        ...

  Isabella            Lee  6th Grade         B-

     Mason         Fisher  6th Grade         C+

    Olivia          Gupta  6th Grade          A

    Sophia       Robinson  6th Grade          A

18 rows × 1 columns 
```

在 R Tidyverse 中，程序是相似的。我们在这里看到的一个[tible](Glossary.xhtml#_idTextAnchor139)，只是一种在 Tidyverse 中首选的数据帧:

```py
%%R

library('tidyverse')

studentsR <- read_csv('data/students-scores.csv')

studentsR 
```

```py
── Column specification ───────────────────────────────────────────────

cols(

  'Last Name' = col_character(),

  'First Name' = col_character(),

  '4th Grade' = col_character(),

  '5th Grade' = col_character(),

  '6th Grade' = col_character()

)

# A tibble: 6 x 5

  'Last Name' 'First Name' '4th Grade' '5th Grade' '6th Grade'

  <chr>       <chr>        <chr>       <chr>       <chr>      

1 Johnson     Mia          A           B+          A-         

2 Lopez       Liam         B           B           A+         

3 Lee         Isabella     C           C-          B-         

4 Fisher      Mason        B           B-          C+         

5 Gupta       Olivia       B           A+          A          

6 Robinson    Sophia       A+          B-          A 
```

在 Tidyverse 内，具体来说是 **tidyr** 包内的，有一个类似熊猫`.melt()`的函数`pivot_longer()`。聚合名称和值具有拼写为`names_to`和`values_to`的参数，但是操作是相同的:

```py
%%R

studentsR <- read_csv('data/students-scores.csv')

studentsR %>% 

  pivot_longer(c('4th Grade', '5th Grade', '6th Grade'), 

               names_to = "Level", 

               values_to = "Score") 
```

```py
── Column specification ───────────────────────────────────────────────

cols(

  'Last Name' = col_character(),

  'First Name' = col_character(),

  '4th Grade' = col_character(),

  '5th Grade' = col_character(),

  '6th Grade' = col_character()

)

# A tibble: 18 x 4

   'Last Name' 'First Name' Level     Score

   <chr>       <chr>        <chr>     <chr>

 1 Johnson     Mia          4th Grade A    

 2 Johnson     Mia          5th Grade B+   

 3 Johnson     Mia          6th Grade A-   

 4 Lopez       Liam         4th Grade B    

 5 Lopez       Liam         5th Grade B    

 6 Lopez       Liam         6th Grade A+   

 7 Lee         Isabella     4th Grade C    

 8 Lee         Isabella     5th Grade C-   

 9 Lee         Isabella     6th Grade B-   

10 Fisher      Mason        4th Grade B    

11 Fisher      Mason        5th Grade B-   

12 Fisher      Mason        6th Grade C+   

13 Gupta       Olivia       4th Grade B    

14 Gupta       Olivia       5th Grade A+   

15 Gupta       Olivia       6th Grade A    

16 Robinson    Sophia       4th Grade A+   

17 Robinson    Sophia       5th Grade B-   

18 Robinson    Sophia       6th Grade A 
```

上面这个简单的例子给了你整理表格数据的第一感觉。要反转将变量(列)移动到值(行)的整理操作，可以使用 tidyr 中的`pivot_wider()`函数。在 Pandas 中，数据帧上有几个相关的方法，包括`.pivot()`、`.pivot_table()`和`.groupby()`结合`.unstack()`，它们可以从行创建列(还可以做许多其他事情)。

已经将整洁作为表格的一般目标，让我们开始研究具体的数据格式，从逗号分隔的值和固定宽度的文件开始。

# 战斗支援车

> 除非根据语音在语言中所执行的任务，否则无法理解、界定、分类和解释语音。
> 
> 罗曼·雅格布森

**概念**:

*   分隔和固定宽度的数据
*   解析问题
*   启发式和“目测”
*   推断数据类型
*   转义特殊字符
*   相关 CSV 文件系列

分隔文本文件，尤其是[逗号分隔值(CSV)](Glossary.xhtml#_idTextAnchor026) 文件，无处不在。这些是文本文件，在每一行放置多个值，并用一些半保留字符，比如逗号，分隔这些值。它们几乎总是用于在其他表格表示形式之间传输数据的交换格式，但是大量数据都以 CSV 格式开始和结束，可能从未通过其他格式。

读取分隔文件不是从磁盘读取到 RAM 内存的最快方式，但也不是最慢的。当然，这种担心只对大型数据集有影响，而对构成我们作为数据科学家的大部分工作的小型数据集没有影响(小型现在意味着大约“少于 100k 行”)。

CSV 文件中有大量的缺陷，但也有一些显著的优势。CSV 文件是第二容易出现结构问题的格式。所有格式一般都同样容易出现内容问题，这与格式本身无关。当然，像 Excel 这样的电子表格是最差的数据完整性格式。

同时，分隔格式(或固定宽度的文本格式)也几乎是唯一可以在文本编辑器中轻松打开并理解的格式，或者是唯一可以使用命令行工具轻松处理文本的格式。因此，分隔文件几乎是唯一不需要专门的阅读器和库就可以完全手动修复的文件。当然，严格执行结构约束的格式*确实避免了一些这样做的需求*。在这一章的后面，以及接下来的两章中，我们将讨论更多的加强结构的格式。

在阅读 CSV 或其他文本文件时，您可能会遇到的一个问题是，实际的字符集编码可能不是您所期望的，或者不是您当前系统的默认编码。在这个 Unicode 时代，这种担心正在减少，但速度很慢，档案文件继续存在。这个主题在 [*第 3 章*](Chapter_3.xhtml#_idTextAnchor005) 、*重新利用数据源*中讨论，在*自定义文本格式*一节中。

## 健全性检查

举个简单的例子，假设您刚刚收到一个中等大小的 CSV 文件，您想要对它执行一个快速的健全性检查。在这个阶段，我们关心的是文件的格式是否正确。我们可以用命令行工具做到这一点，即使大多数库可能会被它们卡住(如下一个代码单元所示)。当然，我们也可以使用 Python、R 或其他通用语言，如果我们最初只是将这些行视为文本的话:

```py
# Use try/except to avoid full traceback in example

try:

    pd.read_csv('data/big-random.csv')

except Exception as err:

    print_err(err) 
```

```py
ParserError

Error tokenizing data. C error: Expected 6 fields in line 75, saw 8 
```

那里出了什么问题？让我们检查一下。

```py
%%bash

# What is the general size/shape of this file?

wc data/big-random.csv 
```

```py
 100000  100000 4335846 data/big-random.csv 
```

太好了！10 万行；但是根据熊猫的说法，在 75 号线上有一些问题(也许在其他线上也有)。使用一个计算每行逗号的管道 Bash 命令可能会有所帮助。我们完全可以用 Python、R 或其他语言来执行同样的分析；然而，熟悉命令行工具对数据科学家执行类似这样的一次性分析是有好处的:

```py
%%bash

cat data/big-random.csv | 

    tr -d -c ',\n' | 

    awk '{ print length; }' | 

    sort | 

    uniq -c 
```

```py
 46 3

  99909 5

     45 7 
```

所以我们已经计算出 99，909 行有预期的 5 个逗号。但是 46 个国家有赤字，45 个国家有盈余。也许我们会简单地丢弃不好的行，但是这并不太多，不需要考虑手工修复，即使是在文本编辑器中。我们需要在每个问题的基础上，对自动化修复相对于手动方法的相对工作量和可靠性做出判断。让我们来看看几个有问题的行:

```py
%%bash

grep -C1 -nP '^([^,]+,){7}' data/big-random.csv | head 
```

```py
74-squarcerai,45,quiescenze,12,scuoieremo,70

75:fantasmagorici,28,immischiavate,44,schiavizzammo,97,sfilzarono,49

76-interagiste,50,repentagli,72,attendato,95

--

712-resettando,58,strisciato,46,insaldai,62

713:aspirasse,15,imbozzimatrici,70,incanalante,93,succhieremo,41

714-saccarometriche,18,stremaste,12,hindi,19

--

8096-squincio,16,biascicona,93,solisti,70

8097:rinegoziante,50,circoncidiamo,83,stringavate,79,stipularono,34 
```

查看这些字段数量略有不同的意大利语单词和整数列表并不能立即阐明问题的本质。我们可能需要更多的领域或问题知识。然而，考虑到少于 1%的行是个问题，也许我们现在应该简单地丢弃它们。如果您决定进行修改，比如删除行，那么对数据进行版本控制，并附带变更历史和原因的文档，对于良好的数据和流程起源来说变得至关重要。

下一个单元格使用正则表达式来过滤“近似 CSV”文件中的行。这种模式可能看起来令人困惑，但是正则表达式提供了一种在文本中描述模式的简洁方式。`pat`中的匹配表示从一行的开始(`^`)到该行的结束(`$`)正好有五个不包括逗号的重复字符序列，每个字符序列后面跟一个逗号(`[^,]+,`):

```py
import re

pat = re.compile(r'^([^,]+,){5}[^,]*$')

with open('data/big-random.csv') as fh:

    lines = [l.strip().split(',') 

             for l in fh if re.match(pat, l)] 

pd.DataFrame(lines) 
```

```py
 0     1                2     3              4    5

    0      infilaste    21        esemplava    15     stabaccavo   73

    1      abbadaste    50        enartrosi    85          iella   54

    2       frustulo    77        temporale    83     scoppianti   91

    3     gavocciolo    84  postelegrafiche    93  inglesizzanti   63

  ...            ...   ...              ...   ...            ...  ...

99905     notareschi    60         paganico    64    esecutavamo   20

99906  rispranghiamo    11       schioccano    44    imbozzarono   80

99907        compone    85   disfronderebbe    19    vaporizzavo   54

99908      ritardata    29         scordare    43   appuntirebbe   24

99909 rows × 6 columns 
```

在 Python 中，我们设法在没有格式问题的情况下读取所有行。我们也可以使用`pd.read_csv()`参数`error_bad_lines=False`来达到同样的效果，但是在普通 Python 和 Bash 中浏览它会让您更好地理解为什么它们被排除在外。

## 好的、坏的和文本数据

让我们回到 CSV 文件的一些优点和不足。这里当我们提到 CSV 时，我们实际上是指任何类型的分隔文件。具体来说，存储表格数据的文本文件几乎总是使用单个字符作为分隔符，用换行符结束 rows/ [记录](Glossary.xhtml#_idTextAnchor108)(或者在传统格式中使用回车和换行符)。除了逗号，您可能会遇到的最常见的分隔符是制表符和管道字符`|`。然而，几乎所有的工具都非常乐意使用任意的字符。

固定宽度文件类似于带分隔符的文件。从技术上讲，它们是不同的，尽管它们是面向行的，但它们将每个数据字段放在每行中特定的字符位置。下面的下一个代码单元格中使用了一个示例。几十年前，当 Fortran 和 Cobol 更流行时，固定宽度的格式更流行；我的感觉是，它们的使用已经减少，取而代之的是分隔文件。在任何情况下，固定宽度的文本数据文件都有和带分隔符的文件一样的缺点和优点。

### 坏事

定界或平面文件中的列不携带数据类型，只是文本值。许多工具将(可选地)对数据类型进行猜测，但是这些都容易出错。此外，即使工具准确地猜测了广泛的类型类别(即，字符串对整数对实数)，它们也无法猜测所需的具体位长度，而这很重要。

同样，用于“实数”的表示也没有编码——大多数系统处理一定长度的 IEEE-754 浮点数，但偶尔某些特定长度的小数更适合某种用途。

类型推断出错的最典型方式是，某个数据集中的初始记录有一个明显的模式，但后来的记录偏离了这个模式。软件库可能推断出一种数据类型，但随后会遇到无法转换成这种类型的字符串。这里的“早”和“晚”可以有几种不同的意思。

对于像 **Vaex** 和 **Dask** (Python 库)这样读取[延迟](Glossary.xhtml#_idTextAnchor068)的核外数据框架库，type 试探法可能会应用于最初的几个记录(可能还有一些其他采样)，但不会看到那些不遵循假定模式的字符串。然而，后来也可能意味着几个月后，当新的数据到达。^(零件编号)

*partnum*

例如，在我以前的一份工作中，我们收到了关于有“零件号”的商业产品的客户数据。这个数字是一个真正的整数，持续了好几个月，直到它不是；它变成了一个字符串，有时混合字母和数字。不幸的是，其他工具已经对未记录的数据类型做出了错误的假设(在本例中是 SQL 模式，但也可能是其他代码)。

大多数数据框库都热衷于推断数据类型-尽管所有数据框库都允许手动指定以简化推断。

对于许多布局，数据框库可以猜测固定宽度的格式并推断列位置和数据类型(如果不能猜测，我们可以手动指定)。但是对数据类型的猜测可能会出错。例如，查看原始文本，我们在`parts.fwf`中看到一个固定宽度的布局:

```py
%%bash

cat data/parts.fwf 
```

```py
Part_No  Description              Maker               Price (USD)

12345    Wankle rotary engine     Acme Corporation    555.55

67890    Sousaphone               Marching Inc.       333.33

2468     Feather Duster           Sweeps Bros         22.22

A9922    Area 51 metal fragment   No Such Agency      9999.99 
```

使用 Pandas 阅读此可以正确推断字段的预期列位置:

```py
df = pd.read_fwf('data/parts.fwf', nrows=3)

df 
```

```py
 Part_No           Description             Maker   Price (USD)

0      12345  Wankle rotary engine  Acme Corporation        555.55

1      67890            Sousaphone     Marching Inc.        333.33

2       2468        Feather Duster       Sweeps Bros         22.22 
```

```py
df.dtypes 
```

```py
Part_No          int64

Description     object

Maker           object

Price (USD)    float64

dtype: object 
```

我们故意只读取`parts.fwf`文件的开头。从最初的几行中，Pandas 为`Part_No`列做出了`int64`的类型推断。

让我们阅读整个文件。Pandas 在这里做了“正确的事情:`Part_No`成为一个通用对象，即 string。然而，如果我们有一百万行，而 Pandas 为了速度和内存效率而使用的试探法恰好将推断限制在前 100，000 行，我们可能就没那么幸运了:

```py
df = pd.read_fwf('data/parts.fwf')

df 
```

```py
 Part_No             Description               Maker    Price (USD)

0    12345    Wankle rotary engine    Acme Corporation         555.55

1    67890              Sousaphone       Marching Inc.         333.33

2     2468          Feather Duster         Sweeps Bros          22.22

3    A9922  Area 51 metal fragment      No Such Agency        9999.99 
```

```py
df.dtypes  # type of 'Part_No' changed 
```

```py
Part_No         object

Description     object

Maker           object

Price (USD)    float64

dtype: object 
```

R tibbles 的行为与 Pandas 相同，稍有不同的是，数据类型插补始终使用 1000 行，如果此后出现不一致，将丢弃值。Pandas 可以被配置为读取所有行进行推断，但是默认情况下读取一个动态确定的数字。熊猫将比 R 取样更多的行，但仍然只有大约几万行。R 集合[数据帧](Glossary.xhtml#_idTextAnchor035)和[数据表](Glossary.xhtml#_idTextAnchor036)同样相似。让我们使用 R 读入与上面相同的文件:

```py
%%R

read_table('data/parts.fwf') 
```

```py
── Column specification ───────────────────────────────────────────────

cols(

  Part_No = col_character(),

  Description = col_character(),

  Maker = col_character(),

  'Price (USD)' = col_double()

)

# A tibble: 4 x 4

  Part_No Description            Maker            'Price (USD)'

  <chr>   <chr>                  <chr>                    <dbl>

1 12345   Wankle rotary engine   Acme Corporation         556\. 

2 67890   Sousaphone             Marching Inc.            333\. 

3 2468    Feather Duster         Sweeps Bros               22.2

4 A9922   Area 51 metal fragment No Such Agency         10000\. 
```

同样，前三个行符合整数数据类型，尽管这对于后面的行是不准确的:

```py
%%R

read_table('data/parts.fwf', 

           n_max = 3, 

           col_types = cols("i", "-", "f", "n")) 
```

```py
# A tibble: 3 x 3

  Part_No Maker            'Price (USD)'

    <int> <fct>                    <dbl>

1   12345 Acme Corporation         556\. 

2   67890 Marching Inc.            333\. 

3    2468 Sweeps Bros               22.2 
```

***

分隔文件——而不是固定宽度的文件——容易出现转义问题。特别是，CSV 通常包含描述性字段，这些字段有时在值本身中包含逗号。如果操作正确，这个逗号应该被转义。在实践中常常做得不对。

CSV 实际上是一个不同方言的家族，大部分在它们的转义约定上有所不同。有时，逗号前后的空格在不同的方言中也会有不同的处理方式。一种转义方法是用引号将每个字符串值、任何类型的每个值或者仅包含禁用逗号的值括起来。这因工具和工具版本而异。当然，如果您引用字段，可能需要转义这些引用；通常，当引号字符是值的一部分时，在引号字符前放置一个反斜杠。

另一种方法是在那些不打算作为分隔符而是作为字符串值(或者可能被格式化的数值，例如`$1,234.56`)的一部分的逗号之前放置一个反斜杠。猜测变体可能会很乱，甚至单个文件在各行之间也不一定是自洽的，在实践中(往往不同的工具或工具版本都接触过数据)。

制表符分隔和管道分隔格式经常被选择，希望避免转义问题。这在一定程度上是可行的。制表符和竖线符号在普通散文中很少出现。但是这两者仍然偶尔出现在文本中，并且所有逃避的问题又回来了。而且，面对逃跑，最简单的工具有时也会失效。例如，Bash 命令`cut -d`在这些情况下无法工作，Python 的`str.split(',')`也是如此。一个更加定制的解析器变得很有必要，尽管与成熟的语法相比这是一个简单的解析器。Python 的标准库`csv`模块就是这样一个定制的解析器。

与分隔文件相比，固定宽度文件的相应危险是值变得太长。在特定的行位置范围内，可以有任何代码点(除了换行符)。但是一旦有人认为永远不会超过 20 个字符的描述或名称变成了 21 个字符，这种格式就失效了。

***

读取日期时间格式时需要特别注意。读取日期时间值的数据框库通常有一个可选开关，用于将某些列解析为日期时间格式。熊猫等库支持 datetime 格式的启发式猜测；这里的问题是对数百万行中的每一行应用启发式算法可能会非常慢。在日期格式统一的情况下，使用手动格式说明符可以使阅读速度提高几个数量级。当然，在格式不同的地方，试探法实际上是神奇的；也许我们应该只是惊讶于狗会说话，而不是批评它的语法。让我们看一只熊猫试图猜测一个用制表符分隔的文件的每一行的日期时间:

```py
%%bash

# Notice many date formats

cat data/parts.tsv 
```

```py
Part_No      Description                  Date   Price (USD)

  12345    Wankle rotary   2020-04-12T15:53:21        555.55

  67890       Sousaphone        April 12, 2020        333.33

   2468   Feather Duster             4/12/2020         22.22

  A9922    Area 51 metal              04/12/20       9999.99 
```

```py
# Let Pandas make guesses for each row

# VERY SLOW for large tables

parts = pd.read_csv('data/parts.tsv', 

            sep='\t', parse_dates=['Date'])

parts 
```

```py
 Part_No      Description                  Date   Price (USD)

0     12345    Wankle rotary   2020-04-12 15:53:21        555.55

1     67890       Sousaphone   2020-04-12 00:00:00        333.33

2      2468   Feather Duster   2020-04-12 00:00:00         22.22

3     A9922    Area 51 metal   2020-04-12 00:00:00       9999.99 
```

我们可以验证数据帧中的日期确实是日期时间数据类型:

```py
parts.dtypes 
```

```py
Part_No                object

Description            object

Date           datetime64[ns]

Price (USD)           float64

dtype: object 
```

我们已经研究了定界和固定宽度格式的一些挑战和限制；让我们也来看看它们相当大的优势。

### 好人

CSV 文件及其分隔或固定宽度的同类文件的最大优势是无处不在的读取和写入工具。每种编程语言中处理数据帧或数组的每个库都知道如何处理它们。大多数时候，这些库很好地解析了古怪的案例。每个电子表格程序都以 CSV 格式导入和导出。每个 RDBMS——以及大多数非关系数据库——都以 CSV 格式导入和导出。大多数程序员的文本编辑器甚至有使编辑 CSV 更容易的工具。Python 有一个名为`csv`的标准库模块，作为逐行记录阅读器处理 CSV(或其他分隔格式)的许多方言。

事实上，如此多的有结构缺陷的 CSV 文件存在于野外表明，并不是每个工具都能完全正确地处理它们。在某种程度上，这可能是因为这种格式足够简单，几乎不需要定制工具就能让 T4 工作。我有自己，在一个“一次性脚本”中，写了无数次；这很好，直到它不工作。当然，抛弃型脚本经常成为数据流的固定标准过程。

***

缺乏类型规范通常是一种优势而不是劣势。例如，几页前提到的零件号可能开始时总是整数，作为一个实际的业务意图，但后来出现了使用非整数“数字”的需求对于具有正式类型说明符的格式，我们通常必须执行迁移和复制，以将旧数据移动到遵循宽松或修订的约束的新格式中。

根据我的经验，数据类型更改经常发生的一个特殊情况是有限宽度字符字段。最初，某个字段被指定为最大长度需要 5、15 或 100 个字符，但后来遇到了对更长字符串的需求，需要修改固定表结构或 SQL 数据库以适应更长的长度。更常见的情况是——尤其是对于数据库——需求没有得到充分的记录，我们最终得到的是一个充满了无用的截断字符串的数据集(可能还有永久丢失的数据)。

在这方面，文本格式通常是灵活的。带分隔符的文件——而不是固定宽度的文件——可以包含任意长度的字段。JSON 数据、YAML 数据、 ^(config) XML 数据、日志文件和其他一些简单使用文本的格式也是如此，通常带有面向行的记录。在所有这些中，数据类型是非常松散的，只真实地存在于数据处理步骤中。这通常是一种伟大的美德。

*配置*

YAML 通常包含相对较短的配置信息，而不是原型意义上的*数据*。在这方面，TOML 是一种类似的格式，更老的 INI 格式也是如此。所有这些都是为手工编辑而设计的，因此通常都很小，尽管读写它们的数据的 API 很常见。虽然您可以将一百万条记录放入这些格式中的任何一种，但在实践中您很少或从未遇到过这种情况。

***

CSV 和类似格式的一个相关“松散性”是，我们经常无限地聚合遵循相同非正式模式的多个 CSV 文件。为每天、每小时或每月正在进行的数据收集编写不同的 CSV 文件是很平常的事情。许多工具，如 Dask 和 **Spark** ，将无缝地把 CSV 文件的集合(匹配文件系统上的 [glob](Glossary.xhtml#_idTextAnchor051) 模式)作为单个数据集。当然，在不直接支持这一点的工具中，手动拼接还是不难的。但是，在拥有包含不确定数量的相关 CSV 快照的目录的模型下，将其呈现为单个公共对象是有帮助的。

无缝处理 CSV 文件系列的库通常是懒惰的和分布式的。也就是说，使用这些工具，您通常不会一次读入所有的 CSV 文件，或者至少不会读入单台机器的主内存。相反，集群中的各种核心或各种节点将分别获得各个文件的文件句柄，并且将仅从一个或几个文件中推断出模式信息，而实际处理将推迟到特定(并行)计算启动后进行。在内核之间分割处理单个 CSV 文件并不容易，因为读取器只能通过扫描来确定新记录的开始位置，直到找到新行。

虽然分布式数据帧库的特定 API 的细节超出了本书的范围，但并行性很容易实现这一事实对于 CSV 格式来说是一个重要的优势。Dask 的具体工作方式是创建许多 Pandas 数据帧，并用一个 API 协调所有这些数据帧(或给定结果所需的数据帧)的计算，该 API 完全复制单个 Pandas 对象的相同方法:

```py
# Generated data files with random values

from glob import glob

# Use glob() function to identify files matching pattern

glob('data/multicsv/2000-*.csv')[:8] # ... and more 
```

```py
['data/multicsv/2000-01-27.csv',

 'data/multicsv/2000-01-26.csv',

 'data/multicsv/2000-01-06.csv',

 'data/multicsv/2000-01-20.csv',

 'data/multicsv/2000-01-13.csv',

 'data/multicsv/2000-01-22.csv',

 'data/multicsv/2000-01-21.csv',

 'data/multicsv/2000-01-24.csv'] 
```

我们将这一系列 CSV 文件读入一个虚拟化的数据帧，该数据帧的行为类似于 Pandas 数据帧，即使加载 Pandas 需要比本地系统允许的更多的内存。在这个具体的例子中，CSV 文件的集合并没有大到现代工作站无法读入内存的程度；但是当它变成这样的时候，使用像 Dask 这样的分布式或核外系统是必要的:

```py
import dask.dataframe as dd

df = dd.read_csv('data/multicsv/2000-*-*.csv', 

                 parse_dates=['timestamp'])

print("Total rows:", len(df))

df.head()

Total rows: 2592000 
```

```py
 Timestamp     id    name          x          y

0  2000-01-01 00:00:00    979   Zelda   0.802163   0.166619

1  2000-01-01 00:00:01   1019  Ingrid  -0.349999   0.704687

2  2000-01-01 00:00:02   1007  Hannah  -0.169853  -0.050842

3  2000-01-01 00:00:03   1034  Ursula   0.868090  -0.190783

4  2000-01-01 00:00:04   1024  Ingrid   0.083798   0.109101 
```

当我们需要计算一些汇总时，Dask 会协调工作人员在每个单独的数据帧上进行汇总，然后汇总这些汇总。还有更微妙的问题，即哪些操作可以用这种“map-reduce”风格重新构造，哪些不可以，但这是一般的想法(Dask 或 Spark 开发人员已经为您考虑到了这一点，所以您不必这样做):

```py
df.mean().compute() 
```

```py
id    999.965606

x       0.000096

y       0.000081

dtype: float64 
```

在了解了使用 CSV 数据的利弊之后，让我们转向存储大量数据的另一种格式。不幸的是，对于电子表格来说，几乎只有缺点。

# 被认为有害的电子表格

> 毒品是不好的，好吧。你不应该吸毒，好吗。如果你做了，你就是坏人，因为毒品是不好的，好吗？吸毒是件坏事，所以不要因为吸毒而变坏，好吗？
> 
> -麦凯先生(南方公园)

**概念**:

*   非强制字段/列标识
*   计算不透明度
*   半表格数据
*   非连续数据
*   不可见的数据和数据类型差异
*   用户界面是吸引人的麻烦

爱德华·塔夫特，这位杰出的信息可视化老前辈，写了一篇名为《PowerPoint 的认知风格:清除 T4 内部的腐败》的文章。在他的观察中，幻灯片演示的方式，特别是 PowerPoint，隐藏了比它揭示的更重要的信息，这是 2003 年哥伦比亚号航天飞机灾难的主要甚至主要原因。PowerPoint 对清晰的信息展示来说是一种诅咒。

在某种程度上，电子表格，尤其是 Excel，是有效数据科学的诅咒。虽然可能没有 CSV 文件中的多，但世界上很大一部分数据存在于 Excel 电子表格中。有许多种数据损坏是电子表格的特殊领域。另外，数据科学工具读取电子表格的速度比其他格式慢得多，而电子表格对其包含的数据量也有其他格式没有的硬性限制。

电子表格为方便用户所做的大部分事情都不利于科学再现性、数据科学、统计学、数据分析和相关领域。电子表格中有明显的行和列，但是没有任何东西强制它们的一致使用，即使是在一张表格中。例如，对于某些行，某些特定的特性通常位于 F 列，但是对于其他行，相同的特性位于 H 列。与 CSV 文件或 SQL 表形成对比；对于后一种格式，虽然一列中的所有数据不一定是*好的*数据，但它通常必须属于同一特性。

*计算*

电子表格的另一个危险根本不在于数据摄取。电子表格中的计算分散在许多单元格中，没有明显或容易检查的顺序，导致了许多大规模的灾难性后果(在金融交易中损失[数十亿美元](https://www.businessinsider.com/excel-partly-to-blame-for-trading-loss-2013-2)；一场[全球经济规划的溃败](https://theconversation.com/the-reinhart-rogoff-error-or-how-not-to-excel-at-economics-13646)；一次[新冠肺炎联系追查大规模失败【英国 )。欧洲电子表格风险利益集团是一个致力于记录此类错误的完整组织。他们提出了许多可爱的报价，包括这个:](https://www.bbc.com/news/technology-54423988)

> 有一篇关于否认的文献，主要关注疾病和许多身患绝症的人否认病情的严重性或采取行动的必要性。显然，非常困难和不愉快的事情是难以想象的。虽然拒绝只是在医学文献中被广泛研究，但它很可能出现在需要采取的行动很困难或繁重的时候。考虑到电子表格测试的费力性质，开发人员可能是否认的受害者，这可能以对准确性过度自信的形式表现出来，因此不需要广泛的测试。
> 
> –[雷·潘科，2003 年](https://arxiv.org/abs/0804.0941)

在过程化编程(包括面向对象编程)中，动作在代码中顺序流动，分支或函数调用的位置清晰；即使在函数式范例中，组合也是明确说明的。在电子表格中，任何人都可以猜测什么计算依赖于什么，以及实际包括什么数据范围。偶尔可以偶然发现错误，但是程序分析和调试*几乎*不可能。只知道或者主要知道电子表格的用户可能会反对*一些*工具的存在是为了识别电子表格中的依赖关系；这在技术上是正确的，因为许多由货运列车运输的货物也可以用手推车运输。

此外，电子表格中的每个单元格都可以有不同的数据类型。通常，类型是通过电子表格界面中的启发式猜测来分配的。这些对所使用的确切击键、输入单元格的顺序、数据是否在块之间复制/粘贴以及其他许多难以预测且在每个电子表格软件程序的每个版本之间会发生变化的事情非常敏感。例如，众所周知，Excel 将基因名称 SEPT2 (Septin 2)解释为日期(至少在广泛的版本中)。使问题更加复杂的是，电子表格的界面使得确定给定单元格的数据类型非常困难。

让我们从一个例子开始。下面的截图是一个普通的电子表格。是的，有些值在它们的单元格中并不完全一致，但这纯粹是一个美学问题。我们首先想到的问题是，一个工作表被用来表示两个不同的(在本例中是相关的)数据表。这已经很难整理了:

![Excel Pitfalls](img/B17126_01_02.png)

图 1.2: Excel 陷阱

如果我们简单地告诉 Pandas(或者具体地说是支持的 openpyxl 库)尝试理解这个文件，它会做出真诚的努力，并应用相当智能的启发式算法。值得称赞的是，它没有崩溃。其他 DataFrame 库也是类似的，只是你需要学习不同的技巧。但是我们最初能看到哪里出了问题？

```py
# Default engine 'xlrd' might have bug in Python 3.9

pd.read_excel('data/Excel-Pitfalls.xlsx',

              sheet_name="Dask Sample", engine="openpyxl") 
```

```py
 Timestamp      id     name          x

0  2000-01-01 00:00:00     979    Zelda   0.802163

1   2000-01-01 0:00:01  1019.5   Ingrid  -0.349999

2  2000-01-01 00:00:02    1007   Hannah  -0.169853

3  2000-01-01 00:00:03    1034   Ursula    0.86809

4            timestamp      id     name          y

5  2000-01-01 00:00:02    1007   Hannah  -0.050842

6  2000-01-01 00:00:03    1034   Ursula  -0.190783

7  2000-01-01 00:00:04    1024   Ingrid   0.109101 
```

我们可以立即注意到,`id`列包含一个在电子表格显示中不可见的值`1019.5`。目前还不清楚该列是浮点型还是整数型。此外，请注意，同一行中的日期在视觉上看起来略有错误。我们将回到这一点。

作为第一步，我们可以通过费力的人工干预，拉出我们真正关心的两个独立的表。Pandas 实际上有一点*太聪明了*——默认情况下，它会忽略电子表格中实际输入的数据，并做出类似于它对 CSV 文件所做的推断。对于这个目的，我们告诉它使用 Excel 实际存储的数据类型。熊猫的推断不是万灵药，但它*有时是*一个有用的选择(它可以解决*一些*，但不是*所有*，我们下面注意到的问题；然而，其他事情变得更糟)。在接下来的几段中，我们希望看到存储在电子表格本身中的原始数据类型:

```py
df1 = pd.read_excel('data/Excel-Pitfalls.xlsx', 

                    nrows=5, dtype=object, engine="openpyxl")

df1.loc[:2]	# Just look at first few rows 
```

```py
 Timestamp       id     name          x

0  2000-01-01 00:00:00      979    Zelda   0.802163

1  2000-01-01 00:00:01   1019.5   Ingrid  -0.349999

2  2000-01-01 00:00:02     1007   Hannah  -0.169853 
```

我们也可以通过使用`pd.read_excel()`参数`skiprows`来读取第二个隐式表:

```py
pd.read_excel('data/Excel-Pitfalls.xlsx', skiprows=7, engine="openpyxl") 
```

```py
 Timestamp    id    name          y

0    2000-01-01 00:00:02  1007  Hannah  -0.050842

1    2000-01-01 00:00:03  1034  Ursula  -0.190783

2    2000-01-01 00:00:04  1024  Ingrid   0.109101 
```

如果我们查看读入的数据类型，我们会看到它们都是 Python 对象，以保留各种单元类型。但是让我们更仔细地看看我们实际拥有的:

```py
df1.dtypes 
```

```py
timestamp    datetime64[ns]

id                   object

name                 object

x                    object

dtype: object 
```

这个小例子中的时间戳对于熊猫来说都是合理的。但是现实生活中的电子表格经常提供更加模糊的信息，通常无法解析为日期。看上面的*图 1.2* ，注意数据类型在电子表格本身中是不可见的。我们可以找到存储在每个单元格中的通用对象的 Python 数据类型:

```py
# Look at the stored data type of each cell

tss = df1.loc[:2, 'timestamp']

for i, ts in enumerate(tss):

    print(f"TS {i}: {ts}\t{ts.__class__.__name__}") 
```

```py
TS 0: 2000-01-01 00:00:00       Timestamp

TS 1: 2000-01-01 00:00:01       Timestamp

TS 2: 2000-01-01 00:00:02       Timestamp 
```

熊猫`to_datetime()`函数是幂等的^(幂等的)，如果我们没有在`pd.read_excel()`调用中使用`dtype=object`特别禁用它，它应该已经运行了。然而，许多电子表格要混乱得多，转换根本不会成功，无论如何都会产生一个`object`列。列中的特定单元格可能包含看起来不像日期的数字、公式或字符串(或者有时字符串看起来非常像日期字符串，人而不是机器可能会猜到其意图；说“Decc 23，，201.9”)。

*等幂*

单词和概念*幂等元*在数学、计算机科学和一般编程中很有用。这意味着在自己的输出上再次调用相同的函数将继续产生相同的答案。这与数学中更为奇特的概念*吸引子*有关。在普通的编程术语中，这意味着您不必担心以幂等的方式重复修改值，这可能会出现在程序流的变迁中。换句话说，不管最初的`x`是什么，你都知道:`python pd.to_datetime(x) == pd.to_datetime(pd.to_datetime(x))`

让我们来看看如何使用`pd.to_datetime()`:

```py
pd.to_datetime(tss) 
```

```py
0   2000-01-01 00:00:00

1   2000-01-01 00:00:01

2   2000-01-01 00:00:02

Name: timestamp, dtype: datetime64[ns] 
```

其他列也有类似的困难。在电子表格视图的`id`列中看起来相同的值实际上是整数、浮点数和字符串的混合。可以想象这是 T4 的意图，但在实践中，这几乎总是电子表格向用户隐藏信息的偶然结果。当这些数据集到达你的数据科学办公桌时，它们仅仅是杂乱的，原因在时间的沙子中消失了。让我们看看`id`列中的数据类型:

```py
# Look at the stored data type of each cell

ids = df1.loc[:3, 'id']

for i, id_ in enumerate(ids):

    print(f"id {i}: {id_}\t{id_.__class__.__name__}") 
```

```py
id 0: 979       int

id 1: 1019.5  float

id 2: 1007      int

id 3: 1034      str 
```

当然，像 Pandas 这样的工具可以在读取后输入转换值，但是我们需要数据集的特定领域知识才能知道什么转换是合适的。让我们使用`.astype()`方法来转换数据:

```py
ids.astype(int) 
```

```py
0     979

1    1019

2    1007

3    1034

Name: id, dtype: int64 
```

将我们提到的清理放在一起，我们可能会以类似于下面的方式小心地键入我们的数据:

```py
# Only rows through index '3' are useful

# We are casting to more specific data types 

#   based on domain and problem knowledge

df1 = df1.loc[0:3].astype(

    {'id': np.uint16, 

     'name': pd.StringDtype(), 

     'x': float})

# datetimes require conversion function, not just type

df1['timestamp'] = pd.to_datetime(df1.timestamp)

print(df1.dtypes) 
```

```py
timestamp    datetime64[ns]

id                   uint16

name                 string

x                   float64

dtype: object 
```

```py
df1.set_index('timestamp') 
```

```py
 timestamp       id     name          x

2000-01-01 00:00:00      979    Zelda   0.802163

2000-01-01 00:00:01     1019   Ingrid  -0.349999

2000-01-01 00:00:02     1007   Hannah  -0.169853

2000-01-01 00:00:03     1034   Ursula   0.868090 
```

电子表格之所以有害，主要不是因为它们潜在的数据格式。非古代版本的 Excel ( `.xlsx`)、LibreOffice (OpenDocument，`.ods`)和 Gnumeric ( `.gnm`)都在字节级采用了类似的格式。也就是说，它们都以 XML 格式存储数据，然后压缩这些数据以节省空间。正如我提到的，这比其他方法要慢，但是这个问题是次要的。

如果这些电子表格格式中的一种纯粹用作结构化工具之间的交换格式，它们将非常适合保存和表示数据。相反，是电子表格的社交和用户界面(UI)元素让它们变得危险。Excel 的“表格”格式结合了非类型化 CSV 和强类型 SQL 数据库的最差元素。它不是按列/特征分配数据类型，而是允许按单元分配类型。

对于任何数据科学目的来说，按单元分类几乎总是错误的。它既不允许通过编程工具(使用推理或类型声明 API)做出灵活的决定，也不强制在存储数据时应该属于同一特性的不同值的一致性。此外，电子表格用户界面中相对自由的输入样式并不能引导用户避免各种类型的输入错误(不仅是数据键入，还有网格中的各种错位、意外的删除或插入等等)。打个比方，电子表格用户界面造成的危险类似于侵权法中“诱人的妨害”的概念——它们不直接造成伤害，但稍不注意就极有可能造成伤害。

不幸的是，目前不存在任何广泛使用的通用数据输入工具。数据库条目表单可以用于加强数据条目的结构，但是它们对于非编程的数据探索是有限的。此外，使用结构化表单，无论数据随后以何种格式存储，目前至少需要少量的软件开发工作，而许多电子表格的普通用户缺乏这种能力。类似于电子表格，但允许在列上锁定数据类型约束的东西，将是一个受欢迎的新功能。也许我的一个或几个读者会创造和推广这样的工具。

目前，现实是许多用户会创建电子表格，作为数据科学家，您需要从中提取数据。与提供不同的格式相比，这将不可避免地增加您的工作量。但是要仔细考虑真正相关的块区域和选项卡/工作表，考虑问题所需的数据类型，以及如何清除不可处理的值。通过努力，数据将进入你的数据管道。

我们现在可以转向结构良好、仔细标注日期的格式；那些存储在关系数据库中的。

# SQL RDBMS

> 当时，尼克松正在与中国实现关系正常化。我想如果他能让关系正常化，那我也能。
> 
> –e . f . Codd(关系数据库理论的发明者)

**概念**:

*   Python DB-API 和 SQL 驱动程序
*   数据类型阻抗不匹配
*   手动转换为精确的数据类型
*   截断和溢出
*   包装与剪裁

关系数据库管理系统(RDBMSs)功能强大，用途广泛。对于大多数人来说，他们对严格的列类型和频繁使用正式外键和约束的需求对于数据科学来说是一大福音。虽然特定的 RDBMSs 在规范化、索引和设计方面有很大的不同——并不是每个组织都有或专门利用数据库工程师——甚至有些非正式的数据库也有许多数据科学所需要的特性。并不是所有的关系数据库都是*整洁的*，但是它们都带你向那个方向迈进了几大步。

使用关系数据库需要一些关于**结构化查询语言** ( **SQL** 的知识。对于小的数据，或者中等大小的数据，您可以将整个表作为数据帧读入内存。可以使用数据框库执行过滤、排序、分组甚至连接等操作。然而，如果您能够在数据库级别直接执行这些操作，效率会高得多；在使用[大数据](Glossary.xhtml#_idTextAnchor017)时，这是绝对必要的。一个数据库拥有数百万或数十亿条记录，分布在数十或数百个相关的表中，它本身可以快速生成您手头任务所需的数十万行([元组](Glossary.xhtml#_idTextAnchor141))。但是将所有这些行加载到内存中不是没有必要就是根本不可能。

有很多关于 SQL 的优秀书籍和教程。我没有特别推荐的文本，但是找到一个合适的文本来跟上进度(如果你还没有的话)并不困难。作为一名数据科学家，`GROUP BY`、`JOIN`和`WHERE`子句的一般概念是你应该知道的主要事情。

如果您对从中提取数据的数据库有更多的控制，了解一些关于如何智能地索引表，以及通过重构和查看`EXPLAIN`输出来优化慢速查询的知识会很有帮助。然而，作为一名数据科学家，您很可能没有数据库管理的完全权限。如果你有这样的权限:小心！

对于这本书，我使用一个本地的 [PostgreSQL](Glossary.xhtml#_idTextAnchor100) 服务器来说明 API。我发现 PostgreSQL 在查询优化方面比它的主要开源竞争对手 [MySQL](Glossary.xhtml#_idTextAnchor080) 要好得多。通过仔细的索引调优，这两种方法表现得一样好，但是对于必须由[查询规划器](Glossary.xhtml#_idTextAnchor103)特别优化的查询，PostgreSQL 通常要快得多。一般来说，无论您使用 PostgreSQL、MySQL、Oracle DB、DB2、SQL Server 还是任何其他 RDBMS，我展示的几乎所有 API 在 Python 或 R(以及大多数其他语言)的驱动程序中都几乎是相同的。尤其是 Python DB-API，它在驱动程序之间实现了很好的标准化。即使是 Python 标准库中包含的单文件 RDBMS [SQLite3](Glossary.xhtml#_idTextAnchor131) ，也几乎符合 DB-API(而且`.sqlite`是非常好的存储格式)。

在由每个章节加载并在源代码库中可用的`setup.py`模块中，执行一些数据库设置。如果您运行其中包含的一些函数，您将能够在您的系统上创建与我在撰写本文的系统上相同的配置。本书不涉及 RDBMS 的实际安装；请参阅数据库软件附带的说明。但是一个关键而简单的步骤是创建一个到数据库的*连接*:

```py
# Similar with adapter other than psycopg2

con = psycopg2.connect(database=db, host=host, 

              user=user, password=pwd) 
```

这个连接对象将在本书的后续代码中使用。我们还创建了一个`engine`对象，它是一个 [SQLAlchemy](Glossary.xhtml#_idTextAnchor130) 包装器，围绕着一个添加了一些增强功能的连接。一些图书馆，比如熊猫图书馆，需要使用引擎而不仅仅是连接。我们可以按如下方式创建:

```py
engine = create_engine(

      f'postgresql://{user}:{pwd}@{host}:{port}/{db}') 
```

## 按摩数据类型

我使用本章前面创建的 Dask 数据用下面的模式填充一个表。这些元数据值是在 RDBMS 本身中定义的。在本节中，我们将使用关系数据库提供的精细而精确的数据类型:

| 圆柱 | 数据类型 | 数据宽度 |
| --- | --- | --- |
| 指数 | 整数 | Thirty-two |
| 时间戳 | 不带时区的时间戳 | 没有人 |
| 身份证明（identification） | 斯莫列特 | Sixteen |
| 名字 | 性格；角色；字母 | Ten |
| x | 数字的 | six |
| y | 真实的 | Twenty-four |

这是在前面的 Dask 讨论中创建的相同的数据结构，但是我在字段上任意添加了更具体的数据类型。显示的 PostgreSQL“数据宽度”有点奇怪；它根据类型混合了位长度和字节长度。而且对于浮点`y`，显示的是尾数的位长，而不是整个 32 位内存字的位长。但是总的来说我们可以看到不同的列有不同的具体类型。

在设计表时，数据库工程师通常会选择足够的数据宽度，但也要在要求允许的范围内尽量小。例如，如果您需要存储数十亿人的年龄，一个 256 位的整数当然可以保存这些数字，但是一个 8 位的整数也可以保存所有可能出现的值，使用 1/32 的存储空间。

使用 Python DB-API 会丢失一些数据类型信息。它做得很好*，但是 Python 没有完整的原生类型。小数被精确地存储为`Decimal`或本地浮点，但是具体的位长度丢失了。同样，该整数是一个无限大小的 Python 整数。`name`字符串的长度总是 10 个字符，但是在大多数情况下，我们可能希望应用`str.rstrip()`(去掉右端的空白)来去掉周围的空白:*

```py
# Function connect_local() spelled out in Chapter 4 (Anomaly Detection)

con, engine = connect_local()

cur = con.cursor()

cur.execute("SELECT * FROM dask_sample")

pprint(cur.fetchmany(2)) 
```

```py
[(3456,

  datetime.datetime(2000, 1, 2, 0, 57, 36),

  941,

  'Alice     ',

  Decimal('-0.612'),

  -0.636485),

 (3457,

  datetime.datetime(2000, 1, 2, 0, 57, 37),

  1004,

  'Victor    ',

  Decimal('0.450'),

  -0.687718)] 
```

不幸的是，使用 Pandas 时，我们丢失了更多的数据类型信息(至少从 Pandas 1.0.1 和 SQLAlchemy 1.3.13 开始，这是本文的最新版本)。Pandas 能够使用 NumPy T2 的完整类型系统，甚至增加了一些自己的自定义类型。这种丰富性可以与 RDBMSs 提供的类型系统相媲美，但不一定完全相同(事实上，它们也互不相同，尤其是在扩展类型方面)。但是，转换层只转换为基本的字符串、浮点、整数和日期类型。

让我们将一个 PostgreSQL 表读入 Pandas，然后检查使用了什么原生数据类型来近似 SQL 数据:

```py
df = pd.read_sql('dask_sample', engine, index_col='index')

df.tail(3) 
```

```py
index            timestamp    id      name       x         y

 5676  2000-01-02 01:34:36  1041   Charlie  -0.587  0.206869

 5677  2000-01-02 01:34:37  1017       Ray   0.311  0.256218

 5678  2000-01-02 01:34:38  1036    Yvonne   0.409  0.535841 
```

数据帧中的特定数据类型有:

```py
df.dtypes 
```

```py
timestamp    datetime64[ns]

id                    int64

name                 object

x                   float64

y                   float64

dtype: object 
```

虽然这有点费力，但我们可以结合这些技术，仍然在一个友好的数据框架内处理我们的数据，但使用更紧密匹配的类型(尽管与数据库不完全匹配)。这里的两个缺点是:

*   我们需要手动决定每列的最佳类型
*   有了`object`列，熊猫的操作会慢很多

让我们努力为我们的数据框架选择更好的数据类型。我们可能需要从 RDBMS 的文档中确定精确的类型，因为很少有人记住 PostgreSQL 类型代码。DB-API 游标对象有一个包含列类型代码的`.description`属性:

```py
cur.execute("SELECT * FROM dask_sample")

cur.description 
```

```py
(Column(name='index', type_code=23),

 Column(name='timestamp', type_code=1114),

 Column(name='id', type_code=21),

 Column(name='name', type_code=1042),

 Column(name='x', type_code=1700),

 Column(name='y', type_code=700)) 
```

我们可以自省来查看结果中使用的 Python 类型。当然，它们并不携带数据库的位长度，所以我们需要手动选择它们。Datetime 足够简单，可以放入 Pandas 的`datetime64[ns]`类型:

```py
rows = cur.fetchall()

[type(v) for v in rows[0]] 
```

```py
[int, datetime.datetime, int, str, decimal.Decimal, float] 
```

与数字打交道比其他类型更棘手。Python 的标准库`decimal`模块符合 [IBM 的通用十进制算术规范](http://speleotrove.com/decimal/)；不幸的是，数据库没有。特别是，IBM 1981 规范(有许多更新)允许每个*操作*在某个选定的“十进制上下文”中执行，该上下文给出精度、舍入规则和其他内容。这与每列有一个小数精度*的*不同，没有对舍入规则的具体控制。我们通常可以忽略这些细微差别；但是当它们咬我们的时候，它们会咬得很厉害。与数据科学相比，土木工程和银行/金融领域出现的问题更多，但这些都是需要注意的问题。**

在下一个单元格中，我们将几列转换为具有特定位宽的特定数字数据类型:

```py
# Read the data with no imposed data types

df = pd.DataFrame(rows, 

                  columns=[col.name for col in cur.description],

                  dtype=object)

# Assign specific int or float lengths to some fields

types = {'index': np.int32, 'id': np.int16, 'y': np.float32}

df = df.astype(types)

# Cast the Python datetime to a Pandas datetime

df['timestamp'] = pd.to_datetime(df.timestamp)

df.set_index('index').head(3) 
```

```py
index            timestamp    id    name       x          y

 3456  2000-01-02 00:57:36   941   Alice  -0.612  -0.636485

 3457  2000-01-02 00:57:37  1004  Victor   0.450  -0.687718

 3458  2000-01-02 00:57:38   980   Quinn   0.552   0.454158 
```

我们可以验证是否使用了这些数据类型。

```py
df.dtypes 
```

```py
index                 int32

timestamp    datetime64[ns]

id                    int16

name                 object

x                    object

y                   float32

dtype: object 
```

Pandas“object”类型隐藏了存储的 Python 对象的底层类的差异。我们可以更具体地看一下:

```py
pprint({repr(x): x.__class__.__name__ 

        for x in df.reset_index().iloc[0]}) 
```

```py
{"'Alice     '": 'str',

 '-0.636485': 'float32',

 '0': 'int64',

 '3456': 'int32',

 '941': 'int16',

 "Decimal('-0.612')": 'Decimal',

 "Timestamp('2000-01-02 00:57:36')": 'Timestamp'} 
```

## 在 R 中重复

在很大程度上，R 中读取 SQL 数据的步骤与 Python 中的类似。获得正确数据类型的陷阱也是如此。我们可以看到，数据类型与 Pandas 生成的实际数据库类型大致相同。显然，在实际代码中，您不应该在源代码中将密码指定为文字值，而是应该使用一些工具来进行秘密管理:

```py
%%R

require("RPostgreSQL")

drv <- dbDriver("PostgreSQL")

con <- dbConnect(drv, dbname = "dirty",

                 host = "localhost", port = 5432,

                 user = "cleaning", password = "data")

sql <- "SELECT id, name, x, y FROM dask_sample LIMIT 3"

data <- tibble(dbGetQuery(con, sql))

data 
```

```py
# A tibble: 3 x 4

     id name              x      y

  <int> <chr>         <dbl>  <dbl>

1   941 "Alice     " -0.612 -0.636

2  1004 "Victor    "  0.45  -0.688

3   980 "Quinn     "  0.552  0.454 
```

有趣的是，我们可能产生的数据帧不是直接的数据库表(也不像这里的例子那样只是前几行)，而是一些更复杂的数据操作或组合。连接可能是这里最有趣的情况，因为它们从多个表中获取数据。但是分组和聚集也经常是有用的，例如，可能将一百万行减少到一千个摘要描述，这可能是我们的目标:

```py
%%R

sql <- "SELECT max(x) AS max_x, max(y) AS max_y, 

               name, count(name) 

        FROM dask_sample 

        WHERE id > 1050 

        GROUP BY name 

        ORDER BY count(name) DESC

        LIMIT 12;"

# Here we simply retrieve a data.frame 

# rather than convert to tibble 

dbGetQuery(con, sql) 
```

```py
 max_x     max_y       name   count

1  0.733  0.768558     Hannah      10

2  0.469  0.849384    Norbert      10

3  0.961  0.735508      Wendy       9

4  0.950  0.673037      Quinn       8

5  0.892  0.853494    Michael       7

6  0.772  0.989233     Yvonne       7

7  0.958  0.859792   Patricia       6

8  0.953  0.865918     Ingrid       6

9  0.998  0.980781     Oliver       6

10 0.050  0.501860      Laura       6

11 0.399  0.808572      Alice       5

12 0.604  0.826401      Kevin       5 
```

## SQL 哪里出错了(以及如何注意到它)

在下面的例子中，我从描述 2012 年美国国家铁路客运公司火车站的数据集开始。最初出现的许多字段被丢弃，但其他一些字段被处理以说明一些要点。请将此视为“假数据”,即使它来自真实的数据集。特别是，列`Visitors`是全布发明的；我从未见过访客计数数据，也不知道它是否在任何地方收集。只有数字会有一个模式:

```py
amtrak = pd.read_sql('bad_amtrak', engine)

amtrak.head() 
```

```py
 Code         StationName          City    State    Visitors

0    ABB    Abbotsford-Colby         Colby       WI       18631

1    ABE            Aberdeen      Aberdeen       MD       12286

2    ABN             Absecon       Absecon       NJ        5031

3    ABQ         Albuquerque   Albuquerque       NM       14285

4    ACA   Antioch-Pittsburg       Antioch       CA       16282 
```

从表面上看——除了我们读到的表的名字——没有什么不合适的。让我们寻找问题。请注意，下面的测试在某种程度上是异常检测，这将在后面的章节中讨论。然而，我们发现的异常是特定于 SQL 数据类型的。

如果给定了特定的字符长度，RDBMSs 中的字符串字段很容易被截断。现代数据库系统也有一个`VARCHAR`或`TEXT`类型的无限长度字符串，但在实践中通常使用特定的长度。在某种程度上，已知文本长度的数据库操作会更有效，所以这个选择并不简单。但是不管是什么原因，你会在实践中经常发现这样的固定长度。特别是，`StationName`列被定义为`CHAR(20)`。问题是:这有问题吗？

知道字符长度不会自动回答我们关心的问题。也许美国国家铁路客运公司法规要求所有站名都有一定长度。这是您作为数据科学家可能不具备的特定领域知识。事实上，领域专家可能也没有它，因为它没有被分析过，或者因为规则已经随着时间而改变。让我们分析数据本身。

此外，即使数据库字段当前是可变长度或很长，也很有可能在数据库的生命周期中某一列发生了改变，或者发生了迁移。不幸的是，多代的旧数据可能都以各自的方式被破坏，这可能会使检测变得模糊不清。

您可能会遇到这种问题数据历史的一个地方是旧数据集中使用两位数年份的日期。对于活动数据库系统，20 年前就必须解决“Y2K”问题——例如，我在 1998 年主要关注这个问题——但是仍然有一些不经常访问的遗留数据存储会因为这种模糊性而失败。如果字符串`'34'`存储在一个名为`YEAR`的列中，它是指 20 世纪发生的事情，还是指本书写作十年后的预期未来事件？回答这个问题需要一些领域知识。

一些相当简洁的熊猫代码可以告诉我们一些有用的东西。第一步是清除固定长度字符字段中的填充。空白填充在我们的代码中通常没有用。之后，我们可以查看每个值的长度，计算每个长度的记录数，并根据这些长度进行排序，以生成直方图:

```py
amtrak['StationName'] = amtrak.StationName.str.strip()

hist = amtrak.StationName.str.len().value_counts().sort_index()

hist 
```

```py
4      15

5      46

6     100

7     114

     ... 

17     15

18     17

19     27

20    116

Name: StationName, Length: 17, dtype: int64 
```

如果我们把它形象化，这个模式会更加引人注目。很明显，站名超过了 20 个字符的宽度。这还不是确凿的证据，但很有启发性:

```py
hist.plot(kind='bar', 

          title="Lengths of Station Names"); 
```

![](img/B17126_01_03.png)

图 1.3:显示站名长度的直方图

我们希望小心不要把潜在的现象归结为[数据假象](Glossary.xhtml#_idTextAnchor033)。例如，在准备这一部分时，我开始分析 Twitter 2015 年的推文。这些很自然地形成了一个类似的 140 个字符的“碰撞”模式——但是我意识到他们这样做是因为精确的底层数据的限制，而不是作为一个数据工件。然而，Twitter 直方图曲线看起来与站名相似。我意识到 Twitter 在 2018 年将其限制翻了一番；我希望随着时间的推移，聚集的集合在 140 和 280 处都显示渐近线，但这是一种“自然”现象。

如果字符宽度限制在我们的数据历史中发生变化，我们可能会看到多个软限制的模式。这些可能更难辨别，尤其是如果这些限制远远大于 20 个字符。在我们完全断定我们有一个*数据工件*而不是，例如，一个美国国家铁路客运公司命名规则之前，让我们看看具体的数据。

当我们从一千行开始时，这并不是不切实际的，但是当有一百万行时，这就变得更加困难了。使用 Pandas 的`.sample()`方法通常是查看匹配某个过滤器的行的随机子集的好方法，但是这里我们只显示第一个和最后几个:

```py
amtrak[amtrak.StationName.str.len() == 20] 
```

```py
 Code            StationName                     City   State   Visitors

28   ARI   Astoria (Shell Stati                  Astoria      OR      27872

31   ART   Astoria (Transit Cen                  Astoria      OR      22116

42   BAL   Baltimore (Penn Stat                Baltimore      MD      19953

50   BCA   Baltimore (Camden St                Baltimore      MD      32767

...  ...                    ...                      ...     ...        ...

965  YOC   Yosemite - Curry Vil   Yosemite National Park      CA      28352

966  YOF   Yosemite - Crane Fla   Yosemite National Park      CA      32767

969  YOV   Yosemite - Visitor C   Yosemite National Park      CA      29119

970  YOW   Yosemite - White Wol   Yosemite National Park      CA      16718

116 rows × 5 columns 
```

从我们检查的数据中可以合理地得出结论，截断是一个真实的问题。许多示例中的单词在字符长度的中间终止。补救它是另一个决定和更多的努力。也许我们可以获得全文作为后续行动；如果幸运的话，前缀将唯一匹配完整的字符串。当然，很有可能，真实的数据只是丢失了。如果我们只关心唯一性，这很可能不是一个大问题(三个字母的代码已经是唯一的)。然而，如果我们的分析涉及丢失的数据本身，我们可能根本无法继续。也许我们可以用一种特定问题的方式来决定前缀仍然是我们正在分析的一个代表性样本。

***

固定长度的数量也会出现类似的问题。浮点数可能会失去预期的精度，但整数可能会换行和/或剪裁。我们可以检查`Visitors`列并确定它存储了一个 16 位的`SMALLINT`。也就是说，它不能表示大于 32，767 的值。也许这比任何一个电视台的访客都要多。

或者，我们可能会看到数据损坏:

```py
max_ = amtrak.Visitors.max()

amtrak.Visitors.plot(

        kind='hist', bins=20, 

        title=f"Visitors per station (max {max_})"); 
```

![](img/B17126_01_04.png)

图 1.4:显示每个站点访客的直方图

在这种情况下，撞击极限是一个强烈的信号。这里的一个额外提示是达到的具体限制。这是你应该学会识别的特殊数字之一。[位长 *N* 的有符号整数](Glossary.xhtml#_idTextAnchor124)范围从 *-2* ^(N-1) 到*2*^(N-1)*-1*。[无符号整数](Glossary.xhtml#_idTextAnchor142)范围从 *0* 到*2*N。32767 是*2*16-1。然而，由于各种编程原因，数据类型界限前的一个(或几个)数字也经常出现。一般来说，如果你曾经看到一个[测量值](Glossary.xhtml#_idTextAnchor074)正好是这些界限中的一个，你应该再看一看，想想它是否可能是一个虚假的数字，而不是一个真实的值。即使在数据库环境之外，这也是一个很好的规则。

一个可能更难解决的问题是值何时换行。根据您使用的工具，大的正整数可能会绕回负整数。许多 RDBMS——包括 PostgreSQL——会简单地拒绝值不可接受的事务，而不是允许它们发生。但是不同的系统有所不同。在本质上非零的计数的情况下，符号换行是显而易见的，但是对于正数和负数都有意义的值，检测起来更困难。

例如，在这个转换为短整型的 Pandas Series 示例中，我们看到正负 15，000 左右的值，既作为真正的元素，也作为类型转换的工件:

```py
ints = pd.Series(

    [100, 200, 15_000, 50_000, -15_000, -50_000])

ints.astype(np.int16) 
```

```py
0      100

1      200

2    15000

3   -15536

4   -15000

5    15536

dtype: int16 
```

在这种情况下，我们只需要获得足够的领域专业知识，就可以知道可能包装的越界值是否是合理的度量。也就是说，50，000 英镑对于这种假设的衡量标准是否合理？如果所有合理的观察都是以百为单位，那么 32，000 的包装就不是一个大问题。可以想象，一些合理的值是从一个不合理的值包装而来的；但是，错误的值可能由于各种原因而出现，这不会是一个过大的问题。

请注意，在广泛使用的计算机体系结构中，整数和浮点数的大小只有 8、16、32、64 和 128 位。对于整数，它们可能是有符号的或无符号的，这将使可表示的最大数减半或加倍。这些在不同位宽内可表示的最大值彼此完全不同。一个经验法则是，如果你可以选择整数表示，就从你*期望*出现的最大幅度开始留下一个数量级的填充。然而，有时即使是一个数量级也不能为意外(但准确)的值设定一个好的界限。

例如，在我们假设的访客数量中，合理预期的最大值可能是 20，000 左右，但多年来，这个数字高达 35，000，导致了我们在*图 1.4* 图(假设数据)中看到的效果。考虑到 9，223，372，036，854，775，807(*2*^(63)*-1*)个访问者访问一个站点，这对于最初的数据库工程师来说可能是不必要的开销。然而，最大值为 2，147，483，647(*2*^(31)*-1*)的 32 位整数将是更好的选择，尽管实际最大值仍然远远大于观察到的值。

现在让我们来看看您可能会用到的一些其他数据格式，通常是二进制数据格式，通常用于科学需求。

# 其他格式

> 让百花齐放；让百家争鸣。
> 
> –儒家谚语

**概念**:

*   二进制柱状数据文件
*   分层数组数据
*   单文件 RDBMS

您可能遇到的各种数据格式可用于保存表格数据。在很大程度上，这些并没有引入我们在前面章节中没有提到的任何特殊的新的清洁问题。数据本身的属性将在后面的章节中讨论。不同的存储格式有不同的数据类型选项，但是我们讨论 RDBMSs 时所关心的问题同样适用于所有的存储格式。总的来说，从本书的角度来看，这些格式只是需要稍微不同的 API 来获取它们的底层数据，但是所有格式都为每一列提供了数据类型。这里所提到的格式并不是一个详尽的列表，很明显，在撰写本文之后，新的格式可能会出现，或者重要性会增加。但是对于未讨论的格式，访问的原则应该是相似的。

密切相关的格式 [HDF5](Glossary.xhtml#_idTextAnchor055) 和 [NetCDF](Glossary.xhtml#_idTextAnchor083) (下面讨论)在很大程度上是可互操作的，并且都提供了存储多个数组的方式，每个数组都与元数据相关联，并且还允许高维数据，而不仅仅是表格形式的二维数组。与数据框模型不同，这些科学格式中的数组始终是同类类型。也就是说，没有机制(通过设计)在同一个对象中存储一个文本列和一个数字列，甚至也没有不同位宽的数字列。但是，由于它们允许在同一个文件中有多个数组，因此完全可以实现通用性，只是方式不同于 SQL 或数据帧模型。

[SQLite](Glossary.xhtml#_idTextAnchor131) (下面讨论)是一种文件格式，它提供了一个关系数据库，在一个文件中可能包含多个表。它的使用非常广泛，从每一个 iOS 和 Android 设备到最大的超级计算机集群，它无处不在。SQLite 的接口是 Python 标准库的一部分，几乎可用于所有其他编程语言。

Apache Parquet (下面讨论)是一个面向列的数据存储。这相当于一种将数据帧或表存储到磁盘的简单方法，但是以一种优化通常沿列而不是沿行矢量化的常见操作的方式。

类似的理念激励着列 RDBMSs，如 Apache Cassandra(T2)和 Monet db(T4)，它们都是 SQL 数据库，只是有不同的查询优化可能性。 [kdb+](Glossary.xhtml#_idTextAnchor067) 是一个老的、非 SQL 的解决类似问题的方法。PostgreSQL 和[Maria db](Glossary.xhtml#_idTextAnchor072)^(Maria db)也都有使用列组织的可选存储格式。通常，这些内部优化与数据科学没有直接关系，但是 Parquet 需要自己的非 SQL APIs。

*马里亚布*

MariaDB 是 MySQL 的一个分支，由 MySQL 创建者 Monty Widenius 创建。在甲骨文 2009 年收购 MySQL 后，这是出于对知识产权自由的担忧。在很大程度上，它的设计和特性与 MySQL 相似，尽管一些高级特性在那次分裂后有所不同。事实上，您可能正在使用 MariaDB，即使您没有意识到这样做，因为 shell 工具和驱动程序通常仍然保留名称`mysql`以实现兼容性。

***

有许多二进制数据格式被广泛使用，但我不会在本书中专门讨论它们。许多其他格式都有自己的优点，但我试图将讨论限制在少数我觉得你作为数据科学家在日常工作中最有可能遇到的格式上。下面列出了一些额外的格式，其特征大多来自各自的主页。你可以在描述中看到它们最相似的格式，通常相同的数据完整性和质量问题适用于我讨论的格式。差异主要在于性能特征:磁盘上的文件有多大，在不同的场景下文件的读写速度有多快，等等:

*   **Feather** (以及 **Arrow** ): Feather 是基本上是 Arrow 内存格式的直接序列化，用非常薄的适配器层存储在磁盘上。Apache Arrow 是一个内存分析开发平台。它为平面和分层数据指定了一种标准化的语言无关的列内存格式，组织起来用于现代硬件上的高效分析操作，如箭头文档[所述](https://arrow.apache.org/docs/)。
*   **Apache Avro** : **它与动态语言集成，不需要代码生成(不像类似的系统，如**节俭**和**协议缓冲**)。(转述自 [Apache Avro 文档](https://avro.apache.org/docs/1.3.3/)。)**
*   **bcolz** : bcolz 提供了柱状、分块的数据容器，既可以在内存中压缩，也可以在磁盘上压缩。列存储允许高效地查询表，以及廉价地添加和删除列。它基于 NumPy，并将其用作与 bcolz 对象通信的标准数据容器，但它也支持与 HDF5/PyTables 和 Pandas 数据帧之间的导入/导出功能，如 bcolz 文档中的[所述。](https://github.com/Blosc/bcolz)
*   **Zarr** : Zarr 提供了用于处理 N 维数组的类和函数，N 维数组的行为类似于 NumPy 数组，但其数据被分成块，并且每个块都被压缩。如果您已经熟悉 HDF5，那么 Zarr 阵列提供了类似的功能，但具有一些额外的灵活性，如 Zarr 文档中的[所述。](https://zarr.readthedocs.io/en/stable/tutorial.html)

## HDF5 和 NetCDF-4

[【HDF】](Glossary.xhtml#_idTextAnchor055)有一段略显复杂的历史，这是由国家超级计算应用中心(NCSA)于 1987 年开始的。HDF4 被明显过度设计，现在远没有被广泛使用。HDF5 简化了 HDF4 的文件结构。它由*数据集*和*组*组成，前者是单一数据类型的多维数组，后者是保存数据集和其他组的容器结构。组和数据集都可以附加属性，这些属性可以是任何命名的元数据。这实际上是在单个文件中模拟一个文件系统。这个虚拟文件系统中的节点或“文件”是数组对象。一般来说，单个 HDF5 文件将包含各种相关数据，用于处理相同的潜在问题。

网络通用数据表单(NetCDF)是一个用于存储和检索数组数据的函数库。该项目本身几乎和 HDF 一样古老，并且是一个由各种科学机构开发和支持的开放标准。从版本 4 开始，支持使用 HDF5 作为存储后端；早期版本使用其他一些文件格式，当前的 NetCDF 软件需要继续支持这些旧格式。偶尔，NetCDF-4 文件对其内容做了足够特殊的处理，以至于用一般的 HDF5 库来读取它们是很笨拙的。

通用 HDF5 文件通常具有扩展名`.h5`、`.hdf5`、`.hdf`或`.he5`。这些应该都代表相同的二进制格式，其他扩展有时也会出现。HDF4 也有一些相应的扩展。奇怪的是，尽管 NetCDF 可以包含许多底层文件格式，但它们似乎都是基于`.nc`扩展名的标准化文件。

### 工具和库

虽然我一般不依赖 GUI 工具，但在查看 HDF5 相当复杂的结构的情况下，它们可以有所帮助。例如，来自 NASA 地球科学数据收集的一个文件包含在本书的样本数据存储库中。用户可以自由注册，从美国宇航局获取数据集，这些数据集总共有数十亿字节的信息。这个特定的 HDF5/NetCDF 文件包含 98 分钟期间的地面压力、垂直温度剖面、地面和垂直风剖面、对流层顶压力、边界层顶部压力和地面重力位的数据集。特别是，一些数据在空间上是三维的。

使用开源查看器 [HDF 罗盘](https://hdf-compass.readthedocs.io/en/latest/index.html)查看一小部分数据说明了一些结构。查看的特定数据集是文件中 16 个数据集中的 1 个。这个 DELP 数据集是关于*压力厚度*，包含一个 32 位值数组和 8 个描述数据集的属性。您可以在下面的屏幕截图中看到，这个特定的 GUI 工具将第 3 个^(和第 5 个)维度显示为一个选择小部件，并将前两个维度显示为一个表格视图。

![HDF Compass NASA data](img/B17126_01_05.png)

图 1.5:美国宇航局的 HDF 罗盘数据

在 Python 中，有两个流行的开源库用于与 HDF5 一起工作的、 [PyTables](Glossary.xhtml#_idTextAnchor102) 和 [h5py](Glossary.xhtml#_idTextAnchor054) 。为了更好地使用 NetCDF，还有一个 [netcdf4-python](Glossary.xhtml#_idTextAnchor082) 库。如果您希望从 HDF5 文件中读取数据，而不是添加 NetCDF 特定的元数据，一个通用的 HDF5 工具就可以了(`h5py`比 PyTables 更好地处理特殊元数据)。

PyTables 和 h5py 的态度略有不同。H5py 接近 HDF5 规范本身，而 PyTables 试图提供更高级别的“Pythonic 式”接口。PyTables 的优势在于它的数据模型借鉴并认可了作者早在 2000 年编写的 XML 访问库；然而，这种优势对普通读者来说可能不如对我个人来说重要。在 R 世界中，库 [rhdf5](Glossary.xhtml#_idTextAnchor113) 是可用的。

在处理 HDF5 数据的库中，当处理大型数据集时，允许一定程度的懒惰。在 Python 接口中，数据集是虚拟化的 NumPy 数组；重要的是，您可以对这些数组执行切片操作，并且实际上只将指定的数据读入内存。您可能正在处理万亿字节的底层信息，但一次只处理或修改(或根本不处理)兆字节的信息，并且从磁盘阵列上的区域高效地读取和写入。

美国宇航局使用的数据文件的名称冗长，但在名称本身包含了数据集的性质的详细指示。让我们打开一个文件，看看它的数据集摘要。我们将显示数据集名称、它的维度、它的数据类型、它的形状，以及这些恰好都具有的“单位”属性。一般来说，属性可以有任何名称，但是 NASA 有使用的约定:

```py
import h5py

h5fname = ('data/earthdata/OMI-Aura_ANC-OMVFPITMET'

           '_2020m0216t225854-o82929_v003'

           '-2020m0217t090311.nc4')

data = h5py.File(h5fname, mode='r')

for name, arr in data.items():

    print(f"{name:6s} | {str(arr.shape):14s} | "

          f"{str(arr.dtype):7s} | {arr.attrs['units'][0]}") 
```

```py
DELP   | (1494, 60, 47) | float32 | Pa

PBLTOP | (1494, 60)     | float32 | Pa

PHIS   | (1494, 60)     | float32 | m+2 s-2

PS     | (1494, 60)     | float32 | Pa

T      | (1494, 60, 47) | float32 | K

TROPPB | (1494, 60)     | float32 | Pa

U      | (1494, 60, 47) | float32 | m s-1

U10M   | (1494, 60)     | float32 | m s-1

V      | (1494, 60, 47) | float32 | m s-1

V10M   | (1494, 60)     | float32 | m s-1

lat    | (1494, 60)     | float32 | degrees_north

lev    | (47,)          | int16   | 1

line   | (1494,)        | int16   | 1

lon    | (1494, 60)     | float32 | degrees_east

sample | (60,)          | int16   | 1

time   | (1494,)        | float64 | seconds since 1993-01-01 00:00:00 
```

我们可以只在一个数据集数组的一部分中创建一个内存视图。在本例中，我们以只读模式打开，但是如果我们使用`'r+'`或`'a'`模式打开，我们可以更改文件。使用`'w'`模式时要非常小心，因为它会覆盖现有文件。如果模式允许在磁盘上修改，调用`data.flush()`或`data.close()`会将任何更改写回 HDF5 源。

让我们仅创建三维 V 数据集的一小部分的视图。我们在这里并不特别关心理解数据的领域，而只是演示 API。具体来说，请注意，我们使用了一维跨距来展示复杂内存视图的一般 NumPy 风格是可用的。只有被引用的数据实际上被放入主存，而其余的数据留在磁盘上:

```py
# A 3-D block from middle of DELP array

middle = data['V'][::500, 10:12, :3]

middle 
```

```py
array([[[17.032158  , 12.763597  ,  3.7710803 ],

        [16.53227   , 12.759642  ,  4.1722884 ]],

       [[ 4.003829  , -1.0843939 , -6.7918572 ],

        [ 3.818467  , -1.0030019 , -6.6708655 ]],

       [[-2.7798688 ,  0.24923703, 20.513933  ],

        [-2.690715  ,  0.2226392 , 20.473366  ]]], dtype=float32) 
```

如果我们修改视图`middle`中的数据，当我们刷新或关闭句柄时(如果不是在只读模式下)，它将被写回。我们还可以将我们的数据切片用于其他计算或数据科学目的。例如，也许这样的选择充当输入到神经网络的张量。

在一个更简单的例子中，也许我们只是想在数据上找到一些统计或简化/抽象:

```py
middle.mean(axis=1) 
```

```py
array([[16.782215  , 12.76162   ,  3.9716845 ],

       [ 3.911148  , -1.0436978 , -6.7313614 ],

       [-2.735292  ,  0.23593812, 20.493649  ]], dtype=float32) 
```

***

用 R 语言——或大多数其他语言——处理 HDF5 文件通常类似于用 Python。让我们一起来看看与 R 库`rhdf5`:

```py
%%R -i h5fname

library(rhdf5)

h5ls(h5fname) 
```

```py
 group   name       otype  dclass            dim

0      /   DELP H5I_DATASET   FLOAT 47 x 60 x 1494

1      / PBLTOP H5I_DATASET   FLOAT      60 x 1494

2      /   PHIS H5I_DATASET   FLOAT      60 x 1494

3      /     PS H5I_DATASET   FLOAT      60 x 1494

4      /      T H5I_DATASET   FLOAT 47 x 60 x 1494

5      / TROPPB H5I_DATASET   FLOAT      60 x 1494

6      /      U H5I_DATASET   FLOAT 47 x 60 x 1494

7      /   U10M H5I_DATASET   FLOAT      60 x 1494

8      /      V H5I_DATASET   FLOAT 47 x 60 x 1494

9      /   V10M H5I_DATASET   FLOAT      60 x 1494

10     /    lat H5I_DATASET   FLOAT      60 x 1494

11     /    lev H5I_DATASET INTEGER             47

12     /   line H5I_DATASET INTEGER           1494

13     /    lon H5I_DATASET   FLOAT      60 x 1494

14     / sample H5I_DATASET INTEGER             60

15     /   time H5I_DATASET   FLOAT           1494 
```

您可能会注意到，在 R 和 Python 中，维度的顺序是颠倒的，因此我们在选择感兴趣的区域时必须考虑到这一点。然而，通常 R 中的切片操作与 NumPy 中的非常相似。函数`h5save()`用于将修改过的数据写回磁盘。

```py
%%R -i h5fname

V = h5read(h5fname, 'V')

V[1:2, 10:12, 10:11] 
```

```py
, , 1

         [,1]     [,2]     [,3]

[1,] 17.69524 17.23481 16.57238

[2,] 12.46370 12.44905 12.47155

, , 2

         [,1]     [,2]     [,3]

[1,] 17.71876 17.25898 16.56942

[2,] 12.42049 12.40599 12.43139 
```

***

所示的 NASA 数据不使用组层次结构，仅使用顶级数据集。让我们看一个玩具数据集，它嵌套了组和数据集。

```py
make_h5_hierarchy()  # initialize the HDF5 file

f = h5py.File('data/hierarchy.h5', 'r+')

dset = f['/deeply/nested/group/my_data']

print(dset.shape, dset.dtype) 
```

```py
(10, 10, 10, 10) int32 
```

我们看到我们有一个整数数据的 4 维数组。也许还附加了一些元数据描述。我们还可以查看——然后修改——在`'r+'`模式下打开后的部分数据。更改数据后，我们可以将其写回磁盘。我们可以类似地在常规字典样式中更改或添加属性，例如:

```py
dset.attrs[mykey] = myvalue 
```

让我们展示数据集的一部分。

```py
for key, val in dset.attrs.items():

    print(key, "→", val)

print()

print("Data block:\n", dset[5, 3, 2:4, 8:]) 
```

```py
author      David Mertz

citation    Cleaning Data Book

shape_type  4-D integer array

Data block:

 [[-93 -53]

 [ 18 -37]] 
```

现在，我们修改显示的同一数据片，然后关闭文件句柄将其写回磁盘:

```py
dset[5, 3, 2:4, 8:] = np.random.randint(-99, 99, (2, 2))

print(dset[5, 3, 2:4, 8:])

f.close()                   # write change to disk 
```

```py
[[-45 -76]

 [-96 -21]] 
```

我们可以遍历 Python 的`h5py`包中的层次结构，但是在路径中循环有点像手工操作。r 的`rhdf5`提供了一个漂亮的实用函数`h5ls()`，让我们看到这个测试文件的更多结构:

```py
%%R 

library(rhdf5)

h5ls('data/hierarchy.h5') 
```

```py
 group       name        otype   dclass                dim

0                       /     deeply    H5I_GROUP                            

1                 /deeply     nested    H5I_GROUP                            

2          /deeply/nested      group    H5I_GROUP                            

3    /deeply/nested/group    my_data  H5I_DATASET  INTEGER  10 x 10 x 10 x 10

4                 /deeply       path    H5I_GROUP                            

5            /deeply/path  elsewhere    H5I_GROUP                            

6  /deeply/path/elsewhere      other  H5I_DATASET  INTEGER                 20

7            /deeply/path  that_data  H5I_DATASET    FLOAT              5 x 5 
```

## SQLite

本质上，从数据科学家的角度来看，SQLite 只是另一个 RDBMS。对于开发人员或系统工程师来说，它有一些特殊的属性，但是对于本书的读者来说，您将通过 SQL 查询从 SQLite 文件中获得数据。与 HDF5 有些类似，一个 SQLite 文件——通常给出扩展名`.sqlite`、`.db`或`.db3`(但不像某些文件类型那样标准化)——可以包含许多表格。在 SQL 中，我们自动获取连接和子查询来组合来自多个表的数据，而没有类似的标准来组合来自多个 HDF5 数据集的数据。

SQLite3 数据格式和服务器非常高效，查询通常也很快。与其他 SQL 数据库一样，它使用*原子事务*进行操作，这些事务要么全部成功，要么全部失败。这可以防止数据库达到逻辑不一致的状态。但是，它没有并发访问模型。

更确切地说，它不像基于服务器的 RDBMSs 那样允许多个同时写入者写入一个公共数据库。许多读者客户端可以毫无困难地同时打开同一个文件；只有当许多客户端希望执行写事务时，它才会停止运行。有很多方法可以解决这种情况，但是它们超出了本书的范围。

与其他 RDBMSs 相比，SQLite 的一个重要优势是分发组成数据库的单个文件非常简单。对于其他系统，您需要添加凭证、防火墙规则等，以便为新用户提供访问权限；或者，您需要将所需的数据导出为另一种格式，通常是 CSV，这种格式既慢又有一些损失(即数据类型)。

SQLite 中的数据类型有点像[嵌合体](Glossary.xhtml#_idTextAnchor024)。有几个基本的数据类型，我们将讨论。然而，与几乎所有其他 SQL 数据库不同的是，SQLite 按值而不是按列携带数据类型。这似乎会遇到围绕电子表格讨论过的同样的脆弱性，但实际上这远没有那些问题严重。每个值的类型不太受关注的一个原因是用于填充它们的接口；在 SQLite 中以交互方式编辑单个值是很少见的，更常见的是发出编程 SQL 命令来插入或更新来自同一数据源的许多行。

然而，除了数据类型，SQLite 还有一个叫做*类型关联*的概念。每个列都被赋予一个首选类型，该类型不会*阻止*其他数据类型的出现，但是会将首选类型推向列的相似性。我们可以从命令行运行工具`sqlite`来进入 interactive SQLite 提示符。例如(改编自 SQLite 文档):

```py
sqlite> CREATE TABLE mytable(a SMALLINT, b VARCHAR(10), c REAL);

sqlite> INSERT INTO mytable(a, b, c) VALUES('123', 456, 789); 
```

这里将插入一行，在`a`列中插入一个整数，`b`列中插入`TEXT`，在`c`列中插入一个浮点。SQL 语法本身是松散类型的，但是底层数据库做出类型/转换决定。其他 RDBMSs 也是如此，但它们对列数据类型的要求更严格。所以我们也可以在 SQLite 中运行这个，这在其他数据库中会失败:

```py
sqlite> INSERT INTO mytable(a, b, c) VALUES('xyz', 3.14, '2.71'); 
```

让我们看看有什么结果:

```py
sqlite> SELECT * FROM mytable;

123|456|789.0

xyz|3.14|2.71 
```

SQLite interactive shell 并没有使数据类型变得完全显而易见，但是在 Python 中运行查询会做到这一点。

```py
import sqlite3

db = sqlite3.connect('data/affinity.sqlite')

cur = db.cursor()

cur.execute("SELECT a, b, c FROM t1")

for row in cur:

    print([f"{x.__class__.__name__} {x}" for x in row]) 
```

```py
['int 123', 'str 456', 'float 789.0']

['str xyz', 'str 3.14', 'float 2.71'] 
```

如果设置了可以解释为整数的内容，列`a`更倾向于保存整数，但是如果需要的话，将返回到更一般的数据类型。同样，列`c`更喜欢浮点型，它可以解释无引号的整数或类似浮点型的字符串。

SQLite 中的实际数据类型为*专有* `NULL`、`INTEGER`、`REAL`、`TEXT`、`BLOB`。但是，其他 SQL 数据库中的大多数类型名称都是这些简单类型的别名。我们在示例中看到，其中`VARCHAR(10)`是`TEXT`的别名，而`SMALLINT`是`INTEGER`的别名。`REAL`值总是用 64 位浮点数表示。在`INTEGER`值内，选择 1、2、3、4、6 或 8 位长度以提高存储效率。SQLite 存储中没有 datetime 类型，但是面向时间的 SQL 函数很乐意接受任何`TEXT` (ISO-8601 字符串)、`REAL`(自公元前 4714 年 11 月 24 日以来的天数)或`INTEGER`(自 1970-01-01T00:00:00 以来的秒数)。

使用 SQLite 数据库的总体收获是，在读取数据时，可能需要额外的小心来仔细检查数据类型，但在大多数情况下，您可以假装每列都是强类型的。不会发生截断、削波和回绕问题。没有实际的十进制数据类型，只有别名；与会计或金融相比，对于数据科学来说，这很少是个问题。但是通常关于浮点舍入问题的警告将适用。

## 阿帕奇拼花地板

Parquet 格式源于 **Hadoop** 生态系统，但本质上只是一种优化的、面向列的文件格式，用于存储类似表格的数据。Parquet 有一个侧重于数字类型的类型系统。它不像 SQLite 那么简单，但也避免了提供每一个可能的位长，例如 NumPy 或 C/C++就是这样做的。所有整数类型都是有符号的。所有非数字的东西都是一个字节数组，它是在应用程序级别(而不是存储格式级别)为所需目的而强制转换的。

从 Hadoop 工具发展而来的 Parquet 特别适合并行计算。一个 Parquet“文件”实际上是一个包含许多数据文件的目录，该目录中有一个描述布局和其他细节的`_metadata`文件。

```py
%%bash

ls -x data/multicsv.parq 
```

```py
 _common_metadata _metadata        part.0.parquet   part.10.parquet

 part.11.parquet  part.12.parquet  part.13.parquet  part.14.parquet

 part.15.parquet  part.16.parquet  part.17.parquet  part.18.parquet

 part.19.parquet  part.1.parquet   part.20.parquet  part.21.parquet

 part.22.parquet  part.23.parquet  part.24.parquet  part.25.parquet

 part.26.parquet  part.27.parquet  part.28.parquet  part.29.parquet

 part.2.parquet   part.3.parquet   part.4.parquet   part.5.parquet

 part.6.parquet   part.7.parquet   part.8.parquet   part.9.parquet 
```

有时，文件系统是一个并行和分布式系统，如 **Hadoop 文件系统** ( **HDFS** )进一步支持大型数据集的计算效率。在这种情况下，Parquet 会进行各种巧妙的数据分片、高效的压缩(使用不同的策略)、连续读取的优化，并且已经过分析和修改，以改进其典型用例的速度和存储大小。

一些支持 Parquet 的工具或库有 **Apache Hive** 、 **Cloudera Impala** 、 **Apache Pig** 和 **Apache Spark** ，它们都生活在并行计算空间中。然而，Python 和 R(以及其他语言)也有可用的接口。许多高级工具通过 SQL 层处理拼花数据。

对于 Python 来说，库 pyarrow 和 **fastparquet** 提供了一个文件格式的直接接口。虽然这些库是通用的，但它们主要是用来将拼花数据转换成数据框的(通常是 Pandas，有时是 Dask、Vaex 或其他)。在 R 世界中， **sparklyr** 是 Spark 的一个接口，但是需要一个运行的 Spark 实例(本地安装也可以)。箭头包是一个直接阅读器，类似于 Python 库。

一般来说，如果您正在处理真正的大数据，Hadoop 或 Spark 工具——伴随着适当的计算集群——是一个不错的选择。Dask 是 Python 上并行化的一种方法，非常好；像 MPI 这样的其他方法也适用于 R、Python 和许多其他语言。然而，Hadoop 和 Spark 是在高效和大规模并行计算方面最受关注的工具。

即使您只需要担心中等大小的数据(几十万到几百万行)而不是大数据(几亿到几十亿行)，Parquet 仍然是一种快速的格式。此外，它还具有按列键入数据的普遍可取的特性，这使得数据至少向干净整洁的目标迈进了一小步。

作为一个例子，让我们读一下我们之前用 Dask 生成的中等大小的数据集。Pandas 和 Dask 都将使用`pyarrow`或`fastparquet`，这取决于安装的是什么。

```py
pd.read_parquet('data/multicsv.parq/') 
```

```py
index             timestamp        id     name           x           y

    0   2000-01-01 00:00:00       979    Zelda    0.802163    0.166619

    1   2000-01-01 00:00:01      1019   Ingrid   -0.349999    0.704687

    2   2000-01-01 00:00:02      1007   Hannah   -0.169853   -0.050842

    3   2000-01-01 00:00:03      1034   Ursula    0.868090   -0.190783

  ...                   ...       ...      ...         ...         ...

86396   2000-01-10 23:59:56       998    Jerry     0.589575   0.412477

86397   2000-01-10 23:59:57      1011   Yvonne     0.047785  -0.202337

86398   2000-01-10 23:59:58      1053   Oliver     0.690303  -0.639954

86399   2000-01-10 23:59:59      1009   Ursula     0.228775   0.750066

2592000 rows × 5 columns 
```

我们可以使用`dask.dataframe`和相同的语法，即`dd.read_parquet(...)`来分发上面的读数。对于大型数据集，这可以将非活动段保留在核心之外，并将工作分配到本地机器上的所有核心上。然而，对于像这样的中小型数据，Pandas 可以更快地避免协调开销。

尽管我们已经利用了数据帧的概念，将 Python 用于 Pandas，将 R 用于 tibbles，但是看看底层抽象由什么组成还是值得的。我们将简要地看一下不同编程语言中的许多不同的数据帧实现，以理解它们有什么共同点(这是很多的)。

# 数据帧

> 每当你着手做一件事时，必须先做另一件事。
> 
> —墨菲的(第一)推论

**概念**:

*   过滤/转换/分组/聚合
*   火花数据帧
*   熊猫及其衍生产品
*   其他 Python 数据框
*   R Tidyverse
*   r 数据表
*   Unix 哲学

几乎所有编程语言中的大量库都支持数据帧抽象。大多数数据科学家发现这种抽象非常强大，甚至是他们处理数据的首选方式。数据帧允许像 SQL 一样简单地表达许多相同的基本概念或操作，但是是在特定的编程语言和程序其余部分的内存空间内。SQL——即使它实际上处理的是 SQLite 之类的纯本地数据库——总是更像是“远程获取”,而不是数据框所允许的交互式探索。

这些操作主要包括过滤、分组、聚合、排序和矢量化功能应用。通常，所有数据框库都支持“流畅”的编程风格，将这些操作以某种顺序链接在一起，以产生最终结果；最终(或至少是工作)结果本身通常是数据帧或标量值。有时，可视化与这样的处理结果相关，并且大多数数据框工具与可视化库无缝集成。

当然，这些流畅的链式操作的目标是描述一个可重复的工作流程。对各种数据修改的探索可以一步一步地建立起来，中间结果通常会提示您可能出错了，或者在一定程度上保证您的路径是正确的。在探索的最后，您将有一个数据复合转换的表达式，它可以与来自您正在解决的领域和问题的新数据一起重用。代码中的注释和伴随的这些链或管道，总是让您和其他代码读者的生活更轻松。

这些分布式和/或核外的库允许无缝地处理大型数据集，也就是说，数据框架抽象几乎可以无限扩展，即使特定的库有一些粗略的限制。在这一节中，我将使用一些数据框库展示类似的代码，并简要评论它们的优点、缺点和差异。

这本书通常将 Python 用于熊猫，并在一定程度上用于 tibbles。我们将看到这些库与 Python/R (Vaex，`data.table`)中的其他库在概念和用法上的相似性，甚至与其他编程语言如 Scala/Spark 或 Bash with [coreutils](Glossary.xhtml#_idTextAnchor028) 的相似性。许多数据科学家特别使用 Spark 无论您使用什么具体工具，贯穿其中的概念都应该很容易翻译，尤其是在数据框方法可用的情况下。

本书大部分代码都会用到熊猫。在撰写本文时，Python 是数据科学中使用最广泛的语言，而 Pandas 是使用最广泛的数据框架库。事实上，几个“竞争”的库本身使用 Pandas 作为内部组件。然而，在这一节中，我想说明并强调所有这些库是多么相似。为此，我将使用许多这样的库来执行相同的任务。

有大量的操作和管道，许多相当复杂，可以用数据帧来完成。这个简短的部分不是关于任何特定库的教程，而只是对表达数据操作的共享风格和不同工具之间的较小差异的一瞥。

对于每个数据框库，我们将执行以下操作:

1.  基于两列`x`和`y`的比较进行过滤
2.  向量化来自比较的一列的导出值，`y`
3.  将另一列中具有公共值的数据分组，`name`
4.  在分组列中聚合数据，`x`
5.  基于计算列对数据进行排序，`Mean_x`
6.  为了便于说明，显示结果的前 5 行

## 火花/标量

作为起点，我想用分布式计算框架 Spark 和它的本地编程语言 Scala 来说明一系列步骤。从 Python、R 和其他语言到 Spark 的绑定也存在，但是会导致一定程度的翻译开销，从而降低操作速度。此管道采用本章其他示例中显示的示例 Dask 数据集，并对数据集执行所有提到的基本操作。^(设置)

*设置*

配置和复制本书使用的 Python 和 R 代码环境在附带的资源库中有描述。然而，配置 Hadoop 和 Spark 是分开的步骤，不太容易封装在几个配置文件中。这些步骤并不难，但是你需要遵循这些工具附带的官方文档，或者其他在线教程。

接下来的几行是在 Spark shell 内部运行的。对于本书的撰写，Hadoop 和 Spark 的本地实例正在运行，但这也可以很容易地连接到远程集群。启动时，您将看到类似于以下内容的内容:

```py
Spark context Web UI available at http://popkdm:4040

Spark context available as 'sc' (master = local[*], app id = local-1582775303458).

Spark session available as 'spark'.

Welcome to

      ____              __

     / __/__  ___ _____/ /__

    _\ \/ _ \/ _ '/ __/  '_/

   /___/ .__/\_,_/_/ /_/\_\   version 2.4.5

      /_/

Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 11.0.6)

Type in expressions to have them evaluated.

Type :help for more information. 
```

在 shell 中，我们可以读入一个公共目录中的 CSV 文件集合。在类似的界面下，许多其他数据源同样可用。我们允许推断数据类型和使用列标题来命名字段。管道符号(`|`)仅仅是的一部分，火花壳接口到表示延续线；它们本身不是 Scala 代码:

```py
scala> val df = spark.read.       // Local file or Hadoop resource

     |     options(Map("inferSchema"->"true","header"->"true")).

     |     csv("data/multicsv/")  // Directory of multiple CSVs

df: org.apache.spark.sql.DataFrame = [

    timestamp: timestamp, id: int ... 3 more fields] 
```

下面的流畅代码只是按顺序执行预期的步骤:

```py
scala> df.  // Working with loaded DataFrame

     | filter($"x" > ($"y" + 1)).  // x more than y+1 (per row)

     | groupBy($"name").           // group together same name

     | agg(avg($"x") as "Mean_x"). // mean within each group

     | sort($"Mean_x").            // order data by new column

     | show(5)

+------+------------------+

|  name|            Mean_x|

+------+------------------+

|   Ray|0.6625697073245446|

|Ursula|0.6628107271270461|

|Xavier|0.6641165295855926|

| Wendy|0.6642381725604264|

| Kevin| 0.664836301676443|

+------+------------------+

only showing top 5 rows 
```

## 熊猫和衍生包装

许多库要么模仿 Pandas API，要么直接利用它作为依赖。Dask 和 **Modin** 都是直接包装熊猫，将一个原生数据帧分割成多个独立的熊猫数据帧。本地数据帧上的方法通常被分派给每个数据帧的底层对应 Pandas 方法。尽管 Modin 可以使用 Dask 或 Ray 作为其并行/集群执行后端，但 Modin 与 Dask 的不同之处在于其执行模型中的 eager。

Dask 是一个通用的执行后端，它的`dask.dataframe`子包只是一个组件。Dask 所做的大部分工作类似于库射线，如果需要的话，Modin 也可以使用它(撰写本文时的基准测试稍微倾向于射线，这取决于用例)。Dask 中大多数 Pandas API 方法调用最初只构建所需操作的**有向无环图** ( **DAG** )。只有在调用构建的 DAG 的`.compute()`方法时，才会执行计算。下面的例子使用了 Dask，但是除了最后的`.compute()`和最初的`import modin.pandas as pd`，它看起来和摩丁完全一样。

cuDF 是另一个非常接近熊猫 API 的库，但是它在 CUDA GPUs 上执行方法。由于底层执行是在一种完全不同的芯片架构上，cuDF 既不与 Pandas 共享代码，也不包装 Pandas。但是几乎所有的 API 调用都是一样的，但是如果你的系统上有一个最新的 CUDA GPU 的话，速度会快很多。

像熊猫和摩丁一样，cuDF 在执行模式上也是跃跃欲试:

```py
import dask.dataframe as dd

dfd = dd.read_csv('data/multicsv/*.csv', parse_dates=['timestamp']) 
```

下面熊猫风格的操作看起来和 Spark 非常相似。访问器`.loc`重载了几种选择样式，但是谓词过滤器就是这样一种允许的样式。另一个在同一行上用于选择列，即名称序列。分组几乎是相同的，除了方法名的大小写。Pandas 甚至有一个`.agg()`方法，我们可以向它传递一个均值函数或字符串`'mean'`；我们只是选择了捷径。聚合中的列不会自动重命名，所以我们这样做是为了更精确地匹配。我们在一个方法中取 5 个最小的，而不是排序和显示。在效果中，概念元素是相同的，拼写略有不同:

```py
(dfd

   .loc[dfd.x > dfd.y+1,            # Row predicate

        ['name', 'x']]              # Column list

   .groupby("name")                 # Grouping column(s)

   .mean()                          # Aggregation

   .rename(columns={'x': 'Mean_x'}) # Naming

   .nsmallest(5, 'Mean_x')          # Selection by order

).compute()                         # Concretize 
```

```py
Name         Mean_x

Ray        0.662570

Ursula     0.662811

Xavier     0.664117

Wendy      0.664238

Kevin      0.664836 
```

## Vaex

Vaex 是一个完全独立于 Pandas 的 Python 库，但是它使用了一个非常相似的 API。相当数量的代码将完全适用于任何一种类型的数据帧，但不会多到你可以简单地用一个替换另一个。Vaex 的哲学和熊猫有些不同。一方面，Vaex 强调懒惰计算和隐式并行；表达式被急切地求值，但是注意不要接触给定操作不需要的数据部分。这与大部分核心外操作密切相关。Vaex 内存不是将数据读入内存，而是将数据映射到磁盘上，只加载操作所需的部分。

Vaex 始终避免制作数据副本，实际上是将选择表示为视图。它有一个*表达式*和*虚拟列*的概念。例如，对几个列的计算，即使分配给一个新列，也不会使用任何重要的新内存，因为只存储函数形式而不是数据。只有当需要这些数据时，才执行计算，而且只针对那些受影响的行。总体结果是 Vaex 在大型数据集上可以非常快；但是，Vaex 只能在一台机器上的多个内核上并行化，而不能在机器集群上并行化。

由于其内存映射方法，Vaex 并不真正希望在内部直接处理 CSV 文件。与将每个数据放在磁盘上可预测位置的序列化 Feather 或 HDF5 不同，CSV 在磁盘上的布局本来就不整齐。虽然`.read_csv()`方法会将单个文件读入内存，但为了处理目录中的一系列 CSV 文件，您可能会希望将它们转换为相应的 HDF5 文件系列。幸运的是，方法`.read_csv_and_convert()`会自动为您完成这项工作。结果是第一次读取这样的集合，转换需要一段时间，但是随后的打开利用现有的 HDF5 文件并立即打开(没有实际读入内存，只有内存映射):

```py
import vaex

dfv = vaex.read_csv_and_convert('data/multicsv/*.csv', copy_index=False) 
```

与 Pandas 的另一个区别是 Vaex 数据帧*整齐*(如本章开头所述)。Pandas 上的许多操作依赖于它们的行索引，这甚至可能是一个包含多个嵌套列的层次索引。Vaex 中的“索引”仅仅是行号。您可以进行筛选、分组和排序等操作，但总是基于常规列。这一理念与 R 中的 tibble 和 data.table 是相同的，它们都拒绝旧 data.frame 的这一方面:

```py
print(

dfv

   [dfv.x > dfv.y + 1]  # Predicate selection of rows

   [['name', 'x']]      # List selection of columns

   .groupby('name')     # Grouping

   .agg({'x': 'mean'})  # Aggregation

   .sort('x')           # Sort (Vaex does not have .nsmallest() method)

   .head(5)             # First 5

) 
```

```py
 #  name           x

  0  Ray      0.66257

  1  Ursula  0.662811

  2  Xavier  0.664117

  3  Wendy   0.664238

  4  Kevin   0.664836 
```

让我们删除这些临时 HDF5 文件，以便讨论除 Vaex 以外的库:

```py
%%bash

rm -f data/multicsv/*.hdf5 
```

现在让我们转向 r 中的模拟数据帧选项。

## R (Tidyverse)中的数据帧

在 Tidyverse 中，tibbles 是首选数据帧对象，而 **dplyr** 是一个关联库，用于——通常是链式的——流水线数据操作。`dplyr`实现流畅风格的方式是不基于链式方法调用。事实上，面向对象编程一般很少在 R 中使用。相反，`dplyr`依赖于“管道”操作符(`%>%`)，它将前一个表达式的结果作为下一个被调用函数的第一个参数。这允许重写紧凑但嵌套很深的表达式，如下所示:

```py
round(exp(diff(log(x))), 1) 
```

在流畅的风格中，这变成了:

```py
x %>% 

  log() %>%

  diff() %>%

  exp() %>%

  round(1) 
```

首先，我们可以读入之前生成的 CSV 文件集合。该数据中总共 250 万行仍然是中等大小的数据，但是下面代码中的模式可以应用于大数据:

```py
%%R 

files <- dir(path = "data/multicsv/", pattern = "*.csv", full.names = TRUE)

read_csv_quiet <- function(file) { 

    read_csv(file, col_types = cols("T", "n", "f", "n", "n"), progress = FALSE) }

data <- files   %>%

  # read_csv() on each file, reduce to one DF with rbind

  map(read_csv_quiet) %>%  

  # If this were genuinely large data, we would process each file individually

  reduce(rbind)  

data 
```

```py
# A tibble: 2,592,000 x 5

   timestamp              id name         x       y

   <dttm>              <dbl> <fct>    <dbl>   <dbl>

 1 2000-01-01 00:00:00   979 Zelda   0.802   0.167 

 2 2000-01-01 00:00:01  1019 Ingrid -0.350   0.705 

 3 2000-01-01 00:00:02  1007 Hannah -0.170  -0.0508

 4 2000-01-01 00:00:03  1034 Ursula  0.868  -0.191 

 5 2000-01-01 00:00:04  1024 Ingrid  0.0838  0.109 

 6 2000-01-01 00:00:05   955 Ingrid -0.757   0.308 

 7 2000-01-01 00:00:06   968 Laura   0.230  -0.913 

 8 2000-01-01 00:00:07   945 Ursula  0.265  -0.271 

 9 2000-01-01 00:00:08  1020 Victor  0.512  -0.481 

10 2000-01-01 00:00:09   992 Wendy   0.862  -0.599 

# ... with 2,591,990 more rows 
```

用于过滤、修改、分组和聚合数据的函数的`dplyr`管道看起来与其他数据框库中使用的链式方法几乎相同。一些函数名与其他库中的略有不同，但是执行的步骤是相同的:

```py
%%R 

summary <- data   %>% 

  filter(x > y+1) %>%   # Predicate selection of rows

  select(name, x) %>%   # Selection of columns

  group_by(name)  %>%   # Grouping

                        # Aggregation and naming

  summarize(Mean_x = mean(x)) %>% 

  arrange(Mean_x) %>%   # Sort data

  head(5)               # First 5

summary 
```

```py
'summarise()' ungrouping output (override with '.groups' argument)

# A tibble: 5 x 2

  name   Mean_x

  <fct>   <dbl>

1 Ray     0.663

2 Ursula  0.663

3 Xavier  0.664

4 Wendy   0.664

5 Kevin   0.665 
```

## R (data.table)中的数据帧

在 Tidyverse 之外，在现代 R 中处理表格数据的主要方法是 data.table。这是对旧的但标准的 R data.frame 的替代。我在本书中不单独讨论 data.frame，因为新代码应该总是更喜欢 tibbles 或 data.tables

与大多数其他数据框方法不同，data.table 不使用流畅或链式样式。相反，它使用了一种极其紧凑的*通用形式`DT[i, j, by]`的*,可以捕获大量可能的操作。不是每个操作的集合都可以用一个单一的通用形式来表达，但是很多操作都可以。此外，因为 data.table 能够优化整个通用形式，所以在大型数据集上，它通常比那些以顺序方式执行操作的库要快得多。

一般形式的每个元素可以省略，以表示“一切”如果使用的话，`i`是描述感兴趣的行的表达式；通常这个`i`会由几个子句组成，这些子句由逻辑连接符`&` (and)、`|` (or)和`!` (not)连接。也可以在该表达式中实施行排序(但不能在派生列中实施)。例如:

```py
dt[(id > 999 | date > '2020-03-01') & !(name == "Lee")] 
```

列选择器`j`可以引用列，包括派生列:

```py
dt[ , .(id, pay_level = round(log(salary), 1)] 
```

最后，`by`表单是一个分组描述，允许对每个行子集进行计算。分组可以遵循[分类](Glossary.xhtml#_idTextAnchor022)值或计算的切割:

```py
dt[, mean(salary), cut(age, quantile(age, seq(0,100,10)))] 
```

将这些表单放在一起，我们可以生成与其他数据框库相同的摘要。然而，第二步必须执行最终订购:

```py
%%R

library(data.table)

dt <- data.table(data)

summary <- dt[

    i = x > y + 1,      # Predicate selection of rows

                        # Aggregation and naming

    j = .(Mean_x = mean(x)), 

    by = .(name)]       # Grouping

# Sort data and first 5

summary[order(Mean_x), .SD[1:5]] 
```

```py
 name    Mean_x

1:    Ray 0.6625697

2: Ursula 0.6628107

3: Xavier 0.6641165

4:  Wendy 0.6642382

5:  Kevin 0.6648363 
```

## 狂欢派对

对于习惯于在命令行执行管道过滤和聚合的读者来说，数据帧使用的管道或流畅风格似乎非常熟悉。事实上，使用命令行工具复制我们的示例并不困难。这里的举重器是 **awk** ，但是它使用的代码非常简单。从概念上讲，这些步骤与我们在数据框库中使用的步骤完全一致。在 Unix 理念下，使用管道组合的小工具可以自然地复制数据帧中使用的相同基本操作:

```py
%%bash

COND='{if ($4 > $5+1) print}'

SHOW='{for(j in count) print j,sum[j]/count[j]}'

AGG='{ count[$1]++; sum[$1]+=$2 }'" END $SHOW"

cat data/multicsv/*.csv | # Create the "data frame"

  grep -v ^timestamp    | # Remove the headers

  awk -F, "$COND"       | # Predicate selection

  cut -d, -f3,4         | # Select columns

  awk -F, "$AGG"        | # Aggregate by group

  sort -k2              | # Sort data

  head -5                 # First 5 
```

```py
Ray 0.66257

Ursula 0.662811

Xavier 0.664117

Wendy 0.664238

Kevin 0.664836 
```

耶鲁安·扬森斯写了一本有趣的书，名为《命令行中的数据科学》[](https://www.datascienceatthecommandline.com/)*,这本书写得非常棒，可以在网上免费获得。你也应该购买印刷版或电子书来支持他的工作。在这一小节中，以及在本书的不同地方，我只是对那本书详细讨论的技术类型做了一些小小的暗示。*

 *数据框架和流畅的编程风格是一个强大的习语，在数据科学中应用尤其广泛。我讨论的每一个特定的库都是具有同等能力的优秀选择。哪种最适合你，很大程度上取决于你的品味，或许还取决于你的同事用什么。

# 练习

把我们在本章中学到的大部分内容放在一起，下面的练习应该能让你利用你读过的技巧和习惯用法。

## 整理 Excel 中的数据

一份 Excel 电子表格，其中包含一些关于授予部电影的奖项的简要信息，可从以下网址获得:

[https://www.gnosis.cx/cleaning/Film_Awards.xlsx](https://www.gnosis.cx/cleaning/Film_Awards.xlsx)

在一个更具体的例子中，我们可能有更多年的数据，更多类型的奖励，更多授予奖励的协会，等等。虽然这个电子表格的组织很像您将在“野外”遇到的许多电子表格，但它很少像我们更愿意处理的整洁数据。在这个简单的例子中，只有 63 个数据值，您可以像编码转换一样快速地手工将它们输入到所需的结构中。然而，本练习的目的是编写可以推广到类似结构的大型数据集的编程代码:

![Film Awards](img/B17126_01_06.png)

图 1.6:电影奖电子表格

在本练习中，您的任务是使用您最熟悉的语言和库将这些数据读入一个规范化的数据框架中。在此过程中，您将需要修复您检测到的任何数据完整性问题。作为需要注意的问题示例:

*   电影 *1917* 被存储为一个数字，而不是一个字符串，当天真地进入一个单元格。
*   某些值的拼写不一致。奥莉薇娅·柯尔曼的名字有一次被错误地转录为“科尔曼”。您需要确定一个值中的间距问题。
*   从结构上看，表面上的相似之处实际上并非如此。人名有时列在协会名称下，但有时列在另一栏下。电影名称有时列在协会下，有时在别处。
*   某些列名在同一个表格区域中出现多次。

当考虑好的数据框组织时，考虑自变量和因变量是什么。在每一年，每个协会为每个类别颁奖。这些是独立的尺寸。人名和电影名有点棘手，因为它们不是完全独立的，但同时有些奖项是给电影的，有些是给人的。此外，一个演员可能在一年内出现在多部电影中(不在这个样本数据中，但不排除这种可能性)。同样，在电影史上，有时多部电影会使用同一个名字。有些人既是导演又是演员(在同一部或不同的电影中)。

有了有用的数据框后，使用它来回答摘要报告中的这些问题:

*   对于涉及多个奖项的每部电影，请列出与其相关的奖项和年份。
*   对于获得多个奖项的每位演员，请列出他们参与的电影和奖项。
*   虽然没有出现在这个小数据集中，但有时男女演员会获得多部电影的奖项(通常在不同的年份)。确保您的代码能够处理这种情况。
*   它是手工工作，但你可能想研究和添加其他年份给予的奖励；特别是增加一些数据会显示演员有多部电影的奖项。您的其他报告是否正确地总结了更大的数据集？

## 整理 SQL 中的数据

SQLite 数据库的简要信息与之前的电子表格大致相同，可从以下网址获得:

[https://www.gnosis.cx/cleaning/Film_Awards.sqlite](https://www.gnosis.cx/cleaning/Film_Awards.sqlite)

然而，数据库版本中的信息相对来说是规范化和类型化的。此外，电子表格中还包含了各种实体的附加信息。该模式中包含的信息仅比电子表格中多一点，但它应该能够容纳大量关于电影、演员、导演和奖项的数据，以及这些数据之间的关系:

```py
sqlite> .tables

actor     award     director  org_name 
```

正如在前面的练习中提到的，一部电影的同一个名字可以被多次使用，甚至是由同一个导演使用。例如，阿贝尔·冈斯使用的标题是【指控！对于他 1919 年和 1938 年拍摄的相关题材的电影:

```py
sqlite> SELECT * FROM director WHERE year < 1950;

Abel Gance|J'accuse!|1919

Abel Gance|J'accuse!|1938 
```

例如，让我们看看从`actor`表中选择的内容。在这个表中，我们有一个列`gender`来区分名称之外的内容。截至本文撰写之时，还没有跨性别演员在性别身份改变前后获得过主要奖项的提名，但这个方案允许这种可能性。在任何情况下，我们都可以使用该字段来区分许多组织授予的“男演员”和“女演员”奖项:

```py
sqlite> .schema actor

CREATE TABLE actor (name TEXT, film TEXT, year INTEGER, gender CHAR(1));

sqlite> SELECT * FROM actor WHERE name="Joaquin Phoenix";

Joaquin Phoenix|Joker|2019|M

Joaquin Phoenix|Walk the Line|2006|M

Joaquin Phoenix|Hotel Rwanda|2004|M

Joaquin Phoenix|Her|2013|M

Joaquin Phoenix|The Master|2013|M 
```

本练习的目标是创建与您在上一练习中创建的相同的整洁的数据框，并回答在该练习中提出的相同问题。如果有些问题可以直接用 SQL 来回答，请随意使用那种方法。对于本练习，仅考虑 2017 年、2018 年和 2019 年的奖励。其他一些报告并不完整，但您的报告是这些年的:

```py
sqlite> SELECT * FROM award WHERE winner="Frances McDormand";

Oscar|Best Actress|2017|Frances McDormand

GG|Actress/Drama|2017|Frances McDormand

Oscar|Best Actress|1997|Frances McDormand 
```

# 结局

> 所有的模型都是错的，但有些模型是有用的。
> 
> 乔治·博克斯

**本章涉及的主题**:分隔文件；电子表格的危险；关系数据库管理系统；HDF5 数据框。

本章介绍了构成世界上绝大多数结构化数据的数据格式。虽然我没有关于数据量细分的确切的硬数据——除了一些专门从事批量数据采集的三字母机构之外，任何人都不能——但我仍然觉得这是一个安全的断言。在存储在 HDF5 和相关格式中的所有科学数据、存储在电子表格中的所有业务数据、存储在 SQL 数据库中的所有交易数据以及从几乎任何地方导出到 CSV 的所有数据之间，这几乎构成了工作数据科学家经常遇到的一切。

在呈现格式时，我们讨论了当前几种语言中用于摄取这些数据源的主要工具。本书的重点仍然是 Python 和 R，它们是数据科学的主要编程语言。也许这种情况在未来会有所改变，而且几乎可以肯定的是，一些新的库将会出现，以更快、更方便的方式处理这些海量数据。即便如此，大多数关于格式的优势和局限性的概念性问题——主要是关于数据类型和存储工件的问题——仍将是那些新语言和库的问题。只有拼写会有轻微的变化。

一个扩展的，但是非常不完整的讨论着眼于在许多工具中使用的数据框架抽象。这里可能会再次出现新的变化，但我相信，在撰写本文后的几十年里，通用抽象将是数据科学中使用的主要抽象。在介绍一些略有不同的库时，我只是触及了其中任何一个库的皮毛。

事实上，即使这一整章仅仅是关于其中一个提到的库，与那些花费全部篇幅讨论一个特定数据框架库的优秀书籍相比，它也是不完整的。尽管如此，我希望这篇关于从过滤、分组、聚合、命名和排序的步骤来思考数据处理问题的介绍，能够很好地帮助读者理解许多摄取任务。

我们在阅读本章讨论的所有格式时使用的数据帧抽象的一个限制是，没有一个以任何有意义的方式看待数据流。在大多数情况下，数据科学需求不是流需求，但偶尔会重叠。如果您的需求处于这一特定边缘，请查看流媒体协议的文档，如 [ActiveMQ](Glossary.xhtml#_idTextAnchor014) 、 [RabbitMQ](Glossary.xhtml#_idTextAnchor105) 和 [Kafka](Glossary.xhtml#_idTextAnchor066) (以及其他)；但是您的关注点主要不是数据格式本身，而是事件处理，以及不断发展的异常和坏数据检测，例如第 4 章 和 [*5*](Chapter_5.xhtml#_idTextAnchor007) 中讨论的 [*和*](Chapter_4.xhtml#_idTextAnchor006) *[*第 6 章*](Chapter_6.xhtml#_idTextAnchor008) 中讨论的值插补。*

在下一章，我们将转向按层次组织的数据格式，而不是表格格式。**