© Andreas François Vermeulen 2018 Andreas François VermeulenPractical Data Science[https://doi.org/10.1007/978-1-4842-3054-1_10](#)

# 10.转换超级步骤

Andreas François Vermeulen<sup class="calibre7">1 </sup> (1)West Kilbride North Ayrshire, UK   The Transform superstep allows you, as a data scientist, to take data from the data vault and formulate answers to questions raised by your investigations. The transformation step is the data science process that converts results into insights. It takes standard data science techniques and methods to attain insight and knowledge about the data that then can be transformed into actionable decisions, which, through storytelling, you can explain to non-data scientists what you have discovered in the data lake. Any source code or other supplementary material referenced by me in this book is available to readers on GitHub, via this book’s product page, located at [www.apress.com/9781484230534](http://www.apress.com/9781484230534) . Please note that this source code assumes that you have completed the source code setup outlined in Chapter [2](02.html).

## 转换超级步骤

The Transform superstep uses the data vault from the process step as its source data. The transformations are tuned to work with the five dimensions of the data vault. As a reminder of what the structure looks like, see Figure [10-1](#Fig1).![A435693_1_En_10_Fig1_HTML.jpg](img/A435693_1_En_10_Fig1_HTML.jpg) Figure 10-1Five categories of data

### 维度合并

The data vault consists of five categories of data, with linked relationships and additional characteristics in satellite hubs. To perform dimension consolidation, you start with a given relationship in the data vault and construct a sun model for that relationship, as shown in Figure [10-2](#Fig2).![A435693_1_En_10_Fig2_HTML.jpg](img/A435693_1_En_10_Fig2_HTML.jpg) Figure 10-2T-P-O-L-E hHigh-level design I will cover the example of a person being born, to illustrate the consolidation process. Note You will need a Python editor to complete this chapter, so please start it, then we can proceed to the data science required. Open a new file in the Python editor and save it as Transform-Gunnarsson_is_Born.py in directory ..\VKHCG\01-Vermeulen\04-Transform. You will require the Python ecosystem , so set it up by adding the following to your editor (you must set up the ecosystem by adding the libraries): import sys import os from datetime import datetime from datetime import timedelta from pytz import timezone, all_timezones import pandas as pd import sqlite3 as sq from pandas.io import sql import uuid pd.options.mode.chained_assignment = None Find the working directory of the examples. ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') Set up the company you are processing. Company='01-Vermeulen' Add the company work space: sDataBaseDir=Base + '/' + Company + '/04-Transform/SQLite' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) sDatabaseName=sDataBaseDir + '/Vermeulen.db' conn1 = sq.connect(sDatabaseName) Add the data vault. sDataVaultDir=Base + '/88-DV' if not os.path.exists(sDataVaultDir):     os.makedirs(sDataVaultDir) sDatabaseName=sDataVaultDir + '/datavault.db' conn2 = sq.connect(sDatabaseName) Add the data warehouse. sDataWarehousetDir=Base + '/99-DW' if not os.path.exists(sDataWarehousetDir):     os.makedirs(sDataWarehousetDir) sDatabaseName=sDataVaultDir + '/datawarehouse.db' conn3 = sq.connect(sDatabaseName) Execute the Python code now, to set up the basic ecosystem. Note The new data structure, called a data warehouse, is in directory ../99-DW. The data warehouse is the only data structure delivered from the Transform step. Let’s look at a real-world scenario. Guðmundur Gunnarsson was born on December 20, 1960, at 9:15 in Landspítali, Hringbraut 101, 101 Reykjavík, Iceland. Following is what I would expect to find in the data vault.

#### 时间

You need a date and time of December 20, 1960, at 9:15 in Reykjavík, Iceland. Enter the following code into your editor (you start with a UTC time): print('Time Category') print('UTC Time') BirthDateUTC = datetime(1960,12,20,10,15,0) BirthDateZoneUTC=BirthDateUTC.replace(tzinfo=timezone('UTC')) BirthDateZoneUTCStr=BirthDateZoneUTC.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)") BirthDateLocal=BirthDate.strftime("%Y-%m-%d %H:%M:%S") print(BirthDateZoneUTCStr) Formulate a Reykjavík local time. print('Birth Date in Reykjavik :') BirthZone = 'Atlantic/Reykjavik' BirthDate = BirthDateZoneUTC.astimezone(timezone(BirthZone)) BirthDateStr=BirthDate.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)") print(BirthDateStr) You have successfully discovered the time key for the time hub and the time zone satellite for Atlantic/Reykjavik.

*   时间中心:您有一个 UTC 日期和时间，时间是 1960 年 12 月 20 日 9:15，在冰岛的雷克雅未克，如下所示:1960-12-20 10:15:00 (UTC) (+0000)
*   时间卫星:雷克雅未克出生日期:1960-12-20 09:15:00 (-01) (-0100)

Now you can save your work, by adding the following to your code. Build a data frame, as follows: IDZoneNumber=str(uuid.uuid4()) sDateTimeKey=BirthDateZoneStr.replace(' ','-').replace(':','-') TimeLine=[('ZoneBaseKey', ['UTC']),               ('IDNumber', [IDZoneNumber]),               ('DateTimeKey', [sDateTimeKey]),               ('UTCDateTimeValue', [BirthDateZoneUTC]),               ('Zone', [BirthZone]),               ('DateTimeValue', [BirthDateStr])] TimeFrame = pd.DataFrame.from_items(TimeLine) Create the time hub. TimeHub=TimeFrame[['IDNumber','ZoneBaseKey','DateTimeKey','DateTimeValue']] TimeHubIndex=TimeHub.set_index(['IDNumber'],inplace=False) sTable = 'Hub-Time-Gunnarsson' print('\n#################################') print('Storing :',sDatabaseName,'\n Table:',sTable) print('\n#################################') TimeHubIndex.to_sql(sTable, conn2, if_exists="replace") sTable = 'Dim-Time-Gunnarsson' TimeHubIndex.to_sql(sTable, conn3, if_exists="replace") Create the time satellite. TimeSatellite=TimeFrame[['IDNumber','DateTimeKey','Zone','DateTimeValue']] TimeSatelliteIndex=TimeSatellite.set_index(['IDNumber'],inplace=False) BirthZoneFix=BirthZone.replace(' ','-').replace('/','-') sTable = 'Satellite-Time-' + BirthZoneFix + '-Gunnarsson' print('\n#################################') print('Storing :',sDatabaseName,'\n Table:',sTable) print('\n#################################') TimeSatelliteIndex.to_sql(sTable, conn2, if_exists="replace") sTable = 'Dim-Time-' + BirthZoneFix + '-Gunnarsson' TimeSatelliteIndex.to_sql(sTable, conn3, if_exists="replace") Well done. You now have a Time category. The next category is Person.

#### 人

You must record that Guðmundur Gunnarsson was born on December 20, 1960, at 9:15 in Iceland. Add the following to your existing code: print('Person Category') FirstName = 'Guðmundur' LastName = 'Gunnarsson' print('Name:',FirstName,LastName) print('Birth Date:',BirthDateLocal) print('Birth Zone:',BirthZone) print('UTC Birth Date:',BirthDateZoneStr) You just created the person in the person hub. IDPersonNumber=str(uuid.uuid4()) PersonLine=[('IDNumber', [IDPersonNumber]),               ('FirstName', [FirstName]),               ('LastName', [LastName]),               ('Zone', ['UTC']),               ('DateTimeValue', [BirthDateZoneStr])] PersonFrame = pd.DataFrame.from_items(PersonLine) ################################################################ TimeHub=PersonFrame TimeHubIndex=TimeHub.set_index(['IDNumber'],inplace=False) ################################################################ sTable = 'Hub-Person-Gunnarsson' print('\n#################################') print('Storing :',sDatabaseName,'\n Table:',sTable) print('\n#################################') TimeHubIndex.to_sql(sTable, conn2, if_exists="replace") sTable = 'Dim-Person-Gunnarsson' TimeHubIndex.to_sql(sTable, conn3, if_exists="replace") Well done. You now have a person hub. Can you create a location hub and satellite for National University Hospital of Iceland?

*   纬度:北纬 64° 08 ' 10.80 "或西经 64.136332788 "-西经 21° 55 ' 22.79 "或-21.9999999997

What else can you process from the address data?

*   大地坐标:Local Name
*   Hringbraut 101:街道地址
*   101:邮政编码
*   雷克雅未克:城市名称
*   冰岛:国家

You can add to the information an entry in the location hub, if the specific latitude and longitude for Reykjavík does not exist yet. You can then add to a satellite named Hospitals the key from the location hub, plus all the extra information. Alternatively, you can simply add it as a satellite called BuildingAt with the same extra information and a type_of_building indicator. Whatever way you perform it, I advise discussing it with your customer. Note You can data-mine many characteristics from a simple connection to a location or building. Can you now create an event hub for “Birth”? Yes, you can—easily—but you have an event called “Born.” Use that. Note I have found that various people have diverse names for similar events, for example, born and birth, death and deceased. I have a library of events that we model for the same event. I keep these in an additional table, and when we create an event for one of the words in a similar list, we also create it for the other lists. For example, if we create an event for born and birth, that facilitates queries from the Transform step. If you asked “Who was born?” or “Whose birth was it?” you would get results retrieved from the data vault into the data warehouse. So, what about Object? Yes, you could use the genus/species data. Guðmundur is a Homo sapiens (human). Try to find the correct record to link to. Tip You should now understand that even a simple business action, such as “Guðmundur Gunnarsson’s Birth,” can generate massive amounts of extra data points. I suggest you try and look at major events in your own life: your birth, first day at school. What if you look at the actions in the event hub, which apply to you? If you understand that with enhancement of the data vault you can achieve major enhancement for the Transform step’s dimension consolidation, you have achieved a major milestone in your capability as a data scientist.

### 太阳模型

The use of sun models is a technique that enables the data scientist to perform consistent dimension consolidation, by explaining the intended data relationship with the business, without exposing it to the technical details required to complete the transformation processing. So, let’s revisit our business statement: Guðmundur Gunnarsson was born on December 20, 1960, at 9:15 in Landspítali, Hringbraut 101, 101 Reykjavík, Iceland.

#### 人对时间孙模型

The following sun model in Figure [10-3](#Fig3) explains the relationship between the Time and Person categories in the data vault.![A435693_1_En_10_Fig3_HTML.jpg](img/A435693_1_En_10_Fig3_HTML.jpg) Figure 10-3Person-to-Time sun model The sun model is constructed to show all the characteristics from the two data vault hub categories you are planning to extract. It explains how you will create two dimensions and a fact via the Transform step from Figure [10-3](#Fig3). You will create two dimensions (Person and Time) with one fact (PersonBornAtTime), as shown in Figure [10-4](#Fig4).![A435693_1_En_10_Fig4_HTML.jpg](img/A435693_1_En_10_Fig4_HTML.jpg) Figure 10-4Person-to-Time sun model (explained) The sun model explains that Guðmundur Gunnarsson was born on December 20, 1960, at 9:15 in Iceland. Tip I practice my sun modeling by taking everyday interactions and drawing sun models for those activities. That way, I can draw most relationships with ease. Practice makes perfect is true of this technique.

#### 模板模型

I also have several printed copies of sun models with blank entries (Figure [10-5](#Fig5)), to quickly record relationships while I am meeting with clients.![A435693_1_En_10_Fig5_HTML.jpg](img/A435693_1_En_10_Fig5_HTML.jpg) Figure 10-5Template for a simple sun model Tip I normally keep a blank copy of this in Microsoft Visio or LibreOffice Draw, to give me a quick head start. I have trained several of my clients to create sun models, and they now write their requirements, by adding these as extra information. It gives them a sense of ownership.

#### 人对物太阳模型

Can you find the dimensions and facts on these already completed sun models? (See Figure [10-6](#Fig6).)![A435693_1_En_10_Fig6_HTML.jpg](img/A435693_1_En_10_Fig6_HTML.jpg) Figure 10-6Sun model for the PersonIsSpecies fact How did you progress? In Figure [10-6](#Fig6), dimensions are Person and Object. Fact is PersonIsSpecies. This describes Guðmundur Gunnarsson as a Homo sapiens.

#### 人到位置太阳模型

If you have dimensions Person and Location and fact PersonAtLocation, you have successfully read the sun model. (Figure [10-7](#Fig7)).![A435693_1_En_10_Fig7_HTML.jpg](img/A435693_1_En_10_Fig7_HTML.jpg) Figure 10-7Sun model for PersonAtLocation fact This describes that Guðmundur Gunnarsson was born at:

*   纬度:北纬 64 度 08 分 10.80 秒或 64 度 58867.88888688886
*   经度:-西经 21° 55 ' 22.79 "或-21.929999998781

#### 个人对事件孙模型

And this event sun model? Can you extract the information in Figure [10-8](#Fig8)?![A435693_1_En_10_Fig8_HTML.jpg](img/A435693_1_En_10_Fig8_HTML.jpg) Figure 10-8Sun model for PersonBorn fact If you have dimensions Person and Event and fact PersonBorn, you have successfully read the sun model (Figure [10-8](#Fig8)).

#### 转换步骤的孙模型

I will guide you through the creation of the Transform step, as shown by the sun model in Figure [10-9](#Fig9).![A435693_1_En_10_Fig9_HTML.jpg](img/A435693_1_En_10_Fig9_HTML.jpg) Figure 10-9Sun model for PersonBornAtTime fact You must build three items: dimension Person, dimension Time, and fact PersonBornAtTime. Open your Python editor and create a file named Transform-Gunnarsson-Sun-Model.py in directory ..\VKHCG\01-Vermeulen\04-Transform. Here is your basic ecosystem: ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os from datetime import datetime from pytz import timezone import pandas as pd import sqlite3 as sq import uuid pd.options.mode.chained_assignment = None ################################################################ if sys.platform == 'linux' or sys.platform == ' Darwin':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ Company='01-Vermeulen' ################################################################ sDataBaseDir=Base + '/' + Company + '/04-Transform/SQLite' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataBaseDir + '/Vermeulen.db' conn1 = sq.connect(sDatabaseName) ################################################################ sDataWarehousetDir=Base + '/99-DW' if not os.path.exists(sDataWarehousetDir):     os.makedirs(sDataWarehousetDir) ################################################################ sDatabaseName=sDataWarehousetDir + '/datawarehouse.db' conn2 = sq.connect(sDatabaseName) ################################################################ Here is your Time dimension: print('\n#################################') print('Time Dimension') BirthZone = 'Atlantic/Reykjavik' BirthDateUTC = datetime(1960,12,20,10,15,0) BirthDateZoneUTC=BirthDateUTC.replace(tzinfo=timezone('UTC')) BirthDateZoneStr=BirthDateZoneUTC.strftime("%Y-%m-%d %H:%M:%S") BirthDateZoneUTCStr=BirthDateZoneUTC.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)") BirthDate = BirthDateZoneUTC.astimezone(timezone(BirthZone)) BirthDateStr=BirthDate.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)") BirthDateLocal=BirthDate.strftime("%Y-%m-%d %H:%M:%S") ################################################################ IDTimeNumber=str(uuid.uuid4()) TimeLine=[('TimeID', [IDTimeNumber]),           ('UTCDate', [BirthDateZoneStr]),           ('LocalTime', [BirthDateLocal]),           ('TimeZone', [BirthZone])] TimeFrame = pd.DataFrame.from_items(TimeLine) ################################################################ DimTime=TimeFrame DimTimeIndex=DimTime.set_index(['TimeID'],inplace=False) ################################################################ sTable = 'Dim-Time' print('\n#################################') print('Storing :',sDatabaseName,'\n Table:',sTable) print('\n#################################') DimTimeIndex.to_sql(sTable, conn1, if_exists="replace") DimTimeIndex.to_sql(sTable, conn2, if_exists="replace") Well done. you have a Time dimension. Let’s build the Person dimension. print('\n#################################') print('Dimension Person') print('\n#################################') FirstName = 'Guðmundur' LastName = 'Gunnarsson' ############################################################### IDPersonNumber=str(uuid.uuid4()) PersonLine=[('PersonID', [IDPersonNumber]),               ('FirstName', [FirstName]),               ('LastName', [LastName]),               ('Zone', ['UTC']),               ('DateTimeValue', [BirthDateZoneStr])] PersonFrame = pd.DataFrame.from_items(PersonLine) ################################################################ DimPerson=PersonFrame DimPersonIndex=DimPerson.set_index(['PersonID'],inplace=False) ################################################################ sTable = 'Dim-Person' print('\n#################################') print('Storing :',sDatabaseName,'\n Table:',sTable) print('\n#################################') DimPersonIndex.to_sql(sTable, conn1, if_exists="replace") DimPersonIndex.to_sql(sTable, conn2, if_exists="replace") Finally, we add the fact, as follows: print('\n#################################') print('Fact - Person - time') print('\n#################################') IDFactNumber=str(uuid.uuid4()) PersonTimeLine=[('IDNumber', [IDFactNumber]),                 ('IDPersonNumber', [IDPersonNumber]),                 ('IDTimeNumber', [IDTimeNumber])] PersonTimeFrame = pd.DataFrame.from_items(PersonTimeLine) ################################################################ FctPersonTime=PersonTimeFrame FctPersonTimeIndex=FctPersonTime.set_index(['IDNumber'],inplace=False) ################################################################ sTable = 'Fact-Person-Time' print('\n#################################') print('Storing:',sDatabaseName,'\n Table:',sTable) print('\n#################################') FctPersonTimeIndex.to_sql(sTable, conn1, if_exists="replace") FctPersonTimeIndex.to_sql(sTable, conn2, if_exists="replace") Can you now understand how to formulate a sun model and build the required dimensions and facts?

## 构建数据仓库

As you have performed so well up to now, I will ask you to open the Transform-Sun-Models.py file from directory ..\VKHCG\01-Vermeulen\04-Transform. Note The Python program will build you a good and solid warehouse with which to try new data science techniques. Please be patient; it will supply you with a big push forward. Execute the program and have some coffee once it runs. You can either code it for yourself or simply upload the code from the samples directory. The code follows, if you want to understand the process. Can you understand the transformation from data vault to data warehouse? ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os from datetime import datetime from pytz import timezone import pandas as pd import sqlite3 as sq import uuid pd.options.mode.chained_assignment = None ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ Company='01-Vermeulen' ################################################################ sDataBaseDir=Base + '/' + Company + '/04-Transform/SQLite' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataBaseDir + '/Vermeulen.db' conn1 = sq.connect(sDatabaseName) ################################################################ sDataVaultDir=Base + '/88-DV' if not os.path.exists(sDataVaultDir):     os.makedirs(sDataVaultDir) ################################################################ sDatabaseName=sDataVaultDir + '/datavault.db' conn2 = sq.connect(sDatabaseName) ################################################################ sDataWarehouseDir=Base + '/99-DW' if not os.path.exists(sDataWarehouseDir):     os.makedirs(sDataWarehouseDir) ################################################################ sDatabaseName=sDataWarehouseDir + '/datawarehouse.db' conn3 = sq.connect(sDatabaseName) ################################################################ sSQL=" SELECT DateTimeValue FROM [Hub-Time];" DateDataRaw=pd.read_sql_query(sSQL, conn2) DateData=DateDataRaw.head(1000) print(DateData) ################################################################ print('\n#################################') print('Time Dimension') print('\n#################################') t=0 mt=DateData.shape[0] for i in range(mt):     BirthZone = ('Atlantic/Reykjavik','Europe/London','UCT')     for j in range(len(BirthZone)):         t+=1         print(t,mt*3)         BirthDateUTC = datetime.strptime(DateData['DateTimeValue'][i],"%Y-%m-%d %H:%M:%S")         BirthDateZoneUTC=BirthDateUTC.replace(tzinfo=timezone('UTC'))         BirthDateZoneStr=BirthDateZoneUTC.strftime("%Y-%m-%d %H:%M:%S")         BirthDateZoneUTCStr=BirthDateZoneUTC.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)")         BirthDate = BirthDateZoneUTC.astimezone(timezone(BirthZone[j]))         BirthDateStr=BirthDate.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)")         BirthDateLocal=BirthDate.strftime("%Y-%m-%d %H:%M:%S")         ################################################################         IDTimeNumber=str(uuid.uuid4())         TimeLine=[('TimeID', [str(IDTimeNumber)]),                   ('UTCDate', [str(BirthDateZoneStr)]),                   ('LocalTime', [str(BirthDateLocal)]),                   ('TimeZone', [str(BirthZone)])]         if t==1:             TimeFrame = pd.DataFrame.from_items(TimeLine)         else:             TimeRow = pd.DataFrame.from_items(TimeLine)             TimeFrame=TimeFrame.append(TimeRow) ################################################################ DimTime=TimeFrame DimTimeIndex=DimTime.set_index(['TimeID'],inplace=False) ################################################################ sTable = 'Dim-Time' print('\n#################################') print('Storing :',sDatabaseName,'\n Table:',sTable) print('\n#################################') DimTimeIndex.to_sql(sTable, conn1, if_exists="replace") DimTimeIndex.to_sql(sTable, conn3, if_exists="replace") ################################################################ sSQL=" SELECT " + \       " FirstName," + \       " SecondName," + \       " LastName," + \       " BirthDateKey " + \       " FROM [Hub-Person];" PersonDataRaw=pd.read_sql_query(sSQL, conn2) PersonData=PersonDataRaw.head(1000) ################################################################ print('\n#################################') print('Dimension Person') print('\n#################################') t=0 mt=DateData.shape[0] for i in range(mt):     t+=1     print(t,mt)     FirstName = str(PersonData["FirstName"])     SecondName = str(PersonData["SecondName"])     if len(SecondName) > 0:         SecondName=""     LastName = str(PersonData["LastName"])     BirthDateKey = str(PersonData["BirthDateKey"])     ###############################################################     IDPersonNumber=str(uuid.uuid4())     PersonLine=[('PersonID', [str(IDPersonNumber)]),                   ('FirstName', [FirstName]),                   ('SecondName', [SecondName]),                   ('LastName', [LastName]),                   ('Zone', [str('UTC')]),                   ('BirthDate', [BirthDateKey])]     if t==1:         PersonFrame = pd.DataFrame.from_items(PersonLine)     else:         PersonRow = pd.DataFrame.from_items(PersonLine)         PersonFrame = PersonFrame.append(PersonRow) ################################################################ DimPerson=PersonFrame print(DimPerson) DimPersonIndex=DimPerson.set_index(['PersonID'],inplace=False) ################################################################ sTable = 'Dim-Person' print('\n#################################') print('Storing :',sDatabaseName,'\n Table:',sTable) print('\n#################################') DimPersonIndex.to_sql(sTable, conn1, if_exists="replace") DimPersonIndex.to_sql(sTable, conn3, if_exists="replace") ############################################################### You should now have a good example of a data vault to data warehouse transformation. Congratulations on your progress!

## 借助数据科学实现转型

You now have a good basis for data exploration and preparation from the data lake into data vault and from the data vault to the data warehouse. I will now introduce you to the basic data science to transform your data into insights. You must understand a selected set of basic investigation practices, to gain insights from your data.

### 数据探索和准备的步骤

You must keep detailed notes of what techniques you employed to prepare the data. Make sure you keep your data traceability matrix up to date after each data engineering step has completed. Update your Data Lineage and Data Providence, to ensure that you have both the technical and business details for the entire process. Now, I will take you through a small number of the standard transform checkpoints, to ensure that your data science is complete.

### 缺失值处理

You must describe in detail what the missing value treatments are for the data lake transformation. Make sure you take your business community with you along the journey. At the end of the process, they must trust your techniques and results. If they trust the process, they will implement the business decisions that you, as a data scientist, aspire to achieve.

#### 为什么需要缺失值处理

Explain with notes on the data traceability matrix why there is missing data in the data lake. Remember: Every inconsistency in the data lake is conceivably the missing insight your customer is seeking from you as a data scientist. So, find them and explain them. Your customer will exploit them for business value.

#### 为什么数据有缺失值

The 5 Whys is the technique that helps you to get to the root cause of your analysis. The use of cause-and-effect fishbone diagrams will assist you to resolve those questions. I have found the following common reasons for missing data:

*   升级期间重命名的数据字段
*   从旧系统到新系统的迁移过程，其中映射不完整
*   主题专家在装载规范中提供的表格不正确
*   数据根本没有记录，因为无法获得
*   法律原因，由于数据保护立法，如一般数据保护法规(GDPR)，导致数据条目上出现“不可处理”标签
*   别人的“坏”数据科学。人和项目都会犯错误，您必须在自己的数据科学中修复他们的错误。

Warning Ensure that your data science processing is not the reason you are missing data. That is the quickest way to lose your customer’s trust.

#### 有哪些方法处理缺失值？

During your progress through the supersteps, you have used many techniques to resolve missing data. Record them in your lineage, but also make sure you collect precisely how each technique applies to the processing flow.

#### 异常检测和处理技术

During the processing, you will have detected several outliers that are not complying with your expected ranges, e.g., you expected “Yes” or “No” but found some “N/A”s, or you expected number ranges between 1 and 10 but got 11, 12, and 13 also. These out-of-order items are the outliers. I suggest you treat them as you treat the missing data. Make sure that your customer agrees with the process, as it will affect the insights you will process and their decisions.

##### 椭圆形信封

I will introduce a function called EllipticEnvelope. The basic idea is to assume that a data set is from a known distribution and then evaluate any entries not complying to that assumption. Fitting an elliptic envelope is one of the more common techniques used to detect outliers in a Gaussian distributed data set. The scikit-learn package provides an object covariance.EllipticEnvelope that fits a robust covariance estimate to the data, and thus fits an ellipse to the central data points, ignoring points outside the central mode. For instance, if the inlier data are Gaussian distributed, it will estimate the inlier location and covariance in a robust way (i.e., without being influenced by outliers). The Mahalanobis distances obtained from this estimate are used to derive a measure of outlyingness. If you want more in-depth details on the function, visit [http://scikit-learn.org/stable/modules/generated/sklearn.covariance.EllipticEnvelope.html](http://scikit-learn.org/stable/modules/generated/sklearn.covariance.EllipticEnvelope.html) . Example: # -*- coding: utf-8 -*- import numpy as np from sklearn.covariance import EllipticEnvelope from sklearn.svm import OneClassSVM import matplotlib.pyplot as plt import matplotlib.font_manager from sklearn.datasets import load_boston # Get data X1 = load_boston()['data'][:, [8, 10]]  # two clusters X2 = load_boston()['data'][:, [5, 12]]  # "banana"-shaped # Define "classifiers" to be used classifiers = {     "Empirical Covariance": EllipticEnvelope(support_fraction=1.,                                              contamination=0.261),     "Robust Covariance (Minimum Covariance Determinant)":     EllipticEnvelope(contamination=0.261),     "OCSVM": OneClassSVM(nu=0.261, gamma=0.05)} The classifiers assume you are testing the normal data entries. Let’s look at EllipticEnvelope(support_fraction=1., contamination=0.261). The support_fraction is the portion of the complete population you want to use to determine the border between inliers and outliers. In this case, we use 1, which means 100%. The contamination is the indication of what portion of the population could be outliers, hence, the amount of contamination of the data set, i.e., the proportion of outliers in the data set. In your case, this is set to 0.261 against a possible 0.5, more generally described as 26.1% contamination. The EllipticEnvelope(contamination=0.261) is only a change of the included population, by using the defaults for all the settings, except for contamination that is set to 26.1%. Third is another type of detection called sklearn.svm.OneClassSVM, which is discussed later in this chapter. colors = ['m', 'g', 'b'] legend1 = {} legend2 = {} # Learn a frontier for outlier detection with several classifiers xx1, yy1 = np.meshgrid(np.linspace(-8, 28, 500), np.linspace(3, 40, 500)) xx2, yy2 = np.meshgrid(np.linspace(3, 10, 500), np.linspace(-5, 45, 500)) for i, (clf_name, clf) in enumerate(classifiers.items()):     fig1a=plt.figure(1)     fig1a.set_size_inches(10, 10)     clf.fit(X1)     Z1 = clf.decision_function(np.c_[xx1.ravel(), yy1.ravel()])     Z1 = Z1.reshape(xx1.shape)     legend1[clf_name] = plt.contour(         xx1, yy1, Z1, levels=[0], linewidths=2, colors=colors[i])     plt.figure(2)     clf.fit(X2)     Z2 = clf.decision_function(np.c_[xx2.ravel(), yy2.ravel()])     Z2 = Z2.reshape(xx2.shape)     legend2[clf_name] = plt.contour(         xx2, yy2, Z2, levels=[0], linewidths=2, colors=colors[i]) legend1_values_list = list(legend1.values()) legend1_keys_list = list(legend1.keys()) # Plot the results (= shape of the data points cloud) fig1b=plt.figure(1)  # two clusters fig1b.set_size_inches(10, 10) plt.title("Outlier detection on a real data set (boston housing)") plt.scatter(X1[:, 0], X1[:, 1], color="black") bbox_args = dict(boxstyle="round", fc="0.8") arrow_args = dict(arrowstyle="->") plt.annotate("several confounded points", xy=(24, 19),              xycoords="data", textcoords="data",              xytext=(13, 10), bbox=bbox_args, arrowprops=arrow_args) plt.xlim((xx1.min(), xx1.max())) plt.ylim((yy1.min(), yy1.max())) plt.legend((legend1_values_list[0].collections[0],             legend1_values_list[1].collections[0],             legend1_values_list[2].collections[0]),            (legend1_keys_list[0], legend1_keys_list[1], legend1_keys_list[2]),            loc="upper center",            prop=matplotlib.font_manager.FontProperties(size=12)) plt.ylabel("accessibility to radial highways") plt.xlabel("pupil-teacher ratio by town") legend2_values_list = list(legend2.values()) legend2_keys_list = list(legend2.keys()) fig2a=plt.figure(2)  # "banana" shape fig2a.set_size_inches(10, 10) plt.title("Outlier detection on a real data set (boston housing)") plt.scatter(X2[:, 0], X2[:, 1], color="black") plt.xlim((xx2.min(), xx2.max())) plt.ylim((yy2.min(), yy2.max())) plt.legend((legend2_values_list[0].collections[0],             legend2_values_list[1].collections[0],             legend2_values_list[2].collections[0]),            (legend2_keys_list[0], legend2_keys_list[1], legend2_keys_list[2]),            loc="upper center",            prop=matplotlib.font_manager.FontProperties(size=12)) plt.ylabel("% lower status of the population") plt.xlabel("average number of rooms per dwelling") plt.show() There are a few other outlier detection techniques you can investigate.

##### 隔离森林

One efficient way of performing outlier detection in high-dimensional data sets is to use random forests. The [ensemble.IsolationForest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html%23sklearn.ensemble.IsolationForest%23sklearn.ensemble.IsolationForest) tool “isolates” observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. Because recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node. This path length, averaged over a forest of such random trees, is a measure of normality and our decision function. Random partitioning produces a noticeably shorter path for anomalies. Hence, when a forest of random trees collectively produces shorter path lengths for particular samples, they are highly likely to be anomalies. See [http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html#sklearn.ensemble.IsolationForest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html%23sklearn.ensemble.IsolationForest) .

##### 新颖性检测

Novelty detection simply performs an evaluation in which we add one more observation to a data set. Is the new observation so different from the others that we can doubt that it is regular? (I.e., does it come from the same distribution?) Or, on the contrary, is it so similar to the other that we cannot distinguish it from the original observations? This is the question addressed by the novelty detection tools and methods. The sklearn.svm.OneClassSVM tool is a good example of this unsupervised outlier detection technique. For more information, see [http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM](http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html%23sklearn.svm.OneClassSVM) .

##### 局部异常因素

An efficient way to perform outlier detection on moderately high-dimensional data sets is to use the local outlier factor (LOF) algorithm. The neighbors.LocalOutlierFactor algorithm computes a score (called a local outlier factor) reflecting the degree of abnormality of the observations. It measures the local density deviation of a given data point with respect to its neighbors. The idea is to detect the samples that have a substantially lower density than their neighbors. In practice, the local density is obtained from the k-nearest neighbors. The LOF score of an observation is equal to the ratio of the average local density of its k-nearest neighbors and its own local density. A normal instance is expected to have a local density like that of its neighbors, while abnormal data are expected to have a much smaller local density. See [http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html#sklearn.neighbors.LocalOutlierFactor](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html%23sklearn.neighbors.LocalOutlierFactor) for additional information. When the amount of contamination is known, this algorithm illustrates three different ways of performing—based on a robust estimator of covariance, which assumes that the data are Gaussian distributed—and performs better than the one-class SVM, in that case. The first is the one-class SVM, which has the ability to capture the shape of the data set and, hence, perform better when the data is strongly non-Gaussian, i.e., with two well-separated clusters. The second is the isolation forest algorithm, which is based on random forests and, hence, better adapted to large-dimensional settings, even if it performs quite well in the example you will perform next. Third is the local outlier factor to measure the local deviation of a given data point with respect to its neighbors, by comparing their local density. Example: Here, the underlying truth about inliers and outliers is given by the points’ colors. The orange-filled area indicates which points are reported as inliers by each method. You assume to know the fraction of outliers in the data sets, and the following provides an example of what you can achieve. Open your Python editor, set up this ecosystem, and investigate the preceding techniques. import numpy as np from scipy import stats import matplotlib.pyplot as plt import matplotlib.font_manager from sklearn import svm from sklearn.covariance import EllipticEnvelope from sklearn.ensemble import IsolationForest from sklearn.neighbors import LocalOutlierFactor rng = np.random.RandomState(42) # Your example settings n_samples = 200 outliers_fraction = 0.25 clusters_separation = [0, 1, 2] # define two outlier detection tools to be compared classifiers = {     "One-Class SVM": svm.OneClassSVM(nu=0.95 * outliers_fraction + 0.05,                                      kernel="rbf", gamma=0.1),     "Robust covariance": EllipticEnvelope(contamination=outliers_fraction),     "Isolation Forest": IsolationForest(max_samples=n_samples,                                         contamination=outliers_fraction,                                         random_state=rng),     "Local Outlier Factor": LocalOutlierFactor(         n_neighbors=35,         contamination=outliers_fraction)} # Compare given classifiers under given settings xx, yy = np.meshgrid(np.linspace(-7, 7, 100), np.linspace(-7, 7, 100)) n_inliers = int((1\. - outliers_fraction) * n_samples) n_outliers = int(outliers_fraction * n_samples) ground_truth = np.ones(n_samples, dtype=int) ground_truth[-n_outliers:] = -1 # Fit the problem with varying cluster separation for i, offset in enumerate(clusters_separation):     np.random.seed(42)     # Data generation     X1 = 0.3 * np.random.randn(n_inliers // 2, 2) - offset     X2 = 0.3 * np.random.randn(n_inliers // 2, 2) + offset     X = np.r_[X1, X2]     # Add outliers     X = np.r_[X, np.random.uniform(low=-6, high=6, size=(n_outliers, 2))]     # Fit the model     plt.figure(figsize=(9, 7))     for i, (clf_name, clf) in enumerate(classifiers.items()):         # fit the data and tag outliers         if clf_name == "Local Outlier Factor":             y_pred = clf.fit_predict(X)             scores_pred = clf.negative_outlier_factor_         else:             clf.fit(X)             scores_pred = clf.decision_function(X)             y_pred = clf.predict(X)         threshold = stats.scoreatpercentile(scores_pred,                                             100 * outliers_fraction)         n_errors = (y_pred != ground_truth).sum()         # plot the levels lines and the points         if clf_name == "Local Outlier Factor":             # decision_function is private for LOF             Z = clf._decision_function(np.c_[xx.ravel(), yy.ravel()])         else:             Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])         Z = Z.reshape(xx.shape)         subplot = plt.subplot(2, 2, i + 1)         subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7),                          cmap=plt.cm.Blues_r)         a = subplot.contour(xx, yy, Z, levels=[threshold],                             linewidths=2, colors="red")         subplot.contourf(xx, yy, Z, levels=[threshold, Z.max()],                          colors='orange')         b = subplot.scatter(X[:-n_outliers, 0], X[:-n_outliers, 1], c="white",                             s=20, edgecolor="k")         c = subplot.scatter(X[-n_outliers:, 0], X[-n_outliers:, 1], c="black",                             s=20, edgecolor="k")         subplot.axis('tight')         subplot.legend(             [a.collections[0], b, c],             ['learned decision function', 'true inliers', 'true outliers'],             prop=matplotlib.font_manager.FontProperties(size=10),             loc='lower right')         subplot.set_xlabel("%d. %s (errors: %d)" % (i + 1, clf_name, n_errors))         subplot.set_xlim((-7, 7))         subplot.set_ylim((-7, 7))     plt.subplots_adjust(0.04, 0.1, 0.96, 0.94, 0.1, 0.26)     plt.suptitle("Outlier detection") plt.show() You can now detect outliers in a data set. That is a major achievement, as the most interesting features of data science work are found in these outliers and why they exist. This supports you in performing the feature engineering of the data sets.

#### 什么是特征工程？

Feature engineering is your core technique to determine the important data characteristics in the data lake and ensure they get the correct treatment through the steps of processing. Make sure that any featuring extraction process technique is documented in the data transformation matrix and the data lineage.

### 常见特征提取技术

I will introduce you to several common feature extraction techniques that will help you to enhance any existing data warehouse, by applying data science to the data in the warehouse.

#### 扔掉

Binning is a technique that is used to reduce the complexity of data sets, to enable the data scientist to evaluate the data with an organized grouping technique. Binning is a good way for you to turn continuous data into a data set that has specific features that you can evaluate for patterns. A simple example is the cost of candy in your local store, which might range anywhere from a penny to ten dollars, but if you subgroup the price into, say, a rounded-up value that then gives you a range of five values against five hundred, you have just reduced your processing complexity to 1/500th of what it was before. There are several good techniques, which I will discuss next. I have two binning techniques that you can use against the data sets. Open your Python editor and try these examples. The first technique is to use the digitize function. import numpy data = numpy.random.random(100) bins = numpy.linspace(0, 1, 10) digitized = numpy.digitize(data, bins) bin_means = [data[digitized == i].mean() for i in range(1, len(bins))] print(bin_means) The second is to use the histogram function. bin_means2 = (numpy.histogram(data, bins, weights=data)[0] /              numpy.histogram(data, bins)[0]) print(bin_means2) This transform technique can be used to reduce the location dimension into three latitude bins and four longitude bins. You will require the NumPy library. import numpy as np Set up the latitude and longitude data sets. LatitudeData = np.array(range(-90,90,1)) LongitudeData = np.array(range(-180,180,1)) Set up the latitude and longitude data bins. LatitudeBins = np.array(range(-90,90,45)) LongitudeBins = np.array(range(-180,180,60)) Digitize the data sets with the data bins. LatitudeDigitized = np.digitize(LatitudeData, LatitudeBins) LongitudeDigitized = np.digitize(LongitudeData, LongitudeBins) Calculate the mean against the bins: LatitudeBinMeans = [LatitudeData[LatitudeDigitized == i].mean() for i in range(1, len(LatitudeBins))] LongitudeBinMeans = [LongitudeData[LongitudeDigitized == i].mean() for i in range(1, len(LongitudeBins))] Well done. You have the three latitude bins and four longitude bins. print(LatitudeBinMeans) print(LongitudeBinMeans) You can also use the histogram function to achieve similar results. LatitudeBinMeans2 = (np.histogram(LatitudeData, LatitudeBins,\              weights=LatitudeData)[0] /              np.histogram(LatitudeData, LatitudeBins)[0]) LongitudeBinMeans2 = (np.histogram(LongitudeData, LongitudeBins,\              weights=LongitudeData)[0] /              np.histogram(LongitudeData, LongitudeBins)[0]) print(LatitudeBinMeans2) print(LongitudeBinMeans2) Now you can apply two different techniques for binning.

#### 求平均值

The use of averaging enables you to reduce the amount of records you require to report any activity that demands a more indicative, rather than a precise, total. Example: Create a model that enables you to calculate the average position for ten sample points. First, set up the ecosystem. import numpy as np import pandas as pd Create two series to model the latitude and longitude ranges. LatitudeData = pd.Series(np.array(range(-90,91,1))) LongitudeData = pd.Series(np.array(range(-180,181,1))) You then select 10 samples for each range: LatitudeSet=LatitudeData.sample(10) LongitudeSet=LongitudeData.sample(10) Calculate the average of each. LatitudeAverage = np.average(LatitudeSet) LongitudeAverage = np.average(LongitudeSet) See your results. print('Latitude') print(LatitudeSet) print('Latitude (Avg):',LatitudeAverage) print('##############') print('Longitude') print(LongitudeSet) print('Longitude (Avg):', LongitudeAverage) You can now calculate the average of any range of numbers. If you run the code several times, you should get different samples. (See Transform-Average-Location.py in ..\VKHCG\01-Vermeulen\04-Transform for code.) Challenge Question One: In directory ..\VKHCG\01-Vermeulen\00-RawData, there is a file called IP_DATA_CORE.csv. Try to import the latitude and longitude columns and calculate the average values. Tip Look at pandas and numpy and have some fun! Challenge Question Two: Try to calculate the average amount of IP addresses:

*   每个国家
*   每个地名
*   每个邮政编码

Tip Amount of IP address = (-1*First IP Number + Last IP Number)

#### 潜在狄利克雷分配

A latent Dirichlet allocation (LDA) is a statistical model that allows sets of observations to be explained by unobserved groups that elucidates why they match or belong together within text documents. This technique is useful when investigating text from a collection of documents that are common in the data lake, as companies store all their correspondence in a data lake. This model is also useful for Twitter or e-mail analysis. Note To run the example, you will require pip install lda. In your Python editor, create a new file named Transform_Latent_Dirichlet_allocation.py in directory .. \VKHCG\01-Vermeulen\04-Transform. Following is an example of what you can achieve: import numpy as np import lda import lda.datasets X = lda.datasets.load_reuters() vocab = lda.datasets.load_reuters_vocab() titles = lda.datasets.load_reuters_titles() X.shape X.sum() You can experiment with ranges of n_topics and n_iter values to observe the impact on the process. model = lda.LDA(n_topics=50, n_iter=1500, random_state=1) model.fit(X) topic_word = model.topic_word_ n_top_words = 10 for i, topic_dist in enumerate(topic_word):     topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]     print('Topic {}: {}'.format(i, ' '.join(topic_words))) Investigate the top-ten topics. doc_topic = model.doc_topic_ for i in range(10):     print("{} (top topic: {})".format(titles[i], doc_topic[i].argmax())) Well done. You can now analyze text documents. Do you think if you had Hillman’s logistics shipping document you could use this technique? Indeed, you could, as this technique will work on any text note fields or e-mail, even Twitter entries. Note If you want to read your Twitter account feed, I suggest using the library twitter. Install it using pip install -i [https://pypi.anaconda.org/pypi/simple](https://pypi.anaconda.org/pypi/simple) twitter. Now, you can read your Twitter accounts. Tip For e-mail, I suggest the standard Python email library. See [https://docs.python.org/3.4/library/email.html](https://docs.python.org/3.4/library/email.html) . Now, you can read e-mail. The complete process is about getting data to the data lake and then guiding it through the steps: retrieve, assess, process, and transform. Tip I have found, on average, that it is only after the third recheck that 90% of the data science is complete. The process is an iterative design process. The methodology is based on a cyclic process of prototyping, testing, analyzing, and refining. Success will be achieved as you close out the prototypes. I will now explain a set of common data science terminology that you will encounter in the field of data science.

## [假设检验](http://analyticsindiamag.com/science-of-analytics/%23_blank)

Hypothesis testing is not precisely an algorithm, but it’s a must-know for any data scientist. You cannot progress until you have thoroughly mastered this technique. Hypothesis testing is the process by which statistical tests are used to check if a hypothesis is true, by using data. Based on hypothetical testing, data scientists choose to accept or reject the hypothesis. When an event occurs, it can be a trend or happen by chance. To check whether the event is an important occurrence or just happenstance, hypothesis testing is necessary. There are many tests for hypothesis testing, but the following two are most popular.

### t 检验

A t-test is a popular statistical test to make inferences about single means or inferences about two means or variances, to check if the two groups’ means are statistically different from each other, where n < 30 and standard deviation is unknown. For more information on the t-test, see [https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.t.html#scipy.stats.t](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.t.html%23scipy.stats.t) and [https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html) . Example: First you set up the ecosystem, as follows: import numpy as np from scipy.stats import ttest_ind, ttest_ind_from_stats from scipy.special import stdtr Create a set of “unknown” data. (This can be a set of data you want to analyze.) In the following example, there are five random data sets. You can select them by having nSet equal 1, 2, 3, 4, or 5. nSet=1 if nSet==1:     a = np.random.randn(40)     b = 4*np.random.randn(50) if nSet==2:     a=np.array([27.1,22.0,20.8,23.4,23.4,23.5,25.8,22.0,24.8,20.2,21.9,22.1,22.9,20.5,24.4])     b=np.array([27.1,22.0,20.8,23.4,23.4,23.5,25.8,22.0,24.8,20.2,21.9,22.1,22.9,20.5,24.41]) if nSet==3:     a=np.array([17.2,20.9,22.6,18.1,21.7,21.4,23.5,24.2,14.7,21.8])     b=np.array([21.5,22.8,21.0,23.0,21.6,23.6,22.5,20.7,23.4,21.8,20.7,21.7,21.5,22.5,23.6,21.5,22.5,23.5,21.5,21.8]) if nSet==4:     a=np.array([19.8,20.4,19.6,17.8,18.5,18.9,18.3,18.9,19.5,22.0])     b=np.array([28.2,26.6,20.1,23.3,25.2,22.1,17.7,27.6,20.6,13.7,23.2,17.5,20.6,18.0,23.9,21.6,24.3,20.4,24.0,13.2]) if nSet==5:     a = np.array([55.0, 55.0, 47.0, 47.0, 55.0, 55.0, 55.0, 63.0])     b = np.array([55.0, 56.0, 47.0, 47.0, 55.0, 55.0, 55.0, 63.0]) First, you will use scipy’s t-test. # Use scipy.stats.ttest_ind. t, p = ttest_ind(a, b, equal_var=False) print("t-Test_ind:            t = %g  p = %g" % (t, p)) Second, you will get the descriptive statistics. # Compute the descriptive statistics of a and b. abar = a.mean() avar = a.var(ddof=1) na = a.size adof = na - 1 bbar = b.mean() bvar = b.var(ddof=1) nb = b.size bdof = nb - 1 # Use scipy.stats.ttest_ind_from_stats. t2, p2 = ttest_ind_from_stats(abar, np.sqrt(avar), na,                               bbar, np.sqrt(bvar), nb,                               equal_var=False) print("t-Test_ind_from_stats: t = %g  p = %g" % (t2, p2)) Look at Welch’s t-test formula. Third, you can use the formula to calculate the test. # Use the formulas directly. tf = (abar - bbar) / np.sqrt(avar/na + bvar/nb) dof = (avar/na + bvar/nb)**2 / (avar**2/(na**2*adof) + bvar**2/(nb**2*bdof)) pf = 2*stdtr(dof, -np.abs(tf)) print("Formula:               t = %g  p = %g" % (tf, pf)) P=1-p if P < 0.001:     print('Statistically highly significant:',P) else:     if P < 0.05:         print('Statistically significant:',P)     else:         print('No conclusion') You should see results like this: t-Test_ind:            t = -1.5827  p = 0.118873 t-Test_ind_from_stats: t = -1.5827  p = 0.118873 Formula:               t = -1.5827  p = 0.118873 No conclusion Your results are as follows. The p means the probability, or how likely your results are occurring by chance. In this case, it’s 11%, or p-value = 0.11. The p-value results can be statistically significant when P < 0.05 and statistically highly significant if P < 0.001 (a less than one-in-a-thousand chance of being wrong). So, in this case, it cannot be noted as either statistically significant or statistically highly significant, as it is 0.11. Go back and change nSet at the beginning of the code you just entered. Remember: I mentioned that you can select them by nSet = 1, 2, 3, 4, or 5. Retest the data sets. You should now see that the p-value changes, and you should also understand that the test gives you a good indicator of whether the two results sets are similar. Can you find the 99.99%?

### 卡方检验

A chi-square (or squared [χ<sup class="calibre7">2</sup>]) test is used to examine if two distributions of categorical variables are significantly different from each other. Example: Try these examples that are generated with five different data sets. First, set up the ecosystem. import numpy as np import scipy.stats as st Create data sets. np.random.seed(1) # Create sample data sets. nSet=1 if nSet==1:     a = abs(np.random.randn(50))     b = abs(50*np.random.randn(50)) if nSet==2:     a=np.array([27.1,22.0,20.8,23.4,23.4,23.5,25.8,22.0,24.8,20.2,21.9,22.1,22.9,20.5,24.4])     b=np.array([27.1,22.0,20.8,23.4,23.4,23.5,25.8,22.0,24.8,20.2,21.9,22.1,22.9,20.5,24.41]) if nSet==3:     a=np.array([17.2,20.9,22.6,18.1,21.7,21.4,23.5,24.2,14.7,21.8])     b=np.array([21.5,22.8,21.0,23.0,21.6,23.6,22.5,20.7,23.4,21.8]) if nSet==4:     a=np.array([19.8,20.4,19.6,17.8,18.5,18.9,18.3,18.9,19.5,22.0])     b=np.array([28.2,26.6,20.1,23.3,25.2,22.1,17.7,27.6,20.6,13.7]) if nSet==5:     a = np.array([55.0, 55.0, 47.0, 47.0, 55.0, 55.0, 55.0, 63.0])     b = np.array([55.0, 56.0, 47.0, 47.0, 55.0, 55.0, 55.0, 63.0]) obs = np.array([a,b]) Perform the test. chi2, p, dof, expected = st.chi2_contingency(obs) Display the results. msg = "Test Statistic : {}\np-value: {}\ndof: {}\n" print( msg.format( chi2, p , dof,expected) ) P=1-p if P < 0.001:     print('Statistically highly significant:',P) else:     if P < 0.05:         print('Statistically significant:',P)     else:         print('No conclusion') Can you understand what the test indicates as you cycle the nSet through samples 1–5?

## 过度拟合和欠拟合

Overfitting and underfitting are major problems when data scientists retrieve data insights from the data sets they are investigating. Overfitting is when the data scientist generates a model to fit a training set perfectly, but it does not generalize well against an unknown future real-world data set, as the data science is so tightly modeled against the known data set, the most minor outlier simply does not get classified correctly. The solution only works for the specific data set and no other data set. For example, if a person earns more than $150,000, that person is rich; otherwise, the person is poor. A binary classification of rich or poor will not work, as can a person earning about $145,000 be poor? Underfitting the data scientist’s results into the data insights has been so nonspecific that to some extent predictive models are inappropriately applied or questionable as regards to insights. For example, your person classifier has a 48% success rate to determine the sex of a person. That will never work, as with a binary guess, you could achieve a 50% rating by simply guessing. Your data science must offer a significant level of insight for you to secure the trust of your customers, so they can confidently take business decisions, based on the insights you provide them.

### 多项式特征

The polynomic formula is the following: ![ $$ \left({a}_1x+{b}_1\right)\left({a}_2x+{b}_2\right)={a}_1{a}_2{x}^2+\left({a}_1{b}_2+{a}_2{b}_1\right)x+{b}_1{b}_2 $$ ](img/A435693_1_En_10_Chapter_IEq1.gif). The polynomial feature extraction can use a chain of polynomic formulas to create a hyperplane that will subdivide any data sets into the correct cluster groups. The higher the polynomic complexity, the more precise the result that can be achieved. Example: import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import Ridge from sklearn.preprocessing import PolynomialFeatures from sklearn.pipeline import make_pipeline def f(x):     """ function to approximate by polynomial interpolation"""     return x * np.sin(x) # generate points used to plot x_plot = np.linspace(0, 10, 100) # generate points and keep a subset of them x = np.linspace(0, 10, 100) rng = np.random.RandomState(0) rng.shuffle(x) x = np.sort(x[:20]) y = f(x) # create matrix versions of these arrays X = x[:, np.newaxis] X_plot = x_plot[:, np.newaxis] colors = ['teal', 'yellowgreen', 'gold'] lw = 2 plt.plot(x_plot, f(x_plot), color="cornflowerblue", linewidth=lw,          label="Ground Truth") plt.scatter(x, y, color="navy", s=30, marker="o", label="training points") for count, degree in enumerate([3, 4, 5]):     model = make_pipeline(PolynomialFeatures(degree), Ridge())     model.fit(X, y)     y_plot = model.predict(X_plot)     plt.plot(x_plot, y_plot, color=colors[count], linewidth=lw,              label="Degree %d" % degree) plt.legend(loc='lower left') plt.show() Now that you know how to generate a polynomic formula to match any curve, I will show you a practical application using a real-life data set.

### 常见数据拟合问题

These higher order polynomic formulas are, however, more prone to overfitting, while lower order formulas are more likely to underfit. It is a delicate balance between two extremes that support good data science. Example: import numpy as np import matplotlib.pyplot as plt from sklearn.pipeline import Pipeline from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_val_score def true_fun(X):     return np.cos(1.5 * np.pi * X) np.random.seed(0) n_samples = 30 degrees = [1, 4, 15] X = np.sort(np.random.rand(n_samples)) y = true_fun(X) + np.random.randn(n_samples) * 0.1 plt.figure(figsize=(14, 5)) for i in range(len(degrees)):     ax = plt.subplot(1, len(degrees), i + 1)     plt.setp(ax, xticks=(), yticks=())     polynomial_features = PolynomialFeatures(degree=degrees[i],                                              include_bias=False)     linear_regression = LinearRegression()     pipeline = Pipeline([("polynomial_features", polynomial_features),                          ("linear_regression", linear_regression)])     pipeline.fit(X[:, np.newaxis], y)     # Evaluate the models using crossvalidation     scores = cross_val_score(pipeline, X[:, np.newaxis], y,                              scoring="neg_mean_squared_error", cv=10)     X_test = np.linspace(0, 1, 100)     plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label="Model")     plt.plot(X_test, true_fun(X_test), label="True function")     plt.scatter(X, y, edgecolor="b", s=20, label="Samples")     plt.xlabel("x")     plt.ylabel("y")     plt.xlim((0, 1))     plt.ylim((-2, 2))     plt.legend(loc="best")     plt.title("Degree {}\nMSE = {:.2e}(+/- {:.2e})".format(         degrees[i], -scores.mean(), scores.std())) plt.show()

## 精确召回

Precision-recall is a useful measure for successfully predicting when classes are extremely imbalanced. In information retrieval,

*   精确度是对结果相关性的一种度量。
*   召回是对返回多少真正相关的结果的度量。

### 精确回忆曲线

The precision-recall curve shows the trade-off between precision and recall for different thresholds. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both shows that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall). A system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the training labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels. An ideal system with high precision and high recall will return many results, with all results labeled correctly. Precision (P) is defined as the number of true positives (Tp) over the number of true positives (Tp) plus the number of false positives (Fp).![ $$ P=\frac{\mathrm{Tp}}{Tp+ Fp} $$ ](img/A435693_1_En_10_Chapter_Equa.gif) Recall (R) is defined as the number of true positives (Tp) over the number of true positives (Tp) plus the number of false negatives (Fn).![ $$ R=\frac{\mathrm{Tp}}{Tp+ Fn} $$ ](img/A435693_1_En_10_Chapter_Equb.gif) The true negative rate (TNR) is the rate that indicates the recall of the negative items.![ $$ TNR=\frac{\mathrm{Tn}}{Tn+ Fp} $$ ](img/A435693_1_En_10_Chapter_Equc.gif) Accuracy (A) is defined as![ $$ A=\frac{\mathrm{Tp}+\mathrm{Tn}}{Tp+ Fp+ Tn+ Fn} $$ ](img/A435693_1_En_10_Chapter_Equd.gif)

### 敏感性和特异性

Sensitivity and specificity are statistical measures of the performance of a [binary classification](https://en.wikipedia.org/wiki/Binary_classification%23Binary%20classification) [test](https://en.wikipedia.org/wiki/Classification_rule%23Classification%20rule), also known in statistics as a [classification function](https://en.wikipedia.org/wiki/Statistical_classification%23Statistical%20classification). Sensitivity (also called the true positive rate, the [recall](https://en.wikipedia.org/wiki/Precision_and_recall%23Definition_.28classification_context.29%23Precision%20and%20recall), or probability of detection) measures the proportion of positives that are correctly identified as such (e.g., the percentage of sick people who are correctly identified as having the condition). Specificity (also called the true negative rate) measures the proportion of negatives that are correctly identified as such (e.g., the percentage of healthy people who are correctly identified as not having the condition).

### f1-测量

The F1-score is a measure that combines precision and recall in the harmonic mean of precision and recall.![ $$ F1=2\ast \frac{\mathrm{P}\ast \mathrm{R}}{P+R} $$ ](img/A435693_1_En_10_Chapter_Eque.gif) Note The precision may not decrease with recall. I have found the following sklearn functions useful when calculation these measures:

*   sk learn . metrics . average _ precision _ score
*   sklearn.metrics.recall_score
*   sk learn . metrics . precision _ score
*   硬化公制 f1_score

I suggest you practice with sets of data and understand these parameters and what affects them positively and negatively. They are the success indicators of your data science. Example: ########################################################################### # In binary classification settings # -------------------------------------------------------- # # Create simple data # .................. # # Try to differentiate the two first classes of the iris data from sklearn import svm, datasets from sklearn.model_selection import train_test_split import numpy as np iris = datasets.load_iris() X = iris.data y = iris.target # Add noisy features random_state = np.random.RandomState(0) n_samples, n_features = X.shape X = np.c_[X, random_state.randn(n_samples, 200 * n_features)] # Limit to the two first classes, and split into training and test X_train, X_test, y_train, y_test = train_test_split(X[y < 2], y[y < 2],                                                     test_size=.5,                                                     random_state=random_state) # Create a simple classifier classifier = svm.LinearSVC(random_state=random_state) classifier.fit(X_train, y_train) y_score = classifier.decision_function(X_test) ########################################################################### # Compute the average precision score # ................................... from sklearn.metrics import average_precision_score average_precision = average_precision_score(y_test, y_score) print('Average precision-recall score: {0:0.2f}'.format(       average_precision)) ########################################################################## # Plot the Precision-Recall curve # ................................ from sklearn.metrics import precision_recall_curve import matplotlib.pyplot as plt precision, recall, _ = precision_recall_curve(y_test, y_score) plt.step(recall, precision, color="b", alpha=0.2,          where='post') plt.fill_between(recall, precision, step="post", alpha=0.2,                  color='b') plt.xlabel('Recall') plt.ylabel('Precision') plt.ylim([0.0, 1.05]) plt.xlim([0.0, 1.0]) plt.title('2-class Precision-Recall curve: AUC={0:0.2f}'.format(           average_precision)) ########################################################################### # In multi-label settings # ------------------------ # # Create multi-label data, fit, and predict # ........................................... # # We create a multi-label dataset, to illustrate the precision-recall in # multi-label settings from sklearn.preprocessing import label_binarize # Use label_binarize to be multi-label like settings Y = label_binarize(y, classes=[0, 1, 2]) n_classes = Y.shape[1] # Split into training and test X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.5,                                        random_state=random_state) # We use OneVsRestClassifier for multi-label prediction from sklearn.multiclass import OneVsRestClassifier # Run classifier classifier = OneVsRestClassifier(svm.LinearSVC(random_state=random_state)) classifier.fit(X_train, Y_train) y_score = classifier.decision_function(X_test) ########################################################################### # The average precision score in multi-label settings # .................................................... from sklearn.metrics import precision_recall_curve from sklearn.metrics import average_precision_score # For each class precision = dict() recall = dict() average_precision = dict() for i in range(n_classes):     precision[i], recall[i], _ = precision_recall_curve(Y_test[:, i],                                                         y_score[:, i])     average_precision[i] = average_precision_score(Y_test[:, i], y_score[:, i]) # A "micro-average": quantifying score on all classes jointly precision["micro"], recall["micro"], _ = precision_recall_curve(Y_test.ravel(),     y_score.ravel()) average_precision["micro"] = average_precision_score(Y_test, y_score,                                                      average="micro") print('Average precision score, micro-averaged over all classes: {0:0.2f}'       .format(average_precision["micro"])) ########################################################################### # Plot the micro-averaged Precision-Recall curve # ............................................... # plt.figure() plt.step(recall['micro'], precision['micro'], color="b", alpha=0.2,          where='post') plt.fill_between(recall["micro"], precision["micro"], step="post", alpha=0.2,                  color='b') plt.xlabel('Recall') plt.ylabel('Precision') plt.ylim([0.0, 1.05]) plt.xlim([0.0, 1.0]) plt.title(     'Average precision score, micro-averaged over all classes: AUC={0:0.2f}'     .format(average_precision["micro"])) ########################################################################### # Plot Precision-Recall curve for each class and iso-f1 curves # ............................................................. # from itertools import cycle # setup plot details colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal']) plt.figure(figsize=(12, 8)) f_scores = np.linspace(0.2, 0.8, num=4) lines = [] labels = [] for f_score in f_scores:     x = np.linspace(0.01, 1)     y = f_score * x / (2 * x - f_score)     l, = plt.plot(x[y >= 0], y[y >= 0], color="gray", alpha=0.2)     plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02)) lines.append(l) labels.append('iso-f1 curves') l, = plt.plot(recall["micro"], precision["micro"], color="gold", lw=2) lines.append(l) labels.append('micro-average Precision-recall (area = {0:0.2f})'               ''.format(average_precision["micro"])) for i, color in zip(range(n_classes), colors):     l, = plt.plot(recall[i], precision[i], color=color, lw=2)     lines.append(l)     labels.append('Precision-recall for class {0} (area = {1:0.2f})'                   ''.format(i, average_precision[i])) fig = plt.gcf() fig.subplots_adjust(bottom=0.25) plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel('Recall') plt.ylabel('Precision') plt.title('Extension of Precision-Recall curve to multi-class') plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14)) plt.show()

### 受试者工作特性(ROC)分析曲线

A receiver operating characteristic (ROC) analysis curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The ROC curve plots the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The true positive rate is also known as sensitivity, recall, or probability of detection. You will find the ROC analysis curves useful for evaluating whether your classification or feature engineering is good enough to determine the value of the insights you are finding. This helps with repeatable results against a real-world data set. So, if you suggest that your customers should take a specific action as a result of your findings, ROC analysis curves will support your advice and insights but also relay the quality of the insights at given parameters. You should now open your Python editor and create the following ecosystem. Example: import numpy as np from scipy import interp import matplotlib.pyplot as plt from sklearn import svm, datasets from sklearn.metrics import roc_curve, auc from sklearn.model_selection import StratifiedKFold # ######################################################################### # Data IO and generation # Import some data to play with iris = datasets.load_iris() X = iris.data y = iris.target X, y = X[y != 2], y[y != 2] n_samples, n_features = X.shape # Add noisy features random_state = np.random.RandomState(0) X = np.c_[X, random_state.randn(n_samples, 200 * n_features)] # ######################################################################### # Classification and ROC analysis # Run classifier with cross-validation and plot ROC curves cv = StratifiedKFold(n_splits=6) classifier = svm.SVC(kernel='linear', probability=True,                      random_state=random_state) tprs = [] aucs = [] mean_fpr = np.linspace(0, 1, 100) i = 0 for train, test in cv.split(X, y):     probas_ = classifier.fit(X[train], y[train]).predict_proba(X[test])     # Compute ROC curve and area the curve     fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])     tprs.append(interp(mean_fpr, fpr, tpr))     tprs[-1][0] = 0.0     roc_auc = auc(fpr, tpr)     aucs.append(roc_auc)     plt.plot(fpr, tpr, lw=1, alpha=0.3,              label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))     i += 1 plt.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r",          label='Luck', alpha=.8) mean_tpr = np.mean(tprs, axis=0) mean_tpr[-1] = 1.0 mean_auc = auc(mean_fpr, mean_tpr) std_auc = np.std(aucs) plt.plot(mean_fpr, mean_tpr, color="b",          label=r'Mean ROC (AUC = %0.2f $\pm$ %0.2f)' % (mean_auc, std_auc),          lw=2, alpha=.8) std_tpr = np.std(tprs, axis=0) tprs_upper = np.minimum(mean_tpr + std_tpr, 1) tprs_lower = np.maximum(mean_tpr - std_tpr, 0) plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color="gray", alpha=.2,                  label=r'$\pm$ 1 std. dev.') plt.xlim([-0.05, 1.05]) plt.ylim([-0.05, 1.05]) plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('Receiver operating characteristic example') plt.legend(loc="lower right") plt.show()

## 交叉验证测试

Cross-validation is a model validation technique for evaluating how the results of a statistical analysis will generalize to an independent data set. It is mostly used in settings where the goal is the prediction. Knowing how to calculate a test such as this enables you to validate the application of your model on real-world, i.e., independent data sets. I will guide you through a test. Open your Python editor and create the following ecosystem: import numpy as np from sklearn.model_selection import cross_val_score from sklearn import datasets, svm import matplotlib.pyplot as plt digits = datasets.load_digits() X = digits.data y = digits.target Let’s pick three different kernels and compare how they will perform. kernels=['linear', 'poly', 'rbf'] for kernel in kernels:     svc = svm.SVC(kernel=kernel)     C_s = np.logspace(-15, 0, 15)     scores = list()     scores_std = list()     for C in C_s:         svc.C = C         this_scores = cross_val_score(svc, X, y, n_jobs=1)         scores.append(np.mean(this_scores))         scores_std.append(np.std(this_scores)) You must plot your results.     Title="Kernel:>" + kernel     fig=plt.figure(1, figsize=(8, 6))     plt.clf()     fig.suptitle(Title, fontsize=20)     plt.semilogx(C_s, scores)     plt.semilogx(C_s, np.array(scores) + np.array(scores_std), 'b--')     plt.semilogx(C_s, np.array(scores) - np.array(scores_std), 'b--')     locs, labels = plt.yticks()     plt.yticks(locs, list(map(lambda x: "%g" % x, locs)))     plt.ylabel('Cross-Validation Score')     plt.xlabel('Parameter C')     plt.ylim(0, 1.1)     plt.show() Well done. You can now perform cross-validation of your results.

## 单变量分析

Univariate analysis is the simplest form of analyzing data. Uni means “one,” so your data has only one variable. It doesn’t deal with causes or relationships, and its main purpose is to describe. It takes data, summarizes that data, and finds patterns in the data. The patterns found in univariate data include central tendency (mean, mode, and median) and dispersion, range, variance, maximum, minimum, quartiles (including the interquartile range), and standard deviation. Example: How many students are graduating with a data science degree? You have several options for describing data using a univariate approach . You can use frequency distribution tables, frequency polygons, histograms, bar charts, or pie charts.

## 双变量分析

Bivariate analysis is when two variables are analyzed together for any possible association or empirical relationship, such as, for example, the correlation between gender and graduation with a data science degree? Canonical correlation in the experimental context is to take two sets of variables and see what is common between the two sets. Graphs that are appropriate for bivariate analysis depend on the type of variable. For two continuous variables, a scatterplot is a common graph. When one variable is categorical and the other continuous, a box plot is common, and when both are categorical, a mosaic plot is common.

## 多变量分析

Multivariate data analysis refers to any statistical technique used to analyze data that arises from more than one variable. This essentially models reality, in which each situation, product, or decision involves more than a single variable. More than two variables are analyzed together for any possible association or interactions. Example: What is the correlation between gender, country of residence, and graduation with a data science degree? Any statistical modeling exercise, such as regression, decision tree, SVM, and clustering are multivariate in nature. The analysis is used when more than two variables determine the final outcome.

## [线性回归](http://analyticsindiamag.com/what-should-one-learn-to-be-a-data-scientist/%23_blank)

Linear regression is a statistical modeling technique that endeavors to model the relationship between an explanatory variable and a dependent variable, by fitting the observed data points on a linear equation, for example, modeling the body mass index (BMI) of individuals by using their weight. Warning A regression analysis with one dependent variable and four independent variables is not a multivariate regression. It’s a multiple regression. Multivariate analysis always refers to the dependent variable.

### 简单线性回归

Linear regression is used if there is a relationship or significant association between the variables. This can be checked by scatterplots. If no linear association appears between the variables, fitting a linear regression model to the data will not provide a useful model. A linear regression line has equations in the following form: Y = a + bX, Where, X = explanatory variable and Y = dependent variable b = slope of the line a = intercept (the value of y when x = 0) Example: ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import pandas as pd import sqlite3 as sq import matplotlib.pyplot as plt import numpy as np ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ ################################################################ Company='01-Vermeulen' ################################################################ sDataBaseDir=Base + '/' + Company + '/04-Transform/SQLite' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataBaseDir + '/Vermeulen.db' conn1 = sq.connect(sDatabaseName) ################################################################ sDataVaultDir=Base + '/88-DV' if not os.path.exists(sDataVaultDir):     os.makedirs(sDataVaultDir) ################################################################ sDatabaseName=sDataVaultDir + '/datavault.db' conn2 = sq.connect(sDatabaseName) ################################################################ sDataWarehouseDir=Base + '/99-DW' if not os.path.exists(sDataWarehouseDir):     os.makedirs(sDataWarehouseDir) ################################################################ sDatabaseName=sDataWarehouseDir + '/datawarehouse.db' conn3 = sq.connect(sDatabaseName) ################################################################ t=0 tMax=((300-100)/10)*((300-30)/5) for heightSelect in range(100,300,10):     for weightSelect in range(30,300,5):         height = round(heightSelect/100,3)         weight = int(weightSelect)         bmi = weight/(height*height)         if bmi <= 18.5:             BMI_Result=1         elif bmi > 18.5 and bmi < 25:             BMI_Result=2         elif bmi > 25 and bmi < 30:             BMI_Result=3         elif bmi > 30:             BMI_Result=4         else:             BMI_Result=0         PersonLine=[('PersonID', [str(t)]),                   ('Height', [height]),                   ('Weight', [weight]),                   ('bmi', [bmi]),                   ('Indicator', [BMI_Result])]         t+=1         print('Row:',t,'of',tMax)         if t==1:             PersonFrame = pd.DataFrame.from_items(PersonLine)         else:             PersonRow = pd.DataFrame.from_items(PersonLine)             PersonFrame = PersonFrame.append(PersonRow) ################################################################ DimPerson=PersonFrame DimPersonIndex=DimPerson.set_index(['PersonID'],inplace=False) ################################################################ sTable = 'Transform-BMI' print('\n#################################') print('Storing :',sDatabaseName,'\n Table:',sTable) print('\n#################################') DimPersonIndex.to_sql(sTable, conn1, if_exists="replace") ################################################################ ################################################################ sTable = 'Person-Satellite-BMI' print('\n#################################') print('Storing :',sDatabaseName,'\n Table:',sTable) print('\n#################################') DimPersonIndex.to_sql(sTable, conn2, if_exists="replace") ################################################################ ################################################################ sTable = 'Dim-BMI' print('\n#################################') print('Storing :',sDatabaseName,'\n Table:',sTable) print('\n#################################') DimPersonIndex.to_sql(sTable, conn3, if_exists="replace") ################################################################ fig = plt.figure() PlotPerson=DimPerson[DimPerson['Indicator']==1] x=PlotPerson['Height'] y=PlotPerson['Weight'] plt.plot(x, y, ".") PlotPerson=DimPerson[DimPerson['Indicator']==2] x=PlotPerson['Height'] y=PlotPerson['Weight'] plt.plot(x, y, "o") PlotPerson=DimPerson[DimPerson['Indicator']==3] x=PlotPerson['Height'] y=PlotPerson['Weight'] plt.plot(x, y, "+") PlotPerson=DimPerson[DimPerson['Indicator']==4] x=PlotPerson['Height'] y=PlotPerson['Weight'] plt.plot(x, y, "^") plt.axis('tight') plt.title("BMI Curve") plt.xlabel("Height(meters)") plt.ylabel("Weight(kg)") plt.plot() Now that we have identified the persons at risk, we can study the linear regression of these diabetics. Note You will use the standard diabetes data sample set that is installed with the sklearn library, the reason being the protection of medical data. As this data is in the public domain, you are permitted to access it. Warning When you process people’s personal information, you are accountable for any issues your processing causes. So, work with great care. Note In the next example, we will use a medical data set that is part of the standard sklearn library. This ensures that you are not working with unauthorized medical results. You set up the ecosystem, as follows: import matplotlib.pyplot as plt import numpy as np from sklearn import datasets, linear_model from sklearn.metrics import mean_squared_error, r2_score Load the data set. # Load the diabetes dataset diabetes = datasets.load_diabetes() Perform feature development. # Use only one feature diabetes_X = diabetes.data[:, np.newaxis, 2] Split the data into train and test data sets. diabetes_X_train = diabetes_X[:-30] diabetes_X_test = diabetes_X[-50:] Split the target into train and test data sets. diabetes_y_train = diabetes.target[:-30] diabetes_y_test = diabetes.target[-50:] Generate a linear regression model. regr = linear_model.LinearRegression() Train the model using the training sets. regr.fit(diabetes_X_train, diabetes_y_train) Create predictions, using the testing set. diabetes_y_pred = regr.predict(diabetes_X_test) Display the coefficients. print('Coefficients: \n', regr.coef_) Display the mean squared error. print("Mean squared error: %.2f"       % mean_squared_error(diabetes_y_test, diabetes_y_pred)) Display the variance score. (Tip: A score of 1 is perfect prediction.) print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred)) Plot outputs. plt.scatter(diabetes_X_test, diabetes_y_test,  color='black') plt.plot(diabetes_X_test, diabetes_y_pred, color="blue", linewidth=3) plt.xticks(()) plt.yticks(()) plt.axis('tight') plt.title("Diabetes") plt.xlabel("BMI") plt.ylabel("Age") plt.show() Well done. You have successfully calculated the BMI and determined the diabetes rate of our staff.

### RANSAC 线性回归

RANSAC (RANdom SAmple Consensus) is an iterative algorithm for the robust estimation of parameters from a subset of inliers from the complete data set. An advantage of RANSAC is its ability to do robust estimation of the model parameters, i.e., it can estimate the parameters with a high degree of accuracy, even when a significant number of outliers is present in the data set. The process will find a solution, because it is so robust. Generally, this technique is used when dealing with image processing, owing to noise in the domain. See [http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html) . Example: import numpy as np from matplotlib import pyplot as plt from sklearn import linear_model, datasets n_samples = 1000 n_outliers = 50 X, y, coef = datasets.make_regression(n_samples=n_samples, n_features=1,                                       n_informative=1, noise=10,                                       coef=True, random_state=0) # Add outlier data np.random.seed(0) X[:n_outliers] = 3 + 0.5 * np.random.normal(size=(n_outliers, 1)) y[:n_outliers] = -3 + 10 * np.random.normal(size=n_outliers) # Fit line using all data lr = linear_model.LinearRegression() lr.fit(X, y) # Robustly fit linear model with RANSAC algorithm ransac = linear_model.RANSACRegressor() ransac.fit(X, y) inlier_mask = ransac.inlier_mask_ outlier_mask = np.logical_not(inlier_mask) # Predict data of estimated models line_X = np.arange(X.min(), X.max())[:, np.newaxis] line_y = lr.predict(line_X) line_y_ransac = ransac.predict(line_X) # Compare estimated coefficients print("Estimated coefficients (true, linear regression, RANSAC):") print(coef, lr.coef_, ransac.estimator_.coef_) lw = 2 plt.scatter(X[inlier_mask], y[inlier_mask], color="yellowgreen", marker='.',             label='Inliers') plt.scatter(X[outlier_mask], y[outlier_mask], color="gold", marker='.',             label='Outliers') plt.plot(line_X, line_y, color="navy", linewidth=lw, label='Linear regressor') plt.plot(line_X, line_y_ransac, color="cornflowerblue", linewidth=lw,          label='RANSAC regressor') plt.legend(loc='lower right') plt.xlabel("Input") plt.ylabel("Response") plt.show() This regression technique is extremely useful when using robotics and robot vision in which the robot requires the regression of the changes between two data frames or data sets.

### 霍夫变换

The Hough transform is a feature extraction technique used in image analysis, computer vision, and digital image processing. The purpose of the technique is to find imperfect instances of objects within a certain class of shapes, by a voting procedure. This voting procedure is carried out in a parameter space, from which object candidates are obtained as local maxima in a so-called accumulator space that is explicitly constructed by the algorithm for computing the Hough transform. See [http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.hough_line](http://scikit-image.org/docs/dev/api/skimage.transform.html%23skimage.transform.hough_line) , [http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.hough_line_peaks](http://scikit-image.org/docs/dev/api/skimage.transform.html%23skimage.transform.hough_line_peaks) , and [http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.probabilistic_hough_line](http://scikit-image.org/docs/dev/api/skimage.transform.html%23skimage.transform.probabilistic_hough_line) . With the help of the Hough transformation, this regression improves the resolution of the RANSAC technique, which is extremely useful when using robotics and robot vision in which the robot requires the regression of the changes between two data frames or data sets to move through an environment.

## [逻辑回归](http://analyticsindiamag.com/developing-a-predictive-model-to-identify-festive-shoppers/%23_blank)

Logistic regression is the technique to find relationships between a set of input variables and an output variable (just like any regression), but the output variable, in this case, is a binary outcome (think of 0/1 or yes/no).

### 简单逻辑回归

I will guide you through a simple logistic regression that only compares two values. A real-word business example would be the study of a traffic jam at a certain location in London, using a binary variable. The output is a categorical: yes or no. Hence, is there a traffic jam? Yes or no? The probability of occurrence of traffic jams can be dependent on attributes such as weather condition, day of the week and month, time of day, number of vehicles, etc. Using logistic regression, you can find the best-fitting model that explains the relationship between independent attributes and traffic jam occurrence rates and predicts probability of jam occurrence. This process is called binary logistic regression . The state of the traffic changes for No = Zero to Yes = One, by moving along a curve modeled by the following code, is illustrated in Figure [10-10](#Fig10). for x in range(-10,10,1): print(math.sin(x/10)) ![A435693_1_En_10_Fig10_HTML.jpg](img/A435693_1_En_10_Fig10_HTML.jpg) Figure 10-10 Binary logistic regression I will now discuss the logistic regression, using a sample data set. Example: from sklearn import datasets, neighbors, linear_model Load the data. digits = datasets.load_digits() X_digits = digits.data y_digits = digits.target n_samples = len(X_digits) Select the train data set. X_train = X_digits[:int(.9 * n_samples)] y_train = y_digits[:int(.9 * n_samples)] X_test = X_digits[int(.9 * n_samples):] y_test = y_digits[int(.9 * n_samples):] Select the K-Neighbor classifier. knn = neighbors.KNeighborsClassifier() Select the logistic regression model. logistic = linear_model.LogisticRegression() Train the model to perform logistic regression. print('KNN score: %f' % knn.fit(X_train, y_train).score(X_test, y_test)) Apply the trained model against the test data set. print('LogisticRegression score: %f'       % logistic.fit(X_train, y_train).score(X_test, y_test)) Well done. You have just completed your next transform step, by successfully deploying a logistic regression model with a K-Neighbor classifier against the sample data set. Tip Using this simple process, I have discovered numerous thought-provoking correlations or busted myths about relationships between data values.

### 多项式逻辑回归

Multinomial logistic regression (MLR) is a form of linear regression analysis conducted when the dependent variable is nominal with more than two levels. It is used to describe data and to explain the relationship between one dependent nominal variable and one or more continuous-level (interval or ratio scale) independent variables. You can consider the nominal variable as a variable that has no intrinsic ordering. This type of data is most common in the business world, as it generally covers most data entries within the data sources and directly indicates what you could expect in the average data lake. The data has no intrinsic order or relationship. I will now guide you through the example, to show you how to deal with these data sets. You need the following libraries for the ecosystem: import time import matplotlib.pyplot as plt import numpy as np from sklearn.datasets import fetch_mldata from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.utils import check_random_state Set up the results target. sPicNameOut=../VKHCG/01-Vermeulen/04-Transform/01-EDS/02-Python/Letters.png' You must tune a few parameters. t0 = time.time() train_samples = 5000 Get the sample data from simulated data lake. mnist = fetch_mldata('MNIST original') Data engineer the data lake data. X = mnist.data.astype('float64') y = mnist.target random_state = check_random_state(0) permutation = random_state.permutation(X.shape[0]) X = X[permutation] y = y[permutation] X = X.reshape((X.shape[0], -1)) Train the data model. X_train, X_test, y_train, y_test = train_test_split(     X, y, train_size=train_samples, test_size=10000) Apply a scaler to training to inhibit overfitting. scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) Turn the tolerance (tol = 0.1) for faster convergence. clf = LogisticRegression(C=50\. / train_samples,                          multi_class='multinomial',                          penalty='l2', solver="sag", tol=0.1) Apply the model to the data set. clf.fit(X_train, y_train) sparsity = np.mean(clf.coef_ == 0) * 100 Score the model. score = clf.score(X_test, y_test) print('Best C % .4f' % clf.C_) print("Sparsity with L1 penalty: %.2f%%" % sparsity) print("Test score with L1 penalty: %.4f" % score) coef = clf.coef_.copy() Display the results. Fig=plt.figure(figsize=(15, 6)) scale = np.abs(coef).max() for i in range(10):     l1_plot = plt.subplot(2, 5, i + 1)     l1_plot.imshow(coef[i].reshape(28, 28), interpolation="nearest",                    cmap=plt.cm.RdBu, vmin=-scale, vmax=scale)     l1_plot.set_xticks(())     l1_plot.set_yticks(())     l1_plot.set_xlabel('Letter %i' % i) plt.suptitle('Classification vector for...') run_time = time.time() - t0 print('Process run in %.3f s' % run_time) plt.show() Save results to disk. Fig.savefig(sPicNameOut,dpi=300) You’re performing well with the examples. Now, you can handle data that has no intrinsic order. If you understand this process, you have successfully achieved a major milestone in your understanding of the transformation of data lakes via data vaults, by using a Transform step. Tip I normally store any results back into the data warehouse as a sun model, which then gets physicalized as facts and dimensions in the data warehouse.

### 有序逻辑回归

Ordinal logistic regression is a type of binomial logistics regression. Ordinal regression is used to predict the dependent variable with ordered multiple categories and independent variables. In other words, it is used to facilitate the interaction of dependent variables (having multiple ordered levels) with one or more independent variables. This data type is an extremely good data set to process, as you already have a relationship between the data entries that is known. Deploying your Transform step’s algorithms will give you insights into how strongly or weakly this relationship supports the data discovery process.

#### 商业问题

Rate a student on a scale from 1 to 5, to determine if he or she has the requisite qualifications (“prestige”) to join the university. The rating introduces a preference or likelihood that these items can be separated by a scale. I will take you through a data science solution, using a set of generated potential students. The question is “What criteria drives the final decision?” Here is the example: Open your Python editor and do the following. Set up the ecosystem. import sys import os import pandas as pd import statsmodels.api as sm import pylab as pl import numpy as np if sys.platform == 'linux':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' Retrieve StudentData. sFileName=Base + '/01-Vermeulen/00-RawData/StudentData.csv' StudentFrame = pd.read_csv(sFileName,header=0) StudentFrame.columns = ["sname", "gre", "gpa", "prestige","admit","QR","VR","gpatrue"] StudentSelect=StudentFrame[["admit", "gre", "gpa", "prestige"]] print('Record select:',StudentSelect.shape[0]) df=StudentSelect I add the following two lines if you want to speed-up the processing, but it does make the predictions less accurate. So use it if you want. #df=StudentSelect.drop_duplicates(subset=None, keep="first", inplace=False) #print('Records Unique:', df.shape[0]) Here are the columns for the data set: print(df.columns) Here is a description of the data profile: print(df.describe()) Did you spot the interesting “skewness” in the admit profile? You can investigate using this command: print(df.std()) Note The admit standard deviation is at +/-0.2, out of a range of 0 to 1.

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"></colgroup> 
| 承认 | 0.201710 |
| 美国研究生入学考试（Graduate Record Examination） | 1.030166 |
| 平均分数 | 145.214383 |
| 声望 | 1.030166 |

This is a very selective admissions model! Not a lot of students make the cut. Let’s investigate the influence prestige has on being selected. print(pd.crosstab(df['admit'], df['prestige'], rownames=['admit']))

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 声望 | one | Two | three | four |
| 承认 |   |   |   |   |
| Zero | Sixteen thousand eight hundred and ten | Fifteen thousand eight hundred and ten | Fifteen thousand eight hundred and ten | Seven thousand nine hundred and five |
| one | Zero | One thousand | One thousand | Five hundred |

There is clearly a criterion related to prestige. A student requires a minimum of Prestige = 2\. Can you see this also? If you create a plot of all the columns, you will see it better. I suggest that we perform a binning reduction for your plot, by using a histogram function, as follows: df.hist() Here are your results: pl.tight_layout() pl.show() You should now create a rank for prestige. dummy_ranks = pd.get_dummies(df['prestige'], prefix="prestige") print(dummy_ranks.head()) You now have a grid that you can populate with the correct values, as per your data. You need to create an empty data frame for the regression analysis as follows: cols_to_keep = ['admit', 'gre', 'gpa'] data = df[cols_to_keep].join(dummy_ranks.loc[:, 'prestige_1':]) print(data.head()) Can you see how the data now transforms? This is called feature engineering. Next, you must add the intercept value. data['intercept'] = 1.00 You have successfully transformed the data into a structure the algorithm understands. Well done! You can now train the model. train_cols = data.columns[1:] logit = sm.Logit(data['admit'], data[train_cols]) You now have a trained model. You can fit the model for the data set. Note that the maxiter = 500 drives the processing time. You could increase or reduce it, to control the processing effort of the model. Experiment on what works for you. result = logit.fit(maxiter=500) You can now investigate the results. print('Results') print(result.summary()) Note The interesting discovery is that GPA (grade point average) score determines admittance. You will now investigate at the confidence interval of each coefficient. print(result.conf_int()) The model identifies GPA. Well done, as that is the true driver: nRank = int(GPA), a.k.a. prestige. You can now investigate the odds ratios. print(np.exp(result.params)) Can you identify the trend?

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
|   | 可能性 | 赔率(真实) |
| 美国研究生入学考试（Graduate Record Examination） | 1.02 年 2 月 | Zero point zero one |
| 平均分数 | 1.06E+00 | One point zero six three |
| 威望 _1 | 1.13E-18 | Zero |
| 威望 _2 | 2.17E-01 | Zero point two one seven |
| 威望 _3 | 2.14E+01 | Twenty-one point three seven six |
| 威望 _4 | 2.10E+03 | Two thousand one hundred and one point eight four nine |

The trend is that if a student has a GPA of > 4, he or she has 2100 to 1 odds of being admitted. Let’s investigate these odds ratios and 95% CI. params = result.params conf = result.conf_int() conf['OR'] = params conf.columns = ['2.5%', '97.5%', 'OR'] print(np.exp(conf)) Can you spot the true nature of the process? GPA is the main driver with higher GPA securing admittance. You can now generate all possible values of GRE (Graduate Record Examinations) and GPA, but I suggest you only use an evenly spaced range of ten values, from the minimum to the maximum. This a simple way of binning data to reduce complexity. Transforming the story results to a smaller result set, while keeping the core findings, is a skill a data scientist requires. Here are your bins: gres = np.linspace(data['gre'].min(), data['gre'].max(), 10) print(gres) gpas = np.linspace(data['gpa'].min(), data['gpa'].max(), 10) print(gpas) Now you must perform some more data engineering. You have to define a Cartesian function to assist with your transform process. def cartesian(arrays, out=None):     arrays = [np.asarray(x) for x in arrays]     dtype = arrays[0].dtype     n = np.prod([x.size for x in arrays])     if out is None:         out = np.zeros([n, len(arrays)], dtype=dtype)     m = int(n / arrays[0].size)     out[:,0] = np.repeat(arrays[0], m)     if arrays[1:]:         cartesian(arrays[1:], out=out[0:m,1:])         for j in range(1, arrays[0].size):             out[j*m:(j+1)*m,1:] = out[0:m,1:]     return out Now that you have your new function, I suggest you apply it to your data, as follows: You must enumerate all possibilities for the investigation. combos = pd.DataFrame(cartesian([gres, gpas, [1, 2, 3, 4], [1,]])) Now you re-create the dummy variables. combos.columns = ['gre', 'gpa', 'prestige', 'intercept'] dummy_ranks = pd.get_dummies(combos['prestige'], prefix="prestige") dummy_ranks.columns = ['prestige_1', 'prestige_2', 'prestige_3', 'prestige_4'] I now suggest that you keep only what you need for making the predictions. cols_to_keep = ['gre', 'gpa', 'prestige', 'intercept'] combos = combos[cols_to_keep].join(dummy_ranks.loc[:, 'prestige_1':]) Now calculate the predictions on the enumerated data set. combos['admit_pred'] = result.predict(combos[train_cols]) Here are your results: print(combos.head()) You need a further function. This function will isolate and plot specific characteristics of the data set. def isolate_and_plot(variable):     # isolate gre and class rank     grouped = pd.pivot_table(combos, values=['admit_pred'], index=[variable, 'prestige'],                 aggfunc=np.mean)     # make a plot     colors = 'rbgyrbgy'     for col in combos.prestige.unique():         plt_data = grouped.loc[grouped.index.get_level_values(1)==col]         pl.plot(plt_data.index.get_level_values(0), \         plt_data['admit_pred'], color=colors[int(col)])     pl.xlabel(variable)     pl.ylabel("P(admit=1)")     pl.legend(['1', '2', '3', '4'], loc='upper left', title="Prestige")     pl.title("Prob(admit=1) isolating " + variable + " and presitge")     pl.show() Now that you have your function, I suggest you investigate these two values: isolate_and_plot('gre') isolate_and_plot('gpa') The story is simple. If your grades (Grade Point Average [GPA]) is low, your Graduate Record Examinations (GRE) (capability) can get you into the university. But a high GPA indicates a successful ambition to achieve success, as the capability is assumed by the university. Congratulations! You have completed the ordinal logistic regression section. Warning Ordinal logistic regression requires massive training data sets, as they have the extra requirement of ordering the steps.

## [聚类技术](http://analyticsindiamag.com/four-hard-topics-in-analytics-are-explained-in-plain-english/%23_blank)

Clustering (or segmentation) is a kind of unsupervised learning algorithm, in which a data set is grouped into unique, differentiated clusters. Let’s say we have customer data spanning 1000 rows. Using clustering, we can group the customers into separate clusters or segments, based on the variables. In the case of customers’ data, the variables can be demographic information or purchasing activities. Clustering is an unsupervised learning algorithm, because the input is unknown to the data scientist as no training set is available to pre-train a model of the solution. You do not train the algorithm on any past input–output information, but you let the algorithm define the output for itself. Therefore, there is no right solution to a clustering algorithm, only a reasonably best-fit solution, based on business usability. Clustering is also known as unsupervised classification. There are two basic types of clustering techniques.

### 分层聚类

Hierarchical clustering is a method of cluster analysis whereby you build a hierarchy of clusters. This works well for data sets that are complex and have distinct characteristics for separated clusters of data. The following would be an example. People on a budget are more attracted to your sale items and multi-buy combinations, while more prosperous shoppers are more brand-orientated. These are two clearly different clusters, with poles-apart driving forces to buy an item. There are two design styles

#### 结块的

This is a bottom-up approach. Each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.

#### 分裂的

This is a top-down approach. All observations start in one cluster, and splits are performed recursively as one moves down the hierarchy. I will take you through the transform process to generate a hierarchical cluster. Note Clustering is applied in the field of biostatistics to assess cluster-based models of DNA sequences. Mapping the human genome was one of humanity’s greatest scientific breakthroughs, but data science is busy taking it to the next level. Investigate bioinformatics’s applications for clusters. Open your Python editor and set up a new ecosystem. from matplotlib import pyplot as plt from scipy.cluster.hierarchy import dendrogram, linkage import numpy as np You will now generate two clusters: (a), with 100 points, and (b), with 50. np.random.seed(4711) a = np.random.multivariate_normal([10, 0], [[3, 1], [1, 4]], size=[100,]) b = np.random.multivariate_normal([0, 20], [[3, 1], [1, 4]], size=[50,]) X = np.concatenate((a, b),) This creates 150 samples with 2 dimensions. print( X.shape) Let’s quickly display it. plt.scatter(X[:,0], X[:,1]) plt.show() You must generate the linkage matrix. The matrix contains the linkage criterion that determines the distance between sets of observations as a function of the pairwise distances between observations. For more information on this matrix, see [https://docs.scipy.org/doc/scipy-0.19.1/reference/generated/scipy.cluster.hierarchy.linkage.html](https://docs.scipy.org/doc/scipy-0.19.1/reference/generated/scipy.cluster.hierarchy.linkage.html) . Z = linkage(X, 'ward') Enhance the ecosystem, by adding extra libraries. from scipy.cluster.hierarchy import cophenet from scipy.spatial.distance import pdist Trigger the cophenetic correlation coefficient. The cophenetic correlation coefficient is a measure of how faithfully a dendrogram preserves the pairwise distances between the original unmodeled data points. In simple terms, how accurate is the measure. c, coph_dists = cophenet(Z, pdist(X)) print('Cophenetic Correlation Coefficient:',c) You will now calculate a full dendrogram. plt.figure(figsize=(25, 10)) plt.title('Hierarchical Clustering Dendrogram') plt.xlabel('Sample Index') plt.ylabel('Distance') dendrogram(     Z,     leaf_rotation=90,     leaf_font_size=8, ) plt.show() Now, you can truncate the cluster (show only the last p merged clusters). plt.title('Hierarchical Clustering Dendrogram (truncated)') plt.xlabel('sample index') plt.ylabel('distance') dendrogram(     Z,     truncate_mode='lastp',     p=12,     show_leaf_counts=False,     leaf_rotation=90,     leaf_font_size=12,     show_contracted=True, ) plt.show() You now must define a new function, to improve the diagram’s display. def fancy_dendrogram(*args, **kwargs):     max_d = kwargs.pop('max_d', None)     if max_d and 'color_threshold' not in kwargs:         kwargs['color_threshold'] = max_d     annotate_above = kwargs.pop('annotate_above', 0)     ddata = dendrogram(*args, **kwargs)     if not kwargs.get('no_plot', False):         plt.title('Hierarchical Clustering Dendrogram (truncated)')         plt.xlabel('sample index or (cluster size)')         plt.ylabel('distance')         for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):             x = 0.5 * sum(i[1:3])             y = d[1]             if y > annotate_above:                 plt.plot(x, y, 'o', c=c)                 plt.annotate("%.3g" % y, (x, y), xytext=(0, -5),                              textcoords='offset points',                              va='top', ha="center")         if max_d:             plt.axhline(y=max_d, c="k")     return ddata You can now use the new function against your clusters. fancy_dendrogram(     Z,     truncate_mode='lastp',     p=12,     leaf_rotation=90.,     leaf_font_size=12.,     show_contracted=True,     annotate_above=10,  # useful in small plots so annotations don't overlap ) plt.show() Let’s set the cutoff to 50. max_d = 50 Now, you just replot the new data. fancy_dendrogram(     Z,     truncate_mode='lastp',     p=12,     leaf_rotation=90.,     leaf_font_size=12.,     show_contracted=True,     annotate_above=10,     max_d=max_d, ) plt.show() Change the cut to 16. fancy_dendrogram(     Z,     truncate_mode='lastp',     p=12,     leaf_rotation=90.,     leaf_font_size=12.,     show_contracted=True,     annotate_above=10,     max_d=16, ) plt.show() Now, you add extra functions to investigate your transformation. from scipy.cluster.hierarchy import inconsistent You can investigate at a depth of five? depth = 5 incons = inconsistent(Z, depth) print(incons[-10:]) What are you seeing? Move to depth of three. depth = 3 incons = inconsistent(Z, depth) print(incons[-10:]) What do you see? You will see it better with a graph. last = Z[-10:, 2] last_rev = last[::-1] idxs = np.arange(1, len(last) + 1) plt.plot(idxs, last_rev) You should now look at the acceleration. acceleration = np.diff(last, 2) acceleration_rev = acceleration[::-1] plt.plot(idxs[:-2] + 1, acceleration_rev) plt.show() k = acceleration_rev.argmax() + 2  # if idx 0 is the max of this we want 2 clusters print ("Clusters:", k) c = np.random.multivariate_normal([40, 40], [[20, 1], [1, 30]], size=[200,]) d = np.random.multivariate_normal([80, 80], [[30, 1], [1, 30]], size=[200,]) e = np.random.multivariate_normal([0, 100], [[100, 1], [1, 100]], size=[200,]) X2 = np.concatenate((X, c, d, e),) plt.scatter(X2[:,0], X2[:,1]) plt.show() Can you see the clusters? Here is a proper cluster diagram: Z2 = linkage(X2, 'ward') plt.figure(figsize=(10,10)) fancy_dendrogram(     Z2,     truncate_mode='lastp',     p=30,     leaf_rotation=90.,     leaf_font_size=12.,     show_contracted=True,     annotate_above=40,     max_d=170, ) plt.show() Well done you can now see the clusters. Let’s look at the data in more detail. last = Z2[-10:, 2] last_rev = last[::-1] idxs = np.arange(1, len(last) + 1) plt.plot(idxs, last_rev) You can now perform more analysis. acceleration = np.diff(last, 2)  # 2nd derivative of the distances acceleration_rev = acceleration[::-1] plt.plot(idxs[:-2] + 1, acceleration_rev) plt.show() k = acceleration_rev.argmax() + 2  # if idx 0 is the max of this we want 2 clusters print ("Clusters:", k) print (inconsistent(Z2, 5)[-10:]) Let’s look at an F-cluster. from scipy.cluster.hierarchy import fcluster max_d = 50 clusters = fcluster(Z, max_d, criterion="distance") print(clusters) Can you see the clusters? k=2 fcluster(Z, k, criterion="maxclust") And now can you spot the clusters? It is not that easy to spot the clusters, is it? Let’s try a different angle. from scipy.cluster.hierarchy import fcluster fcluster(Z, 8, depth=10) plt.figure(figsize=(10, 8)) plt.scatter(X[:,0], X[:,1], c=clusters, cmap="winter" plt.show() You can see them now! Note Visualize insights. People understand graphics much more easily. Columned data sets require explaining and additional contextual information. In Chapter [11](11.html), I will discuss how to report your insights in an easy but effective manner. You have successfully completed hierarchical clustering. This should enable you to understand that there are numerous subsets of clusters that can be combined to form bigger cluster structures.

### 分割聚类

A partitional clustering is simply a division of the set of data objects into non-overlapping subsets (clusters), such that each data object is in exactly one subset. Remember when you were at school? During breaks, when you played games, you could only belong to either the blue team or the red team. If you forgot which team was yours, the game normally ended in disaster! Open your Python editor, and let’s perform a transformation to demonstrate how you can create a proper partitional clustering solution. As always, you will require the ecosystem. import numpy as np from sklearn.cluster import DBSCAN from sklearn import metrics from sklearn.datasets.samples_generator import make_blobs from sklearn.preprocessing import StandardScaler You can generate some sample data, as follows: centers = [[4, 4], [-4, -4], [4, -4],[6,0],[0,0]] X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.5,                             random_state=0) Let’s apply a scaler transform. X = StandardScaler().fit_transform(X) You can now apply the DBSCAN transformation. db = DBSCAN(eps=0.3, min_samples=10).fit(X) core_samples_mask = np.zeros_like(db.labels_, dtype=bool) core_samples_mask[db.core_sample_indices_] = True labels = db.labels_ Select the number of clusters in labels. You can ignore noise, if present. n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0) Here are your findings: print('Estimated number of clusters: %d' % n_clusters_) print("Homogeneity: %0.3f" % metrics.homogeneity_score(labels_true, labels)) print("Completeness: %0.3f" % metrics.completeness_score(labels_true, labels)) print("V-measure: %0.3f" % metrics.v_measure_score(labels_true, labels)) print("Adjusted Rand Index: %0.3f"       % metrics.adjusted_rand_score(labels_true, labels)) print("Adjusted Mutual Information: %0.3f"       % metrics.adjusted_mutual_info_score(labels_true, labels)) print("Silhouette Coefficient: %0.3f"       % metrics.silhouette_score(X, labels)) You can also plot it. Remember: Graphics explains better. import matplotlib.pyplot as plt unique_labels = set(labels) colors = [plt.cm.Spectral(each)           for each in np.linspace(0, 1, len(unique_labels))] for k, col in zip(unique_labels, colors):     if k == -1:         col = [0, 0, 0, 1]     class_member_mask = (labels == k)     xy = X[class_member_mask & core_samples_mask]     plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),              markeredgecolor='k', markersize=14)     xy = X[class_member_mask & ~core_samples_mask]     plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),              markeredgecolor='k', markersize=6) plt.title('Estimated number of clusters: %d' % n_clusters_) plt.show() If you see this layout, well done. You can perform partitional clustering. This type of clustering is commonly used, as it places any data entry in distinct clusters without overlay (see Figure [10-11](#Fig11)).![A435693_1_En_10_Fig11_HTML.jpg](img/A435693_1_En_10_Fig11_HTML.jpg) Figure 10-11Partitional clustering Open a new file in the Python editor and try the following location cluster. Set up the ecosystem. import sys import os from math import radians, cos, sin, asin, sqrt from scipy.spatial.distance import pdist, squareform from sklearn.cluster import DBSCAN import matplotlib.pyplot as plt import pandas as pd Remember this distance measures function? It calculates the great circle distance between two points on the earth, when specified in decimal degrees. def haversine(lonlat1, lonlat2):     # convert decimal degrees to radians     lat1, lon1 = lonlat1     lat2, lon2 = lonlat2     lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])     # haversine formula     dlon = lon2 - lon1     dlat = lat2 - lat1     a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2     c = 2 * asin(sqrt(a))     r = 6371 # Radius of earth in kilometers. Use 3956 for miles     return c * r Let’s get the location data. if sys.platform == 'linux':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' sFileName=Base + '/01-Vermeulen/00-RawData/IP_DATA_ALL.csv' print('Loading :',sFileName) scolumns=("Latitude","Longitude") RawData = pd.read_csv(sFileName,usecols=scolumns,header=0,low_memory=False) print(RawData) You now need a sample of 1000 data points. X=RawData.sample(n=1000) Now calculate the cluster distance parameter. distance_matrix = squareform(pdist(X, (lambda u,v: haversine(u,v)))) print(distance_matrix) Next, apply the clustering. db = DBSCAN(eps=0.2, min_samples=2, metric="precomputed", algorithm="auto") y_db = db.fit_predict(distance_matrix) X['cluster'] = y_db C = X.cluster.unique() Let’s visualize the data. fig=plt.figure(1, figsize=(20, 20)) plt.title('Estimated number of clusters: %d' % len(C)) plt.scatter(X['Latitude'], X['Longitude'], c=X['cluster'],marker='D') plt.show() Now save your results. sImageFile = Base + '/01-Vermeulen/04-Transform/01-EDS/02-Python/Location_Cluster.jpg' fig.savefig(sImageFile) plt.close(fig) You should see a layout similar to Figure [10-12](#Fig12). ![A435693_1_En_10_Fig12_HTML.jpg](img/A435693_1_En_10_Fig12_HTML.jpg) Figure 10-12 Location cluster Note The latent Dirichlet allocation (LDA), discussed earlier in this chapter, would be classed as a cluster algorithm and should be mentioned among other clustering algorithms. You have completed the partitional clustering solution.

## 方差分析

The one-way analysis of variance (ANOVA) test is used to determine whether the mean of more than two groups of data sets is significantly different from each data set. Example: A BOGOF (buy-one-get-one-free) campaign is executed on 5 groups of 100 customers each. Each group is different in terms of its demographic attributes. We would like to determine whether these five respond differently to the campaign. This would help us optimize the right campaign for the right demographic group, increase the response rate, and reduce the cost of the campaign. The analysis of variance works by comparing the variance between the groups to that within the group. The core of this technique lies in assessing whether all the groups are in fact part of one larger population or a completely different population with different characteristics. Open your Python editor and set up the following ecosystem: import pandas as pd datafile='../VKHCG/01-Vermeulen/00-RawData/PlantGrowth.csv' data = pd.read_csv(datafile) Now you must create a boxplot against the data. data.boxplot('weight', by="group", figsize=(12, 8)) You must now perform feature extraction and engineering. ctrl = data['weight'][data.group == 'ctrl'] grps = pd.unique(data.group.values) d_data = {grp:data['weight'][data.group == grp] for grp in grps} k = len(pd.unique(data.group))  # number of conditions N = len(data.values)  # conditions times participants n = data.groupby('group').size()[0] #Participants in each condition You now need extra funtions from extra library: from scipy import stats Now activate the one-way ANOVA test for the null hypothesis that two or more groups have the same population mean transformation. F, p = stats.f_oneway(d_data['ctrl'], d_data['trt1'], d_data['trt2']) You need to set up a few parameters. DFbetween = k - 1 DFwithin = N - k DFtotal = N - 1 You can now further investigate the results from the transformation. SSbetween = (sum(data.groupby('group').sum()['weight']**2)/n) \     - (data['weight'].sum()**2)/N sum_y_squared = sum([value**2 for value in data['weight'].values]) SSwithin = sum_y_squared - sum(data.groupby('group').sum()['weight']**2)/n SStotal = sum_y_squared - (data['weight'].sum()**2)/N MSbetween = SSbetween/DFbetween MSwithin = SSwithin/DFwithin F = MSbetween/MSwithin eta_sqrd = SSbetween/SStotal omega_sqrd = (SSbetween - (DFbetween * MSwithin))/(SStotal + MSwithin) Here are the results of your investigation: print(F,p,eta_sqrd,omega_sqrd) Well done. You have performed a successful one-way ANOVA test.

## 主成分分析

Dimension (variable) reduction techniques aim to reduce a data set with higher dimension to one of lower dimension, without the loss of features of information that are conveyed by the data set. The dimension here can be conceived as the number of variables that data sets contain. Two commonly used variable reduction techniques follow.

### 要素分析

The crux of PCA lies in measuring the data from the perspective of a principal component. A principal component of a data set is the direction with the largest variance. A PCA analysis involves rotating the axis of each variable to the highest Eigen vector/Eigen value pair and defining the principal components, i.e., the highest variance axis or, in other words, the direction that most defines the data. Principal components are uncorrelated and orthogonal. PCA is fundamentally a dimensionality reduction algorithm, but it is just as useful as a tool for visualization, for noise filtering, for feature extraction, and engineering. Here is an example. Open your Python editor and set up this ecosystem: import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D from sklearn import datasets from sklearn.decomposition import PCA Import some data to apply your skills against. iris = datasets.load_iris() Take only the first two features. X = iris.data[:, :2] You need the following target: y = iris.target x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5 You can quickly visualize the data set. plt.clf() plt.figure(2, figsize=(8, 6)) Now plot the training points. plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1,             edgecolor='k') plt.xlabel('Sepal length') plt.ylabel('Sepal width') plt.xlim(x_min, x_max) plt.ylim(y_min, y_max) plt.xticks(()) plt.yticks(()) I suggest getting a better understanding of the interaction of the dimensions. Plot the first three PCA dimensions. fig = plt.figure(1, figsize=(8, 6)) ax = Axes3D(fig, elev=-150, azim=110) X_reduced = PCA(n_components=3).fit_transform(iris.data) ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=y,            cmap=plt.cm.Set1, edgecolor="k", s=40) ax.set_title("First three PCA directions") ax.set_xlabel("1st eigenvector") ax.w_xaxis.set_ticklabels([]) ax.set_ylabel("2nd eigenvector") ax.w_yaxis.set_ticklabels([]) ax.set_zlabel("3rd eigenvector") ax.w_zaxis.set_ticklabels([]) plt.show() Can you see the advantage of reducing the features in a controlled manner? Let’s deploy your new knowledge against a more complex data set. Example: Hillman Ltd has introduced a CCTV tracking process on the main gates, as it suspects unauthorized offloading of goods is happening through its harbor port’s warehouse. You will load a set of digits from the cameras, to assist the company to read the numbers on the side of the containers that left the warehouse. Open your Python editor and set up this ecosystem: from sklearn.datasets import load_digits from sklearn.decomposition import PCA import matplotlib.pyplot as plt import numpy as np Load the digits from the preprocessed image software. digits = load_digits() You now apply the transformation to project from 64 to 2 dimensions. pca = PCA(2) projected = pca.fit_transform(digits.data) print(digits.data.shape) print(projected.shape) Display your findings. plt.figure(figsize=(10, 15)) plt.scatter(projected[:, 0], projected[:, 1],             c=digits.target, edgecolor="none", alpha=0.5,             cmap=plt.cm.get_cmap('nipy_spectral_r', 10)) plt.xlabel('Value 1') plt.ylabel('Value 2') plt.colorbar() Apply the PCA transform. pca = PCA().fit(digits.data) plt.plot(np.cumsum(pca.explained_variance_ratio_)) plt.xlabel('Number of components') plt.ylabel('Cumulative explained variance'); You will require the following function to plot the digits: def plot_digits(data):     fig, axes = plt.subplots(4, 10, figsize=(10, 4),                              subplot_kw={'xticks':[], 'yticks':[]},                              gridspec_kw=dict(hspace=0.1, wspace=0.1))     for i, ax in enumerate(axes.flat):         ax.imshow(data[i].reshape(8, 8),                   cmap='binary', interpolation="nearest",                   clim=(0, 16)) You can now plot results. plot_digits(digits.data) One of the cameras, however, has not generated perfect images. np.random.seed(42) noisy = np.random.normal(digits.data, 4) plot_digits(noisy) You can still use the data; it just needs more processing. pca = PCA(0.50).fit(noisy) pca.n_components_ You can determine the distortion by filtering the noise factors. components = pca.transform(noisy) filtered = pca.inverse_transform(components) Visualize your results: plot_digits(filtered) Question Can you see the digits from the cameras? Do you think we have a workable solution? You have now proven that you can take a complex 64-dimension data set , and using PCA, reduce it to 2 dimensions and still achieve good data science results. Good progress on your side.

### [联合分析](http://analyticsindiamag.com/applying-conjoint-analysis-provisioning-of-a-new-restaurant/%23_blank)

Conjoint analysis is widely used in market research to identify customers’ preference for various attributes that make up a product. The attributes can be various features, such as size, color, usability, price, etc. Using conjoint (trade-off) analysis, brand managers can identify which features customers would trade off for a certain price point. Thus, such analysis is a highly used technique in new product design or pricing strategies. Example: The data is a ranking of three different features (TV size, TV type, TV color).

*   电视尺寸选项有 42 英寸、47 英寸或 60 英寸。
*   电视类型选项有液晶或等离子。
*   电视颜色选项有红色、蓝色或粉色。

The data rates the different stimuli types for each customer. You are tasked with determining which TVs to display on Krennwallner’s billboards. Open your Python editor and set up the following ecosystem: import numpy as np import pandas as pd Retrieve the customer buy choices. sFile='C:/VKHCG/01-Vermeulen/00-RawData/BuyChoices.txt' caInputeDF = pd.read_csv(sFile, sep = ";") You can display the choices. print(caInputeDF) You need a new data structure, and you must create dummy variables for every stimulus. In total, you require 9 different stimuli and 18 different combinations. ConjointDummyDF = pd.DataFrame(np.zeros((18,9)), columns=["Rank","A1", "A2", "A3","B1","B2","C1", "C2","C3"]) You now need to feature engineer the data choices into the new structure: ConjointDummyDF.Rank = caInputeDF.Rank for index, row in caInputeDF.iterrows():     stimuli1 = str(caInputeDF["Stimulus"].iloc[index][:2])     stimuli2 = str(caInputeDF["Stimulus"].iloc[index][2:4])     stimuli3 = str(caInputeDF["Stimulus"].iloc[index][4:6])     stimuliLine=[stimuli1,stimuli2,stimuli3]     Columns=ConjointDummyDF.columns     for stimuli in stimuliLine:         for i in range(len(Columns)):             if stimuli == Columns[i]:                 ConjointDummyDF.iloc[index, i] = 1 Well done. Let’s look at your new structure. print(ConjointDummyDF.head()) You only have unknown numbers. I suggest you add suitable stimulus names. Be mindful of the Transform step you normally present to businesspeople. They understand plasma TV is not good with TVType3. fullNames = {"Rank":"Rank", "A1": "42\" (106cm)","A2": "47\" (120cm)","A3": "60\" (152cm)","B1": "Plasma","B2":"LCD","C1":"Blue","C2":"Red","C3": "Pink",} ConjointDummyDF.rename(columns=fullNames, inplace=True) That’s better. Now look at the data structure. #ConjointDummyDF.head() Next, you estimate the main effects with a linear regression. You need extra libraries to achieve this action. import statsmodels.api as sm You can select any of these columns for your model. print(ConjointDummyDF.columns) I suggest you select these: X = ConjointDummyDF[[u'42" (106cm)', u'47" (120cm)',\  u'60" (152cm)', u'Plasma', u'LCD', u'Red', u'Blue', u'Pink']] You now need to set up the model. X = sm.add_constant(X) Y = ConjointDummyDF.Rank You now activate the model against your new data structure. linearRegression = sm.OLS(Y, X).fit() You can now look at the results. print(linearRegression.summary())

*   你可以研究很多指标。我不打算详细讨论其中的任何一个，因为有几个需要复杂的数学来完全解释它们。我们感兴趣的指标是刺激的部分价值和相对重要性:刺激的重要性=最大(β)-最小(β)
*   刺激的相对重要性=刺激的重要性/总和(所有刺激的重要性)

You will now investigate these indicators. To do this, you need some data engineering. You now require several basic indicator variables to start the process. importance = [] relative_importance = [] rangePerFeature = [] begin = "A" tempRange = [] You now need to load the stimuli. for stimuli in fullNames.keys():     if stimuli[0] == begin:         tempRange.append(linearRegression.params[fullNames[stimuli]])     elif stimuli == "Rank":         rangePerFeature.append(tempRange)     else:         rangePerFeature.append(tempRange)         begin = stimuli[0]         tempRange = [linearRegression.params[fullNames[stimuli]]] Then, you need the feature ranges. for item in rangePerFeature:     importance.append( max(item) - min(item)) You then calculate the importance of a feature. for item in importance:     relative_importance.append(100* round(item/sum(importance),3)) Start a base data structure for the part worth values. partworths = [] item_levels = [1,3,5,8] You now calculate the part worth values. for i in range(1,4):     part_worth_range = linearRegression.params[item_levels[i-1]:item_levels[i]]     print (part_worth_range) You need to determine the mean rank of the data set. meanRank = [] for i in ConjointDummyDF.columns[1:]:     newmeanRank = ConjointDummyDF["Rank"].loc[ConjointDummyDF[i] == 1].mean()     meanRank.append(newmeanRank) I suggest you use total mean known as “basic utility” to be used as the “zero alternative.” totalMeanRank = sum(meanRank) / len(meanRank) You will now rank the value of each part of the stimuli, i.e., what features of the TV are important? partWorths = {} for i in range(len(meanRank)):     name = fullNames[sorted(fullNames.keys())[i]]     partWorths[name] = meanRank[i] - totalMeanRank You now have a result set. print(partWorths) Now, I will help you to develop the results into insights and deliver the basis for a summary and results report. print ("Relative Importance of Feature:\n\nMonitor Size:",relative_importance[0], "%","\nType of Monitor:", relative_importance[1], "%", "\nColor of TV:", relative_importance[2], "%\n\n") print ("--"*30) print ("Importance of Feature:\n\nMonitor Size:",importance[0],"\nType of Monitor:", importance[1],  "\nColor of TV:", importance[2]) So, what is the optimal product bundle? I suggest 60", LCD, Red. Do you agree? Let’s test that assumed fact. optBundle = [1,0,0,1,0,1,0,1,0] print ("The best possible Combination of Stimuli would have the highest rank:",linearRegression.predict(optBundle)[0]) optimalWorth = partWorths["60\" (152cm)"] + partWorths["LCD"] + partWorths["Red"] print ("Choosing the optimal Combination brings the Customer an extra ", optimalWorth, "'units' of utility") Does your science support my assumed fact? Congratulations, you have just completed a conjoint analysis. Tip I suggest you try the equivalent analysis with other data sets, as this is a common question I am asked. What makes people pick one product and not another? You should practice with data sets of all sizes, to understand how the formulas react to changes in the stimuli factors. I have confidence that there is a future for you at Krennwallner as a data scientist.

## [决策树](http://analyticsindiamag.com/predictive-attrition-model/%23_blank)

Decision trees, as the name suggests, are a tree-shaped visual representation of routes you can follow to reach a particular decision, by laying down all options and their probability of occurrence. Decision trees are exceptionally easy to understand and interpret. At each node of the tree, one can interpret what would be the consequence of selecting that node or option. The series of decisions leads you to the end result, as shown in Figure [10-13](#Fig13).![A435693_1_En_10_Fig13_HTML.jpg](img/A435693_1_En_10_Fig13_HTML.jpg) Figure 10-13Simple decision tree Before you start the example , I must discuss a common add-on algorithm to decision trees called AdaBoost. AdaBoost, short for “adaptive boosting,” is a machine-learning meta-algorithm. The classifier is a meta-estimator, because it begins by fitting a classifier on the original data set and then fits additional copies of the classifier on the same data set, but where the weights of incorrectly classified instances are adjusted, such that subsequent classifiers focus more on difficult cases. It boosts the learning impact of less clear differences in the specific variable, by adding a progressive weight to boost the impact. See [http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) . This boosting enables the data scientist to force decisions down unclear decision routes for specific data entities, to enhance the outcome. Example of a Decision Tree using AdaBoost: Open your Python editor and set up this ecosystem: import numpy as np import matplotlib.pyplot as plt from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import AdaBoostRegressor You need a data set. So, you can build a random data set. rng = np.random.RandomState(1) X = np.linspace(0, 6, 1000)[:, np.newaxis] y = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0]) You then fit the normal regression model to the data set. regr_1 = DecisionTreeRegressor(max_depth=4) You then also apply an AdaBoost model to the data set. The parameters can be changed, if you want to experiment. regr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),                           n_estimators=300, random_state=rng) You then train the model. regr_1.fit(X, y) regr_2.fit(X, y) You activate the predict. y_1 = regr_1.predict(X) y_2 = regr_2.predict(X) You plot the results. plt.figure(figsize=(15, 10)) plt.scatter(X, y, c="k", label="Training Samples") plt.plot(X, y_1, c="g", label="n_Estimators=1", linewidth=2) plt.plot(X, y_2, c="r", label="n_Estimators=300", linewidth=2) plt.xlabel("Data") plt.ylabel("Target") plt.title("Boosted Decision Tree Regression") plt.legend() plt.show() Congratulations! You just built a decision tree. You can now test your new skills against a more complex example. Once more, open your Python editor and set up this ecosystem: import numpy as np import matplotlib.pyplot as plt from sklearn.ensemble import AdaBoostClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.datasets import make_gaussian_quantiles You are building a more complex data set. X1, y1 = make_gaussian_quantiles(cov=2.,                                  n_samples=2000, n_features=2,                                  n_classes=2, random_state=1) X2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5,                                  n_samples=3000, n_features=2,                                  n_classes=2, random_state=1) X = np.concatenate((X1, X2)) y = np.concatenate((y1, - y2 + 1)) You now have a data set ready. Create and fit an AdaBoosted decision tree to the data set. bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),                          algorithm="SAMME",                          n_estimators=200) You train the model. bdt.fit(X, y) You now need to set up a few parameters that you will require. plot_colors = "br" plot_step = 0.02 class_names = "AB" You now create a visualization of the results. plt.figure(figsize=(10, 5)) Add the plot for the decision boundaries. plt.subplot(121) x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),                      np.arange(y_min, y_max, plot_step)) Z = bdt.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired) plt.axis("tight") Plot your training points. for i, n, c in zip(range(2), class_names, plot_colors):     idx = np.where(y == i)     plt.scatter(X[idx, 0], X[idx, 1],                 c=c, cmap=plt.cm.Paired,                 s=20, edgecolor="k",                 label="Class %s" % n) plt.xlim(x_min, x_max) plt.ylim(y_min, y_max) plt.legend(loc='upper right') plt.xlabel('x') plt.ylabel('y') plt.title('Decision Boundary') Plot the two-class decision scores. twoclass_output = bdt.decision_function(X) plot_range = (twoclass_output.min(), twoclass_output.max()) plt.subplot(122) for i, n, c in zip(range(2), class_names, plot_colors):     plt.hist(twoclass_output[y == i],              bins=10,              range=plot_range,              facecolor=c,              label='Class %s' % n,              alpha=.5,              edgecolor='k') x1, x2, y1, y2 = plt.axis() plt.axis((x1, x2, y1, y2 * 1.2)) plt.legend(loc='upper right') plt.ylabel('Samples') plt.xlabel('Score') plt.title('Decision Scores') plt.tight_layout() plt.subplots_adjust(wspace=0.35) plt.show() Well done, you have just completed a complex decision tree.

## 支持向量机、网络、聚类和网格

The support vector machine (SVM) constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can be used for classification and regression. The support vector network (SVN) daisy chains more than one SVM together to form a network. All the data flows through the same series of SVMs. The support vector cluster (SVC) runs SVM on different clusters of the data in parallel. Hence, not all data flows through all the SVMs. The support vector grid (SVG) is an SVC of an SVN or an SVN of an SVC. This solution is the most likely configuration you will develop at a customer site. It uses SVMs to handle smaller clusters of the data, to apply specific transform steps. As a beginner data scientist, you only need to note that they exist.

### 支持向量机

A support vector machine is a discriminative classifier formally defined by a separating hyperplane. The method calculates an optimal hyperplane (Figure [10-14](#Fig14)) with a maximum margin, to ensure it classifies the data set into separate clusters of data points.![A435693_1_En_10_Fig14_HTML.jpg](img/A435693_1_En_10_Fig14_HTML.jpg) Figure 10-14Simple support vector machine’s optimal hyperplane I will guide you through a sample SVM. Open your Python editor and create this ecosystem: import numpy as np import matplotlib.pyplot as plt from sklearn import svm Here is your data set and targets. X = np.c_[(.4, -.7),           (-1.5, -1),           (-1.4, -.9),           (-1.3, -1.2),           (-1.1, -.2),           (-1.2, -.4),           (-.5, 1.2),           (-1.5, 2.1),           (1, 1),           (1.3, .8),           (1.2, .5),           (.2, -2),           (.5, -2.4),           (.2, -2.3),           (0, -2.7),           (1.3, 2.1)].T Y = [0] * 8 + [1] * 8 You display the results as Figure Number 1. fignum = 1 You have several kernels you can use to fit the data model. I will take you through all of them. for kernel in ('linear', 'poly', 'rbf'):     clf = svm.SVC(kernel=kernel, gamma=2)     clf.fit(X, Y) You now plot the line, the points, and the nearest vectors to the plane.     plt.figure(fignum, figsize=(8, 6))     plt.clf()     plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,                 facecolors='none', zorder=10, edgecolors="k")     plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired,                 edgecolors='k')     plt.axis('tight')     x_min = -3     x_max = 3     y_min = -3     y_max = 3     XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]     Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()]) You now apply the result into a color plot.     Z = Z.reshape(XX.shape)     plt.figure(fignum, figsize=(8, 6))     plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)     plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],                 levels=[-.5, 0, .5])     plt.xlim(x_min, x_max)     plt.ylim(y_min, y_max)     plt.xticks(())     plt.yticks(())     fignum = fignum + 1 You now show your plots. plt.show() Well done. You have just completed three different types of SVMs.

### 支持向量网络

The support vector network is the ensemble of a network of support vector machines that together classify the same data set, by using different parameters or even different kernels. This a common method of creating feature engineering, by creating a chain of SVMs. Tip You can change kernels on the same flow to expose new features. Practice against the data sets with the different kernels, to understand what they give you.

### 支持向量聚类

Support vector clustering is used were the data points are classified into clusters, with support vector machines performing the classification at the cluster level. This is commonly used in highly dimensional data sets, where the clustering creates a grouping that can then be exploited by the SVM to subdivide the data points, using different kernels and other parameters. I have seen SVC, SVN, SVM, and SVG process in many of the deep-learning algorithms that I work with every day. The volume, variety, and velocity of the data require that the deep learning do multistage classifications, to enable the more detailed analysis of the data points to occur after a primary result is published.

## 数据挖掘

Data mining is processing data to pinpoint patterns and establish relationships between data entities. Here are a small number of critical data mining theories you need to understand about data patterns, to be successful with data mining.

### 关联模式

This involves detecting patterns in which one occasion is associated to another. If, for example, a loading bay door is opened, it is fair to assume that a truck is loading goods. Pattern associations simply discover the correlation of occasions in the data. You will use some core statistical skills for this processing. Warning “Correlation does not imply causation.” Correlation is only a relationship or indication of behavior between two data sets. The relationship is not a cause-driven action. Example: If you discover a relationship between hot weather and ice cream sales, it does not mean high ice cream sales cause hot weather, or vice versa. It is only an observed relationship. This is commonly used in retail basket analysis and recommender systems. I will discuss this causation factor later, in Chapter [11](11.html), when we start to prepare our insights and actionable knowledge extractions. I will guide you through an example now. Please open your Python editor and create this ecosystem: import pandas as pd df1 = pd.DataFrame({'A': range(8), 'B': [2*i for i in range(8)]}) df2 = pd.DataFrame({'A': range(8), 'B': [-2*i for i in range(8)]}) Here is your data. print('Positive Data Set') print(df1) print('Negative Data Set') print(df2) Here are your results. print('Results') print('Correlation Positive:', df1['A'].corr(df1['B'])) print('Correlation Negative:', df2['A'].corr(df2['B'])) You should see a correlation of either +1 or -1\. If it is +1, this means there is a 100% correlation between the two values, hence they change at the same rate. If it is -1, this means there is a 100% negative correlation, indicating that the two values have a relationship if one increases while the other decreases. Tip In real-world data sets, such two extremes as +1 and -1 do not occur. The range is normally i-1 < C < +1. You will now apply changes that will interfere with the correlation. df1.loc[2, 'B'] = 10 df2.loc[2, 'B'] = -10 You can now see the impact. print('Positive Data Set') print(df1) print('Negative Data Set') print(df2) print('Results') print('Correlation Positive:', df1['A'].corr(df1['B'])) print('Correlation Negative:', df2['A'].corr(df2['B'])) Can you sense the impact a minor change has on the model? So, let’s add a bigger change. df1.loc[3, 'B'] = 100 df2.loc[3, 'B'] = -100 You check the impact. print('Positive Data Set') print(df1) print('Negative Data Set') print(df2) print('Results') print('Correlation Positive:', df1['A'].corr(df1['B'])) print('Correlation Negative:', df2['A'].corr(df2['B'])) Well done, if you understood the changes that interfere with the relationship. If you see the relationship, you have achieved an understanding of association patterns. These “What if?” analyses are common among data scientists’ daily tasks. For example, one such analysis might relate to the following: What happens when I remove 300 of my 1000 staff? And at what point does the removal of staff have an impact? These small-step increases of simulated impact can be used to simulate progressive planned changes in the future.

### 分类模式

This technique discovers new patterns in the data, to enhance the quality of the complete data set. Data classification is the process of consolidating data into categories, for its most effective and efficient use by the data processing. For example, if the data is related to the shipping department, you must then augment a label on the data that states that fact. A carefully planned data-classification system creates vital data structures that are easy to find and retrieve. You do not want to scour your complete data lake to find data every time you want to analyze a new data pattern.

#### 聚类模式

Clustering is the discovery and labeling of groups of specifics not previously known. An example of clustering is if, when your customers buy bread and milk together on a Monday night, you group, or cluster, these customers as “start-of-the-week small-size shoppers,” by simply looking at their typical basic shopping basket. Any combination of variables that you can use to cluster data entries into a specific group can be viewed as some form of clustering. For data scientists, the following clustering types are beneficial to master.

#### 基于连通性的聚类

You can discover the interaction between data items by studying the connections between them. This process is sometimes also described as hierarchical clustering.

#### 基于质心的聚类(K 均值聚类)

”Centroid-based” describes the cluster as a relationship between data entries and a virtual center point in the data set. K-means clustering is the most popular centroid-based clustering algorithm. I will guide you through an example. Open your Python editor and set up this ecosystem: import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D from sklearn.cluster import KMeans from sklearn import datasets You now set up a data set to study. np.random.seed(5) iris = datasets.load_iris() X = iris.data y = iris.target Configure your estimators for the clustering. estimators = [('k_means_iris_8', KMeans(n_clusters=8)),               ('k_means_iris_3', KMeans(n_clusters=3)),               ('k_means_iris_bad_init', KMeans(n_clusters=3, n_init=1,                                                init='random'))] Get ready to virtualize your results. fignum = 1 titles = ['8 clusters', '3 clusters', '3 clusters, bad initialization'] for name, est in estimators:     fig = plt.figure(fignum, figsize=(4, 3))     ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)     est.fit(X)     labels = est.labels_     ax.scatter(X[:, 3], X[:, 0], X[:, 2],                c=labels.astype(np.float), edgecolor="k")     ax.w_xaxis.set_ticklabels([])     ax.w_yaxis.set_ticklabels([])     ax.w_zaxis.set_ticklabels([])     ax.set_xlabel('Petal width')     ax.set_ylabel('Sepal length')     ax.set_zlabel('Petal length')     ax.set_title(titles[fignum - 1])     ax.dist = 12     fignum = fignum + 1 Plot your results. fig = plt.figure(fignum, figsize=(4, 3)) ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134) for name, label in [('Setosa', 0),                     ('Versicolour', 1),                     ('Virginica', 2)]:     ax.text3D(X[y == label, 3].mean(),               X[y == label, 0].mean(),               X[y == label, 2].mean() + 2, name,               horizontalalignment='center',               bbox=dict(alpha=.2, edgecolor="w", facecolor="w")) Reorder the labels to have colors matching the cluster results. y = np.choose(y, [1, 2, 0]).astype(np.float) ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y, edgecolor="k") ax.w_xaxis.set_ticklabels([]) ax.w_yaxis.set_ticklabels([]) ax.w_zaxis.set_ticklabels([]) ax.set_xlabel('Petal width') ax.set_ylabel('Sepal length') ax.set_zlabel('Petal length') ax.set_title('Ground Truth') ax.dist = 12 fig.show() Well done. You should see a set of results for your hard work. Can you see how the connectivity-based clustering uses the interaction between data points to solve the transformation steps?

#### 基于分布的聚类

This type of clustering model relates data sets with statistics onto distribution models. The most widespread density-based clustering technique is DBSCAN.

#### 基于密度的聚类

In density-based clustering , an area of higher density is separated from the remainder of the data set. Data entries in sparse areas are placed in separate clusters. These clusters are considered to be noise, outliers, and border data entries.

#### 基于网格方法

Grid-based approaches are common for mining large multidimensional space clusters having denser regions than their surroundings. The grid-based clustering approach differs from the conventional clustering algorithms in that it does not use the data points but a value space that surrounds the data points.

### 贝叶斯分类

Naive Bayes (NB) classifiers are a group of probabilistic classifiers established by applying Bayes’s theorem with strong independence assumptions between the features of the data set. There is one more specific Bayesian classification you must take note of, and it is called tree augmented naive Bayes (TAN). Tree augmented naive Bayes is a semi-naive Bayesian learning method. It relaxes the naive Bayes attribute independence assumption by assigning a tree structure, in which each attribute only depends on the class and one other attribute. A maximum weighted spanning tree that maximizes the likelihood of the training data is used to perform classification. The naive Bayesian (NB) classifier and the tree augmented naive Bayes (TAN) classifier are well-known models that will be discussed next. Here is an example: Open your Python editor and start with this ecosystem. import numpy as np import urllib Load data via the web interface. url = "http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data" Download the file. raw_data = urllib.request.urlopen(url) Load the CSV file into a numpy matrix. dataset = np.loadtxt(raw_data, delimiter=",") Separate the data from the target attributes in the data set. X = dataset[:,0:8] y = dataset[:,8] Add extra processing capacity. from sklearn import metrics from sklearn.naive_bayes import GaussianNB model = GaussianNB() model.fit(X, y) print(model) Produce predictions. expected = y predicted = model.predict(X) # summarize the fit of the model print(metrics.classification_report(expected, predicted)) print(metrics.confusion_matrix(expected, predicted)) You have just built your naive Bayes classifiers. Good progress!

### 序列或路径分析

This identifies patterns in which one event leads to another, later resulting in insights into the business. Path analysis is a chain of consecutive events that a given business entity performs during a set period, to understand behavior, in order to gain actionable insights into the data. I suggest you use a combination of tools to handle this type of analysis. I normally model the sequence or path with the help of a graph database, or, for smaller projects, I use a library called networkx in Python. Example: Your local telecommunications company is interested in understanding the reasons or flow of events that resulted in people churning their telephone plans to their competitor. Open your Python editor and set up this ecosystem. ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import pandas as pd import sqlite3 as sq import networkx as nx import datetime pd.options.mode.chained_assignment = None ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ Company='01-Vermeulen' ################################################################ sDataBaseDir=Base + '/' + Company + '/04-Transform/SQLite' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataBaseDir + '/Vermeulen.db' conn = sq.connect(sDatabaseName) ################################################################ You must create a new graph to track the individual paths of each customer through their journey over the last nine months. G=nx.Graph() ################################################################ The following loop structure enables you to compare two sequential months and determine the changes: for M in range(1,10):     print('Month: ', M)     MIn = str(M - 1)     MOut = str(M)     sFile0 = 'ConnectionsChurn' + MIn + '.csv'     sFile1 = 'ConnectionsChurn' + MOut + '.csv'     sTable0 = 'ConnectionsChurn' + MIn     sTable1 = 'ConnectionsChurn' + MOut     sFileName0=Base + '/' + Company + '/00-RawData/' + sFile0     ChurnData0=pd.read_csv(sFileName0,header=0,low_memory=False, encoding="latin-1")     sFileName1=Base + '/' + Company + '/00-RawData/' + sFile1     ChurnData1=pd.read_csv(sFileName1,header=0,low_memory=False, encoding="latin-1")     ################################################################ Owing to an error during extraction, the files dates are not correct, so you perform a data-quality correction.     dt1 = datetime.datetime(year=2017, month=1, day=1)     dt2 = datetime.datetime(year=2017, month=2, day=1)     ChurnData0['Date'] = dt1.strftime('%Y/%m/%d')     ChurnData1['Date'] = dt2.strftime('%Y/%m/%d')     ################################################################ You now compare all the relevant features of the customer, to see what actions were taken during the month under investigation.     TrackColumns=['SeniorCitizen', 'Partner',            'Dependents', 'PhoneService', 'MultipleLines',            'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',            'StreamingTV', 'PaperlessBilling', 'StreamingMovies', 'InternetService',            'Contract', 'PaymentMethod', 'MonthlyCharges']     ################################################################     for i in range(ChurnData0.shape[0]):         for TColumn in TrackColumns:             t=0             if ChurnData0[TColumn][i] == 'No':                 t+=1             if t > 4:                 ChurnData0['Churn'][i] = 'Yes'             else:                 ChurnData0['Churn'][i] = 'No'     ################################################################     for i in range(ChurnData1.shape[0]):         for TColumn in TrackColumns:             t=0             if ChurnData1[TColumn][i] == 'No':                 t+=1             if t > 4:                 ChurnData1['Churn'][i] = 'Yes'             else:                 ChurnData1['Churn'][i] = 'No'     ################################################################     print('Store CSV Data')     ChurnData0.to_csv(sFileName0, index=False)     ChurnData1.to_csv(sFileName1, index=False)     ################################################################     print('Store SQLite Data')     ChurnData0.to_sql(sTable0, conn, if_exists="replace")     ChurnData1.to_sql(sTable1, conn, if_exists="replace")     ################################################################     for TColumn in TrackColumns:         for i in range(ChurnData0.shape[0]): You always start with a “Root” node and then attach the customers. Then you connect each change in status and perform a complete path analysis.             Node0 = 'Root'             Node1 = '(' + ChurnData0['customerID'][i] + '-Start)'             G.add_edge(Node0, Node1)             Node5 = '(' + ChurnData0['customerID'][i] + '-Stop)'             G.add_node(Node5)             if ChurnData0['Churn'][i] == 'Yes':                 NodeA = '(' + ChurnData0['customerID'][i] + '-Start)'                 NodeB = '(' + ChurnData0['customerID'][i] + '-Stop)'                 if nx.has_path(G, source=NodeA, target=NodeB) == False:                     NodeC = '(' + ChurnData0['customerID'][i] + '):(Churn)=>(' + ChurnData1['Churn'][i] + ')'                     G.add_edge(NodeA, NodeC)                     G.add_edge(NodeC, NodeB)             else:                 if ChurnData0[TColumn][i] != ChurnData1[TColumn][i]:                     #print(M,ChurnData0['customerID'][i],ChurnData0['Date'][i],ChurnData1['Date'][i],TColumn, ChurnData0[TColumn][i], ChurnData1[TColumn][i])                     Node2 = '(' + ChurnData0['customerID'][i] + ')-(' + ChurnData0['Date'][i] + ')'                     G.add_edge(Node1, Node2)                     Node3 = Node2 + '-(' + TColumn + ')'                     G.add_edge(Node2, Node3)                     Node4 = Node3 + ':(' + ChurnData0[TColumn][i] + ')=>(' + ChurnData1[TColumn][i] + ')'                     G.add_edge(Node3, Node4)                     if M == 9:                         Node6 = '(' + ChurnData0['customerID'][i] + '):(Churn)=>(' + ChurnData1['Churn'][i] + ')'                         G.add_edge(Node4, Node6)                         G.add_edge(Node6, Node5)                     else:                         G.add_edge(Node4, Node5) You can use these lines to investigate the nodes and the edges you created. for n in G.nodes():     print(n) for e in G.edges():     print(e) You must now store your graph for future use. sGraphOutput=Base + '/' + Company + \     '/04-Transform/01-EDS/02-Python/Transform_ConnectionsChurn_Graph.gml.gz' nx.write_gml(G, sGraphOutput) You now investigate the paths taken by a customer over the nine months and produce an output of all the steps taken by the customer over the period. sFile0 = 'ConnectionsChurn9.csv' sFileName0=Base + '/' + Company + '/00-RawData/' + sFile0 ChurnData0=pd.read_csv(sFileName0,header=0,low_memory=False, encoding="latin-1") c=0 for i in range(ChurnData0.shape[0]):     sCustomer = ChurnData0['customerID'][i]     NodeX = '(' + ChurnData0['customerID'][i] + '-Start)'     NodeY = '(' + ChurnData0['customerID'][i] + '-Stop)'     if nx.has_path(G, source=NodeX, target=NodeY) == False:         NodeZ = '(' + ChurnData0['customerID'][i] + '):(Churn)=>(' + ChurnData0['Churn'][i] + ')'         G.add_edge(NodeX, NodeZ)         G.add_edge(NodeZ, NodeX)     if nx.has_path(G, source=NodeX, target=NodeY) == True: This function enables you to expose all the paths between the two nodes you created for each customer.         pset = nx.all_shortest_paths(G, source=NodeX, target=NodeY, weight=None)         t=0         for p in pset:             t=0             ps = 'Path: ' + str(p)             for s in p:                 c+=1                 t+=1                 ts = 'Step: ' + str(t)                 #print(NodeX, NodeY, ps, ts, s)                 if c == 1:                     pl = [[sCustomer, ps, ts, s]]                 else:                     pl.append([sCustomer, ps, ts, s]) You now store the path analysis results into a CSV for later use. sFileOutput=Base + '/' + Company + \     '/04-Transform/01-EDS/02-Python/Transform_ConnectionsChurn.csv' df = pd.DataFrame(pl, columns=['Customer', 'Path', 'Step', 'StepName']) df.index.name = 'RecID' sTable = 'ConnectionsChurnPaths' df.to_sql(sTable, conn, if_exists="replace") df.to_csv(sFileOutput) ################################################################ print('### Done!! ############################################') ################################################################ Well done. You have just completed your first path analysis over a period of nine months. Insights: Can you understand why the routes are different for each customer but still have the same outcome? Can you explain why one customer churns and another does not? The common activity you will spot is that customers begin to remove services from your client as they start to churn, to enable their own new churned services, they nolonger will support you services 100%. If they have fewer than five services, customers normally churn, as they are now likely with the other telecommunications company. If you were advising the client, I would suggest you highlight that the optimum configuration for the trigger that a customer is about to churn is if the customer has changed his or her configuration to include fewer services from your client. You can still prevent a churn, if you intervene before the customer hits the five-minimum level. Well done. You can now perform Transform steps for path analysis, get a simple list of node and edges (relationships) between them, and model a graph and ask pertinent questions.

### 预测

This technique is used to discover patterns in data that result in practical predictions about a future result, as indicated, by predictive analytics of future probabilities and trends. We have been performing forecasting at several points in the book.

## 模式识别

Pattern recognition identifies regularities and irregularities in data sets. The most common application of this is in text analysis, to find complex patterns in the data. I will guide you through an example for text extraction. The example will extract text files from a common 20 newsgroups data set and then create categories of text that was found together in the same document. This will provide you with the most common word that is used in the newsgroups. Open your Python editor and set up this ecosystem: from pprint import pprint from time import time import logging from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfTransformer from sklearn.linear_model import SGDClassifier from sklearn.model_selection import GridSearchCV from sklearn.pipeline import Pipeline Modify your logging, to display progress logs on stdout (standard output). logging.basicConfig(level=logging.INFO,                     format='%(asctime)s %(levelname)s %(message)s') You can now load your categories from the training set categories = [     'sci.space',     'comp.sys.ibm.pc.hardware',     'sci.electronics',     'sci.crypt',     'rec.autos',     'rec.motorcycles', ] You could use this for all the categories, but I would perform it as a later experiment, as it slows down your processing, by using larger volumes of data from your data lake. #categories = None You can now investigate what you loaded. print("Loading 20 newsgroups dataset for categories:") print(categories) You must now load the training data. data = fetch_20newsgroups(subset='train', categories=categories) print("%d documents" % len(data.filenames)) print("%d categories" % len(data.target_names)) You now have to define a pipeline, combining a text feature extractor with a simple classifier. pipeline = Pipeline([     ('vect', CountVectorizer()),     ('tfidf', TfidfTransformer()),     ('clf', SGDClassifier()), ]) Warning Uncommenting more of the following parameters will provide improved exploring power but will increase processing time in a combinatorial way. parameters = {     'vect__max_df': (0.5, 0.75, 1.0),     #'vect__max_features': (None, 5000, 10000, 50000),     'vect__ngram_range': ((1, 1), (1, 2)),     #'tfidf__use_idf': (True, False),     #'tfidf__norm': ('l1', 'l2'),     'clf__alpha': (0.00001, 0.000001),     'clf__penalty': ('l2', 'elasticnet'),     #'clf__n_iter': (10, 50, 80), } You can now build the main processing engine. if __name__ == "__main__":     grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)     print("Performing grid search...")     print("pipeline:", [name for name, _ in pipeline.steps])     print("parameters:")     pprint(parameters)     t0 = time()     grid_search.fit(data.data, data.target)     print("done in %0.3fs" % (time() - t0))     print()     print("Best score: %0.3f" % grid_search.best_score_)     print("Best parameters set:")     best_parameters = grid_search.best_estimator_.get_params()     for param_name in sorted(parameters.keys()):         print("\t%s: %r" % (param_name, best_parameters[param_name])) When you execute the program, you will have successfully completed a text extraction. The data sources for this type of extract are typically documents, e-mail, Twitter, or note fields in databases. Any test source can receive a transform step.

## 机器学习

The business world is bursting with activities and philosophies about machine learning and its application to various business environments. Machine learning is the capability of systems to learn without explicit software development. It evolved from the study of pattern recognition and computational learning theory. The impact is that with the appropriate processing and skills, you can amplify your own data capabilities, by training a processing environment to accomplish massive amounts of discovery of data into actionable knowledge, while you have a cup of coffee, for example. This skill is an essential part of achieving major gains in shortening the data-to-knowledge cycle. I will cover a limited rudimentary theory, but machine learning encompasses a wide area of expertise that merits a book by itself. So, I will introduce you only to the core theories.

### 监督学习

Supervised learning is the machine-learning task of inferring a function from labeled training data. The training data consists of a set of training examples. In supervised learning, each example is a pair consisting of an input object and a desired output value. You use this when you know the required outcome for a set of input features. Example: If you buy bread and jam, you can make a jam sandwich. Without either, you have no jam sandwich. If you investigate this data set, you can easily spot from the indicators what is bread and what is jam. A data science model could perform the same task, using supervised learning.

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| BaseFoodName | 填充名称 | 是面包 | 伊斯兰教 | IsJamSandwich |
| --- | --- | --- | --- | --- |
| 艾什·梅拉拉 | 苹果派 | one | one | one |
| 艾什·梅拉拉 | 苹果和木瓜 | one | one | one |
| 荞麦面包 | 苹果派 | one | one | one |
| 荞麦面包 | 苹果和木瓜 | one | one | one |
| 艾什·梅拉拉 | 全能侠 | one | Zero | Zero |
| 艾什·梅拉拉 | 烤奶酪 | one | Zero | Zero |
| 荞麦面包 | 全能侠 | one | Zero | Zero |
| 荞麦面包 | 烤奶酪 | one | Zero | Zero |
| 天使蛋糕 | 苹果派 | Zero | one | Zero |
| 天使蛋糕 | 苹果和木瓜 | Zero | one | Zero |
| 天使蛋糕 | 苹果派 | Zero | one | Zero |
| 天使蛋糕 | 苹果和木瓜 | Zero | one | Zero |
| 天使蛋糕 | 全能侠 | Zero | Zero | Zero |
| 天使蛋糕 | 烤奶酪 | Zero | Zero | Zero |
| 天使蛋糕 | 全能侠 | Zero | Zero | Zero |
| 天使蛋糕 | 烤奶酪 | Zero | Zero | Zero |

If you investigate the bigger data set stored in directory .. \VKHCG\03-Hillman\00-RawData in file KitchenData.csv, you can see how the preceding knowledge can be used to infer what the correct selection is for items that will result in a jam sandwich.

### 无监督学习

Unsupervised learning is the machine-learning task of inferring a function to describe hidden structures from unlabeled data. This encompasses many other techniques that seek to summarize and explain key features of the data. Example: You can take a bag of marble with different colors, sizes, and materials and split them into three equal groups, by applying a set of features and a model. You do not know up front what the criteria are for splitting the marbles.

### 强化学习

Reinforcement learning (RL) is an area of machine learning inspired by behavioral psychology that is concerned with how software agents should take actions in an environment, so as to maximize, more or less, a notion of cumulative reward. This is used in several different areas, such as game theory, swarm intelligence, control theory, operations research, simulation-based optimization, multi-agent systems, statistics, and genetic algorithms (Figure [10-15](#Fig15)).![A435693_1_En_10_Fig15_HTML.jpg](img/A435693_1_En_10_Fig15_HTML.jpg) Figure 10-15Reinforced learning diagram The process is simple. Your agent extracts features from the environment that are either “state” or “reward.” State features indicate that something has happened. Reward features indicate that something happened that has improved or worsened to the perceived gain in reward. The agent uses the state and reward to determine actions to change the environment. This process of extracting state and reward, plus responding with action, will continue until a pre-agreed end reward is achieved. The real-world application for these types of reinforced learning is endless. You can apply reinforced learning to any environment which you can control with an agent. I build many RL systems that monitor processes, such as a sorting system of purchases or assembly of products. It is also the core of most robot projects, as robots are physical agents that can interact with the environment. I also build many “soft-robots” that take decisions on such data processing as approval of loans, payments of money, and fixing of data errors.

## 打包数据

Bootstrap aggregating, also called bagging, is a machine-learning ensemble meta-algorithm that aims to advance the stability and accuracy of machine-learning algorithms used in statistical classification and regression. It decreases variance and supports systems to avoid overfitting. I want to cover this concept, as I have seen many data science solutions over the last years that suffer from overfitting, because they were trained with a known data set that eventually became the only data set they could process. Thanks to inefficient processing and algorithms, we naturally had a lead way for variance in the data. The new GPU (graphics processing unit) -based systems are so accurate that they overfit easily, if the training data is a consistent set, with little or no major changes in the patterns within the data set. You will now see how to perform a simple bagging process. Open your Python editor and create this ecosystem: import numpy as np import matplotlib.pyplot as plt from sklearn.ensemble import BaggingRegressor from sklearn.tree import DecisionTreeRegressor You need a few select settings. n_repeat = 100       # Number of iterations for processing n_train = 100        # Size of the Training Data set n_test = 10000       # Size of the Test Data set noise = 0.1          # Standard deviation of the noise introduced np.random.seed(0) You will select two estimators to compare. estimators = [("Tree", DecisionTreeRegressor()),               ("Bagging(Tree)", BaggingRegressor(DecisionTreeRegressor()))] n_estimators = len(estimators) You will need a set of data to perform the bagging against, so generate a random data set. def f(x):     x = x.ravel()     return np.exp(-x ** 2) - 2 * np.exp(-(x - 2) ** 2) You can experiment with other data configurations, if you want to see the process working. You need to create a function to add the noise to the data. def generate(n_samples, noise, n_repeat=1):     X = np.random.rand(n_samples) * 10 - 5     X = np.sort(X)     if n_repeat == 1:         y = f(X) + np.random.normal(0.0, noise, n_samples)     else:         y = np.zeros((n_samples, n_repeat))         for i in range(n_repeat):             y[:, i] = f(X) + np.random.normal(0.0, noise, n_samples)     X = X.reshape((n_samples, 1))     return X, y You can now train the system using these Transform steps. X_train = [] y_train = [] You train the system with the bagging data set, by taking a sample each cycle. This exposes the model to a more diverse data-training spread of the data. for i in range(n_repeat):     X, y = generate(n_samples=n_train, noise=noise)     X_train.append(X)     y_train.append(y) You can now test your models. X_test, y_test = generate(n_samples=n_test, noise=noise, n_repeat=n_repeat) You can now loop over estimators to compare the results, by computing your predictions. for n, (name, estimator) in enumerate(estimators):     y_predict = np.zeros((n_test, n_repeat))     for i in range(n_repeat):         estimator.fit(X_train[i], y_train[i])         y_predict[:, i] = estimator.predict(X_test)     # Bias^2 + Variance + Noise decomposition of the mean squared error     y_error = np.zeros(n_test)     for i in range(n_repeat):         for j in range(n_repeat):             y_error += (y_test[:, j] - y_predict[:, i]) ** 2     y_error /= (n_repeat * n_repeat)     y_noise = np.var(y_test, axis=1)     y_bias = (f(X_test) - np.mean(y_predict, axis=1)) ** 2     y_var = np.var(y_predict, axis=1) You can now display your results.     print("{0}: {1:.4f} (error) = {2:.4f} (bias^2) "           " + {3:.4f} (var) + {4:.4f} (noise)".format(name,          np.mean(y_error),          np.mean(y_bias),          np.mean(y_var),          np.mean(y_noise))) You can now plot your results.     plt.subplot(2, n_estimators, n + 1)     plt.plot(X_test, f(X_test), "b", label="$f(x)$")     plt.plot(X_train[0], y_train[0], ".b", label="LS ~ $y = f(x)+noise$")     for i in range(n_repeat):         if i == 0:             plt.plot(X_test, y_predict[:, i], "r", label="$\^y(x)$")         else:             plt.plot(X_test, y_predict[:, i], "r", alpha=0.05)     plt.plot(X_test, np.mean(y_predict, axis=1), "c",              label="$\mathbb{E}_{LS} \^y(x)$")     plt.xlim([-5, 5])     plt.title(name)     if n == 0:         plt.legend(loc="upper left", prop={"size": 11})     plt.subplot(2, n_estimators, n_estimators + n + 1)     plt.plot(X_test, y_error, "r", label="$error(x)$")     plt.plot(X_test, y_bias, "b", label="$bias^2(x)$"),     plt.plot(X_test, y_var, "g", label="$variance(x)$"),     plt.plot(X_test, y_noise, "c", label="$noise(x)$")     plt.xlim([-5, 5])     plt.ylim([0, 0.1])     if n == 0:         plt.legend(loc="upper left", prop={"size": 11}) Display your hard work! plt.show() Well done. You have completed you bagging example. Remember: The bagging enables the training engine to train against different sets of the data you expect it to process. This eliminates the impact of outliers and extremes on the data model. So, remember that you took a model and trained it against several training sets sampled from the same population of data.

## 随机森林

Random forests, or random decision forests, are an ensemble learning method for classification and regression that works by constructing a multitude of decision trees at training time and outputting the results the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees’ habit of overfitting to their training set. The result is an aggregation of all the trees’ results, by performing a majority vote against the range of results. So, if five trees return three yeses and two nos, it passes a yes out of the Transform step. Sometimes, this is also called tree bagging, as you take a bagging concept to the next level by not only training the model on a range of samples from the data population but by actually performing the complete process with the data bag and then aggregating the data results. Warning If you perform a ten-factor random forest, it means you are performing ten times more processing. So, investigate what the optimum spread is for your forests. Tip I have found that a 15+ spread on the average data set sizes I process improves the result’s optimal. But please experiment, as I have also had solutions in which 5 worked perfectly and a painful process in which, owing to remarkably unpredictable outliers, a 100+ spread in conclusion achieved proper results. Let me guide you through this process. Open your Python editor and prepare the following ecosystem: from sklearn.ensemble import RandomForestClassifier import pandas as pd import numpy as np import sys import os import datetime as dt import calendar as cal Set up the data location and load the data. if sys.platform == 'linux':     Base=os.path.expanduser('~') + 'VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') basedate = dt.datetime(2018,1,1,0,0,0) Company='04-Clark' InputFileName='Process_WIKI_UPS.csv' InputFile=Base+'/'+Company+'/03-Process/01-EDS/02-Python/' + InputFileName ShareRawData=pd.read_csv(InputFile,header=0,\                          usecols=['Open','Close','UnitsOwn'], \                          low_memory=False) You must perform some preprocessing to reveal features in the data. ShareRawData.index.names = ['ID'] ShareRawData['nRow'] = ShareRawData.index ShareRawData['TradeDate']=ShareRawData.apply(lambda row:\             (basedate - dt.timedelta(days=row['nRow'])),axis=1) ShareRawData['WeekDayName']=ShareRawData.apply(lambda row:\             (cal.day_name[row['TradeDate'].weekday()])\              ,axis=1) ShareRawData['WeekDayNum']=ShareRawData.apply(lambda row:\             (row['TradeDate'].weekday())\              ,axis=1) ShareRawData['sTarget']=ShareRawData.apply(lambda row:\             'true' if row['Open'] < row['Close'] else 'false'\             ,axis=1) Here is your data set: print(ShareRawData.head()) Select a data frame with the two feature variables. sColumns=['Open','Close','WeekDayNum'] df = pd.DataFrame(ShareRawData, columns=sColumns) Let’s look at the top-five rows. print(df.head()) You need to select the target column. df2 = pd.DataFrame(['sTarget']) df2.columns =['WeekDayNum'] You must select a training data set. df['is_train'] = np.random.uniform(0, 1, len(df)) <= .75 Now create two new data frames, one with the training rows and the other with the test rows. train, test = df[df['is_train']==True], df[df['is_train']==False] Here is the number of observations for the test and training data frames: print('Number of observations in the training data:', len(train)) print('Number of observations in the test data:',len(test)) Start processing the data by creating a list of the feature column's names features = df.columns[:3] Display your features. print(features) You must factorize your target to use the model I selected. y = pd.factorize(train['WeekDayNum'])[0] You can now view the target values. print(y) You now train The Random Forest Classifier Create a random forest classifier. Tip By convention, data scientists use “clf” for “Classifier.” That way, they can simply change the clf by changing the type of classifier. clf = RandomForestClassifier(n_jobs=2, random_state=0) You now train the classifier to take the training features and learn how they relate to the training y (weekday number). clf.fit(train[features], y) Now apply the classifier to test data. This action is called “scoring.” clf.predict(test[features]) You can look at the predicted probabilities of the first ten observations. print(clf.predict_proba(test[features])[0:10]) Evaluate the classifier. Is it any good? preds = clf.predict(test[features])[3:4] Look at the PREDICTED Week Day Number for the first ten observations. print('PREDICTED Week Number:',preds[0:10]) Look at the ACTUAL WeekDayName for the first ten observations. print(test['WeekDayNum'].head(10)) I suggest you create a confusion matrix. c=pd.crosstab(df2['WeekDayNum'], preds, rownames=['Actual Week Day Number'], colnames=['Predicted Week Day Number']) print(c) You can also look at a list of the features and their importance scores. print(list(zip(train[features], clf.feature_importances_))) You have completed the Transform steps for a random forest solution. At his point, I want to explain an additional aspect of random forests. This is simply the daisy-chaining of a series of random forests to create a solution. I have found these to become more popular over the last two years, as solutions become more demanding and data sets become larger. The same principles apply; you are simply repeating them several times in a chain.

## 计算机视觉

Computer vision is a complex feature extraction area, but once you have the features exposed, it simply becomes a matrix of values. Note In this book, I offer only a basic introduction to the computer vision aspect of data science. This is a massive field in its own right. I simply want you to note that will be a part of your future field of work. Open your Python editor and enter this quick example: import matplotlib.pyplot as plt from PIL import Image import numpy as np sPicNameIn='C:/VKHCG/01-Vermeulen/00-RawData/AudiR8.png' imageIn = Image.open(sPicNameIn) fig1=plt.figure(figsize=(10, 10)) fig1.suptitle('Audi R8', fontsize=20) imgplot = plt.imshow(imageIn) plt.show() You should see a car. imagewidth, imageheight = imageIn.size imageMatrix=np.asarray(imageIn) pixelscnt = (imagewidth * imageheight) print('Pixels:', pixelscnt) print('Size:', imagewidth, ' x', imageheight,) print(imageMatrix) This is what your computer sees! You have achieved computer vision. Remember how I showed you that movies convert several frames? Each frame becomes a matrix’s entry, and now you can make your data science “see.”

## 自然语言处理

Natural language processing is the area in data science that investigates the process we as humans use to communicate with each other. This covers mainly written and spoken words that form bigger concepts. Your data science is aimed at intercepting or interacting with humans, to react to the natural language. Note This is a major field of study in data science. I am only providing a basic introduction to this interesting subject. There are two clear requirements in natural language processing. First is the direct interaction with humans, such as when you speak to your smartphone, and it responds with an appropriate answer. For example, you request “phone home,” and the phone calls the number set as “home.” The second type of interaction is taking the detailed content of the interaction and understanding its context and relationship with other text or recorded information. Examples of these are news reports that are examined, and common trends are found among different news reports. This a study of the natural language’s meaning, not simply a response to a human interaction.

### 基于文本的

If you want to process text, you must set up an ecosystem to perform the basic text processing. I recommend that you use library nltk (conda install -c anaconda nltk). Warning This process generates a large text reference library on your local PC. Make sure that you have more than 3.3GB to download the complete data set. Open your Python editor and set up your ecosystem. You then require the base data. import nltk nltk.download() You will see a program that enables you to download several text libraries, which will assist the process to perform text analysis against any text you submit for analysis. The basic principle is that the library matches your text against the text stored in the data libraries and will return the correct matching text analysis. Open your Python editor and create the following ecosystem, to enable you to investigate this library: from nltk.tokenize import sent_tokenize, word_tokenize Txt = "Good Day Mr. Vermeulen,\  how are you doing today?\  The weather is great, and Data Science is awesome.\  You are doing well!" print(Txt,'\n') print('Identify sentences') print(sent_tokenize(Txt),'\n') print('Identify Word') print(word_tokenize(Txt))

### 基于语音的

There is a major demand for speech-to-text conversion , to extract features. I suggest looking at the SpeechRecognition library ( [https://pypi.python.org/pypi/SpeechRecognition/](https://pypi.python.org/pypi/SpeechRecognition/) ). You can install it by using conda install -c conda-forge speechrecognition. This transform area is highly specialized, and I will not provide any further details on this subject.

## [神经网络](http://analyticsindiamag.com/machine-learning-a-unifying-perspective-new-paths/%23_blank)

Neural networks (also known as artificial neural networks) are inspired by the human nervous system. They simulate how complex information is absorbed and processed by the human system. Just like humans, neural networks learn by example and are configured to a specific application. Neural networks are used to find patterns in extremely complex data and, thus, deliver forecasts and classify data points. Neural networks are usually organized in layers. Layers are made up of a number of interconnected “nodes.” Patterns (features) are presented to the network via the “input layer,” which communicates to one or more “hidden layers,” where the actual processing is done. The hidden layers then link to an “output layer,” where the answer is output, as shown in Figure [10-16](#Fig16).![A435693_1_En_10_Fig16_HTML.jpg](img/A435693_1_En_10_Fig16_HTML.jpg) Figure 10-16General artificial neural network Tip When I perform feature development for a neural network, I frame a question so as to receive a simple response (yes/no), but I create hundreds, or even thousands, of questions. Let me offer an example of feature development for neural networks. Suppose you have to select three colors for your dog’s new ball: blue, yellow, or pink. The features would be

*   球是蓝色的吗？是/否
*   球是黄色的吗？是/否
*   这个球是粉红色的吗？是/否

When you feed these to the neural network, it will use them as simple 0 or 1 values, and this is what neural networks really excel at solving. Unfortunately, the most important feature when buying a ball for a dog is “Does the dog fit under the house?” It took me two hours to retrieve one dog and one black ball from a space I did not fit! The lesson: You must change criteria as you develop the neural network. If you keep the question simple, you can just add more questions, or even remove questions, that result in features. Warning Neural networks can get exceptionally gigantic when you start to work with them. I have a solution that has nearly 5 million features and, during specific processing cycles, is busy with 250,000 hidden layers and delivers just over a thousand outputs to a conjoint analysis of a product range. I have found that the average network has in the range of three to ten hidden layers, with fewer than twenty features. Any bigger networks are mostly outliers on the size scale. Note, too, that you can daisy-chain neural networks , and I design such systems on a regular basis. I call it “neural pipelining and clustering,” as I sell it as a service. Tip Investigate how GPUs improve neural network performance. You also want to look at TensorFlow on GPUs (covered later in this chapter). Before you start your example, you must understand two concepts.

### 梯度下降

Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient of the function at the current point. Example: Open your Python editor and investigate this code: cur_x = 3 # The algorithm starts at x=3 gamma = 0.01 # step size multiplier precision = 0.00001 previous_step_size = cur_x df = lambda x: 4 * x**3 - 9 * x**2 while previous_step_size > precision:     prev_x = cur_x     cur_x += -gamma * df(prev_x)     previous_step_size = abs(cur_x - prev_x)     print("Current X at %f" % cur_x, " with step at %f" % previous_step_size ) print("The local minimum occurs at %f" % cur_x) Can you see how the steps start to improve the process to find the local minimum? The gradient descent can take many iterations to compute a local minimum with a required accuracy. This process is useful when you want to find an unknown value that fits your model. In neural networks this supports the weighting of the hidden layers, to calculate to a required accuracy.

### 正则化强度

Regularization strength is the parameter that prevents overfitting of the neural network. The parameter enables the neural network to match the best set of weights for a general data set. The common name for this setting is the epsilon parameter, also known as the learning rate.

### 简单神经网络

Now that you understand these basic parameters, I will show you an example of a neural network. You can now open a new Python file and your Python editor. Let’s build a simple neural network. Setup the eco-system. import numpy as np from sklearn import datasets, linear_model import matplotlib.pyplot as plt You need a visualization procedure. def plot_decision_boundary(pred_func):     # Set min and max values and give it some padding     x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5     y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5     h = 0.01     # Generate a grid of points with distance h between them     xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))     # Predict the function value for the whole gid     Z = pred_func(np.c_[xx.ravel(), yy.ravel()])     Z = Z.reshape(xx.shape)     # Plot the contour and training examples     plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)     plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral) You will generate a data set for the example. I suggest 200 points, but feel free to increase or decrease as you experiment with your neural network. np.random.seed(0) X, y = datasets.make_moons(200, noise=0.20) You can plot the data to see what you generated. plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral) I suggest we train a logistic regression classifier to feed the features. clf = linear_model.LogisticRegressionCV() clf.fit(X, y)  Now you can plot the decision boundary plot_decision_boundary(lambda x: clf.predict(x)) plt.title("Logistic Regression") You now configure the neural network. I kept it simple, with two inputs and two outputs. num_examples = len(X) # training set size nn_input_dim = 2 # input layer dimensionality nn_output_dim = 2 # output layer dimensionality You set the gradient descent parameters. This drives the speed at which you resolve the neural network. Set learning rate for gradient descent and regularization strength, experiment with this as it drives the transform speeds. epsilon = 0.01 reg_lambda = 0.01 # You must engineer a helper function, to evaluate the total loss on the data set. def calculate_loss(model):     W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']     # Forward propagation to calculate our predictions     z1 = X.dot(W1) + b1     a1 = np.tanh(z1)     z2 = a1.dot(W2) + b2     exp_scores = np.exp(z2)     probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)     # Calculating the loss     corect_logprobs = -np.log(probs[range(num_examples), y])     data_loss = np.sum(corect_logprobs)     # Add regulatization term to loss (optional)     data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))     return 1./num_examples * data_loss You also require a helper function, to predict an output (0 or 1). def predict(model, x):     W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']     # Forward propagation     z1 = x.dot(W1) + b1     a1 = np.tanh(z1)     z2 = a1.dot(W2) + b2     exp_scores = np.exp(z2)     probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)     return np.argmax(probs, axis=1) Your next function to engineer is central to the neural network. This function learns parameters for the neural network and returns the model.

*   nn_hdim:隐藏层中的节点数
*   num_passes:通过梯度下降的训练数据的次数。我建议 20000，但是你可以尝试不同的尺寸。
*   print_loss:如果为 True，则每 1000 次迭代打印一次损失。

def build_model(nn_hdim, num_passes=20000, print_loss=False):     # Initialize the parameters to random values. We need to learn these.     np.random.seed(0)     W1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)     b1 = np.zeros((1, nn_hdim))     W2 = np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)     b2 = np.zeros((1, nn_output_dim))     # This is what we return at the end     model = {}     # Gradient descent. For each batch...     for i in range(0, num_passes):         # Forward propagation         z1 = X.dot(W1) + b1         a1 = np.tanh(z1)         z2 = a1.dot(W2) + b2         exp_scores = np.exp(z2)         probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)         # Backpropagation         delta3 = probs         delta3[range(num_examples), y] -= 1         dW2 = (a1.T).dot(delta3)         db2 = np.sum(delta3, axis=0, keepdims=True)         delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))         dW1 = np.dot(X.T, delta2)         db1 = np.sum(delta2, axis=0)         # Add regularization terms (b1 and b2 don't have regularization terms)         dW2 += reg_lambda * W2         dW1 += reg_lambda * W1         # Gradient descent parameter update         W1 += -epsilon * dW1         b1 += -epsilon * db1         W2 += -epsilon * dW2         b2 += -epsilon * db2         # Assign new parameters to the model         model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}         # Optionally print the loss.         # This is expensive because it uses the whole dataset, so we don't want to do it too often.         if print_loss and i % 1000 == 0:           print ("Loss after iteration %i: %f" %(i, calculate_loss(model)))     return model You now define the model with a three-dimensional hidden layer. model = build_model(3, print_loss=True) You can now plot the decision boundary. plot_decision_boundary(lambda x: predict(model, x)) plt.title("Decision Boundary for hidden layer size 3") You can now visualize what you have achieved. plt.figure(figsize=(16, 32)) hidden_layer_dimensions = [1, 2, 3, 4, 5, 20, 50] for i, nn_hdim in enumerate(hidden_layer_dimensions):     plt.subplot(5, 2, i+1)     plt.title('Hidden Layer size %d' % nn_hdim)     print('Model:',nn_hdim)     model = build_model(nn_hdim, print_loss=True)     plot_decision_boundary(lambda x: predict(model, x)) plt.show() You can now build neural networks. Well done. Advice You should spend time on understanding how to construct neural networks, as I have experienced a massive increase in the amount of solutions that use neural networks to process data science. The rapid advances with GPU-driven processing is pushing this trend even faster. At a minimum, I must build 100 inputs and 10 hidden layers, to handle the smallest networks I receive requests for on a regular basis. Also, look at daisy-chaining via other transform processes I discuss in this chapter. The preceding is neural networking in its simplest form.

## TensorFlow

TensorFlow is an open source software library for numerical computation using data-flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google’s machine intelligence research organization, for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains. I will guide you through a few examples, to demonstrate the capability. I have several installations that use this ecosystem, and it is gaining popularity in the data science communities. To use it, you will require a library named tensorflow. You install it using conda install -c conda-forge tensorflow. Details about the library are available at [www.tensorflow.org](http://www.tensorflow.org) . The next big advantage is the Cloud Tensor Processing Unit (TPU) ( [https://cloud.google.com/tpu/](https://cloud.google.com/tpu/) ) hardware product, which was specifically designed to calculate tensor processing at better performance levels than standard CPU or GPU hardware. The TPU supports the TensorFlow process with an extremely effective hardware ecosystem. The TensorFlow Research Cloud provides users with access to these second-generation cloud TPUs , each of which provides 180 teraflops of machine-learning acceleration. Note I use the open research platform at [www.tensorflow.org/tfrc/](http://www.tensorflow.org/tfrc/) to test any new model I have, and it provides a great R&D ecosystem. I suggest you investigate it for your own ecosystem.

### Basic TensorFlow

I will take you through a basic example by explaining, as a starting point, how to convert the following mathematical equation into a TensorFlow:![ $$ \mathrm{a}=\left(\mathrm{b}+\mathrm{c}\right)\ast \left(\mathrm{c}+2\right)\mathrm{a}=\left(\mathrm{b}+\mathrm{c}\right)\ast \left(\mathrm{c}+2\right) $$ ](img/A435693_1_En_10_Chapter_Equf.gif) I will calculate the following for you:

*   B = 2.5
*   C = 10

Open your Python editor and create the following ecosystem: import tensorflow as tf Create a TensorFlow constant. const = tf.constant(2.0, name="const") Create TensorFlow variables. b = tf.Variable(2.5, name="b") c = tf.Variable(10.0, name="c") You must now create the operations. d = tf.add(b, c, name="d") e = tf.add(c, const, name="e") a = tf.multiply(d, e, name="a") Next, set up the variable initialization. init_op = tf.global_variables_initializer() You can now start the session. with tf.Session() as sess:     # initialise the variables     sess.run(init_op)     # compute the output of the graph     a_out = sess.run(a)     print("Variable a is {}".format(a_out)) Well done. You have just successfully deployed a TensorFlow solution. I will now guide you through a more advance example: how to feed a range of values into a TensorFlow.![ $$ \mathrm{a}=\left(\mathrm{b}+\mathrm{c}\right)\ast \left(\mathrm{c}+22\right)\mathrm{a}=\left(\mathrm{b}+\mathrm{c}\right)\ast \left(\mathrm{c}+22\right) $$ ](img/A435693_1_En_10_Chapter_Equg.gif) I will calculate for the following:

*   B =范围(-5，-4，-3，-2，-1，0，1，2，3，4，5)
*   C = 3

Open your Python editor and create the following ecosystem: import tensorflow as tf import numpy as np Create a TensorFlow constant. const = tf.constant(22.0, name="const") Now create the TensorFlow variables. Note the range format for variable b. b = tf.placeholder(tf.float32, [None, 1], name="b") c = tf.Variable(3.0, name="c") You will create the required operations next. d = tf.add(b, c, name="d") e = tf.add(c, const, name="e") a = tf.multiply(d, e, name="a") Start the setup of the variable initialization. init_op = tf.global_variables_initializer() Start the session construction. with tf.Session() as sess:     # initialise the variables     sess.run(init_op)     # compute the output of the graph     a_out = sess.run(a, feed_dict={b: np.arange(-5, 5)[:, np.newaxis]})     print("Variable a is {}".format(a_out)) Did you notice how with minor changes TensorFlow handles larger volumes of data with ease? The advantage of TensorFlow is the simplicity of the basic building block you use to create it, and the natural graph nature of the data pipelines, which enable you to easily convert data flows from the real world into complex simulations within the TensorFlow ecosystem. I will use a fun game called the One-Arm Bandits to offer a sample real-world application of this technology. Open your Python editor and create the following ecosystem: import tensorflow as tf import numpy as np Let’s construct a model for your bandits. There are four one-arm bandits, and currently, bandit 4 is set to provide a positive reward most often. bandits = [0.2,0.0,-0.2,-2.0] num_bandits = len(bandits) You must model the bandit by creating a pull-bandit action. def pullBandit(bandit):     #Get a random number.     result = np.random.randn(1)     if result > bandit:         #return a positive reward.         return 1     else:         #return a negative reward.         return -1 Now, you reset the ecosystem. tf.reset_default_graph() You need the following two lines to establish the feed-forward part of the network. You perform the actual selection using this formula. weights = tf.Variable(tf.ones([num_bandits])) chosen_action = tf.argmax(weights,0) These next six lines establish the training procedure. You feed the reward and chosen action into the network by computing the loss and using it to update the network. reward_holder = tf.placeholder(shape=[1],dtype=tf.float32) action_holder = tf.placeholder(shape=[1],dtype=tf.int32) responsible_weight = tf.slice(weights,action_holder,[1]) loss = -(tf.log(responsible_weight)*reward_holder) optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001) update = optimizer.minimize(loss) Now, you must train the system to perform as a one-arm bandit. total_episodes = 1000 #Set total number of episodes to train the bandit. total_reward = np.zeros(num_bandits) #Set scoreboard for bandits to 0. e = 0.1 #Set the chance of taking a random action. Initialize the ecosystem now. init = tf.initialize_all_variables() Launch the TensorFlow graph processing. with tf.Session() as sess:     sess.run(init)     i = 0     while i < total_episodes:         #Choose either a random action or one from our network.         if np.random.rand(1) < e:             action = np.random.randint(num_bandits)         else:             action = sess.run(chosen_action)         reward = pullBandit(bandits[action]) Collect your reward from picking one of the bandits and update the network.         _,resp,ww = sess.run([update,responsible_weight,weights], feed_dict={reward_holder:[reward],action_holder:[action]}) Update your running tally of the scores.         total_reward[action] += reward         if i % 50 == 0:             print ("Running reward for the " + str(num_bandits) + " bandits: " + str(total_reward))         i+=1 print ("The agent thinks bandit " + str(np.argmax(ww)+1) + " is the most promising....") if np.argmax(ww) == np.argmax(-np.array(bandits)):     print ("...and it was right!") else:     print ("...and it was wrong!") Congratulations! You have a fully functional TensorFlow solution. Can you think of three real-life examples you can model using this ecosystem?

### 张量流的真实用途

Over the last 12 months, I have used the TensorFlow process for the following three models:

*   篮子分析:人们买什么？他们一起买什么？
*   外汇交易:为公司需求提供何时购买外汇的建议。
*   商品交易:买卖期货。

### 独臂强盗(上下文版本)

To try your hand at a more complex bandit, I have prepared the following example. Let’s play Bandits. Open your Python editor and create the following ecosystem: import tensorflow as tf import tensorflow.contrib.slim as slim import numpy as np Create the contextual bandits. class contextual_bandit():     def __init__(self):         self.state = 0         #List out our bandits.         self.bandits = np.array([\                                  [0.4,0,-0.0,-8],\                                  [0.9,-55,1,0.75],\                                  [-15,5,9,5],\                                  [-5,1,9,55],\                                  [15,51,9,5],\                                  [-15,51,9,55],\                                  [-5,51,99,55],\                                  [-65,51,9,55],\                                  [0.9,-55,1,0.75]\                                  ])         self.num_bandits = self.bandits.shape[0]         self.num_actions = self.bandits.shape[1] Create the bandit.     def getBandit(self):         self.state = np.random.randint(0,len(self.bandits)) #Returns a random state for each episode.         return self.state Create the pull-arm action.     def pullArm(self,action):         #Get a random number.         bandit = self.bandits[self.state,action]         result = np.random.randn(1)         if result > bandit:             #return a positive reward.             return 1         else:             #return a negative reward.             return -1 Configure the rules for the bandits. class agent():     def __init__(self, lr, s_size,a_size):         #These lines established the feed-forward part of the network. The agent takes a state and produces an action.         self.state_in= tf.placeholder(shape=[1],dtype=tf.int32)         state_in_OH = slim.one_hot_encoding(self.state_in,s_size)         output = slim.fully_connected(state_in_OH,a_size,\             biases_initializer=None,activation_fn=tf.nn.sigmoid,weights_initializer=tf.ones_initializer())         self.output = tf.reshape(output,[-1])         self.chosen_action = tf.argmax(self.output,0) The next six lines establish the training procedure. Once again, feed the reward and chosen action into the network.         #to compute the loss, and use it to update the network.         self.reward_holder = tf.placeholder(shape=[1],dtype=tf.float32)         self.action_holder = tf.placeholder(shape=[1],dtype=tf.int32)         self.responsible_weight = tf.slice(self.output,self.action_holder,[1])         self.loss = -(tf.log(self.responsible_weight)*self.reward_holder)         optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)         self.update = optimizer.minimize(self.loss) Train your agent now. tf.reset_default_graph() #Clear the Tensorflow graph. cBandit = contextual_bandit() #Load the bandits. myAgent = agent(lr=0.001,s_size=cBandit.num_bandits,a_size=cBandit.num_actions) #Load the agent. weights = tf.trainable_variables()[0] #The weights we will evaluate to look into the network. total_episodes = 10000 #Set total number of episodes to train agent on. total_reward = np.zeros([cBandit.num_bandits,cBandit.num_actions]) #Set scoreboard for bandits to 0. e = 0.1 #Set the chance of taking a random action. init = tf.initialize_all_variables() Launch the TensorFlow graph. with tf.Session() as sess:     sess.run(init)     i = 0     while i < total_episodes:         s = cBandit.getBandit() #Get a state from the environment.         #Choose either a random action or one from our network.         if np.random.rand(1) < e:             action = np.random.randint(cBandit.num_actions)         else:             action = sess.run(myAgent.chosen_action,feed_dict={myAgent.state_in:[s]})         reward = cBandit.pullArm(action) #Get our reward for taking an action given a bandit.         #Update the network.         feed_dict={myAgent.reward_holder:[reward],myAgent.action_holder:[action],myAgent.state_in:[s]}         _,ww = sess.run([myAgent.update,weights], feed_dict=feed_dict)         #Update our running tally of scores.         total_reward[s,action] += reward         if i % 500 == 0:             print ("Mean reward for each of the " + str(cBandit.num_bandits) + " bandits: " + str(np.mean(total_reward,axis=1)))         i+=1 for a in range(cBandit.num_bandits):     print ("The agent thinks action " + str(np.argmax(ww[a])+1) + " for bandit " + str(a+1) + " is the most promising....")     if np.argmax(ww[a]) == np.argmin(cBandit.bandits[a]):         print ("...and it was right!")     else:         print ("...and it was wrong!") Can you understand how the process assists you to model the real-life activities?

### 处理容器上的数字

Here is a more practical application. You are required to identify the digits on the side of a container as it moves around your shipping yard at Hillman Ltd’s Edinburgh warehouse. As it rains most of the time, the scanning equipment is not producing optimum images as the containers move past the camera recording the digits. Can you resolve the problem of identifying the digits? Open your Python editor and create the following ecosystem: import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets("MNIST_data/", one_hot=True) You must have these optimization variables set. You can later experiment with different numbers to understand what impact they will have on your results. learning_rate = 0.5 epochs = 10 batch_size = 100 You must declare the training data format. Set input x—for 28 × 28 pixels = 784. x = tf.placeholder(tf.float32, [None, 784]) Now declare the output data placeholder—ten digits y = tf.placeholder(tf.float32, [None, 10]) Declare the weights connecting the input to the hidden layer. Yes, you guessed right; this is a neural network. W1 = tf.Variable(tf.random_normal([784, 300], stddev=0.03), name="W1") b1 = tf.Variable(tf.random_normal([300]), name="b1") Now add the weights connecting the hidden layer to the output layer. W2 = tf.Variable(tf.random_normal([300, 10], stddev=0.03), name="W2") b2 = tf.Variable(tf.random_normal([10]), name="b2") Calculate the output of the hidden layer. hidden_out = tf.add(tf.matmul(x, W1), b1) hidden_out = tf.nn.relu(hidden_out) Use a Softmax regression activated output layer. y_ = tf.nn.softmax(tf.add(tf.matmul(hidden_out, W2), b2)) y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999) cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)                          + (1 - y) * tf.log(1 - y_clipped), axis=1)) You will need to add an optimizer. optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy) Next, set up the initialization operator. init_op = tf.global_variables_initializer() Set up the accuracy assessment operation. correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) All you need now is to start the session. with tf.Session() as sess:     # initialise the variables     sess.run(init_op)     total_batch = int(len(mnist.train.labels) / batch_size)     for epoch in range(epochs):          avg_cost = 0          for i in range(total_batch):              batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)              _, c = sess.run([optimiser, cross_entropy],                           feed_dict={x: batch_x, y: batch_y})              avg_cost += c / total_batch          print("Epoch:", (epoch + 1), "cost =", "{:.3f}".format(avg_cost))     print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})) Can you see how the TensorFlow assists you to build complex operations with ease?

#### 使用张量流的线性回归

Do you feel secure enough to perform a linear regression in TensorFlow? Open your Python editor and create the following ecosystem: import tensorflow as tf import numpy import matplotlib.pyplot as plt rng = numpy.random Set the parameters. learning_rate = 0.01 training_epochs = 10000 display_step = 50 Get your training data. train_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,                          7.042,10.791,5.313,7.997,5.654,9.27,3.1]) train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,                          2.827,3.465,1.65,2.904,2.42,2.94,1.3]) n_samples = train_X.shape[0] Set up the TensorFlow graph input. X = tf.placeholder("float") Y = tf.placeholder("float") Set the model weights. W = tf.Variable(rng.randn(), name="weight") b = tf.Variable(rng.randn(), name="bias") Now construct a linear model. pred = tf.add(tf.multiply(X, W), b) Calculate a mean squared error. cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples) Calculate a gradient descent. optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) Initialize the variables. init = tf.global_variables_initializer() Launch the graph. with tf.Session() as sess:     sess.run(init)     # Fit all training data     for epoch in range(training_epochs):         for (x, y) in zip(train_X, train_Y):             sess.run(optimizer, feed_dict={X: x, Y: y})         # Display logs per epoch step         if (epoch+1) % display_step == 0:             c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})             print("Epoch:", '%04d' % (epoch+1), "cost=", "{:.9f}".format(c), \                 "W=", sess.run(W), "b=", sess.run(b))     print("Optimization Finished!")     training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})     print("Training cost=", training_cost, "W=", sess.run(W), "b=", sess.run(b), '\n')     # Graphic display     plt.plot(train_X, train_Y, 'ro', label='Original data')     plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')     plt.legend()     plt.show()     # Testing example, as requested (Issue #2)     test_X = numpy.asarray([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])     test_Y = numpy.asarray([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])     print("Testing... (Mean square loss Comparison)")     testing_cost = sess.run(         tf.reduce_sum(tf.pow(pred - Y, 2)) / (2 * test_X.shape[0]),         feed_dict={X: test_X, Y: test_Y})  # same function as cost above     print("Testing cost=", testing_cost)     print("Absolute mean square loss difference:", abs(         training_cost - testing_cost))     plt.plot(test_X, test_Y, 'bo', label='Testing data')     plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')     plt.legend()     plt.show() Congratulations! You just completed the linear model in TensorFlow with the Transform step. The use of TensorFlow enhances your capacity in a very positive manner. The TensorFlow ecosystem has opened up many new opportunities for processing large and complex data sets in the cloud, returning results to the customer’s equipment.

## 摘要

This chapter was a marathon through complex and important Transform steps. So, let’s review what you have learned.

*   您了解了如何将数据仓库从流程步骤转换成包含维度和事实的数据仓库。
*   向您介绍了 sun 模型，以及它们如何通过使用模型作为与客户和主题专家讨论的基础来帮助传达您的调查结果。
*   已经向您介绍了几个转换步骤和其他技术，您可以对 data vault 甚至数据仓库应用这些步骤和技术，以获得对数据对业务的意义的新见解。

At this point, I want to congratulate you on your progress, as you now can build a stable pipeline from the data lake, via the Retrieve step, Assess step, and Process step, into the data vault. You now can transform data into the warehouse. The next chapter will show you how to take your insights and organize them into specific groupings for our business entities. You will also see how to report the insights to the business community. You can now celebrate with a well-deserved refreshment and then start Chapter [11](11.html).