© Andreas François Vermeulen 2018 Andreas François VermeulenPractical Data Science[https://doi.org/10.1007/978-1-4842-3054-1_3](03.html)

# 3.分层框架

Andreas François Vermeulen<sup class="calibre7">1 </sup> (1)West Kilbride North Ayrshire, UK   In this chapter, I will introduce you to new concepts that enable us to share insights on a common understanding and terminology. I will define the Data Science Framework in detail, while introducing the Homogeneous Ontology for Recursive Uniform Schema (HORUS) . I will take you on a high-level tour of the top layers of the framework, by explaining the fundamentals of the business layer, utility layer, operational management layer, plus audit, balance, and control layers. I will discuss how to engineer a layered framework for improving the quality of data science when you are working in a large team in parallel with common business requirements. Warning  If you are doing small-scale data science, you may be tempted to perform a quick analysis or pilot of the data at hand, without following the suggested framework. Please proceed with caution. I have spent countless hours restructuring small-scale data science pilots that cause problems when they are transferred to large-scale production ecosystems. I am not suggesting that you build the complete set of layers only for smaller projects, only that you ensure that your project will effortlessly fit into the big processing world of production-size data science. You must understand that successful data science is based on the capability to go from pilot project to production at rapid speed. Note Do not be surprised, after you demonstrate your state-of-the-art data science discovery that saves the business millions, to be faced with demands to deploy within hours. The ability to perform this feat is a highly valued skill. The framework recommended will assist in accomplishing this with ease!

## 数据科学框架的定义

Data science is a series of discoveries. You work toward an overall business strategy of converting raw unstructured data from your data lake into actionable business data. This process is a cycle of discovering and evolving your understanding of the data you are working with to supply you with metadata that you need. My suggestion is that you build a basic framework that you use for your data processing. This will enable you to construct a data science solution and then easily transfer it to your data engineering environments. I use the following framework for my projects. It works for projects from small departmental to at-scale internationally distributed deployments, as the framework has a series of layers that enables you to follow a logical building process and then use your data processing and discoveries across many projects.

## 数据挖掘的跨行业标准流程(CRISP-DM)

CRISP-DM was generated in 1996, and by 1997, it was extended via a European Union project, under the ESPRIT funding initiative. It was the majority support base for data scientists until mid-2015\. The web site that was driving the Special Interest Group disappeared on June 30, 2015, and has since reopened. Since then, however, it started losing ground against other custom modeling methodologies. The basic concept behind the process is still valid, but you will find that most companies do not use it as is, in any projects, and have some form of modification that they employ as an internal standard. An overview of the basics is provided in Figure [3-1](#Fig1).![A435693_1_En_3_Fig1_HTML.jpg](Images/A435693_1_En_3_Fig1_HTML.jpg) Figure 3-1CRISP-DM flowchart

### 商业理解

This initial phase indicated in Figure [3-1](#Fig1) concentrates on discovery of the data science goals and requests from a business perspective. I will discuss the approach I use in Chapter [4](04.html). Many businesses use a decision model or process mapping tool that is based on the Decision Model and Notation (DMN) standard. DMN is a standard published by the [Object Management Group](https://en.wikipedia.org/wiki/Object_Management_Group%23Object%20Management%20Group) (see [www.omg.org/spec/DMN/](http://www.omg.org/spec/DMN/) ). The DMN standard offers businesses a modeling notation for describing decision management flows and business rules. I will not venture deeper into this notation in this book, but take note of it, as I have noted its use by my clients.

### 数据理解

The data understanding phase noted in Figure [3-1](#Fig1) starts with an initial data collection and continues with actions to discover the characteristics of the data. This phase identifies data quality complications and insights into the data. In Chapters [7](07.html)–[11](11.html), I will discuss the approach I use, to give you an understanding of the data and to help you identify the required data sources and their characteristics via data science–driven discovery methods.

### 数据准备

The data preparation phase indicated in Figure [3-1](#Fig1) covers all activities to construct the final data set for modeling tools. This phase is used in a cyclical order with the modeling phase, to achieve a complete model. This ensures you have all the data required for your data science. In Chapters [7](07.html) and [8](08.html), I will discuss the approach I use to prepare the data for processing via data science models.

### 建模

In this phase noted in Figure [3-1](#Fig1), different data science modeling techniques are nominated and evaluated for accomplishing the prerequisite outcomes, as per the business requirements. It returns to the data preparation phase in a cyclical order until the processes achieve success. In Chapters [9](09.html)–[11](11.html), I will discuss the approach I use to formulate and build the different models that apply to the data sets.

### 估价

At this stage shown in Figure [3-1](#Fig1), the process should deliver high-quality data science. Before proceeding to final deployment of the data science, validation that the proposed data science solution achieves the business objectives is required. If this fails, the process returns to the data understanding phase, to improve the delivery.

### 部署

Creation of the data science is generally not the end of the project. Once the data science is past the development and pilot phases, it has to go to production. In the rest of the book, I will discuss how you successfully reach the end goal of delivering a practical data science solution to production. At this point, you should have a basic understanding of CRISP-DM, and we can now proceed to look at the process I have been using with success to create my data science projects. I will now proceed to the core framework I use.

## 递归统一模式的同质本体

The Homogeneous Ontology for Recursive Uniform Schema (HORUS) is used as an internal data format structure that enables the framework to reduce the permutations of transformations required by the framework. The use of HORUS methodology results in a hub-and-spoke data transformation approach. External data formats are converted to HORUS format, and then a HORUS format is transformed into any other external format. The basic concept is to take native raw data and then transform it first to a single format. That means that there is only one format for text files, one format for JSON or XML, one format for images and video. Therefore, to achieve any-to-any transformation coverage , the framework’s only requirements are a data-format-to-HORUS and HURUS-to-data-format converter. Table [3-1](#Tab1) demonstrates how by using of the hub-and-spoke methodology on text files to reduce the amount of convertor scripts you must achieve an effective data science solution. You can see that at only 100 text data sets, you can save 98% on a non-hub-and-spoke framework. I advise that you invest time in getting the hub-and-spoke framework to work in your environment, as the savings will reward anytime you invest. Similar deductions are feasible by streamlining other data types.Table 3-1Advantages of Using Hub-and-Spoke Data Conversions

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 数据格式 | 需要普通转换器 | 需要 horus cconverter 中心辐射型) | %已保存 |
| --- | --- | --- | --- |
| Two | four | four | 0.00% |
| three | nine | six | 33.33% |
| four | Sixteen | eight | 50.00% |
| five | Twenty-five | Ten | 60.00% |
| six | Thirty-six | Twelve | 66.67% |
| seven | forty-nine | Fourteen | 71.43% |
| eight | Sixty-four | Sixteen | 75.00% |
| nine | Eighty-one | Eighteen | 77.78% |
| Ten | One hundred | Twenty | 80.00% |
| Twenty-five | Six hundred and twenty-five | Fifty | 92.00% |
| Fifty | Two thousand five hundred | One hundred | 96.00% |
| One hundred | ten thousand | Two hundred | 98.00% |

Details of the requirements of HORUS will be covered in later chapters. At this point, simply take note that you will need some form of common data format internal to your data science process, to pass data from one format to another. I use HORUS principally for data processing and have had success with it for years.

## 分层框架的顶层

The top layers are to support a long-term strategy of creating a Center of Excellence for your data science work. The layers shown in Figure [3-1](#Fig1) enable you to keep track of all the processing and findings you achieve. The use of a Center of Excellence framework is required, if you are working in a team environment. Using the framework will prepare you to work in an environment, with multiple data scientists working on a common ecosystem. If you are just data wrangling a few data files, and your data science skills must yield quick insights, I would advise that you single out what you require from the overall framework. Remember: All big systems were a small pilot at some point. The framework will enable you to turn your small project into a big success, without having to do major restructuring along the route to production. Let’s discuss the framework (Figure [3-2](#Fig2)).![A435693_1_En_3_Fig2_HTML.jpg](Images/A435693_1_En_3_Fig2_HTML.jpg) Figure 3-2Top layers of the data science framework So, now that you’ve seen them, let’s look at these layers in further detail.

### 业务层的基础

The business layer indicated in Figure [3-2](#Fig2) is the principal source of the requirements and business information needed by data scientists and engineers for processing. Tip You can never have too much information that explains the detailed characteristics and requirements of your business. Data science has matured. The years of providing a quick half-baked analysis, then rushing off to the next analysis are, to a great extent, over. You are now required to ensure the same superior quality and consistency to analysis as you would for any other business aspect. Remember You are employed by the business. Look after it, and it will look after you. Material you would expect in the business layer includes the following:

*   最新组织结构图
*   您正在调查的业务流程的业务描述
*   您的主题专家列表
*   项目计划
*   预算
*   功能需求
*   非功能需求
*   数据项的标准

Remember Data science should apply identical sets of rules, as in software engineering and statistics data research. I will discuss the business layer in detail and share my experiences with it in Chapter [4](04.html).

### 实用程序层的基础

I define a utility as a process that is used by many data science projects to achieve common processing methodologies. The utility layer, as shown in Figure [3-2](#Fig2), is a common area in which you store all your utilities. You will be faced regularly with immediate requirements demanding that you fix something in the data or run your “magical” algorithm. You may have spent hours preparing for this moment, only to find that you cannot locate that awesome utility, or you have a version that you cannot be sure of working properly. Been there—and have got the scars to prove it. Tip Collect your utilities (including source code) in one central location. Keep detailed records of every version. If you have data algorithms that you use regularly, I strongly suggest you keep any proof and credentials that show the process to be a high-quality and industry-accepted algorithm. Painful experience has proven to me that you will get tested, and it is essential to back up your data science 100%. The additional value created is the capability to get multiple teams to work on parallel projects and know that each data scientist or engineer is working with clear standards. In Chapter [5](05.html), I will discuss the interesting and useful utilities that I believe every single data scientist has a responsibility to know.

### 运营管理层的基础知识

As referred to in Figure [3-2](#Fig2), operations management is an area of the ecosystem concerned with designing and controlling the process chains of a production environment and redesigning business procedures. This layer stores what you intend to process. It is where you plan your data science processing pipelines. The operations management layer is where you record

*   处理流定义和管理
*   因素
*   行程安排
*   监视
*   沟通
*   发信号

The operations management layer is the common location where you store any of the processing chains you have created for your data science. Note Operations management is how you turn data science experiments into long-term business gains. I will discuss the particulars of a number of subcomponents later in Chapter [6](06.html), covering operations management.

### 审计、平衡和控制层的基础知识

The audit, balance, and control layer, represented in Figure [3-2](#Fig2), is the area from which you can observe what is currently running within your data science environment. It records

*   流程执行统计
*   平衡和控制
*   拒绝和错误处理
*   代码管理

The three subareas are utilized in following manner.

#### 审计

The audit sublayer, indicated in Figure [3-2](#Fig2), records any process that runs within the environment. This information is used by data scientists and engineers to understand and plan improvements to the processing. Tip Make sure your algorithms and processing generate a good and complete audit trail. My experiences have taught me that a good audit trail is essential. The use of the native audit capability of the Data Science Technology Stack will provide you with a quick and effective base for your auditing. The audit information I find essential and useful is discussed in detail in Chapter [6](06.html). Following is a quick introduction to what you could expect in this topic.

#### 保持平衡

The balance sublayer in Figure [3-2](#Fig2) ensures that the ecosystem is balanced across the available processing capability or has the capability to top-up capability during periods of extreme processing. The processing on demand capability of a cloud ecosystem is highly desirable for this purpose. Tip Plan your capability as a combination of always-on and top-up processing. That way, you get maximum productivity from your processing cycle. Using the audit trail, it is possible to adapt to changing requirements and forecast what you will require to complete the schedule of work you submitted to the ecosystem. In the always-on and top-up ecosystem you can build, you can balance your processing requirements by removing or adding resources dynamically as you move through the processing pipe. I suggest that you enable your code to balance its ecosystem as regularly as possible. An example would be during end-of-month processing, you increase your processing capacity to sixty nodes, to handle the extra demand of the end-of-month run. The rest of the month, you run at twenty nodes during business hours. During weekends and other slow times, you only run with five nodes. Massive savings can be generated in this manner.

#### 控制

The control sublayer, indicated in Figure [3-2](#Fig2), controls the execution of the current active data science processes in a production ecosystem. The control elements are a combination of the control element within the Data Science Technology Stack’s individual tools plus a custom interface to control the primary workflow. The control also ensures that when processing experiences an error, it can attempt a recovery, as per your requirements, or schedule a clean-up utility to undo the error. Chapter [6](06.html) will discuss in more detail how, what, and why you need specific interfaces and utilities. Most of my bigger customers have separate discovery environments set up, in which you can run jobs without any limiting control mechanisms. This enables data scientists to concentrate on the models and processing and not on complying with the more controlled production requirements.

### 功能层的基础

The functional layer of the data science ecosystem shown in Figure [3-2](#Fig2) is the main layer of programming required. The functional layer is the part of the ecosystem that executes the comprehensive data science. It consists of several structures.

*   数据模型
*   处理算法
*   基础设施的供应

My processing algorithm is spread across six supersteps of processing, as follows:

1.  1.Retrieve:这个超级步骤包含了通过更结构化的格式从原始数据湖中检索数据的所有处理链。
2.  2.评估:这个超级步骤包含质量保证和额外数据增强的所有处理链。
3.  3.Process:这个超级步骤包含构建数据仓库的所有处理链。
4.  4.Transform:这个超级步骤包含构建数据仓库的所有处理链。
5.  5.Organize:这个超级步骤包含构建数据集市的所有处理链。
6.  6.报告:这个超级步骤包含构建虚拟化和报告可操作知识的所有处理链。

These supersteps are discussed in detail in chapters specifically devoted to them (Chapters [6](06.html)–[11](11.html)), to enable you to master the individual supersteps and the tools from the Data Science Technology Stack that are relevant.

## 高层数据科学与工程的分层框架

Any source code or other supplementary material referenced by me in this book is available to readers on GitHub, via this book’s product page, located at [www.apress.com/9781484230534](http://www.apress.com/9781484230534) . The layered framework is engineered with the following structures. The following scripts will create for you a complete framework in the C:\VKHCG\05-DS\00-Framework directory, if you are on Windows, or ./VKHCG/05-DS/00-Framework, if you are using a Linux environment.

### Windows 操作系统

If you prefer Windows and Python, in the source code for Chapter [2](02.html), you will find the relevant Python script, identified as Build_Win_Framework.py. If you prefer Windows and R, in the source code for Chapter [2](02.html), you will find the relevant Python script, identified as Build_Win_Framework.r.

### Linux 操作系统

If you prefer Linux and Python, in the source code for Chapter [2](02.html), you will find the relevant Python script, identified as Build_ Linux _Framework.py. If you prefer Linux and R, in the source code for Chapter [2](02.html), you will find the relevant Python script, identified as Build_ Linux_Framework.r.

## 摘要

I have explained the basic layered framework that the data science solution requires. You should have a general high-level understanding of how your solution fits together, to achieve success in your own data science solution. In the next chapters, I will provide details for individual components, with practical examples.