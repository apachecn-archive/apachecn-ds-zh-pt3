© Andreas François Vermeulen 2018 Andreas François VermeulenPractical Data Science[https://doi.org/10.1007/978-1-4842-3054-1_6](06.html)

# 6.三个管理层

Andreas François Vermeulen<sup class="calibre7">1 </sup> (1)West Kilbride North Ayrshire, UK   This chapter is about the three management layers that are must-haves for any large-scale data science system. I will discuss them at a basic level. I suggest you scale-out these management capabilities, as your environment grows.

## 运营管理层

The operational management layer is the core store for the data science ecosystem’s complete processing capability. The layer stores every processing schedule and workflow for the all-inclusive ecosystem. This area enables you to see a singular view of the entire ecosystem. It reports the status of the processing. The operations management layer is the layer where I record the following.

### 处理流定义和管理

The processing-stream definitions are the building block of the data science ecosystem. I store all my current active processing scripts in this section. Definition management describes the workflow of the scripts through the system, ensuring that the correct execution order is managed, as per the data scientists’ workflow design. Tip Keep all your general techniques and algorithms in a source-control-based system, such as GitHub or SVN, in the format of importable libraries. That way, you do not have to verify if they work correctly every time you use them. Advice I spend 10% of my time generating new processing building blocks every week and 10% improving existing building blocks. I can confirm that this action easily saves more than 20% of my time on processing new data science projects when I start them, as I already have a base set of tested code to support the activities required. So, please invest in your own and your team’s future, by making this a standard practice for the team. You will not regret the investment. Warning When you replace existing building blocks, check for impacts downstream. I suggest you use a simple versioning scheme of mylib_001_01\. That way, you can have 999 versions with 99 sub-versions. This also ensures that your new version can be orderly rolled out into your customer base. The most successful version is to support the process with a good version-control process that can support multiple branched or forked code sets.

### 因素

The parameters for the processing are stored in this section, to ensure a single location for all the system parameters. You will see in all the following examples that there is an ecosystem setup phase. if sys.platform == 'linux':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' ################################################################ print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ sFileDir=Base + '/01-Vermeulen/01-Retrieve/01-EDS/02-Python' if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ sFileName=Base + '/01-Vermeulen/00-RawData/Country_Currency.xlsx' In my production system, for each customer, we place all these parameters in a single location and then simply call the single location. Two main designs are used:

1.  1.一个简单的文本文件，然后我们将其导入到每个 Python 脚本中
2.  2.由标准参数设置脚本支持的参数数据库，然后我们将它包含到每个脚本中

I will also admit to having several parameters that follow the same format as the preceding examples, and I simply collect them in a section at the top of the code. Advice Find a way that works best for your team and standardize that method across your team.

### 行程安排

The scheduling plan is stored in this section, to enable central control and visibility of the complete scheduling plan for the system. In my solution, I use a Drum-Buffer-Rope (Figure [6-1](#Fig1)) methodology. The principle is simple.![A435693_1_En_6_Fig1_HTML.jpg](Images/A435693_1_En_6_Fig1_HTML.jpg) Figure 6-1Original Drum-Buffer-Rope use Similar to a troop of people marching, the Drum-Buffer-Rope methodology is a standard practice to identify the slowest process and then use this process to pace the complete pipeline. You then tie the rest of the pipeline to this process to control the eco-system’s speed. So, you place the “drum” at the slow part of the pipeline, to give the processing pace, and attach the “rope” to the beginning of the pipeline, and the end by ensuring that no processing is done that is not attached to this drum. This ensures that your processes complete more efficiently, as nothing is entering or leaving the process pipe without been recorded by the drum’s beat. I normally use an independent Python program that employs the directed acyclic graph (DAG) that is provided by the network libraries’ DiGraph structure. This automatically resolves duplicate dependencies and enables the use of a topological sort, which ensures that tasks are completed in the order of requirement. Following is an example: Open this in your Python editor and view the process. import networkx as nx Here you construct your network in any order. DG = nx.DiGraph([         ('Start','Retrieve1'),         ('Start','Retrieve2'),         ('Retrieve1','Assess1'),         ('Retrieve2','Assess2'),         ('Assess1','Process'),         ('Assess2','Process'),         ('Process','Transform'),         ('Transform','Report1'),         ('Transform','Report2')         ]) Here is your network unsorted: print("Unsorted Nodes") print(DG.nodes()) You can test your network for valid DAG. print("Is a DAG?",nx.is_directed_acyclic_graph(DG)) Now you sort the DAG into a correct order. sOrder=nx.topological_sort(DG) print("Sorted Nodes") print(sOrder) You can also visualize your network. pos=nx.spring_layout(DG) nx.draw_networkx_nodes(DG,pos=pos,node_size = 1000) nx.draw_networkx_edges(DG,pos=pos) nx.draw_networkx_labels(DG,pos=pos) You can add some extra nodes and see how this resolves the ordering. I suggest that you experiment with networks of different configurations, as this will enable you to understand how the process truly assists with the processing workload. Tip I normally store the requirements for the nodes in a common database. That way, I can upload the requirements for multiple data science projects and resolve the optimum requirement with ease.

### 监视

The central monitoring process is in this section to ensure that there is a single view of the complete system. Always ensure that you monitor your data science from a single point. Having various data science processes running on the same ecosystem without central monitoring is not advised. Tip I always get my data science to simply set an active status in a central table when it starts and a not-active when it completes. That way, the entire team knows who is running what and can plan its own processing better. If you are running on Windows, try the following: conda install -c primer wmi import wmi c = wmi.WMI () for process in c.Win32_Process ():   print (process.ProcessId, process.Name) For Linux, try this import os pids = [pid for pid in os.listdir('/proc') if pid.isdigit()] for pid in pids:     try:         print open(os.path.join('/proc', pid, 'cmdline'), 'rb').read()     except IOError: # proc has already terminated         continue This will give you a full list of all running processes. I normally just load this into a table every minute or so, to create a monitor pattern for the ecosystem.

### 沟通

All communication from the system is handled in this one section, to ensure that the system can communicate any activities that are happening. We are using a complex communication process via Jira, to ensure we have all our data science tracked. I suggest you look at the Conda install -c conda-forge jira. I will not provide further details on this subject, as I have found that the internal communication channel in any company is driven by the communication tools it uses. The only advice I will offer is to communicate! You would be alarmed if at least once a week, you lost a project owing to someone not communicating what they are running.

### 发信号

The alerting section uses communications to inform the correct person, at the correct time, about the correct status of the complete system. I use Jira for alerting, and it works well. If any issue is raised, alerting provides complete details of what the status was and the errors it generated. I will now discuss each of these sections in more detail and offer practical examples of what to expect or create in each section.

## 审计、平衡和控制层

The audit, balance, and control layer controls any processing currently under way. This layer is the engine that ensures that each processing request is completed by the ecosystem as planned. The audit, balance, and control layer is the single area in which you can observe what is currently running within your data scientist environment. It records

*   流程执行统计
*   平衡和控制
*   拒绝和错误处理
*   故障代码管理

The three subareas are utilized in following manner.

### 审计

First, let’s define what I mean by audit. An audit is a systematic and independent examination of the ecosystem. The audit sublayer records the processes that are running at any specific point within the environment. This information is used by data scientists and engineers to understand and plan future improvements to the processing. Tip Make sure your algorithms and processing generate a good and complete audit trail. My experience shows that a good audit trail is extremely crucial. The use of the built-in audit capability of the data science technology stack’s components supply you with a rapid and effective base for your auditing. I will discuss what audit statistics are essential to the success of your data science. In the data science ecosystem, the audit consists of a series of observers that record preapproved processing indicators regarding the ecosystem. I have found the following to be good indicators for audit purposes.

#### 内置日志记录

I advise you to design your logging into an organized preapproved location, to ensure that you capture every relevant log entry. I also recommend that you do not change the internal or built-in logging process of any of the data science tools , as this will make any future upgrades complex and costly. I suggest that you handle the logs in same manner you would any other data source. Normally, I build a controlled systematic and independent examination of all the built-in logging vaults. That way, I am sure I can independently collect and process these logs across the ecosystem. I deploy five independent watchers for each logging location, as logging usually has the following five layers.

##### 调试监视器

This is the maximum verbose logging level. If I discover any debug logs in my ecosystem, I normally raise an alarm, as this means that the tool is using precise processing cycles to perform low-level debugging. Warning Tools running debugging should not be part of a production system.

##### 信息观察者

The information level is normally utilized to output information that is beneficial to the running and management of a system. I pipe these logs to the central Audit, Balance, and Control data store, using the ecosystem as I would any other data source.

##### 警告观察者

Warning is often used for handled “exceptions” or other important log events. Usually this means that the tool handled the issue and took corrective action for recovery. I pipe these logs to the central Audit, Balance, and Control data store, using the ecosystem as I would any other data source. I also add a warning to the Performing a Cause and Effect Analysis System data store. I will discuss this critical tool later in this chapter.

##### 错误监视器

Error is used to log all unhandled exceptions in the tool. This is not a good state for the overall processing to be in, as it means that a specific step in the planned processing did not complete as expected. Now, the ecosystem must handle the issue and take corrective action for recovery. I pipe these logs to the central Audit, Balance, and Control data store, using the ecosystem as I would any other data source. I also add an error to the Performing a Cause and Effect Analysis System data store. I will discuss this critical tool later in this chapter.

##### 致命守望者

Fatal is reserved for special exceptions/conditions for which it is imperative that you quickly identify these events. This is not a good state for the overall processing to be in, as it means a specific step in the planned processing has not completed as expected. This means the ecosystem must now handle the issue and take corrective action for recovery. Once again, I pipe these logs to the central Audit, Balance, and Control data store, using the ecosystem as I would any other data source. I also add an error to the Performing a Cause and Effect Analysis System data store, which I will discuss later in this chapter. I have discovered that by simply using built-in logging and a good cause-and-effect analysis system, I can handle more than 95% of all issues that arise in the ecosystem.

#### 基本日志记录

Following is a basic logging process I normally deploy in my data science. Open your Python editor and enter the following logging example. You will require the following libraries: import sys import os import logging import uuid import shutil import time You next set up the basic ecosystem, as follows: if sys.platform == 'linux':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' You need the following constants to cover the ecosystem: sCompanies=['01-Vermeulen','02-Krennwallner','03-Hillman','04-Clark'] sLayers=['01-Retrieve','02-Assess','03-Process','04-Transform','05-Organise','06-Report'] sLevels=['debug','info','warning','error'] You can now build the loops to perform a basic logging run. for sCompany in sCompanies:     sFileDir=Base + '/' + sCompany     if not os.path.exists(sFileDir):         os.makedirs(sFileDir)     for sLayer in sLayers:         log = logging.getLogger()  # root logger         for hdlr in log.handlers[:]:  # remove all old handlers             log.removeHandler(hdlr)         sFileDir=Base + '/' + sCompany + '/' + sLayer + '/Logging'         if os.path.exists(sFileDir):             shutil.rmtree(sFileDir)         time.sleep(2)         if not os.path.exists(sFileDir):             os.makedirs(sFileDir)         skey=str(uuid.uuid4())                sLogFile=Base + '/' + sCompany + '/' + sLayer + '/Logging/Logging_'+skey+'.log'         print('Set up:',sLogFile) You set up logging to file, as follows:         logging.basicConfig(level=logging.DEBUG,                             format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',                             datefmt='%m-%d %H:%M',                             filename=sLogFile,                             filemode='w') You define a handler, which writes all messages to sys.stderr.         console = logging.StreamHandler()         console.setLevel(logging.INFO) You set a format for console use.         formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s') You activate the handler to use this format.         console.setFormatter(formatter) Now, add the handler to the root logger.         logging.getLogger('').addHandler(console) Test your root logging.         logging.info('Practical Data Science is fun!.') Test all the other levels.         for sLevel in sLevels:             sApp='Application-'+ sCompany + '-' + sLayer + '-' + sLevel             logger = logging.getLogger(sApp)             if sLevel == 'debug':                 logger.debug('Practical Data Science logged a debugging message.')             if sLevel == 'info':                 logger.info('Practical Data Science logged information message.')             if sLevel == 'warning':                 logger.warning('Practical Data Science logged a warning message.')             if sLevel == 'error':                 logger.error('Practical Data Science logged an error message.') This logging enables you to log everything that occurs in your data science processing to a central file, for each run of the process.

### 过程跟踪

I normally build a controlled systematic and independent examination of the process for the hardware logging. There is numerous server-based software that monitors temperature sensors, voltage, fan speeds, and load and clock speeds of a computer system. I suggest you go with the tool with which you and your customer are most comfortable. I do, however, advise that you use the logs for your cause-and-effect analysis system.

### 起始日期

Keep records for every data entity in the data lake, by tracking it through all the transformations in the system. This ensures that you can reproduce the data, if needed, in the future and supplies a detailed history of the data’s source in the system.

### 数据谱系

Keep records of every change that happens to the individual data values in the data lake. This enables you to know what the exact value of any data record was in the past. It is normally achieved by a valid-from and valid-to audit entry for each data set in the data science environment.

## 保持平衡

The balance sublayer ensures that the ecosystem is balanced across the accessible processing capability or has the capability to top up capability during periods of extreme processing. The processing on-demand capability of a cloud ecosystem is highly desirable for this purpose. Tip Plan your capability as a combination of always-on and top-up processing. By using the audit trail, it is possible to adapt to changing requirements and forecast what you will require to complete the schedule of work you submitted to the ecosystem. I have found that deploying a deep reinforced learning algorithm against the cause-and-effect analysis system can handle any balance requirements dynamically. Note In my experience, even the best pre-plan solution for processing will disintegrate against a good deep-learning algorithm, with reinforced learning capability handling the balance in the ecosystem.

## 控制

The control sublayer controls the execution of the current active data science. The control elements are a combination of the control element within the Data Science Technology Stack’s individual tools plus a custom interface to control the overarching work. The control sublayer also ensures that when processing experiences an error, it can try a recovery, as per your requirements, or schedule a clean-up utility to undo the error. The cause-and-effect analysis system is the core data source for the distributed control system in the ecosystem. I normally use a distributed yoke solution to control the processing. I create an independent process that is created solely to monitor a specific portion of the data processing ecosystem control. So, the control system consists of a series of yokes at each control point that uses Kafka messaging to communicate the control requests. The yoke then converts the requests into a process to execute and manage in the ecosystem. The yoke system ensures that the distributed tasks are completed, even if it loses contact with the central services. The yoke solution is extremely useful in the Internet of things environment, as you are not always able to communicate directly with the data source.

## 轭溶液

The yoke solution is a custom design I have worked on over years of deployments. Apache Kafka is an open source stream processing platform developed to deliver a unified, high-throughput, low-latency platform for handling real-time data feeds. Kafka provides a publish-subscribe solution that can handle all activity-stream data and processing. The Kafka environment enables you to send messages between producers and consumers that enable you to transfer control between different parts of your ecosystem while ensuring a stable process. I will give you a simple example of the type of information you can send and receive.

### 生产者

The producer is the part of the system that generates the requests for data science processing, by creating structures messages for each type of data science process it requires. The producer is the end point of the pipeline that loads messages into Kafka. Note This is for your information only. You do not have to code this and make it run. from kafka import KafkaProducer producer = KafkaProducer(bootstrap_servers='localhost:1234') for _ in range(100):      producer.send('Retrieve', b'Person.csv') # Block until a single message is sent (or timeout) future = producer.send('Retrieve', b'Last_Name.json') result = future.get(timeout=60) # Block until all pending messages are at least put on the network # NOTE: This does not guarantee delivery or success! It is really # only useful if you configure internal batching using linger_ms producer.flush() # Use a key for hashed-partitioning producer.send('York', key=b'Retrieve', value=b'Run') # Serialize json messages import json producer = KafkaProducer(value_serializer=lambda v: json.dumps(v).encode('utf-8')) producer.send('Retrieve', {'Retrieve': 'Run'}) # Serialize string keys producer = KafkaProducer(key_serializer=str.encode) producer.send('Retrieve', key="ping", value=b'1234') # Compress messages producer = KafkaProducer(compression_type='gzip') for i in range(1000):      producer.send('Retrieve', b'msg %d' % i)

### 消费者

The consumer is the part of the process that takes in messages and organizes them for processing by the data science tools. The consumer is the end point of the pipeline that offloads the messages from Kafka. Note This is for your information only. You do not have to code this and make it run. from kafka import KafkaConsumer import msgpack consumer = KafkaConsumer('Yoke') for msg in consumer:      print (msg) # join a consumer group for dynamic partition assignment and offset commits from kafka import KafkaConsumer consumer = KafkaConsumer('Yoke', group_id="Retrieve") for msg in consumer:      print (msg) # manually assign the partition list for the consumer from kafka import TopicPartition consumer = KafkaConsumer(bootstrap_servers='localhost:1234') consumer.assign([TopicPartition('Retrieve', 2)]) msg = next(consumer) # Deserialize msgpack-encoded values consumer = KafkaConsumer(value_deserializer=msgpack.loads) consumer.subscribe(['Yoke']) for msg in consumer:      assert isinstance(msg.value, dict)

### 有向无环图调度

This solution uses a combination of graph theory and publish-subscribe stream data processing to enable scheduling. You can use the Python NetworkX library to resolve any conflicts, by simply formulating the graph into a specific point before or after you send or receive messages via Kafka. That way, you ensure an effective and an efficient processing pipeline. Tip I normally publish the request onto three different message queues, to ensure that the pipeline is complete. The extra redundancy outweighs the extra processing, as the message is typically every small.

### 轭示例

Following is a simple simulation of what I suggest you perform with your processing. Open your Python editor and create the following three parts of the yoke processing pipeline: Create a file called Run-Yoke.py in directory ..\VKHCG\77-Yoke. Enter this code into the file and save it. ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import shutil ################################################################ def prepecosystem():     if sys.platform == 'linux':         Base=os.path.expanduser('~') + '/VKHCG'     else:         Base='C:/VKHCG'     ############################################################     sFileDir=Base + '/77-Yoke'     if not os.path.exists(sFileDir):         os.makedirs(sFileDir)     ############################################################     sFileDir=Base + '/77-Yoke/10-Master'     if not os.path.exists(sFileDir):         os.makedirs(sFileDir)     ############################################################     sFileDir=Base + '/77-Yoke/20-Slave'     if not os.path.exists(sFileDir):         os.makedirs(sFileDir)     ############################################################     return Base ################################################################ def makeslavefile(Base,InputFile):     sFileNameIn=Base + '/77-Yoke/10-Master/'+InputFile     sFileNameOut=Base + '/77-Yoke/20-Slave/'+InputFile     if os.path.isfile(sFileNameIn):         shutil.move(sFileNameIn,sFileNameOut) ################################################################ if __name__ == '__main__':     ################################################################     print('### Start ############################################')     ################################################################     Base = prepecosystem()     sFiles=list(sys.argv)     for sFile in sFiles:         if sFile != 'Run-Yoke.py':             print(sFile)             makeslavefile(Base,sFile)     ################################################################     print('### Done!! ############################################')     ################################################################ Next, create the Master Producer Script. This script will place nine files in the master message queue simulated by a directory called 10-Master. Create a file called Master-Yoke.py in directory ..\VKHCG\77-Yoke. ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import sqlite3 as sq from pandas.io import sql import uuid import re from multiprocessing import Process ################################################################ def prepecosystem():     if sys.platform == 'linux':         Base=os.path.expanduser('~') + '/VKHCG'     else:         Base='C:/VKHCG'     ############################################################     sFileDir=Base + '/77-Yoke'     if not os.path.exists(sFileDir):         os.makedirs(sFileDir)     ############################################################     sFileDir=Base + '/77-Yoke/10-Master'     if not os.path.exists(sFileDir):         os.makedirs(sFileDir)     ############################################################     sFileDir=Base + '/77-Yoke/20-Slave'     if not os.path.exists(sFileDir):         os.makedirs(sFileDir)     ############################################################     sFileDir=Base + '/77-Yoke/99-SQLite'     if not os.path.exists(sFileDir):         os.makedirs(sFileDir)     ############################################################     sDatabaseName=Base + '/77-Yoke/99-SQLite/Yoke.db'     conn = sq.connect(sDatabaseName)     print('Connecting :',sDatabaseName)     sSQL='CREATE TABLE IF NOT EXISTS YokeData (\      PathFileName VARCHAR (1000) NOT NULL\      );'     sql.execute(sSQL,conn)     conn.commit()     conn.close()        return Base,sDatabaseName ################################################################ def makemasterfile(sseq,Base,sDatabaseName):     sFileName=Base + '/77-Yoke/10-Master/File_' + sseq +\     '_' + str(uuid.uuid4()) + '.txt'     sFileNamePart=os.path.basename(sFileName)     smessage="Practical Data Science Yoke \n File: " + sFileName     with open(sFileName, "w") as txt_file:         txt_file.write(smessage)     connmerge = sq.connect(sDatabaseName)     sSQLRaw="INSERT OR REPLACE INTO YokeData(PathFileName)\             VALUES\             ('" + sFileNamePart + "');"     sSQL=re.sub('\s{2,}', ' ', sSQLRaw)     sql.execute(sSQL,connmerge)     connmerge.commit()     connmerge.close()         ################################################################ if __name__ == '__main__':     ################################################################     print('### Start ############################################')     ################################################################     Base,sDatabaseName = prepecosystem()     for t in range(1,10):         sFile='{num:06d}'.format(num=t)         print('Spawn:',sFile)         p = Process(target=makemasterfile, args=(sFile,Base,sDatabaseName))         p.start()         p.join()     ################################################################     print('### Done!! ############################################')     ################################################################ Execute the master script to load the messages into the yoke system. Next, create the Slave Consumer Script. This script will place nine files in the master message queue simulated by a directory called 20-Slave. Create a file called Slave-Yoke.py in directory ..\VKHCG\77-Yoke. ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import sqlite3 as sq from pandas.io import sql import pandas as pd from multiprocessing import Process ################################################################ def prepecosystem():     if sys.platform == 'linux':         Base=os.path.expanduser('~') + '/VKHCG'     else:         Base='C:/VKHCG'     ############################################################     sFileDir=Base + '/77-Yoke'     if not os.path.exists(sFileDir):         os.makedirs(sFileDir)     ############################################################     sFileDir=Base + '/77-Yoke/10-Master'     if not os.path.exists(sFileDir):         os.makedirs(sFileDir)     ############################################################     sFileDir=Base + '/77-Yoke/20-Slave'     if not os.path.exists(sFileDir):         os.makedirs(sFileDir)     ############################################################     sFileDir=Base + '/77-Yoke/99-SQLite'     if not os.path.exists(sFileDir):         os.makedirs(sFileDir)     ############################################################     sDatabaseName=Base + '/77-Yoke/99-SQLite/Yoke.db'     conn = sq.connect(sDatabaseName)     print('Connecting :',sDatabaseName)     sSQL='CREATE TABLE IF NOT EXISTS YokeData (\      PathFileName VARCHAR (1000) NOT NULL\      );'     sql.execute(sSQL,conn)     conn.commit()     conn.close()        return Base,sDatabaseName ################################################################ def makeslavefile(Base,InputFile):     sExecName=Base + '/77-Yoke/Run-Yoke.py'     sExecLine='python ' + sExecName + ' ' + InputFile     os.system(sExecLine) ################################################################ if __name__ == '__main__':     ################################################################     print('### Start ############################################')     ################################################################     Base,sDatabaseName = prepecosystem()     connslave = sq.connect(sDatabaseName)     sSQL="SELECT PathFileName FROM YokeData;"     SlaveData=pd.read_sql_query(sSQL, connslave)     for t in range(SlaveData.shape[0]):         sFile=str(SlaveData['PathFileName'][t])         print('Spawn:',sFile)         p = Process(target=makeslavefile, args=(Base,sFile))         p.start()         p.join()     ################################################################     print('### Done!! ############################################')     ################################################################ Execute the script and observe the slave script retrieving the messages and then using the Run-Yoke to move the files between the 10-Master and 20-Slave directories. This is a simulation of how you could use systems such as Kafka to send messages via a producer and, later, via a consumer to complete the process, by retrieving the messages and executing another process to handle the data science processing. Well done, you just successfully simulated a simple message system. Tip In this manner, I have successfully passed messages between five data centers across four time zones. It is worthwhile to invest time and practice to achieve a high level of expertise with using messaging solutions.

## 因果分析系统

The cause-and-effect analysis system is the part of the ecosystem that collects all the logs, schedules, and other ecosystem-related information and enables data scientists to evaluate the quality of their system. Advice Apply the same data science techniques to this data set, to uncover the insights you need to improve your data science ecosystem. You have now successfully completed the management sections of the ecosystem. I will now introduce the core data science process for this book.

## 功能层

The functional layer of the data science ecosystem is the largest and most essential layer for programming and modeling. In Chapters [7](07.html)–[11](11.html), I will cover the complete functional layer in detail. Any data science project must have processing elements in this layer. The layer performs all the data processing chains for the practical data science. Before I officially begin the discussion of the functional layer, I want to share my successful fundamental data science process.

## 数据科学过程

Following are the five fundamental data science process steps that are the core of my approach to practical data science.

### 从一个假设问题开始

Decide what you want to know, even if it is only the subset of the data lake you want to use for your data science, which is a good start. For example, let’s consider the example of a small car dealership. Suppose I have been informed that Bob was looking at cars last weekend. Therefore, I ask: “What if I know what car my customer Bob will buy next?”

### 猜测一种可能的模式

Use your experience or insights to guess a pattern you want to discover, to uncover additional insights from the data you already have. For example, I guess Bob will buy a car every three years, and as he currently owns a three-year-old Audi, he will likely buy another Audi. I have no proof; it’s just a guess or so-called gut feeling. Something I could prove via my data science techniques.

### 收集观察结果，并利用它们提出假设

So, I start collecting car-buying patterns on Bob and formulate a hypothesis about his future behavior. For those of you who have not heard of a hypothesis, it is a proposed explanation, prepared on the basis of limited evidence, as a starting point for further investigation. “I saw Bob looking at cars last weekend in his Audi” then becomes “Bob will buy an Audi next, as his normal three-year buying cycle is approaching.”

### 使用真实世界的证据来验证假设

Now, we verify our hypothesis with real-world evidence. On our CCTV, I can see that Bob is looking only at Audis and returned to view a yellow Audi R8 five times over last two weeks. On the sales ledger, I see that Bob bought an Audi both three years previous and six previous. Bob’s buying pattern, then, is every three years. So, our hypothesis is verified. Bob wants to buy my yellow Audi R8.

### 当您获得洞察力时，及时并定期地与客户和主题专家协作

The moment I discover Bob’s intentions, I contact the salesperson, and we successfully sell Bob the yellow Audi R8. These five steps work, but I will acknowledge that they serve only as my guide while prototyping. Once you start working with massive volumes, velocities, and variance in data, you will need a more structured framework to handle the data science. In Chapters [7](07.html)–[11](11.html), I will return to the data science process and explain how the preceding five steps fit into the broader framework. So, let’s discuss the functional layer of the framework in more detail. As previously mentioned, the functional layer of the data science ecosystem is the largest and most essential layer for programming and modeling. The functional layer is the part of the ecosystem that runs the comprehensive data science ecosystem. Warning When database administrators refer to a data model, they are referring to data schemas and data formats in the data science world. Data science views data models as a set of algorithms and processing rules applied as part of data processing pipelines. So, make sure that when you talk to people, they are clear on what you are talking about in all communications channels. It consists of several structures, as follows:

*   数据模式和数据格式:功能数据模式和数据格式部署到数据湖的原始数据上，通过功能层执行所需的模式查询。
*   数据模型:这些构成了未来处理的基础，通过存储已经处理过的数据源供数据湖的其他进程将来使用，增强了数据湖的处理能力。
*   处理算法:功能处理是通过处理链中一系列设计良好的算法来执行的。
*   基础设施的供应:功能性基础设施供应使框架能够使用诸如 Apache Mesos 之类的技术向生态系统添加处理能力，这使得能够对处理工作单元进行动态供应。

The processing algorithms and data models are spread across six supersteps for processing the data lake.

1.  1.恢复
    *   这个超级步骤包含从原始数据湖中检索数据到更结构化格式的所有处理链。这个超级步骤的细节在第 [7](07.html) 章中讨论。 
2.  2.评定
    *   这个超级步骤包含质量保证和额外数据增强的所有处理链。这一超级步骤的细节将在第 [8](08.html) 章中讨论。 
3.  3.过程
    *   这个超级步骤包含构建数据仓库的所有处理链。这个超级步骤的细节在第 [9](09.html) 章中讨论。 
4.  4.改变
    *   这个超级步骤包含从核心数据仓库构建数据仓库的所有处理链。第 [10](10.html) 章讨论了这一超级步骤的细节。 
5.  5.组织
    *   这个超级步骤包含从核心数据仓库构建数据集市的所有处理链。第 [11](11.html) 章讨论了这一超级步骤的细节。 
6.  6.报告
    *   这个超级步骤包含构建虚拟化和报告可操作知识的所有处理链。第 [11](11.html) 章讨论了这一超级步骤的细节。 

These six supersteps, discussed in detail in individual chapters devoted to them, enable the reader to master both them and the relevant tools from the Data Science Technology Stack.

## 摘要

This chapter covered the three management layers of any data science ecosystem. Operational management ensures that your system has everything ready to process the data with the data techniques you require for your data science. It stores and has all the processing capacity of your solution ready to process data. The audit, balance, and control layer controls any processing that is currently being performed and keeps records of what happened and is happening in the system. Note this area is the biggest source of information for optimizing your own solutions for improving the performance of the ecosystem. The functional layer of the data science ecosystem is the largest and most essential layer for programming and modeling. These are the specific combinations of the techniques, algorithms, and methods deployed against each customer’s data sets. This single layer is the biggest part of the ecosystem. I also discussed other important concepts. I discussed the yoke solution that I personally use for my data science, as it supports distributed processing, using a messaging ecosystem. This is useful, when employing data from various distributed data lakes, to formulate a holistic data science solution. I related the basic data science process that I follow when completing a data science project, regardless of the scale or complexity of the data. It is my guide to good practical data science. In the next chapter, I will expand on the techniques, algorithms. and methods used in the functional layer, to enable you to process data lakes to gain business insights.