© Andreas François Vermeulen 2018 Andreas François VermeulenPractical Data Science[https://doi.org/10.1007/978-1-4842-3054-1_11](#)

# 11.组织和报告超级步骤

Andreas François Vermeulen<sup class="calibre7">1 </sup> (1)West Kilbride North Ayrshire, UK   This chapter will cover the Organize superstep first, then proceed to the Report superstep. The two sections will enable you, as the data scientist, first to collect the relevant information from your prepared data warehouse, to match the requirement of a specific segment of the customer’s decision makers. For example, you will use the same data warehouse to report to the chief financial officer (CFO) and the accountant in Wick, Scotland, but the CFO will receive an overall view, in addition to detailed views of all regions, while the accountant in Wick will see only details related to Wick. This is called organizing your data into a smaller data structure called a “data mart .” The second part of the chapter will introduce you to various reporting techniques that you can use to visualize the data for the various customers. I will discuss what each is used for and supply examples of the individual report techniques. So, let’s start our organization of data the warehouse’s data into data marts.

## 组织超级步骤

The Organize superstep takes the complete data warehouse you built at the end of the Transform superstep and subsections it into business-specific data marts. A data mart is the access layer of the data warehouse environment built to expose data to the users. The data mart is a subset of the data warehouse and is generally oriented to a specific business group. Any source code or other supplementary material referenced by me in this book is available to readers on GitHub, via this book’s product page, located at [www.apress.com/9781484230534](http://www.apress.com/9781484230534) . Please note that this source code assumes you have completed the source code setup outlined in Chapter [2](02.html).

### 水平样式

Performing horizontal-style slicing or subsetting of the data warehouse is achieved by applying a filter technique that forces the data warehouse to show only the data for a specific preselected set of filtered outcomes against the data population. The horizontal-style slicing selects the subset of rows from the population while preserving the columns. That is, the data science tool can see the complete record for the records in the subset of records. Let’s look at the technique, using the following example: Start with the standard ecosystem. ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import pandas as pd import sqlite3 as sq ################################################################ If sys.platform == 'linux' or sys.platform == ' darwin':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ ################################################################ Company='01-Vermeulen' ################################################################ sDataWarehouseDir=Base + '/99-DW' if not os.path.exists(sDataWarehouseDir):     os.makedirs(sDataWarehouseDir) ################################################################ sDatabaseName=sDataWarehouseDir + '/datawarehouse.db' conn1 = sq.connect(sDatabaseName) ################################################################ sDatabaseName=sDataWarehouseDir + '/datamart.db' conn2 = sq.connect(sDatabaseName) ################################################################ Load the complete BMI data set from the data warehouse. Note You are loading the data into memory for processing. The code assumes you have enough memory to perform the analysis in memory. To assist with removing unwanted or invalid data, you will reduce the memory requirements. The next query loads all the data into memory, and that means you will have the complete data set ready in memory, but it also means you will have allocated memory to the data set that you may not need for your further analysis. print('################') sTable = 'Dim-BMI' print('Loading :',sDatabaseName,' Table:',sTable) sSQL="SELECT * FROM [Dim-BMI];" PersonFrame0=pd.read_sql_query(sSQL, conn1) The following introduces a solution, by using a data-slicing technique. Let’s look first at “horizontal” slicing, by loading only any person taller than 1.5 meters and having an indicator of 1\. This will reduce the data set, to make it less demanding on memory. Load the horizontal data slice for body mass index (BMI) from the data warehouse. print('################') sTable = 'Dim-BMI' print('Loading :',sDatabaseName,' Table:',sTable) sSQL="SELECT PersonID,\        Height,\        Weight,\        bmi,\        Indicator\   FROM [Dim-BMI]\   WHERE \   Height > 1.5 \   and Indicator = 1\   ORDER BY  \        Height,\        Weight;" PersonFrame1=pd.read_sql_query(sSQL, conn1) ################################################################ DimPerson=PersonFrame1 DimPersonIndex=DimPerson.set_index(['PersonID'],inplace=False) ################################################################ Note You will now replace the data in the database with the reduced data slice. This will make all future queries result in a reduced memory requirement. Store the horizontal data slice for BMI into the data warehouse. sTable = 'Dim-BMI' print('\n#################################') print('Storing :',sDatabaseName,'\n Table:',sTable) print('\n#################################') DimPersonIndex.to_sql(sTable, conn2, if_exists="replace") ################################################################ print('################') sTable = 'Dim-BMI' print('Loading :',sDatabaseName,' Table:',sTable) sSQL="SELECT * FROM [Dim-BMI];" PersonFrame2=pd.read_sql_query(sSQL, conn2) You can show your results by printing the following code . You can see the improvement you achieved. print('Full Data Set (Rows):', PersonFrame0.shape[0]) print('Full Data Set (Columns):', PersonFrame0.shape[1]) print('Horizontal Data Set (Rows):', PersonFrame2.shape[0]) print('Horizontal Data Set (Columns):', PersonFrame2.shape[1]) You should return the following: ################################ Full Data Set (Rows): 1080 Full Data Set (Columns): 5 ################################ Horizontal Data Set (Rows): 194 Horizontal Data Set (Columns): 5 ################################ This shows that you successfully reduced the records from 1080 to 194 relevant for your specific data slice, while keeping the 5 columns of data. Well done. You have successfully achieved horizontal-style slicing by modifying the amount of records in the data warehouse. Warning When performing horizontal-style slicing, make sure you do not overwrite the complete data set when you write your results back into the data warehouse. Horizontal-style slicing is the most common organizational procedure that you will perform. This is because most dimensions are set up to normally contain all the columns the business requires. Note You can use the table-creation route or the view-creation route when generating the organize subsets. This ensures your personal preference on what works for your specific customer requirements. In performing the example, you replaced the table permanently, but it is also possible to organize the creation of a view that then performs the horizontal slicing every time you call the view. This protects the original data set but requires extra database processing with every call into memory, while still reducing the memory requirements. I use both in equal measure.

### 垂直样式

Performing vertical-style slicing or subsetting of the data warehouse is achieved by applying a filter technique that forces the data warehouse to show only the data for specific preselected filtered outcomes against the data population. The vertical-style slicing selects the subset of columns from the population, while preserving the rows. That is, the data science tool can see only the preselected columns from a record for all the records in the population. Note The use of vertical-style data slicing is common in systems in which specific data columns may not be shown to everybody, owing to security or privacy regulations. This is the most common way to impose different groups of security on the same data set. Let’s look at the technique, using the following example. You will return only the weight, height, and indicator, to prevent the other columns from being seen, owing to privacy rules. Start with the standard ecosystem. ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import pandas as pd import sqlite3 as sq ################################################################ if sys.platform == 'linux' or sys.platform == ' darwin':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ ################################################################ Company='01-Vermeulen' ################################################################ sDataWarehouseDir=Base + '/99-DW' if not os.path.exists(sDataWarehouseDir):     os.makedirs(sDataWarehouseDir) ################################################################ sDatabaseName=sDataWarehouseDir + '/datawarehouse.db' conn1 = sq.connect(sDatabaseName) ################################################################ sDatabaseName=sDataWarehouseDir + '/datamart.db' conn2 = sq.connect(sDatabaseName) ################################################################ Load the complete BMI data set from the data warehouse. print('################') sTable = 'Dim-BMI' print('Loading :',sDatabaseName,' Table:',sTable) sSQL="SELECT * FROM [Dim-BMI];" PersonFrame0=pd.read_sql_query(sSQL, conn1) Load the vertical data slice for BMI from the data warehouse. print('################') sTable = 'Dim-BMI' print('Loading :',sDatabaseName,' Table:',sTable) sSQL="SELECT \        Height,\        Weight,\        Indicator\   FROM [Dim-BMI];" PersonFrame1=pd.read_sql_query(sSQL, conn1) ################################################################ DimPerson=PersonFrame1 DimPersonIndex=DimPerson.set_index(['Indicator'],inplace=False) ################################################################ sTable = 'Dim-BMI' print('\n#################################') print('Storing :',sDatabaseName,'\n Table:',sTable) print('\n#################################') DimPersonIndex.to_sql(sTable, conn2, if_exists="replace") ################################################################ print('################') sTable = 'Dim-BMI' print('Loading :',sDatabaseName,' Table:',sTable) sSQL="SELECT * FROM [Dim-BMI];" PersonFrame2=pd.read_sql_query(sSQL, conn2) ################################################################ You can show your results by printing the following: print('Full Data Set (Rows):', PersonFrame0.shape[0]) print('Full Data Set (Columns):', PersonFrame0.shape[1]) print('Horizontal Data Set (Rows):', PersonFrame2.shape[0]) print('Horizontal Data Set (Columns):', PersonFrame2.shape[1]) You should return the following: ################################ Full Data Set (Rows): 1080 Full Data Set (Columns): 5 ################################ Horizontal Data Set (Rows): 1080 Horizontal Data Set (Columns): 3 ################################ This shows that you successfully reduced the columns from 5 to 3 relevant for your specific data slice, while keeping the 1080 rows of data. Well done. You have successfully achieved vertical-style slicing by modifying the amount of columns in the data warehouse . Note Vertical slicing is common for data warehouses in which the dimensions are broad and contain dimensional attributes that cover several dissimilar business requirements.

### 岛屿风格

Performing island-style slicing or subsetting of the data warehouse is achieved by applying a combination of horizontal- and vertical-style slicing. This generates a subset of specific rows and specific columns reduced at the same time. Note These types of island slices are typical for snapshotting a reduced data set of data each month. Items may include unpaid accounts, overdue deliveries, and damaged billboards. You do not have to store the complete warehouse. You only want the specific subset of data. The technique generates a set of data islands that are specific to particular requirements within the business. Start with the standard ecosystem. ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import pandas as pd import sqlite3 as sq ################################################################ if sys.platform == 'linux' or sys.platform == ' darwin':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ ################################################################ Company='01-Vermeulen' ################################################################ sDataWarehouseDir=Base + '/99-DW' if not os.path.exists(sDataWarehouseDir):     os.makedirs(sDataWarehouseDir) ################################################################ sDatabaseName=sDataWarehouseDir + '/datawarehouse.db' conn1 = sq.connect(sDatabaseName) ################################################################ sDatabaseName=sDataWarehouseDir + '/datamart.db' conn2 = sq.connect(sDatabaseName) Load the complete BMI data set from the data warehouse. print('################') sTable = 'Dim-BMI' print('Loading :',sDatabaseName,' Table:',sTable) sSQL="SELECT * FROM [Dim-BMI];" PersonFrame0=pd.read_sql_query(sSQL, conn1) Load the island of BMI from the data warehouse. print('################') sTable = 'Dim-BMI' print('Loading :',sDatabaseName,' Table:',sTable) Note Here is the island. You are taking only selective columns of the data warehouse that are bigger than an indicator of two and creating an island. sSQL="SELECT \        Height,\        Weight,\        Indicator\   FROM [Dim-BMI]\   WHERE Indicator > 2\   ORDER BY  \        Height,\        Weight;" PersonFrame1=pd.read_sql_query(sSQL, conn1) ################################################################ DimPerson=PersonFrame1 DimPersonIndex=DimPerson.set_index(['Indicator'],inplace=False) ################################################################ sTable = 'Dim-BMI' print('\n#################################') print('Storing :',sDatabaseName,'\n Table:',sTable) print('\n#################################') DimPersonIndex.to_sql(sTable, conn2, if_exists="replace") ################################################################ print('################') sTable = 'Dim-BMI-Vertical' print('Loading :',sDatabaseName,' Table:',sTable) sSQL="SELECT * FROM [Dim-BMI-Vertical];" PersonFrame2=pd.read_sql_query(sSQL, conn2) You can show your results by printing the following: print('Full Data Set (Rows):', PersonFrame0.shape[0]) print('Full Data Set (Columns):', PersonFrame0.shape[1]) print('Horizontal Data Set (Rows):', PersonFrame2.shape[0]) print('Horizontal Data Set (Columns):', PersonFrame2.shape[1]) You should return the following: ################################ Full Data Set (Rows): 1080 Full Data Set (Columns): 5 ################################ Horizontal Data Set (Rows): 771 Horizontal Data Set (Columns): 3 ################################ Observe that you have successfully organized an island of data that fits your requirements . You reduced both the records and the columns.

### 安全保管库样式

The secure vault is a version of one of the horizontal, vertical, or island slicing techniques, but the outcome is also attached to the person who performs the query. This is common in multi-security environments, where different users are allowed to see different data sets. This process works well, if you use a role-based access control (RBAC) approach to restricting system access to authorized users. The security is applied against the “role,” and a person can then, by the security system, simply be added or removed from the role, to enable or disable access. The security in most data lakes I deal with is driven by an RBAC model that is an approach to restricting system access to authorized users by allocating them to a layer of roles that the data lake is organized into to support security access. It is also possible to use a time-bound RBAC that has different access rights during office hours than after hours. Warning RBAC is a security process that is in effect at most of the customers I deal with daily. Make sure you understand this process, to ensure security compliance. Start with the standard ecosystem. ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import pandas as pd import sqlite3 as sq ################################################################ if sys.platform == 'linux' or sys.platform == ' darwin':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ ################################################################ Company='01-Vermeulen' ################################################################ sDataWarehouseDir=Base + '/99-DW' if not os.path.exists(sDataWarehouseDir):     os.makedirs(sDataWarehouseDir) ################################################################ sDatabaseName=sDataWarehouseDir + '/datawarehouse.db' conn1 = sq.connect(sDatabaseName) ################################################################ sDatabaseName=sDataWarehouseDir + '/datamart.db' conn2 = sq.connect(sDatabaseName) Load the complete BMI data set from the data warehouse. print('################') sTable = 'Dim-BMI' print('Loading :',sDatabaseName,' Table:',sTable) sSQL="SELECT * FROM [Dim-BMI];" PersonFrame0=pd.read_sql_query(sSQL, conn1) Load the security BMI data set from the data warehouse. print('################') sTable = 'Dim-BMI' print('Loading :',sDatabaseName,' Table:',sTable) sSQL="SELECT \        Height,\        Weight,\        Indicator,\        CASE Indicator\        WHEN 1 THEN 'Pip'\        WHEN 2 THEN 'Norman'\        WHEN 3 THEN 'Grant'\        ELSE 'Sam'\        END AS Name\   FROM [Dim-BMI]\   WHERE Indicator > 2\   ORDER BY  \        Height,\        Weight;" PersonFrame1=pd.read_sql_query(sSQL, conn1) ################################################################ DimPerson=PersonFrame1 DimPersonIndex=DimPerson.set_index(['Indicator'],inplace=False) ################################################################ sTable = 'Dim-BMI-Secure' print('\n#################################') print('Storing :',sDatabaseName,'\n Table:',sTable) print('\n#################################') DimPersonIndex.to_sql(sTable, conn2, if_exists="replace") Load Sam’s view of the BMI data set from the data warehouse. print('################################') sTable = 'Dim-BMI-Secure' print('Loading :',sDatabaseName,' Table:',sTable) print('################################') sSQL="SELECT * FROM [Dim-BMI-Secure] WHERE Name = 'Sam';" PersonFrame2=pd.read_sql_query(sSQL, conn2) ################################################################ print('################################') print('Full Data Set (Rows):', PersonFrame0.shape[0]) print('Full Data Set (Columns):', PersonFrame0.shape[1]) print('################################') print('Horizontal Data Set (Rows):', PersonFrame2.shape[0]) print('Horizontal Data Set (Columns):', PersonFrame2.shape[1]) print('Only Sam Data') print(PersonFrame2.head()) print('################################') ################################################################ ################################ Full Data Set (Rows): 1080 Full Data Set (Columns): 5 ################################ Horizontal Data Set (Rows): 692 Horizontal Data Set (Columns): 4 Only Sam’s data appears, as follows:

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 指示器 | 高度 | 重量 | 名字 |
| --- | --- | --- | --- |
| four | one | Thirty-five | 山姆 |
| four | one | Forty | 山姆 |
| four | one | Forty-five | 山姆 |
| four | one | Fifty | 山姆 |
| four | one | Fifty-five | 山姆 |

Note It is better to create roles than named references, as done in the example. The principle is the same, but the security is more flexible.

### 关联规则挖掘

Association rule learning is a rule-based machine-learning method for discovering interesting relations between variables in large databases, similar to the data you will find in a data lake. The technique enables you to investigate the interaction between data within the same population. This example I will discuss is also called “market basket analysis .” It will investigate the analysis of a customer’s purchases during a period of time . The new measure you need to understand is called “lift.” Lift is simply estimated by the ratio of the joint probability of two items x and y, divided by the product of their individual probabilities:![$$ Lift=\frac{\mathrm{P}\left(\mathrm{x},\mathrm{y}\right)}{\mathrm{P}\left(\mathrm{x}\right)P(y)} $$](img/A435693_1_En_11_Chapter_Equa.gif) If the two items are statistically independent, then P(x,y) = P(x)P(y), corresponding to Lift = 1, in that case. Note that anti-correlation yields lift values less than 1, which is also an interesting discovery, corresponding to mutually exclusive items that rarely co-occur. You will require the following additional library: conda install -c conda-forge mlxtend. The general algorithm used for this is the Apriori algorithm for frequent item set mining and association rule learning over the content of the data lake. It proceeds by identifying the frequent individual items in the data lake and extends them to larger and larger item sets, as long as those item sets appear satisfactorily frequently in the data lake. The frequent item sets determined by Apriori can be used to determine association rules that highlight common trends in the overall data lake. I will guide you through an example. Start with the standard ecosystem. ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import pandas as pd from mlxtend.frequent_patterns import apriori from mlxtend.frequent_patterns import association_rules ################################################################ if sys.platform == 'linux' or sys.platform == ' darwin':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ Company='01-Vermeulen' InputFileName='Online-Retail-Billboard.xlsx' EDSAssessDir='02-Assess/01-EDS' InputAssessDir=EDSAssessDir + '/02-Python' ################################################################ sFileAssessDir=Base + '/' + Company + '/' + InputAssessDir if not os.path.exists(sFileAssessDir):     os.makedirs(sFileAssessDir) ################################################################ sFileName=Base+'/'+ Company + '/00-RawData/' + InputFileName ################################################################ Import the Excel worksheet into the ecosystem. df = pd.read_excel(sFileName) print(df.shape) Perform some feature engineering to formulate the basket’s simulation in your model. df['Description'] = df['Description'].str.strip() df.dropna(axis=0, subset=['InvoiceNo'], inplace=True) df['InvoiceNo'] = df['InvoiceNo'].astype('str') df = df[~df['InvoiceNo'].str.contains('C')] basket = (df[df['Country'] =="France"]           .groupby(['InvoiceNo', 'Description'])['Quantity']           .sum().unstack().reset_index().fillna(0)           .set_index('InvoiceNo')) ################################################################ def encode_units(x):     if x <= 0:         return 0     if x >= 1:         return 1 ################################################################ basket_sets = basket.applymap(encode_units) basket_sets.drop('POSTAGE', inplace=True, axis=1) Apply the Apriori algorithm to the data model. frequent_itemsets = apriori(basket_sets, min_support=0.07, use_colnames=True) rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1) print(rules.head()) rules[ (rules['lift'] >= 6) &        (rules['confidence'] >= 0.8) ] You can now check what the results are for a “Green Clock .” sProduct1='ALARM CLOCK BAKELIKE GREEN' print(sProduct1) print(basket[sProduct1].sum()) And a “Red Clock” . . . sProduct2='ALARM CLOCK BAKELIKE RED' print(sProduct2) print(basket[sProduct2].sum()) You can now check what the results for a basket for “Germany” are. basket2 = (df[df['Country'] =="Germany"]           .groupby(['InvoiceNo', 'Description'])['Quantity']           .sum().unstack().reset_index().fillna(0)           .set_index('InvoiceNo')) basket_sets2 = basket2.applymap(encode_units) basket_sets2.drop('POSTAGE', inplace=True, axis=1) frequent_itemsets2 = apriori(basket_sets2, min_support=0.05, use_colnames=True) rules2 = association_rules(frequent_itemsets2, metric="lift", min_threshold=1) print(rules2[ (rules2['lift'] >= 4) &         (rules2['confidence'] >= 0.5)]) You have successfully used a complex Apriori algorithm to organize the data science into a model that you can use to extract insights into the purchasing behavior of customers. Note There are several algorithms and techniques that a good data scientist can deploy to organize the data into logical data sets. I advise you to investigate algorithms that your customer’s data fits best and practice against them, to become an expert.

### 设计一个实用的组织超级步骤

Now that I have explained the various aspects of the Organize superstep, I will show you how to help our company with their processing.

#### vermeulen plc 公司

I will show you how to apply the Organize superstep to our VKHCG companies. So, I will start with a networking requirement. Can you generate a network routing diagram for the VKHCG companies?

##### 创建网络路由图

I will guide you through a possible solution for the requirement, by constructing an island-style Organize superstep that uses a graph data model to reduce the records and the columns on the data set. As a bonus, it creates a visually improved graph that can be used to illustrate the routes. Open your Python editor and start with a standard ecosystem. ################################################################ import sys import os import pandas as pd import networkx as nx import matplotlib.pyplot as plt ################################################################ pd.options.mode.chained_assignment = None ################################################################ if sys.platform == 'linux' or sys.platform == ' darwin':     Base=os.path.expanduser('~') + 'VKHCG' else:     Base='C:/VKHCG' ################################################################ print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ sInputFileName='02-Assess/01-EDS/02-Python/Assess-Network-Routing-Company.csv' ################################################################ sOutputFileName1='05-Organise/01-EDS/02-Python/Organise-Network-Routing-Company.gml' sOutputFileName2='05-Organise/01-EDS/02-Python/Organise-Network-Routing-Company.png' Company='01-Vermeulen' ################################################################ Now load the company information from a comma-delimited value file . ################################################################ ### Import Country Data ################################################################ sFileName=Base + '/' + Company + '/' + sInputFileName print('################################') print('Loading :',sFileName) print('################################') CompanyData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") print('################################') Now, you can inspect the company information. ################################################################ print(CompanyData.head()) print(CompanyData.shape) ################################################################ Let’s start a new graph to load the company information. G=nx.Graph() for i in range(CompanyData.shape[0]):     for j in range(CompanyData.shape[0]):         Node0=CompanyData['Company_Country_Name'][i]         Node1=CompanyData['Company_Country_Name'][j]         if Node0 != Node1:             G.add_edge(Node0,Node1) for i in range(CompanyData.shape[0]):     Node0=CompanyData['Company_Country_Name'][i]     Node1=CompanyData['Company_Place_Name'][i] + '('+ CompanyData['Company_Country_Name'][i] + ')'     if Node0 != Node1:         G.add_edge(Node0,Node1) Now you can inspect the company information within the graph . print('Nodes:', G.number_of_nodes()) print('Edges:', G.number_of_edges()) I suggest you store your graph to disk now. ################################################################ sFileName=Base + '/' + Company + '/' + sOutputFileName1 print('################################') print('Storing :',sFileName) print('################################') nx.write_gml(G, sFileName) ################################################################ You can now supply insights against your requirement. I suggest you create an image of the graph, to support your findings. sFileName=Base + '/' + Company + '/' + sOutputFileName2 print('################################') print('Storing Graph Image:',sFileName) print('################################') plt.figure(figsize=(15, 15)) pos=nx.spectral_layout(G,dim=2) nx.draw_networkx_nodes(G,pos, node_color="k", node_size=10, alpha=0.8) nx.draw_networkx_edges(G, pos,edge_color='r', arrows=False, style="dashed") nx.draw_networkx_labels(G,pos,font_size=12,font_family='sans-serif',font_color='b') plt.axis('off') plt.savefig(sFileName,dpi=600) plt.show() ################################################################ print('################################') print('### Done!! #####################') print('################################') ################################################################ Congratulations! You can now answer the requirement. Can you generate a network routing diagram for the VKHCG companies? Yes, you just did. Your network is a core of three nodes with a hub structure like that of a wheel (Figure [11-1](#Fig1)).![A435693_1_En_11_Fig1_HTML.jpg](img/A435693_1_En_11_Fig1_HTML.jpg) Figure 11-1Node diagram for United States, United Kingdom, and Germany

#### Krennwallner 股份有限公司

The Krennwallner group requires picking content for the billboards. Can you organize a model to support them with their inquiry?

##### 为广告牌挑选内容

To enable the marketing salespeople to sell billboard content , they will require a diagram to show what billboards connect to which office content publisher. Each of Krennwallner’s billboards has a proximity sensor that enables the content managers to record when a registered visitor points his/her smartphone at the billboard content or touches the near-field pad with a mobile phone. So, I will assist you in building an organized graph of the billboards’ locations data to help you to gain insights into the billboard locations and content picking process. Open your Python editor and start with a standard ecosystem. ################################################################ import sys import os import pandas as pd import networkx as nx import matplotlib.pyplot as plt import numpy as np ################################################################ pd.options.mode.chained_assignment = None ################################################################ if sys.platform == 'linux' or sys.platform == ' darwin':     Base=os.path.expanduser('~') + 'VKHCG' else:     Base='C:/VKHCG' ################################################################ print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ sInputFileName='02-Assess/01-EDS/02-Python/Assess-DE-Billboard-Visitor.csv' ################################################################ sOutputFileName1='05-Organise/01-EDS/02-Python/Organise-Billboards.gml' sOutputFileName2='05-Organise/01-EDS/02-Python/Organise-Billboards.png' Billboard='02-Krennwallner' ################################################################ Load the proximity sensor data for each billboard. ################################################################ ### Import Proximity Sensor Data ################################################################ sFileName=Base + '/' + Billboard + '/' + sInputFileName print('################################') print('Loading :',sFileName) print('################################') BillboardDataRaw=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") print('################################') Let’s get insights into the data . ################################################################ print(BillboardDataRaw.head()) print(BillboardDataRaw.shape) BillboardData=BillboardDataRaw I suggest extracting a sample from the population, to create a training and test set of data. sSample=list(np.random.choice(BillboardData.shape[0],20)) I suggest generating a graph data set from the sample. G=nx.Graph() for i in sSample:     for j in sSample:         Node0=BillboardData['BillboardPlaceName'][i] + '('+ BillboardData['BillboardCountry'][i] + ')'         Node1=BillboardData['BillboardPlaceName'][j] + '('+ BillboardData['BillboardCountry'][i] + ')'         if Node0 != Node1:             G.add_edge(Node0,Node1) for i in sSample:     Node0=BillboardData['BillboardPlaceName'][i] + '('+ BillboardData['VisitorPlaceName'][i] + ')'     Node1=BillboardData['BillboardPlaceName'][i] + '('+ BillboardData['VisitorCountry'][i] + ')'     if Node0 != Node1:         G.add_edge(Node0,Node1) You can now get various insights from the graph . print('Nodes:', G.number_of_nodes()) print('Edges:', G.number_of_edges()) Save your hard work. ################################################################ sFileName=Base + '/' + Billboard + '/' + sOutputFileName1 print('################################') print('Storing :',sFileName) print('################################') nx.write_gml(G, sFileName) I will now guide you through a visualization process. ################################################################ sFileName=Base + '/' + Billboard + '/' + sOutputFileName2 print('################################') print('Storing Graph Image:',sFileName) print('################################') plt.figure(figsize=(15, 15)) pos=nx.circular_layout(G,dim=2) nx.draw_networkx_nodes(G,pos, node_color="k", node_size=150, alpha=0.8) nx.draw_networkx_edges(G, pos,edge_color='r', arrows=False, style="solid") nx.draw_networkx_labels(G,pos,font_size=12,font_family='sans-serif',font_color='b') plt.axis('off') plt.savefig(sFileName,dpi=600) plt.show() ################################################################ print('################################') print('### Done!! #####################') print('################################') ################################################################ Success! You have a graph that can support any further inquiries and a picture you can distribute to the marketing departments (Figure [11-2](#Fig2)).![A435693_1_En_11_Fig2_HTML.jpg](img/A435693_1_En_11_Fig2_HTML.jpg) Figure 11-2Krennwallner’s marketing departments

#### 希尔曼有限公司

Hillman is now expanding its operations to cover a new warehouse and shops. Can you assist the company with a standard planning pack for activating the logistics for Warehouse 13? Tip The system records Warehouse 13 as WH-KA13\. Have you seen this before? Yes, you are right; you processed it earlier in the book.

##### 创建交货路线

Hillman requires a new delivery route plan from HQ-KA13’s delivery region . The managing director has to know the following:

*   如果成本是每英里 1.50 美元，每天计划两次旅行，那么他最昂贵的路线是什么
*   该地区每月 30 天的平均旅行距离是多少英里

With your newfound knowledge in building the technology stack for turning data lakes into business assets, can you convert the graph stored in the Assess step called “Assess_Best_Logistics ” into the shortest path between the two points? Open your Python editor and start with a standard ecosystem. # -*- coding: utf-8 -*- ################################################################ import sys import os import pandas as pd ################################################################ if sys.platform == 'linux' or sys.platform == ' darwin':     Base=os.path.expanduser('~') + 'VKHCG' else:     Base='C:/VKHCG' ################################################################ print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ sInputFileName='02-Assess/01-EDS/02-Python/Assess_Shipping_Routes.txt' ################################################################ sOutputFileName='05-Organise/01-EDS/02-Python/Organise-Routes.csv' Company='03-Hillman' ################################################################ You now import the possible routes that you processed before and stored to disk earlier in the book. ################################################################ ### Import Routes Data ################################################################ sFileName=Base + '/' + Company + '/' + sInputFileName print('################################') print('Loading :',sFileName) print('################################') RouteDataRaw=pd.read_csv(sFileName,header=0,low_memory=False, sep='|', encoding="latin-1") print('################################') Isolate with a horizontal-style slice the specific WH-KA13-related data. ################################################################ RouteStart=RouteDataRaw[RouteDataRaw['StartAt']=='WH-KA13'] ################################################################ With a secondary horizontal-style slice, isolate WH-KA13’s distance in miles data . RouteDistance=RouteStart[RouteStart['Cost']=='DistanceMiles'] Now organize it into a logical order. RouteDistance=RouteDistance.sort_values(by=['Measure'], ascending=False) Determine the maximum distance, and calculate the cost for performing two trips at £1.50 per mile. ################################################################ RouteMax=RouteStart["Measure"].max() RouteMaxCost=round((((RouteMax/1000)*1.5*2)),2) print('################################') print('Maximum (£) per day:') print(RouteMaxCost) print('################################') Determine the mean distance, and calculate the 30-day monthly distance in miles. ################################################################ RouteMean=RouteStart["Measure"].mean() RouteMeanMonth=round((((RouteMean/1000)*2*30)),6) print('################################') print('Mean per Month (Miles):') print(RouteMeanMonth) print('################################') You have just used existing data science outcomes and a series of extra organize processes to answer the logistics questions from the MD . Well done. Warehouse 13 will be a big success , if your numbers are correct.

#### 克拉克有限公司

Our financial services company has been tasked to investigate the options to convert 1 million pounds sterling into extra income. Mr. Clark Junior suggests using the simple variance in the daily rate between the British pound sterling and the US dollar, to generate extra income from trading. Your chief financial officer wants to know if this is feasible?

##### 简单的外汇交易规划

Your challenge is to take 1 million US dollars or just over six hunderd thou sand pounds sterling and, by simply converting it between pounds sterling and US dollars, achieve a profit . Are you up to this challenge? I will show you how to model this problem and achieve a positive outcome. The forex data has been collected on a daily basis by Clark’s accounting department, from previous overseas transactions. Open your Python editor and start with a standard ecosystem. # -*- coding: utf-8 -*- ################################################################ import sys import os import pandas as pd import sqlite3 as sq import re ################################################################ if sys.platform == 'linux' or sys.platform == ' darwin':     Base=os.path.expanduser('~') + 'VKHCG' else:     Base='C:/VKHCG' ################################################################ print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ sInputFileName='03-Process/01-EDS/02-Python/Process_ExchangeRates.csv' ################################################################ sOutputFileName='05-Organise/01-EDS/02-Python/Organise-Forex.csv' Company='04-Clark' ################################################################ sDatabaseName=Base + '/' + Company + '/05-Organise/SQLite/clark.db' conn = sq.connect(sDatabaseName) #conn = sq.connect(':memory:') ################################################################ Import the forex data into the ecosystem. ################################################################ ### Import Forex Data ################################################################ sFileName=Base + '/' + Company + '/' + sInputFileName print('################################') print('Loading :',sFileName) print('################################') ForexDataRaw=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") print('################################') ################################################################ ForexDataRaw.index.names = ['RowID'] sTable='Forex_All' print('Storing :',sDatabaseName,' Table:',sTable) ForexDataRaw.to_sql(sTable, conn, if_exists="replace") ################################################################ You now seed the process with £1,000,000.00. sSQL="SELECT 1 as Bag\        , CAST(min(Date) AS VARCHAR(10)) as Date \        ,CAST(1000000.0000000 as NUMERIC(12,4)) as Money \        ,'USD' as Currency \        FROM Forex_All \        ;" sSQL=re.sub("\s\s+", " ", sSQL) nMoney=pd.read_sql_query(sSQL, conn) ################################################################ nMoney.index.names = ['RowID'] sTable='MoneyData' print('Storing :',sDatabaseName,' Table:',sTable) nMoney.to_sql(sTable, conn, if_exists="replace") ################################################################ sTable='TransactionData' print('Storing :',sDatabaseName,' Table:',sTable) nMoney.to_sql(sTable, conn, if_exists="replace") ################################################################ You now simulate the trading model Clark Jr. suggested. Note Clark Jr.’s hypothesis, or proposed explanation prepared on the basis of limited evidence as a starting point for further investigation by you, is that if you simply keep on exchanging the same “bag” of money, you will produce a profit. ForexDay=pd.read_sql_query("SELECT Date FROM Forex_All GROUP BY Date;", conn) ################################################################ t=0 for i in range(ForexDay.shape[0]):     sDay=ForexDay['Date'][i]     sSQL='\     SELECT M.Bag as Bag, \            F.Date as Date, \            round(M.Money * F.Rate,6) AS Money, \            F.CodeIn AS PCurrency, \            F.CodeOut AS Currency \     FROM MoneyData AS M \     JOIN \     ( \         SELECT \         CodeIn, CodeOut, Date, Rate \         FROM \         Forex_All \         WHERE\         CodeIn = "USD" AND CodeOut = "GBP" \         UNION \         SELECT \         CodeOut AS CodeIn, CodeIn AS CodeOut,  Date, (1/Rate) AS Rate \         FROM \         Forex_All \         WHERE\         CodeIn = "USD" AND CodeOut = "GBP" \     ) AS F \     ON \     M.Currency=F.CodeIn \     AND \     F.Date ="' + sDay + '";'     sSQL=re.sub("\s\s+", " ", sSQL)     ForexDayRate=pd.read_sql_query(sSQL, conn)     for j in range(ForexDayRate.shape[0]):         sBag=str(ForexDayRate['Bag'][j])         nMoney=str(round(ForexDayRate['Money'][j],2))         sCodeIn=ForexDayRate['PCurrency'][j]         sCodeOut=ForexDayRate['Currency'][j]     sSQL='UPDATE MoneyData SET Date= "' + sDay + '", '     sSQL= sSQL + ' Money = ' + nMoney + ', Currency="' + sCodeOut + '"'     sSQL= sSQL + ' WHERE Bag=' + sBag + ' AND Currency="' + sCodeIn + '";'     sSQL=re.sub("\s\s+", " ", sSQL)     cur = conn.cursor()     cur.execute(sSQL)     conn.commit()     t+=1     print('Trade :', t, sDay, sCodeOut, nMoney)     sSQL=' \     INSERT INTO TransactionData ( \                                 RowID, \                                 Bag, \                                 Date, \                                 Money, \                                 Currency \                             )  \     SELECT ' + str(t) + ' AS RowID, \        Bag, \        Date, \        Money, \        Currency \     FROM MoneyData \     ;'     sSQL=re.sub("\s\s+", " ", sSQL)     cur = conn.cursor()     cur.execute(sSQL)     conn.commit() ################################################################ Let’s load the transaction log, to investigate the insights into doing this process of achieving extra income. sSQL="SELECT RowID, Bag, Date, Money, Currency FROM TransactionData ORDER BY RowID;" sSQL=re.sub("\s\s+", " ", sSQL) TransactionData=pd.read_sql_query(sSQL, conn) OutputFile=Base + '/' + Company + '/' + sOutputFileName TransactionData.to_csv(OutputFile, index = False) ################################################################ The challenge was to take $1,000,000 or £603,189.41 on January 4, 1999, and make a profit by converting the complete amount into either British pounds or US dollars. The end result: You made $119,828.98, or a 19.87% return on investment, by May 5, 2017\. Well done. Warning This is a simple example. Trading foreign exchange is a high-risk enterprise. Please seek professional advice before attempting this for your customers.

## 报告超级步骤

The Report superstep is the step in the ecosystem that enhances the data science findings with the art of storytelling and data visualization. You can perform the best data science , but if you cannot execute a respectable and trustworthy Report step by turning your data science into actionable business insights, you have achieved no advantage for your business. Let me guide you through an example, to show you what you have learned up to now in this book.

### 结果摘要

The most important step in any analysis is the summary of the results. Your data science techniques and algorithms can produce the most methodically, most advanced mathematical or most specific statistical results to the requirements, but if you cannot summarize those into a good story, you have not achieved your requirements.

#### 理解上下文

What differentiates good data scientists from the best data scientists are not the algorithms or data engineering; it is the ability of the data scientist to apply the context of his findings to the customer. Example: The bar has served last rounds at 23:45 and there is one person left. The person drinking has less than 5% of the beer still in his glass at 23:50. From the context of the drinker, he will have to drink slower to stay till midnight or drink the rest immediately, if he has to catch the midnight bus. From the context of the bar staff, they want to close at 23:45, and they have to get this last patron out the door to lock up, as they, too, want to catch the midnight bus. It is clear that if you can determine if both parties want to be on the midnight bus, you will have the context of the amount of beer in the drinker’s glass. The sole data science measure of the 5% left in the glass is of no value. I had a junior data scientist try to use image processing from a brewery’s CCTV system to determine the levels of people’s beers, to enable his data science to notify the bar staff what their rate of beer consumption was during the night. The intention was to generate 3% more profit on the late shift. The issue was more sinister, as every night, several local regulars would stay after midnight, to catch the new subsidized “Drive-Safe” night bus from the bus terminal across the street home after 12:30, hence not having to pay the normal bus fares. So, they got beer for less than their fare home and traveled at no cost, because they smelled like beer. The brewery ended paying extra hours of overtime at double rates to six staff members in their three bars, owing to it being after midnight, and staff are not allowed to be in the bar alone with customers, for security reasons. The staff was always late returning home, as, not qualifying for the late charities’ bus, they had to use taxis. It was only after the brewery supplied the data scientist with the external CCTV that we found the real issue. The solution was to supply a coupon for a normal bus trip if customers took it before 23:30 and had beer at the brewery. The total profit equaled +7.3% on the late shift. No complex data science needs only some context. I have seen too many data scientists spend hours producing great results using the most complex algorithms but being unable to articulate their findings to the business in a manner it understood. Or even worse, not able to get their results into production, because at a bigger scale, they simply did not work as designed by the data scientist—which immediately made the whole exercise pointless in the first place . Caution Experience has taught me that this is the one area where data scientists achieve success or failure. Always determine the complete contextual setting of your results. Understand the story behind the results.

#### 适当的可视化

It is true that a picture tells a thousand words. But in data science, you only want your visualizations to tell one story: the findings of the data science you prepared. It is absolutely necessity to ensure that your audience will get your most important message clearly and without any other meanings. Warning Beware of a lot of colors and unclear graph format choices. You will lose the message! Keep it simple and to the point. Practice with your visual tools and achieve a high level of proficiency. I have seen numerous data scientists lose the value of great data science results because they did not perform an appropriate visual presentation .

#### 消除杂乱

Have you ever attended a presentation where the person has painstakingly prepared 50 slides to feedback his data science results? The most painful image is the faces of the people suffering through such a presentation for over two hours. Tip On average, it takes five minutes to cover one slide. You should never present more than ten slides. The biggest task of a data scientist is to eliminate clutter in the data sets. There are various algorithms, such as principal component analysis (PCA) , multicollinearity using the variance inflation factor to eliminate dimensions and impute or eliminate missing values, decision trees to subdivide, and backward feature elimination, but the biggest contributor to eliminating clutter is good and solid feature engineering. Tip If you do not need it, lose it! Applying appropriate feature engineering wins every time.

#### 在你想要的地方吸引注意力

Remember: Your purpose as a data scientist is to deliver insights to your customer, so that they can implement solutions to resolve a problem they may not even know about. You must place the attention on the insight and not the process. However, you must ensure that your process is verified and can support an accredited algorithm or technique. Warning Do not get into a situation in which your findings are questioned solely because your methods and procedures are not clear and precise. Tell only the story that is important right now. You can always come back to tell the next story later.

#### 讲故事(弗莱塔格的金字塔)

Under Freytag’s pyramid , the plot of a story consists of five parts: exposition, rising action, climax, falling action, and resolution. This is used by writers of books and screenplays as the basic framework of any story. In the same way, you must take your business through the data science process. Exposition is the portion of a story that introduces important background information to the audience. In data science, you tell the background of the investigation you performed. Rising action refers to a series of events that build toward the point of greatest interest . In data science, you point out the important findings or results. Keep it simple and to the point. The climax is the turning point that determines a good or bad outcome for the story’s characters. In data science, you show how your solution or findings will change the outcome of the work you performed. During the falling action, the conflict between what occurred before and after the climax takes place. In data science , you prove that after your suggestion has been implemented in a pilot, the same techniques can be used to find the issues now proving that the issues can inevitably be resolved. Resolution is the outcome of the story. In data science, you produce the solution and make the improvements permanent. Decide what repeatable sound bite you can use to help your core message stick with your audience. People remember the core things by your repeating them. You must ensure one thing: that the customer remembers your core message. Make sure you deliver that message clearly and without any confusion. Drive the core message home by repeating sound bites throughout the meeting. The art of a good data scientist lies in the ability to tell a story about the data and get people to remember its baseline. Tip Practice your storytelling with friends and family. Get them to give you feedback. Make sure you are comfortable speaking in front of people. Now that you can tell the story with ease, I will supply you with more examples of how you create the Report step for the data science .

### 工程实践报告超级步骤

Any source code or other supplementary material referenced by me in this book is available to readers on GitHub, via this book’s product page, located at [www.apress.com/9781484230534](http://www.apress.com/9781484230534) . Please note that this source code assumes you have completed the source code setup outlined in Chapter [2](02.html). Let’s check your graphics capacity . import matplotlib.pyplot as plt import matplotlib.patheffects as path_effects fig = plt.figure(figsize=(21, 3)) t = fig.text(0.2, 0.5, 'Practical Data Science', fontsize=75, weight=1000, va="center") t.set_path_effects([path_effects.PathPatchEffect(offset=(4, -4), hatch="xxxx",           facecolor='gray'),         path_effects.PathPatchEffect(edgecolor='white', linewidth=1.1,          facecolor='black')]) plt.plot() plt.show() Now that I have explained the various aspects of the Report superstep, I will show you how to help VKHC group companies with their processing. Try this code to find out your progress on the data science company scale : import matplotlib.pyplot as plt from numpy.random import randn z = randn(8) plt.figure(figsize=(15, 5)) red_dot, = plt.plot(z, "ro-", markersize=18) white_cross, = plt.plot(z[:5], "w+", markeredgewidth=3, markersize=15) black_cross, = plt.plot(z[:2], "y+", markeredgewidth=3, markersize=15) plt.legend([red_dot, (red_dot, white_cross),(red_dot,black_cross)],\             ["Krennwallner", "Clark","Hillman"]) Run it a few times , if you want better results.

#### vermeulen plc 公司

Vermeulen requires a map of all their customers’ data links. Can you provide a report to deliver this? I will guide you through an example that delivers this requirement.

##### 创建网络路由图

Let’s load the customers’ data and create a graph database. You can use the graph database to then create an image to support your story about the customers. Open your Python editor and create the standard ecosystem. ################################################################ import sys import os import pandas as pd import networkx as nx import matplotlib.pyplot as plt ################################################################ pd.options.mode.chained_assignment = None ################################################################ if sys.platform == 'linux' or sys.platform == ' darwin':     Base=os.path.expanduser('~') + 'VKHCG' else:     Base='C:/VKHCG' ################################################################ print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ sInputFileName='02-Assess/01-EDS/02-Python/Assess-Network-Routing-Customer.csv' ################################################################ sOutputFileName1='06-Report/01-EDS/02-Python/Report-Network-Routing-Customer.gml' sOutputFileName2='06-Report/01-EDS/02-Python/Report-Network-Routing-Customer.png' Company='01-Vermeulen' ################################################################ Import the data from the Access step for the networking customers. ################################################################ ### Import Country Data ################################################################ sFileName=Base + '/' + Company + '/' + sInputFileName print('################################') print('Loading :',sFileName) print('################################') CustomerDataRaw=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") CustomerData=CustomerDataRaw.head(100) print('Loaded Country:',CustomerData.columns.values) print('################################') ################################################################ print(CustomerData.head()) print(CustomerData.shape) ################################################################ G=nx.Graph() for i in range(CustomerData.shape[0]):     for j in range(CustomerData.shape[0]):         Node0=CustomerData['Customer_Country_Name'][i]         Node1=CustomerData['Customer_Country_Name'][j]         if Node0 != Node1:             G.add_edge(Node0,Node1) for i in range(CustomerData.shape[0]):     Node0=CustomerData['Customer_Country_Name'][i]     Node1=CustomerData['Customer_Place_Name'][i] + '('+ CustomerData['Customer_Country_Name'][i] + ')'     Node2='('+ "{:.9f}".format(CustomerData['Customer_Latitude'][i]) + ')\     ('+ "{:.9f}".format(CustomerData['Customer_Longitude'][i]) + ')'     if Node0 != Node1:         G.add_edge(Node0,Node1)     if Node1 != Node2:         G.add_edge(Node1,Node2) Here are your insights into the networking customers: print('Nodes:', G.number_of_nodes()) print('Edges:', G.number_of_edges()) ################################################################ Export the data for the networking customers. sFileName=Base + '/' + Company + '/' + sOutputFileName1 print('################################') print('Storing :',sFileName) print('################################') nx.write_gml(G, sFileName) ################################################################ sFileName=Base + '/' + Company + '/' + sOutputFileName2 print('################################') print('Storing Graph Image:',sFileName) print('################################') You can now prepare an image for the networking customers , to explain the insights you achieved. plt.figure(figsize=(25, 25)) pos=nx.spectral_layout(G,dim=2) nx.draw_networkx_nodes(G,pos, node_color="k", node_size=10, alpha=0.8) nx.draw_networkx_edges(G, pos,edge_color='r', arrows=False, style="dashed") nx.draw_networkx_labels(G,pos,font_size=12,font_family='sans-serif',font_color='b') plt.axis('off') plt.savefig(sFileName,dpi=600) plt.show() ################################################################ print('################################') print('### Done!! #####################') print('################################') ################################################################ Well done. You have a report for the management of Vermeulen on their network requirements for their customers.

#### Krennwallner 股份有限公司

The Krennwallner marketing department wants to deploy the locations of the billboards onto the company web server. Can you prepare three versions of the locations’ web pages?

*   缩小时，位置聚集成气泡
*   位置作为大头针
*   位置作为热图

To achieve this, I must introduce you to a new library: folium. This library assists with the creation of web pages by using locations. Install it using conda install -c conda-forge folium.

##### 为广告牌挑选内容

Open your Python editor and set up the following ecosystem: ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import pandas as pd from folium.plugins import FastMarkerCluster, HeatMap from folium import Marker, Map import webbrowser ################################################################ if sys.platform == 'linux' or sys.platform == ' darwin':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ Import the required data for the companies’ billboards in Germany . sFileName=Base+'/02-Krennwallner/01-Retrieve/01-EDS/02-Python/Retrieve_DE_Billboard_Locations.csv' df = pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") df.fillna(value=0, inplace=True) print(df.shape) The data requires some missing data treatment. I suggest simply removing the “bad”-quality records by performing a try-except process. ################################################################ t=0 for i in range(df.shape[0]):     try:         sLongitude=df["Longitude"][i]         sLongitude=float(sLongitude)     except Exception:         sLongitude=float(0.0)     try:         sLatitude=df["Latitude"][i]         sLatitude=float(sLatitude)     except Exception:         sLatitude=float(0.0)     try:         sDescription=df["Place_Name"][i] + ' (' + df["Country"][i]+')'     except Exception:         sDescription='VKHCG'     if sLongitude != 0.0 and sLatitude != 0.0:         DataClusterList=list([sLatitude, sLongitude])         DataPointList=list([sLatitude, sLongitude, sDescription])         t+=1         if t==1:             DataCluster=[DataClusterList]             DataPoint=[DataPointList]         else:             DataCluster.append(DataClusterList)             DataPoint.append(DataPointList) data=DataCluster You now have a data set that can be used to create the web content. You can start with the cluster web page first. pins=pd.DataFrame(DataPoint) pins.columns = [ 'Latitude','Longitude','Description'] ################################################################ billbords_map1 = Map(location=[48.1459806, 11.4985484], zoom_start=5) marker_cluster = FastMarkerCluster(data).add_to(billbords_map1) sFileNameHtml=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/Billboard1.html' billbords_map1.save(sFileNameHtml) Now that you saved the web page, let’s open it and see your achievement. webbrowser.open('file://' + os.path.realpath(sFileNameHtml)) Next, you can produce the pins or marker-only web page. ################################################################ billbords_map2 = Map(location=[48.1459806, 11.4985484], zoom_start=5) for name, row in pins.iloc[:100].iterrows():     Marker([row["Latitude"],row["Longitude"]], popup=row["Description"]).add_to(billbords_map2) sFileNameHtml=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/Billboard2.html' billbords_map2.save(sFileNameHtml) webbrowser.open('file://' + os.path.realpath(sFileNameHtml)) We’re making good progress with the example. You can also prepare the heat map version of the data into a web page. ################################################################ billbords_heatmap = Map(location=[48.1459806, 11.4985484], zoom_start=5) billbords_heatmap.add_child(HeatMap([[row["Latitude"], row["Longitude"]] for name, row in pins.iloc[:100].iterrows()])) sFileNameHtml=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/Billboard_heatmap.html' billbords_heatmap.save(sFileNameHtml) webbrowser.open('file://' + os.path.realpath(sFileNameHtml)) ################################################################ print('### Done!! ############################################') ################################################################

#### 希尔曼有限公司

Dr. Hillman Sr. has just installed a camera system that enables the company to capture video and, therefore, indirectly, images of all containers that enter or leave the warehouse. Can you convert the number on the side of the containers into digits?

##### 读取容器

I will assist you with a quick experiment to prove that you can perform the task required. Open your Python editor and set up this ecosystem: from time import time import numpy as np import matplotlib.pyplot as plt from matplotlib import offsetbox from sklearn import (manifold, datasets, decomposition, ensemble,                      discriminant_analysis, random_projection) digits = datasets.load_digits(n_class=6) X = digits.data y = digits.target n_samples, n_features = X.shape n_neighbors = 30 Create a function for scaling and visualizing the embedding vectors. def plot_embedding(X, title=None):     x_min, x_max = np.min(X, 0), np.max(X, 0)     X = (X - x_min) / (x_max - x_min)     plt.figure(figsize=(10, 10))     ax = plt.subplot(111)     for i in range(X.shape[0]):         plt.text(X[i, 0], X[i, 1], str(digits.target[i]),                  color=plt.cm.Set1(y[i] / 10.),                  fontdict={'weight': 'bold', 'size': 9})     if hasattr(offsetbox, 'AnnotationBbox'):         # only print thumbnails with matplotlib > 1.0         shown_images = np.array([[1., 1.]])  # just something big         for i in range(digits.data.shape[0]):             dist = np.sum((X[i] - shown_images) ** 2, 1)             if np.min(dist) < 4e-3:                 # don't show points that are too close                 continue             shown_images = np.r_[shown_images, [X[i]]]             imagebox = offsetbox.AnnotationBbox(                 offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),                 X[i])             ax.add_artist(imagebox)     plt.xticks([]), plt.yticks([])     if title is not None:         plt.title(title) You can now plot images of the digits you identified. n_img_per_row = 20 img = np.zeros((10 * n_img_per_row, 10 * n_img_per_row)) for i in range(n_img_per_row):     ix = 10 * i + 1     for j in range(n_img_per_row):         iy = 10 * j + 1         img[ix:ix + 8, iy:iy + 8] = X[i * n_img_per_row + j].reshape((8, 8)) plt.figure(figsize=(10, 10)) plt.imshow(img, cmap=plt.cm.binary) plt.xticks([]) plt.yticks([]) plt.title('A selection from the 64-dimensional digits dataset') You can process with a random 2D projection using a random unitary matrix. print("Computing random projection") rp = random_projection.SparseRandomProjection(n_components=2, random_state=42) X_projected = rp.fit_transform(X) plot_embedding(X_projected, "Random Projection of the digits") Now add a projection onto the first two principal components . print("Computing PCA projection") t0 = time() X_pca = decomposition.TruncatedSVD(n_components=2).fit_transform(X) plot_embedding(X_pca,                "Principal Components projection of the digits (time %.2fs)" %                (time() - t0)) Add a projection on to the first two linear discriminant components. print("Computing Linear Discriminant Analysis projection") X2 = X.copy() X2.flat[::X.shape[1] + 1] += 0.01  # Make X invertible t0 = time() X_lda = discriminant_analysis.LinearDiscriminantAnalysis(n_components=2).fit_transform(X2, y) plot_embedding(X_lda,                "Linear Discriminant projection of the digits (time %.2fs)" %                (time() - t0)) Add an Isomap projection of the digits data set. print("Computing Isomap embedding") t0 = time() X_iso = manifold.Isomap(n_neighbors, n_components=2).fit_transform(X) print("Done.") plot_embedding(X_iso,                "Isomap projection of the digits (time %.2fs)" %                (time() - t0)) Add a locally linear embedding of the digits data set. print("Computing LLE embedding") clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,                                       method='standard') t0 = time() X_lle = clf.fit_transform(X) print("Done. Reconstruction error: %g" % clf.reconstruction_error_) plot_embedding(X_lle,                "Locally Linear Embedding of the digits (time %.2fs)" %                (time() - t0)) Add a modified locally linear embedding of the digits data set . print("Computing modified LLE embedding") clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,                                       method='modified') t0 = time() X_mlle = clf.fit_transform(X) print("Done. Reconstruction error: %g" % clf.reconstruction_error_) plot_embedding(X_mlle,                "Modified Locally Linear Embedding of the digits (time %.2fs)" %                (time() - t0)) Add a Hessian LLE embedding of the digits data set. print("Computing Hessian LLE embedding") clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,                                       method='hessian') t0 = time() X_hlle = clf.fit_transform(X) print("Done. Reconstruction error: %g" % clf.reconstruction_error_) plot_embedding(X_hlle,                "Hessian Locally Linear Embedding of the digits (time %.2fs)" %                (time() - t0)) Add an LTSA embedding of the digits data set. print("Computing LTSA embedding") clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,                                       method='ltsa') t0 = time() X_ltsa = clf.fit_transform(X) print("Done. Reconstruction error: %g" % clf.reconstruction_error_) plot_embedding(X_ltsa,                "Local Tangent Space Alignment of the digits (time %.2fs)" %                (time() - t0)) Add an MDS embedding of the digits data set. print("Computing MDS embedding") clf = manifold.MDS(n_components=2, n_init=1, max_iter=100) t0 = time() X_mds = clf.fit_transform(X) print("Done. Stress: %f" % clf.stress_) plot_embedding(X_mds,                "MDS embedding of the digits (time %.2fs)" %                (time() - t0)) Add a random trees embedding of the digits data set . print("Computing Totally Random Trees embedding") hasher = ensemble.RandomTreesEmbedding(n_estimators=200, random_state=0,                                        max_depth=5) t0 = time() X_transformed = hasher.fit_transform(X) pca = decomposition.TruncatedSVD(n_components=2) X_reduced = pca.fit_transform(X_transformed) plot_embedding(X_reduced,                "Random forest embedding of the digits (time %.2fs)" %                (time() - t0)) Add a spectral embedding of the digits data set. print("Computing Spectral embedding") embedder = manifold.SpectralEmbedding(n_components=2, random_state=0,                                       eigen_solver="arpack") t0 = time() X_se = embedder.fit_transform(X) plot_embedding(X_se,                "Spectral embedding of the digits (time %.2fs)" %                (time() - t0)) Add a t-SNE embedding of the digits data set. print("Computing t-SNE embedding") tsne = manifold.TSNE(n_components=2, init="pca", random_state=0) t0 = time() X_tsne = tsne.fit_transform(X) plot_embedding(X_tsne,                "t-SNE embedding of the digits (time %.2fs)" %                (time() - t0)) plt.show() Great job! You have successfully completed the container experiment. Which display format do you think is the best? The right answer is your choice, as it has to be the one that matches your own insight into the data, and there is not really a wrong answer .

#### 克拉克有限公司

The financial company in VKHCG is the Clark accounting firm that VKHCG owns with a 60% stake. The accountants are the financial advisers to the group and handle everything to do with the complex work of international accounting.

##### 金融的

The VKHCG companies did well last year, and the teams at Clark must prepare a balance sheet for each company in the group. The companies require a balance sheet for each company, to be produced using the template (Balance-Sheet-Template.xlsx) that can be found in the example directory (..\VKHCG\04-Clark\00-RawData). Figure [11-3](#Fig3) shows the basic layout.![A435693_1_En_11_Fig3_HTML.jpg](img/A435693_1_En_11_Fig3_HTML.jpg) Figure 11-3Basic internal balance sheet for VKHCG companies I will guide you through a process that will enable you to merge the data science with preformatted Microsoft Excel template, to produce a balance sheet for each of the VKHCG companies. Note A new library named openpyxl supports you in editing Microsoft Excel workbooks. Install the new library by using "conda install -c anaconda openpyxl". Open your Python ecosystem and build the following code into it. Set up a standard ecosystem. # -*- coding: utf-8 -*- ################################################################ import sys import os import pandas as pd import sqlite3 as sq import re from openpyxl import load_workbook ################################################################ if sys.platform == 'linux' or sys.platform == ' darwin':     Base=os.path.expanduser('~') + 'VKHCG' else:     Base='C:/VKHCG' ################################################################ print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ Set the input data for the example. sInputTemplateName='00-RawData/Balance-Sheet-Template.xlsx' Set up the preamble for the output of the example. sOutputFileName='05-Organise/01-EDS/02-Python/Report-Balance-Sheet' Company='04-Clark' ################################################################ sDatabaseName=Base + '/' + Company + '/06-Report/SQLite/clark.db' You have a choice here. You can use a disk-bound SQLite database that will store the data to the hard disk for use at later stages but is not as fast as a second option. conn = sq.connect(sDatabaseName) The second option is keeping the data in a memory-bound database, which is faster to access but not permanent. #conn = sq.connect(':memory:') ################################################################ ### Import Balance Sheet Data ################################################################ for y in range(1,13):     sInputFileName='00-RawData/BalanceSheets' + str(y).zfill(2) + '.csv'     sFileName=Base + '/' + Company + '/' + sInputFileName     print('################################')     print('Loading :',sFileName)     print('################################')     ForexDataRaw=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1")     print('################################')     ################################################################     ForexDataRaw.index.names = ['RowID']     sTable='BalanceSheets'     print('Storing :',sDatabaseName,' Table:',sTable)     if y == 1:         print('Load Data')         ForexDataRaw.to_sql(sTable, conn, if_exists="replace")     else:         print('Append Data')         ForexDataRaw.to_sql(sTable, conn, if_exists="append") ################################################################ sSQL="SELECT \             Year, \             Quarter, \             Country, \             Company, \             CAST(Year AS INT) || 'Q' || CAST(Quarter AS INT) AS sDate, \             Company || ' (' || Country || ')' AS sCompanyName , \             CAST(Year AS INT) || 'Q' || CAST(Quarter AS INT) || '-' ||\             Company || '-' || Country AS sCompanyFile \         FROM BalanceSheets \         GROUP BY \             Year, \             Quarter, \             Country, \             Company \         HAVING Year is not null \        ;" sSQL=re.sub("\s\s+", " ", sSQL) sDatesRaw=pd.read_sql_query(sSQL, conn) print(sDatesRaw.shape) sDates=sDatesRaw.head(5) ################################################################ ## Loop Dates ################################################################ for i in range(sDates.shape[0]):     sFileName=Base + '/' + Company + '/' + sInputTemplateName     wb = load_workbook(sFileName)     ws=wb.get_sheet_by_name("Balance-Sheet")     sYear=sDates['sDate'][i]     sCompany=sDates['sCompanyName'][i]     sCompanyFile=sDates['sCompanyFile'][i]     sCompanyFile=re.sub("\s+", "", sCompanyFile)     ws['D3'] = sYear     ws['D5'] = sCompany     sFields = pd.DataFrame(             [            ['Cash','D16', 1],            ['Accounts_Receivable','D17', 1],            ['Doubtful_Accounts','D18', 1],            ['Inventory','D19', 1],            ['Temporary_Investment','D20', 1],            ['Prepaid:Expenses','D21', 1],            ['Long_Term_Investments','D24', 1],            ['Land','D25', 1],            ['Buildings','D26', 1],            ['Depreciation_Buildings','D27', -1],            ['Plant_Equipment','D28', 1],            ['Depreciation_Plant_Equipment','D29', -1],            ['Furniture_Fixtures','D30', 1],            ['Depreciation_Furniture_Fixtures','D31', -1],            ['Accounts_Payable','H16', 1],            ['Short_Term_Notes','H17', 1],            ['Current_Long_Term_Notes','H18', 1],            ['Interest_Payable','H19', 1],            ['Taxes_Payable','H20', 1],            ['Accrued_Payroll','H21', 1],            ['Mortgage','H24', 1],            ['Other_Long_Term_Liabilities','H25', 1],            ['Capital_Stock','H30', 1]            ]             )     nYear=str(int(sDates['Year'][i]))     nQuarter=str(int(sDates['Quarter'][i]))     sCountry=str(sDates['Country'][i])     sCompany=str(sDates['Company'][i])     sFileName=Base + '/' + Company + '/' + sOutputFileName + \     '-' + sCompanyFile + '.xlsx'     print(sFileName)     for j in range(sFields.shape[0]):         sSumField=sFields[0][j]         sCellField=sFields[1][j]         nSumSign=sFields[2][j]         sSQL="SELECT  \                Year, \                Quarter, \                Country, \                Company, \                SUM(" + sSumField + ") AS nSumTotal \             FROM BalanceSheets \             GROUP BY \                Year, \                Quarter, \                Country, \                Company \             HAVING \                 Year=" + nYear + " \             AND \                 Quarter=" + nQuarter + " \             AND \                 Country='" + sCountry + "' \             AND \                 Company='" + sCompany + "' \            ;"         sSQL=re.sub("\s\s+", " ", sSQL)         sSumRaw=pd.read_sql_query(sSQL, conn)         ws[sCellField] = sSumRaw["nSumTotal"][0] * nSumSign         print('Set cell',sCellField,' to ', sSumField,'Total')     wb.save(sFileName) Well done. You now have all the reports you need.

## 制图法

Until now, you have seen little graphical visualization in the examples. I will now guide you through a number of visualizations that I find particularly useful when I present my data to my customers.

### 绘图选项

First, let’s look at several plotting options within the ecosystem. Open your Python editor and set up the following ecosystem : ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import pandas as pd import matplotlib as ml from matplotlib import pyplot as plt ################################################################ if sys.platform == 'linux' or sys.platform == ' darwin':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ GBase = Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/' ml.style.use('ggplot') Here is a special data set to demonstrate the visualization. data=[ ['London',    29.2, 17.4], ['Glasgow',    18.8, 11.3], ['Cape Town',    15.3, 9.0], ['Houston',    22.0, 7.8], ['Perth',    18.0, 23.7], ['San Francisco',    11.4, 33.3] ] os_new=pd.DataFrame(data) pd.Index(['Item', 'Value', 'Value Percent', 'Conversions', 'Conversion Percent',        'URL', 'Stats URL'],       dtype='object') os_new.rename(columns = {0 : "Warehouse Location"}, inplace=True) os_new.rename(columns = {1 : "Profit 2016"}, inplace=True) os_new.rename(columns = {2 : "Profit 2017"}, inplace=True)

#### 饼图

Let’s start with a simple pie graph. explode = (0, 0, 0, 0, 0, 0.1) labels=os_new['Warehouse Location'] colors_mine = ['yellowgreen', 'gold', 'lightskyblue', 'lightcoral', 'lightcyan','lightblue'] os_new.plot(figsize=(10, 10),kind="pie", y="Profit 2017",autopct='%.2f%%', \             shadow=True, explode=explode, legend = False, colors = colors_mine,\             labels=labels, fontsize=20) sPicNameOut1=GBase+'pie_explode.png' plt.savefig(sPicNameOut1,dpi=600) Figure [11-4](#Fig4) shows the resulting pie graph.![A435693_1_En_11_Fig4_HTML.jpg](img/A435693_1_En_11_Fig4_HTML.jpg) Figure 11-4Pie graph

#### 双层馅饼

With the following, you can put two pie graphs side by side: explode = (0, 0, 0, 0, 0, 0) colors_mine = ['yellowgreen', 'gold', 'lightskyblue', 'lightcoral', 'lightcyan','lightblue'] os_new.plot(figsize=(10, 5),kind="pie", y=['Profit 2016','Profit 2017'],autopct='%.2f%%', \             shadow=True, explode=explode, legend = False, colors = colors_mine,\             subplots=True, labels=labels, fontsize=10) sPicNameOut2=GBase+'pie.png' plt.savefig(sPicNameOut2,dpi=600) Figure [11-5](#Fig5) shows the resulting double pie graph.![A435693_1_En_11_Fig5_HTML.jpg](img/A435693_1_En_11_Fig5_HTML.jpg) Figure 11-5Double pie graph

#### 线图

We can also visualize the data in the form of a line graph. os_new.iloc[:5].plot(figsize=(10, 10),kind='Line',x='Warehouse Location',\            y=['Profit 2016','Profit 2017']); sPicNameOut3=GBase+'line.png' plt.savefig(sPicNameOut3,dpi=600) Figure [11-6](#Fig6) shows the resulting line graph .![A435693_1_En_11_Fig6_HTML.jpg](img/A435693_1_En_11_Fig6_HTML.jpg) Figure 11-6Line graph

#### 条形图

The following results in a bar graph: os_new.iloc[:5].plot(figsize=(10, 10),kind='bar',x='Warehouse Location',\            y=['Profit 2016','Profit 2017']); sPicNameOut4=GBase+'bar.png' plt.savefig(sPicNameOut4,dpi=600) Figure [11-7](#Fig7) shows the resulting bar graph.![A435693_1_En_11_Fig7_HTML.jpg](img/A435693_1_En_11_Fig7_HTML.jpg) Figure 11-7 Bar graph

#### 水平条形图

You can also create a horizontal bar graph. os_new.iloc[:5].plot(figsize=(10, 10),kind='barh',x='Warehouse Location',\            y=['Profit 2016','Profit 2017']); sPicNameOut5=GBase+'hbar.png' plt.savefig(sPicNameOut5,dpi=600) Figure [11-8](#Fig8) shows the resulting horizontal bar graph.![A435693_1_En_11_Fig8_HTML.jpg](img/A435693_1_En_11_Fig8_HTML.jpg) Figure 11-8 Horizontal bar graph

#### 面积图

An area graph format is also possible. os_new.iloc[:5].plot(figsize=(10, 10),kind='area',x='Warehouse Location',\            y=['Profit 2016','Profit 2017'],stacked=False); sPicNameOut6=GBase+'area.png' plt.savefig(sPicNameOut6,dpi=600) Figure [11-9](#Fig9) shows the resulting area graph.![A435693_1_En_11_Fig9_HTML.jpg](img/A435693_1_En_11_Fig9_HTML.jpg) Figure 11-9 Area graph

#### 发散图

We could also visualize using scatter graphs, if you want. os_new.iloc[:5].plot(figsize=(10, 10),kind='scatter',x='Profit 2016',\            y='Profit 2017',color='DarkBlue',marker='D'); sPicNameOut7=GBase+'scatter.png' plt.savefig(sPicNameOut7,dpi=600) Figure [11-10](#Fig10) shows the resulting scatter graph.![A435693_1_En_11_Fig10_HTML.jpg](img/A435693_1_En_11_Fig10_HTML.jpg) Figure 11-10Scatter graph

#### 十六进制 Bin 图

Here is something interesting I discovered—a hex bin graph. The hex graph is included because, if you can perform the data science, you can

*   内置刻度，轻松判断距离
*   轻松判断兴趣点的密度，也是由于内置的规模
*   轻松放大或缩小地图，并根据地理空间报告格式将内容大致放置在需要的位置

Let’s look at a simple example for a hex graph. We have a farm of about 25 × 25 miles. If we measure the profit generated by each square mile of farming, we can discover the areas of the farm that are profitable or not. os_new.iloc[:5].plot(figsize=(13, 10),kind='hexbin',x='Profit 2016',\            y='Profit 2017', gridsize=25); sPicNameOut8=GBase+'hexbin.png' plt.savefig(sPicNameOut8,dpi=600) Figure [11-11](#Fig11) shows the resulting hex bin graph, which indicates that there are five fields that generated profits both in 2016 and 2017.![A435693_1_En_11_Fig11_HTML.jpg](img/A435693_1_En_11_Fig11_HTML.jpg) Figure 11-11 Hex bin graph for farm

### 更复杂的图形

I will next show you some more complex graph techniques that I have used over my time working with data from various data sources. Often, a normal pie, line, or bar graph simply does not convey the full visualization of the results, or the data is so complex that it cannot be visualized without some extra data science applied to resolve the complexities. I will now guide you through a few of the more common techniques I use regularly.

#### 核密度估计(KDE)图

Kernel density estimation is an essential data-smoothing technique in which interpretations about the population are prepared, based on a finite data sample. ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import pandas as pd import matplotlib as ml import numpy as np from matplotlib import pyplot as plt ################################################################ if sys.platform == 'linux' or sys.platform == ' darwin':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ ml.style.use('ggplot') fig1=plt.figure(figsize=(10, 10)) ser = pd.Series(np.random.randn(1000)) ser.plot(figsize=(10, 10),kind='kde') sPicNameOut1=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/kde.png' plt.savefig(sPicNameOut1,dpi=600) plt.tight_layout() plt.show() Figure [11-12](#Fig12) shows the resulting KDE graph, which illustrates the estimate of the probability density function of a random variable. It reduces the random thousand points into a curve that indicates how the values change across the population , without displaying each value.![A435693_1_En_11_Fig12_HTML.jpg](img/A435693_1_En_11_Fig12_HTML.jpg) Figure 11-12Kernel density estimation graph

#### 散布矩阵图

Scatter matrix is a statistic tool that is used to estimate the covariance matrix. fig2=plt.figure(figsize=(10, 10)) from pandas.plotting import scatter_matrix df = pd.DataFrame(np.random.randn(1000, 5), columns=['Y2014','Y2015', 'Y2016', 'Y2017', 'Y2018']) scatter_matrix(df, alpha=0.2, figsize=(10, 10), diagonal="kde") sPicNameOut2=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/scatter_matrix.png' plt.savefig(sPicNameOut2,dpi=600) plt.tight_layout() plt.show() Figure [11-13](#Fig13) shows the resulting scatter matrix graph.![A435693_1_En_11_Fig13_HTML.jpg](img/A435693_1_En_11_Fig13_HTML.jpg) Figure 11-13 Scatter matrix graph

#### 安德鲁斯曲线

Visualization with Andrews’ curve is a way to visualize structure in high-dimensional data. Andrews’ curves are examples of the space transformed visualization (STV) techniques for visualizing multivariate data, which represent k-dimensional data points by a profile line curve in two- or three-dimensional space, using orthogonal basis functions. Andrews’ curves are based on a Fourier series in which the coefficients are the observation’s values. In simple terms, they allow you, in a structured way, to reduce dimensional results of more than three dimensions into a visualized format. Open your Python editor and set up this basic ecosystem: import sys import os import pandas as pd from matplotlib import pyplot as plt ################################################################ if sys.platform == 'linux' or sys.platform == ' darwin':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ sDataFile=Base+'/01-Vermeulen/00-RawData/irisdata.csv' data = pd.read_csv(sDataFile) from pandas.plotting import andrews_curves plt.figure(figsize=(10, 10)) andrews_curves(data, 'Name') sPicNameOut1=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/andrews_curves.png' plt.savefig(sPicNameOut1,dpi=600) plt.tight_layout() plt.show() Figure [11-14](#Fig14) shows the result: a four- to two-dimensional reduction graph.![A435693_1_En_11_Fig14_HTML.jpg](img/A435693_1_En_11_Fig14_HTML.jpg) Figure 11-14Graph produced using Andrews’ curves

#### 平行坐标

Parallel coordinates are a regularly used for visualizing high-dimensional geometry and analyzing multivariate data. from pandas.plotting import parallel_coordinates plt.figure(figsize=(10, 10)) parallel_coordinates(data, 'Name') sPicNameOut2=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/parallel_coordinates.png' plt.savefig(sPicNameOut2,dpi=600) plt.tight_layout() plt.show() Figure [11-15](#Fig15) shows the results of this graph.![A435693_1_En_11_Fig15_HTML.jpg](img/A435693_1_En_11_Fig15_HTML.jpg) Figure 11-15Graph produced using parallel coordinates

#### 拉德维兹方法

The RADVIZ method maps a set of multidimensional points onto two-dimensional space. from pandas.plotting import radviz plt.figure(figsize=(10, 10)) radviz(data, 'Name') sPicNameOut3=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/radviz.png' plt.savefig(sPicNameOut3,dpi=600) plt.tight_layout() plt.show() Figure [11-16](#Fig16) shows the results.![A435693_1_En_11_Fig16_HTML.jpg](img/A435693_1_En_11_Fig16_HTML.jpg) Figure 11-16Graph produced by the RADVIZ method

#### 创建绘图

A lag plot allows the data scientist to check whether a data set is random. Creating a lag plot enables you to check for randomness. Random data will spread fairly evenly, both horizontally and vertically. If you cannot see a pattern in the graph, your data is most probably random. This example will show you how it works in practical data science: ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import pandas as pd from matplotlib import style from matplotlib import pyplot as plt import numpy as np ################################################################ if sys.platform == 'linux' or sys.platform == ' darwin':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ style.use('ggplot') from pandas.plotting import lag_plot plt.figure(figsize=(10, 10)) data = pd.Series(0.1 * np.random.rand(1000) + \                  0.9 * np.sin(np.linspace(-99 * np.pi, 99 * np.pi, num=1000))) lag_plot(data) sPicNameOut1=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/lag_plot.png' plt.savefig(sPicNameOut1,dpi=600) plt.tight_layout() plt.show() Figure [11-17](#Fig17) shows the results.![A435693_1_En_11_Fig17_HTML.jpg](img/A435693_1_En_11_Fig17_HTML.jpg) Figure 11-17 Lag plot graph

#### 自相关图

An autocorrelation plot is a design of the sample autocorrelations vs. the time lags. from pandas.plotting import autocorrelation_plot plt.figure(figsize=(10, 10)) data = pd.Series(0.7 * np.random.rand(1000) + \                  0.3 * np.sin(np.linspace(-9 * np.pi, 9 * np.pi, num=1000))) autocorrelation_plot(data) sPicNameOut2=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/autocorrelation_plot.png' plt.savefig(sPicNameOut2,dpi=600) plt.tight_layout() plt.show() Figure [11-18](#Fig18) shows the results .![A435693_1_En_11_Fig18_HTML.jpg](img/A435693_1_En_11_Fig18_HTML.jpg) Figure 11-18 Autocorrelation plot graph

#### 自举图

To generate a bootstrap uncertainty estimate for a given statistic from a set of data, a subsample of a size less than or equal to the size of the data set is generated from the data, and the statistic is calculated. This subsample is generated with replacement, so that any data point can be sampled multiple times or not sampled at all. This process is repeated for many subsamples. Here is an example for 500 samples of 50 elements out of a population of 1000 points. from pandas.plotting import bootstrap_plot data = pd.Series(np.random.rand(1000)) plt.figure(figsize=(10, 10)) bootstrap_plot(data, size=50, samples=500, color="grey") sPicNameOut3=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/bootstrap_plot.png' plt.savefig(sPicNameOut3,dpi=600) plt.tight_layout() plt.show() Figure [11-19](#Fig19) shows the results of the proposed bootstrap. This enables the data scientist to show what the distribution of the samples used for the bootstrap processing of the data is.![A435693_1_En_11_Fig19_HTML.jpg](img/A435693_1_En_11_Fig19_HTML.jpg) Figure 11-19Bootstrap plot graph

#### 等高线图

A contour plot is a graphical technique for demonstrating a three-dimensional surface by plotting constant z slices, called contours , on a two-dimensional format. The latest geospatial insights for new technology such as 5G and robotics requires data scientists to start taking the terrain around the customers into consideration when performing their data analysis. Data science has also shown that previous beliefs about concentric circles with a common center of influence were incorrect. The latest findings show that results similar to the contours of the earth as the patterns of the influence. Following is an example to show you how to visualize these contour maps. Open your Python editor and set up this ecosystem : ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import matplotlib import numpy as np import matplotlib.cm as cm import matplotlib.mlab as mlab import matplotlib.pyplot as plt ################################################################ if sys.platform == 'linux' or sys.platform == ' darwin':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ matplotlib.rcParams['xtick.direction'] = 'out' matplotlib.rcParams['ytick.direction'] = 'out' Here is your data set . delta = 0.025 x = np.arange(-3.0, 3.0, delta) y = np.arange(-2.0, 2.0, delta) X, Y = np.meshgrid(x, y) Z1 = mlab.bivariate_normal(X, Y, 1.0, 1.0, 0.0, 0.0) Z2 = mlab.bivariate_normal(X, Y, 1.5, 0.5, 1, 1) # difference of Gaussians Z = 10.0 * (Z2 - Z1) You can now create a simple contour graph with labels. plt.figure(figsize=(10, 10)) CS = plt.contour(X, Y, Z) plt.clabel(CS, inline=1, fontsize=10) plt.title('Simply default with labels') sPicNameOut0=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/contour0.png' plt.savefig(sPicNameOut0,dpi=600) plt.tight_layout() plt.show() Here is the output (Figure [11-20](#Fig20)). A clear contour was achieved.![A435693_1_En_11_Fig20_HTML.jpg](img/A435693_1_En_11_Fig20_HTML.jpg) Figure 11-20Contour plot graph Can you see how matplotlib assists you with creating a complex visualization? This type of contour can be used for the normal mapping of the contours of the land. I also use them to show the area of impact of a specific event or action, for example, Hillman opening a new warehouse or Krennwallner putting up a new billboard. The other use is when you can tie the data set to move across a period of time. In that case, I simply generate the same code with a loop that generates, say, 12 images. Then I simply perform a command to produce a movie in MP4 format. Look at ffmpeg -f image2 -r 1/5 -i yourimage%03d.png -vcodec mpeg4 -y yourmovie.mp4\. It simply takes images with three-digit padding (yourimage001.png) into an PEG4 QuickTime movie. Tip People can follow a quick movie clip quickly, to visualize the effects or the context of the results you are presenting. Now we can look at changing our contour format. I found the forcing of manual locations for the labels a useful feature, as it supports the better control of the presentation format when you make images for your movie series. plt.figure(figsize=(10, 10)) CS = plt.contour(X, Y, Z) manual_locations = [(-1, -1.4), (-0.62, -0.7), (-2, 0.5),\                     (1.7, 1.2), (2.0, 1.4), (2.4, 1.7)] plt.clabel(CS, inline=1, fontsize=10, manual=manual_locations) plt.title('Labels at selected locations') sPicNameOut1=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/contour1.png' plt.savefig(sPicNameOut1,dpi=600) plt.tight_layout() plt.show() In Figure [11-21](#Fig21), can you see how the contour labels are now pinned to a specific location ?![A435693_1_En_11_Fig21_HTML.jpg](img/A435693_1_En_11_Fig21_HTML.jpg) Figure 11-21Graph with pinned contour labels Tip Only move or change items that you want to communicate. A person’s eye picks up on the most minor changes, and they can distract the person’s interest. You can also change the line types, to communicate new aspects of the process . plt.figure(figsize=(10, 10)) CS = plt.contour(X, Y, Z, 6,                  colors='k',  # negative contours will be dashed by default                  ) plt.clabel(CS, fontsize=9, inline=1) plt.title('Single color - negative contours dashed') sPicNameOut2=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/contour2.png' plt.savefig(sPicNameOut2,dpi=600) plt.tight_layout() plt.show() The results are shown in Figure [11-22](#Fig22).![A435693_1_En_11_Fig22_HTML.jpg](img/A435693_1_En_11_Fig22_HTML.jpg) Figure 11-22Graph with contrasted contours As you can see, you can tune many aspects of the graph . plt.figure(figsize=(10, 10)) matplotlib.rcParams['contour.negative_linestyle'] = 'solid' plt.figure(figsize=(10, 10)) CS = plt.contour(X, Y, Z, 6,                  colors='k',                  ) plt.clabel(CS, fontsize=9, inline=1) plt.title('Single color - negative contours solid') sPicNameOut3=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/contour3.png' plt.savefig(sPicNameOut3,dpi=600) plt.tight_layout() plt.show() More results are shown in Figure [11-23](#Fig23).![A435693_1_En_11_Fig23_HTML.jpg](img/A435693_1_En_11_Fig23_HTML.jpg) Figure 11-23Graph with solid contours So, let’s construct a contour to show you what you could achieve. plt.figure(figsize=(10, 10)) CS = plt.contour(X, Y, Z, 6,                  linewidths=np.arange(.5, 4, .5),                  colors=('r', 'green', 'blue', \                          (1, 1, 0), '#afeeee', '0.5')                  ) plt.clabel(CS, fontsize=9, inline=1) plt.title('Crazy lines') sPicNameOut4=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/contour4.png' plt.savefig(sPicNameOut4,dpi=600) plt.tight_layout() plt.show() What do you think? Can you see the benefits of ensuring a change in line color or line type in Figure [11-24](#Fig24)? I find it a good communication tool. Tip Gray out but never remove unimportant information, as it gives the audience a point of comparison about what you think is important or not, without losing the data set. ![A435693_1_En_11_Fig24_HTML.jpg](img/A435693_1_En_11_Fig24_HTML.jpg) Figure 11-24Graph with highlighted contour You can do some amazing graphs. plt.figure(figsize=(12, 10)) im = plt.imshow(Z, interpolation="bilinear", origin="lower",                 cmap=cm.gray, extent=(-3, 3, -2, 2)) levels = np.arange(-1.2, 1.6, 0.2) CS = plt.contour(Z, levels,                  origin='lower',                  linewidths=2,                  extent=(-3, 3, -2, 2)) Let’s now thicken the zero contours. zc = CS.collections[6] plt.setp(zc, linewidth=4) plt.clabel(CS, levels[1::2],  # label every second level            inline=1,            fmt='%1.1f',            fontsize=14) Now add a color bar for the contour lines. CB = plt.colorbar(CS, shrink=0.8, extend="both") plt.title('Lines with colorbar') #plt.hot()  # Now change the colormap for the contour lines and colorbar plt.flag() You can still add a color bar for the image also. CBI = plt.colorbar(im, orientation="horizontal", shrink=0.8) That makes the original color bar look a bit out of place, so let’s recover its location. l, b, w, h = plt.gca().get_position().bounds ll, bb, ww, hh = CB.ax.get_position().bounds CB.ax.set_position([ll, b + 0.1*h, ww, h*0.8]) sPicNameOut5=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/contour5.png' plt.savefig(sPicNameOut5,dpi=600) plt.tight_layout() plt.show() Wow, you have created a super contour graph (Figure [11-25](#Fig25)).![A435693_1_En_11_Fig25_HTML.jpg](img/A435693_1_En_11_Fig25_HTML.jpg) Figure 11-25 Multidimensional complex contour graph I have a question. Did the extra work to create this super contour graph add any more value to the graph’s message? I would not create graphs of this complexity, unless I want to demonstrate my ability to generate complex visualization. Warning Always make sure that your message is passed on! I am, however, also a scientist at heart, so when you have the spare time, make different changes, try other colors, and explore what each parameter controls. In short, have fun with the process.

#### 三维图形

Let’s go 3D with our graphs. Many data sets have more than two influencing factors, and a 3D graph will assist in communicating those. Open your Python editor and build this ecosystem: ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D from sklearn import decomposition from sklearn import datasets if sys.platform == 'linux' or sys.platform == ' darwin':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') Now generate a data set. np.random.seed(5) centers = [[1, 1], [-1, -1], [1, -1]] iris = datasets.load_iris() X = iris.data y = iris.target Now let’s go 3D. fig = plt.figure(1, figsize=(16, 12)) plt.clf() ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134) plt.cla() pca = decomposition.PCA(n_components=3) pca.fit(X) X = pca.transform(X) for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:     ax.text3D(X[y == label, 0].mean(),               X[y == label, 1].mean() + 1.5,               X[y == label, 2].mean(), name,               horizontalalignment='center',               bbox=dict(alpha=.5, edgecolor="w", facecolor="w")) I suggest you reorder the labels, to have colors matching the cluster results. y = np.choose(y, [1, 2, 0]).astype(np.float) ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.spectral,            edgecolor='k',marker='p',s=300) ax.w_xaxis.set_ticklabels([]) ax.w_yaxis.set_ticklabels([]) ax.w_zaxis.set_ticklabels([]) sPicNameOut0=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/3DPlot.png' plt.savefig(sPicNameOut0,dpi=600) plt.show() Well done. You can now create 3D graphs. See Figure [11-26](#Fig26) for the result.![A435693_1_En_11_Fig26_HTML.jpg](img/A435693_1_En_11_Fig26_HTML.jpg) Figure 11-26Three-dimensional graph

### 摘要

You have successfully progressed thought the graph section of this chapter. Let’s summarize what you have learned.

*   您现在可以创建各种 2D 和 3D 图形类型。
*   您可以使用我在本书前面讨论的技术将您的图表转换成电影。你也可以简单地在你最喜欢的演示软件上添加一系列图片。
*   您可以通过更改颜色、线型或其他标准来增强部分图表。

Tip I have only introduced you to the world of visualization using graphs. Please perform more research and look at other people’s work. I have a graphics expert working for me. I normally pass all my final graphs to her to fix, as she is an artist skilled at preparing good-quality presentations. Use every skill available to you, to ensure that your communication is perfect. The next section is part of visualization, as it shows you how to use pre-prepared visualizations in your data science.

## 图片

Pictures are an interesting data science specialty. The processing of movies and pictures is a science on its own. I will give you an introduction into the area but admit that it is merely a peek into this subfield of data science.

### 图像通道

The interesting fact about any picture is that it is a complex data set in every image. Pictures are built using many layers or channels that assists the visualization tools to render the required image. Open your Python editor, and let’s investigate the inner workings of an image. import sys import os import matplotlib.pyplot as plt import matplotlib.image as mpimg ################################################################ if sys.platform == 'linux' or sys.platform == ' darwin':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ Here is a typical image in Portable Network Graphics (PNG) format: sPicName=Base+'/01-Vermeulen/00-RawData/AudiR8.png' You can load it into a matrix of value. t=0 img=mpimg.imread(sPicName) print('Size:', img.shape) plt.figure(figsize=(10, 10)) t+=1 sTitle= '(' + str(t) + ') Original' plt.title(sTitle) plt.imshow(img) plt.show() for c in range(img.shape[2]):     t+=1     plt.figure(figsize=(10, 10))     sTitle= '(' + str(t) + ') Channel: ' + str(c)     plt.title(sTitle)     lum_img = img[:,:,c]     plt.imshow(lum_img)     plt.show() You can clearly see that the image is of the following size: 1073, 1950, 4\. This means you have 1073 × 1950 pixels per channel. That is a total of 2,092,350 pixels per layer. As you have four layers, that equals a total of 8,369,400 values describing image size. You can get this size by executing img.size. Let’s look into transforming the images.

### 前沿

One of the most common techniques that most data science projects require is the determination of the edge of an item’s image . This is useful in areas such as robotics object selection and face recognition, to cite two special cases. I will show you how to determine the edge of the image you saw before. Open your Python editor and create this ecosystem: ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import matplotlib.pyplot as plt from PIL import Image ################################################################ if sys.platform == 'linux' or sys.platform == ' darwin':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ sPicNameIn=Base+'/01-Vermeulen/00-RawData/AudiR8.png' sPicNameOut=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/AudiR8Edge1.png' You can now open the image. imageIn = Image.open(sPicNameIn) fig1=plt.figure(figsize=(10, 10)) fig1.suptitle('Audi R8', fontsize=20) imgplot = plt.imshow(imageIn) You now convert the image to black and white. mask=imageIn.convert("L") The next value must be adjusted for an image with which to experiment with outcomes. You can select a range between 0 and 255, to inspect the outcome of your investigation. th=49 imageOut = mask.point(lambda i: i < th and 255) You can save your experiment or try another. imageOut.save(sPicNameOut) imageTest = Image.open(sPicNameOut) fig2=plt.figure(figsize=(10, 10)) fig2.suptitle('Audi R8 Edge', fontsize=20) imgplot = plt.imshow(imageTest) Figure [11-27](#Fig27) shows the result.![A435693_1_En_11_Fig27_HTML.jpg](img/A435693_1_En_11_Fig27_HTML.jpg) Figure 11-27Image with edges extracted I will now show you a slightly different route. This method is common in robotics , because it is a quick way to identify the existence of an object in the line of sight. Open your Python editor and set up this ecosystem: ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import numpy import scipy from scipy import ndimage ################################################################ if sys.platform == 'linux' or sys.platform == ' darwin':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ sPicNameIn=Base+'/01-Vermeulen/00-RawData/AudiR8.png' sPicNameOut=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/AudiR8Edge2.png' ################################################################ You now load the image . im = scipy.misc.imread(sPicNameIn) You can now apply limited mathematics calculations, and you have an edge image. im = im.astype('int32') dx = ndimage.sobel(im, 0) # horizontal derivative dy = ndimage.sobel(im, 1) # vertical derivative mag = numpy.hypot(dx, dy) # magnitude mag *= 255.0 / numpy.max(mag) # normalize (Q&D) You can now save your image. scipy.misc.imsave(sPicNameOut, mag) You can now compare your two images . import matplotlib.pyplot as plt from PIL import Image imageIn = Image.open(sPicNameIn) plt.figure(figsize=(10, 10)) imgplot = plt.imshow(imageIn) imageTest = Image.open(sPicNameOut) plt.figure(figsize=(10, 10)) imgplot = plt.imshow(imageTest) The result is shown in Figure [11-28](#Fig28).![A435693_1_En_11_Fig28_HTML.jpg](img/A435693_1_En_11_Fig28_HTML.jpg) Figure 11-28 Edged image Well done. You can now do edges and compare results. These are two important skills for processing images.

### 一种尺寸不适合所有人

The images we get to process are mostly of different sizes and quality . You will have to size images to specific sizes for most of your data science. Warning If you change an image, make sure you save your result under a new name. I have seen too much image processing that simply overwrites the originals. The following examples will demonstrate what happens to an image if you reduce the pixel quality. Open your Python editor and set up this ecosystem: ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import matplotlib.pyplot as plt from PIL import Image ################################################################ if sys.platform == 'linux' or sys.platform == ' darwin':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ sPicName=Base+'/01-Vermeulen/00-RawData/AudiR8.png' nSize=4 ################################################################ Let’s load the original image . img = Image.open(sPicName) plt.figure(figsize=(nSize, nSize)) sTitle='Unchanges' plt.title(sTitle) imgplot = plt.imshow(img) You now apply a thumbnail function that creates a 64 × 64 pixel thumbnail image. img.thumbnail((64, 64), Image.ANTIALIAS) # resizes image in-place plt.figure(figsize=(nSize, nSize)) sTitle='Resized' plt.title(sTitle) imgplot = plt.imshow(img) plt.figure(figsize=(nSize, nSize)) sTitle='Resized with Bi-Cubic' plt.title(sTitle) imgplot = plt.imshow(img, interpolation="bicubic") ################################################################ print('### Done!! ############################################') ################################################################ Can you see the impact it has on the quality of the image ?

## 显示差异

I want to show you this technique, to enable you to produce two overlaying results but still show that both exist. In a data science presentation showing that two sets of nodes on a graph are the same, you must make each set marginally different, in an orderly manner, to facilitate their visualization. Without this slight shift, the two sets will simply overlay each other. import numpy as np from matplotlib import pyplot as plt from matplotlib.collections import LineCollection from sklearn import manifold from sklearn.metrics import euclidean_distances from sklearn.decomposition import PCA n_samples = 25 seed = np.random.RandomState(seed=3) X_true = seed.randint(0, 20, 2 * n_samples).astype(np.float) X_true = X_true.reshape((n_samples, 2)) # Center the data X_true -= X_true.mean() similarities = euclidean_distances(X_true) You simply add noise to the similarities. noise = np.random.rand(n_samples, n_samples) noise = noise + noise.T noise[np.arange(noise.shape[0]), np.arange(noise.shape[0])] = 0 similarities += noise mds = manifold.MDS(n_components=2, max_iter=3000, eps=1e-9, random_state=seed,                    dissimilarity="precomputed", n_jobs=1) pos = mds.fit(similarities).embedding_ nmds = manifold.MDS(n_components=2, metric=False, max_iter=3000, eps=1e-12,                     dissimilarity="precomputed", random_state=seed, n_jobs=1,                     n_init=1) npos = nmds.fit_transform(similarities, init=pos) You then rescale the data. pos *= np.sqrt((X_true ** 2).sum()) / np.sqrt((pos ** 2).sum()) npos *= np.sqrt((X_true ** 2).sum()) / np.sqrt((npos ** 2).sum()) Next, you rotate the data by a small margin . clf = PCA(n_components=2) X_true = clf.fit_transform(X_true) pos = clf.fit_transform(pos) npos = clf.fit_transform(npos) fig = plt.figure(1) ax = plt.axes([0., 0., 1., 1.]) s = 100 plt.scatter(X_true[:, 0], X_true[:, 1], color="navy", s=s, lw=0,             label='True Position') plt.scatter(pos[:, 0], pos[:, 1], color="turquoise", s=s, lw=0, label="MDS") plt.scatter(npos[:, 0], npos[:, 1], color="darkorange", s=s, lw=0, label="NMDS") plt.legend(scatterpoints=1, loc="best", shadow=False) similarities = similarities.max() / similarities * 100 similarities[np.isinf(similarities)] = 0 # Plot the edges start_idx, end_idx = np.where(pos) # a sequence of (*line0*, *line1*, *line2*), where:: #            linen = (x0, y0), (x1, y1), ... (xm, ym) segments = [[X_true[i, :], X_true[j, :]]             for i in range(len(pos)) for j in range(len(pos))] values = np.abs(similarities) lc = LineCollection(segments,                     zorder=0, cmap=plt.cm.Blues,                     norm=plt.Normalize(0, values.max())) lc.set_array(similarities.flatten()) lc.set_linewidths(0.5 * np.ones(len(segments))) ax.add_collection(lc) plt.show() You should see the same result as that shown in Figure [11-29](#Fig29).![A435693_1_En_11_Fig29_HTML.jpg](img/A435693_1_En_11_Fig29_HTML.jpg) Figure 11-29Improved visualization by a shift of values Success, you have improved the visualization .

## 摘要

Congratulations! You have reached the end of the process I use for my own data science projects. You should now be able to construct a pipeline for any data in the data lake, via the complete Retrieve ➤ Assess ➤ Process ➤ Transform ➤ Organize ➤ Report steps to building the technology stack for turning data lakes into business assets.

## 结束语

You have successfully reached the end of this book. Well done on all your efforts and achievements. Thank you for following me along the entire pipeline, from the data lake to the insights for the business. I hope that your practical data science provides you with everything you wish for in the future. The biggest achievement you could accomplish is . . .

> 一路上玩得开心！