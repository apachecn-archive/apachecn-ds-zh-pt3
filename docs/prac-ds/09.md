© Andreas François Vermeulen 2018 Andreas François VermeulenPractical Data Science[https://doi.org/10.1007/978-1-4842-3054-1_9](#)

# 9.过程超级步骤

Andreas François Vermeulen<sup class="calibre7">1 </sup> (1)West Kilbride North Ayrshire, UK   The Process superstep adapts the assess results of the retrieve versions of the data sources into a highly structured data vault that will form the basic data structure for the rest of the data science steps. This data vault involves the formulation of a standard data amalgamation format across a range of projects. Tip If you follow the rules of the data vault, it results in a clean and stable structure for your future data science. The Process superstep is the amalgamation process that pipes your data sources into five main categories of data (Figure [9-1](#Fig1)).![A435693_1_En_9_Fig1_HTML.jpg](Images/A435693_1_En_9_Fig1_HTML.jpg) Figure 9-1Five categories of data Using only these five hubs in your data vault, and with good modeling, you can describe most activities of your customers. This enable you to then fine-tune your data science algorithms, to simply understand the five hubs’ purpose and relationships that enable good data science .

## 数据库

Data vault modeling is a database modeling method designed by Dan Linstedt. The data structure is designed to be responsible for long-term historical storage of data from multiple operational systems. It supports chronological historical data tracking for full auditing and enables parallel loading of the structures.

### 中心

Data vault hubs contain a set of unique business keys that normally do not change over time and, therefore, are stable data entities to store in hubs. Hubs hold a surrogate key for each hub data entry and metadata labeling the source of the business key.

### 链接

Data vault links are associations between business keys. These links are essentially many-to-many joins, with additional metadata to enhance the particular link.

### 卫星

Data vault satellites hold the chronological and descriptive characteristics for a specific section of business data. The hubs and links form the structure of the model but have no chronological characteristics and hold no descriptive characteristics. Satellites consist of characteristics and metadata linking them to their specific hub. Metadata labeling the origin of the association and characteristics, along with a time line with start and end dates for the characteristics, is put in safekeeping, for future use from the data section. Each satellite holds an entire chronological history of the data entities within the specific satellite.

### 参考卫星

Reference satellites are referenced from satellites but under no circumstances bound with metadata for hub keys. They prevent redundant storage of reference characteristics that are used regularly by other satellites. Typical reference satellites are

*   标准代码:这些代码如 ISO 3166 代表国家代码，ISO 4217 代表货币，ISO 8601 代表时区。
*   特定特征的固定列表:这些可以是减少其他标准列表的标准列表。例如，您的企业设有办事处的国家列表可能是 ISO 3166 列表中的一个缩减的固定列表。您还可以根据自己的报告结构生成自己的列表，比如业务区域。
*   转换查找:查看全球定位系统(GPS)变换，如莫洛金斯基变换、赫尔默特变换、莫洛金斯基-巴德卡斯变换或高斯-克鲁格坐标系。

The most common is WGS84, the standard U.S. Department of Defense definition of a global location system for geospatial information, and is the reference system for the GPS.

## 时间-人员-对象-位置-事件数据库

The data vault we use is based on the Time-Person-Object-Location-Event (T-P-O-L-E) design principle (Figure [9-2](#Fig2)).![A435693_1_En_9_Fig2_HTML.jpg](Images/A435693_1_En_9_Fig2_HTML.jpg) Figure 9-2T-P-O-L-E—high-level design Note The entire data structures discussed over the next pages can be found in the datavault.db file, which can be found in directory \VKHCG\88-DV.

### 时间段

The time section contains the complete data structure for all data entities related to recording the time at which everything occurred.

#### 时间中心

The time hub consists of the following fields: CREATE TABLE [Hub-Time] (     IDNumber      VARCHAR (100) PRIMARY KEY,     IDTimeNumber  Integer,     ZoneBaseKey   VARCHAR (100),     DateTimeKey   VARCHAR (100),     DateTimeValue DATETIME ); This time hub acts as the core connector between time zones and other date-time associated values.

#### 时间链接

The time links link the time hub to the other hubs (Figure [9-3](#Fig3)).![A435693_1_En_9_Fig3_HTML.jpg](Images/A435693_1_En_9_Fig3_HTML.jpg) Figure 9-3Time links The following links are supported.

##### 时间-人联系

This connects date-time values within the person hub to the time hub. The physical link structure is stored as a many-to-many relationship time within the data vault. Later in this chapter, I will supply details on how to construct the tables. Dates such as birthdays, marriage anniversaries, and the date of reading this book can be recorded as separate links in the data vault. The normal format is BirthdayOn, MarriedOn, or ReadBookOn. The format is simply a pair of keys between the time and person hubs.

##### 时间-对象链接

This connects date-time values within the object hub to the time hub. Dates such as those on which you bought a car, sold a car, and read this book can be recorded as separate links in the data vault. The normal format is BoughtCarOn, SoldCarOn, or ReadBookOn. The format is simply a pair of keys between the time and object hubs.

##### 时间-位置链接

This connects date-time values in the location hub to the time hub. Dates such as moved to post code SW1, moved from post code SW1, and read book at post code SW1 can be recorded as separate links in the data vault. The normal format is MovedToPostCode, MovedFromPostCode, or ReadBookAtPostCode. The format is simply a pair of keys between the time and location hubs .

##### 时间-事件链接

This connects date-time values in the event hub with the time hub. Dates such as those on which you have moved house and changed vehicles can be recorded as separate links in the data vault. The normal format is MoveHouse or ChangeVehicle. The format is simply a pair of keys between the time and event hubs .

#### 时间卫星

Time satellites are the part of the vault that stores the following fields. Note The <Time Zone> part of the table name is from a list of time zones that I will discuss later in the chapter. CREATE TABLE [Satellite-Time-<Time Zone>] (     IDZoneNumber     VARCHAR (100) PRIMARY KEY,     IDTimeNumber     INTEGER,     ZoneBaseKey      VARCHAR (100),     DateTimeKey      VARCHAR (100),     UTCDateTimeValue DATETIME,     Zone             VARCHAR (100),     DateTimeValue    DATETIME ); Time satellites enable you to work more easily with international business patterns. You can move between time zones to look at such patterns as “In the morning . . . ” or “At lunchtime . . . ” These capabilities will be used during the Transform superstep, to discover patterns and behaviors around the world.

### 个人部分

The person section contains the complete data structure for all data entities related to recording the person involved.

#### 个人中心

The person hub consists of a series of fields that supports a “real” person. The person hub consists of the following fields: CREATE TABLE [Hub-Person] (     IDPersonNumber     INTEGER,     FirstName          VARCHAR (200),     SecondName         VARCHAR (200),     LastName           VARCHAR (200),     Gender             VARCHAR (20),     TimeZone           VARCHAR (100),     BirthDateKey       VARCHAR (100),     BirthDate          DATETIME );

#### 人员链接

This links the person hub to the other hubs (Figure [9-4](#Fig4)).![A435693_1_En_9_Fig4_HTML.jpg](Images/A435693_1_En_9_Fig4_HTML.jpg) Figure 9-4Person links The following links are supported in person links.

##### 人-时间联系

This link joins the person to the time hub , to describe the relationships between the two hubs. The link consists of the following fields: CREATE TABLE [Link-Person-Time] (     IDPersonNumber   INTEGER,     IDTimeNumber     INTEGER,     ValidDate        DATETIME );

##### 人-物链接

This link joins the person to the object hub to describe the relationships between the two hubs. The link consists of the following fields : CREATE TABLE [Link-Person-Object] (     IDPersonNumber   INTEGER,     IDObjectNumber   INTEGER,     ValidDate        DATETIME );

##### 人-位置链接

This link joins the person to the location hub , to describe the relationships between the two hubs. The link consists of the following fields: CREATE TABLE [Link-Person-Time] (     IDPersonNumber       INTEGER,     IDLocationNumber     INTEGER,     ValidDate            DATETIME );

##### 人-事件链接

This link joins the person to the event hub , to describe the relationships between the two hubs. The link consists of the following fields: CREATE TABLE [Link-Person-Time] (     IDPersonNumber   INTEGER,     IDEventNumber    INTEGER,     ValidDate        DATETIME );

#### 载人卫星

The person satellites are the part of the vault that stores the temporal attributes and descriptive attributes of the data. The satellite is of the following format: CREATE TABLE [Satellite-Person-Gender] ( PersonSatelliteID VARCHAR (100), IDPersonNumber INTEGER, FirstName VARCHAR (200), SecondName VARCHAR (200), LastName VARCHAR (200), BirthDateKey VARCHAR (20), Gender VARCHAR (10), );

### 对象部分

The object section contains the complete data structure for all data entities related to recording the object involved.

#### 对象中枢

The object hub consists of a series of fields that supports a “real” object. The object hub consists of the following fields: CREATE TABLE [Hub-Object-Species] ( IDObjectNumber INTEGER, ObjectBaseKey VARCHAR (100), ObjectNumber VARCHAR (100), ObjectValue VARCHAR (200), ); This structure enables you as a data scientist to categorize the objects in the business environment.

#### 对象链接

These link the object hub to the other hubs (Figure [9-5](#Fig5)).![A435693_1_En_9_Fig5_HTML.jpg](Images/A435693_1_En_9_Fig5_HTML.jpg) Figure 9-5Object links The following links are supported:

##### 对象-时间链接

This link joins the object to the time hub, to describe the relationships between the two hubs. The link consists of the following fields: CREATE TABLE [Link-Object-Time] (     IDObjectNumber   INTEGER,     IDTimeNumber     INTEGER,     ValidDate        DATETIME );

##### 物-人联系

This link joins the object to the person hub to describe the relationships between the two hubs. The link consists of the following fields: CREATE TABLE [Link-Object-Person] (     IDObjectNumber   INTEGER,     IDPersonNumber   INTEGER,     ValidDate        DATETIME );

##### 目标位置链接

This link joins the object to the location hub , to describe the relationships between the two hubs. The link consists of the following fields: CREATE TABLE [Link-Object-Location] (     IDObjectNumber   INTEGER,     IDLocationNumber INTEGER,     ValidDate        DATETIME );

##### 对象-事件链接

This link joins the object to the event hub to describe the relationships between the two hubs.

#### 目标卫星

Object satellites are the part of the vault that stores and provisions the detailed characteristics of objects. The typical object satellite has the following data fields: CREATE TABLE [Satellite-Object-Make-Model] ( IDObjectNumber INTEGER, ObjectSatelliteID VARCHAR (200), ObjectType VARCHAR (200), ObjectKey VARCHAR (200), ObjectUUID VARCHAR (200), Make VARCHAR (200), Model VARCHAR (200) ); The object satellites will hold additional characteristics, as each object requires additional information to describe the object. I keep each set separately, to ensure future expansion, as the objects are the one hub that evolves at a high rate of change, as new characteristics are discovered by your data science.

### 位置部分

The location section contains the complete data structure for all data entities related to recording the location involved.

#### 位置中枢

The location hub consists of a series of fields that supports a GPS location. The location hub consists of the following fields: CREATE TABLE [Hub-Location] (     IDLocationNumber INTEGER,     ObjectBaseKey  VARCHAR (200),     LocationNumber INTEGER,     LocationName   VARCHAR (200),     Longitude      DECIMAL (9, 6),     Latitude       DECIMAL (9, 6) ); The location hub enables you to link any location, address, or geospatial information to the rest of the data vault.

#### 位置链接

The location links join the location hub to the other hubs (Figure [9-6](#Fig6)).![A435693_1_En_9_Fig6_HTML.jpg](Images/A435693_1_En_9_Fig6_HTML.jpg) Figure 9-6Location links The following links are supported.

##### 位置-时间链接

The link joins the location to the time hub , to describe the relationships between the two hubs. The link consists of the following fields: CREATE TABLE [Link-Location-Time] (     IDLocationNumber   INTEGER,     IDTimeNumber       INTEGER,     ValidDate          DATETIME ); These links support business actions such as ArrivedAtShopAtDateTime or ShopOpensAtTime.

##### 位置-人员链接

This link joins the location to the person hub , to describe the relationships between the two hubs. The link consists of the following fields: CREATE TABLE [Link-Location-Person] (     IDLocationNumber   INTEGER,     IDPersonNumber     INTEGER,     ValidDate          DATETIME ); These links support such business actions as ManagerAtShop or SecurityAtShop.

##### 位置-对象链接

This link joins the location to the object hub , to describe the relationships between the two hubs. The link consists of the following fields: CREATE TABLE [Link-Location-Object] (     IDLocationNumber   INTEGER,     IDObjectNumber     INTEGER,     ValidDate          DATETIME ); These links support such business actions as ShopDeliveryVan or RackAtShop.

##### 位置-事件链接

This link joins the location to the event hub , to describe the relationships between the two hubs. The link consists of the following fields: CREATE TABLE [Link-Location-Event] (     IDLocationNumber   INTEGER,     IDEventNumber      INTEGER,     ValidDate          DATETIME ); These links support such business actions as ShopOpened or PostCodeDeliveryStarted.

#### 定位卫星

The location satellites are the part of the vault that stores and provisions the detailed characteristics of where entities are located. The typical location satellite has the following data fields : CREATE TABLE [Satellite-Location-PostCode] ( IDLocationNumber INTEGER, LocationSatelliteID VARCHAR (200), LocationType VARCHAR (200), LocationKey VARCHAR (200), LocationUUID VARCHAR (200), CountryCode VARCHAR (20), PostCode VARCHAR (200) ); The location satellites will also hold additional characteristics that are related only to your specific customer. They may split their business areas into their own regions, e.g., Europe, Middle-East, and China. These can be added as a separate location satellite.

### 事件部分

The event section contains the complete data structure for all data entities related to recording the event that occurred.

#### 活动中心

The event hub consists of a series of fields that supports events that happens in the real world. The event hub consists of the following fields: CREATE TABLE [Hub-Event] (     IDEventNumber    INTEGER,     EventType        VARCHAR (200),     EventDescription VARCHAR (200) );

#### 事件链接

Event links join the event hub to the other hubs (see Figure [9-7](#Fig7)).![A435693_1_En_9_Fig7_HTML.jpg](Images/A435693_1_En_9_Fig7_HTML.jpg) Figure 9-7Event links The following links are supported.

##### 事件-时间链接

This link joins the event to the time hub, to describe the relationships between the two hubs . The link consists of the following fields: CREATE TABLE [Link-Event-Time] (     IDEventNumber   INTEGER,     IDTimeNumber     INTEGER,     ValidDate        DATETIME ); These links support such business actions as DeliveryDueAt or DeliveredAt.

##### 事件-人员链接

This link joins the event to the person hub , to describe the relationships between the two hubs. The link consists of the following fields: CREATE TABLE [Link-Event-Person] (     IDEventNumber   INTEGER,     IDPersonNumber  INTEGER,     ValidDate       DATETIME ); These links support such business actions as ManagerAppointAs or StaffMemberJoins.

##### 事件-对象链接

This link joins the event to the object hub , to describe the relationships between the two hubs. The link consists of the following fields: CREATE TABLE [Link-Event-Object] (     IDEventNumber   INTEGER,     IDObjectNumber  INTEGER,     ValidDate       DATETIME ); These links support such business actions as VehicleBuy, VehicleSell, or ItemInStock.

##### 事件位置链接

The link joins the event to the location hub to describe the relationships between the two hubs. The link consists of the following fields: CREATE TABLE [Link-Event-Location] (     IDEventNumber   INTEGER,     IDTimeNumber    INTEGER,     ValidDate       DATETIME ); These links support such business actions as DeliveredAtPostCode or PickupFromGPS.

#### 活动卫星

The event satellites are the part of the vault that stores the details related to all the events that occur within the systems you will analyze with your data science. I suggest that you keep to one type of event per satellite. This enables future expansion and easier long-term maintenance of the data vault .

### 设计一个实用的过程超级步骤

I will now begin with a series of informational and practical sections with the end goal of having you use the Process step to generate a data vault using the T-P-O-L-E design model previously discussed. Note I use the T-P-O-L-E model, but the “pure” data vault can use various other hubs, links, or satellites. The model is a simplified one that enables me to reuse numerous utilities and processes I have developed over years of working with data science.

### 时间

Time is the characteristic of the data that is directly associated with time recording. ISO 8601-2004 describes an international standard for data elements and interchange formats for dates and times. Following is a basic set of rules to handle the data interchange. The calendar is based on the Gregorian calendar . The following data entities (year, month, day, hour, minute, second, and fraction of a second) are officially part of the ISO 8601-2004 standard. The data is recorded from largest to smallest. Values must have a pre-agreed fixed number of digits that are padded with leading zeros. I will now introduce you to these different data entities.

#### 年

The standard year must use four-digit values within the range 0000 to 9999, with the optional agreement that any date denoted by AC/BC requires a conversion whereby AD 1 = year 1, 1 BC = year 0, 2 BC = year -1\. That means that 2017AC becomes +2017, but 2017BC becomes -2016. Valid formats are: YYYY and +/-YYYY. The rule for a valid year is 20 October 2017 becomes 2017-10-20 or +2017-10-20 or a basic 20171020\. The rule for missing data on a year is 20 October becomes --10-20 or --1020. The use of YYYYMMDD is common in source systems. I advise you to translate to YYYY-MM-DD during the Assess step, to generate a consistency with the date in later stages of processing. Warning Discuss with your customer the dates before October 1582 (the official start of the Gregorian calendar). The Julian calendar was used from January 1, 45 BC. If the earlier dates are vital to your customer’s data processing, I suggest that you perform an in-depth study into the other, more complex differences in dates. This is very important when you work with text mining on older documents. Open your IPython editor and investigate the following date format: from datetime import datetime from pytz import timezone, all_timezones now_date = datetime(2001,2,3,4,5,6,7) now_utc=now_date.replace(tzinfo=timezone('UTC')) print('Date:',str(now_utc.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) print('Year:',str(now_utc.strftime("%Y"))) You should see Date: 2001-02-03 04:05:06 (UTC) (+0000) Year: 2001 This enables you to easily extract a year value from a given date.

#### 月

The standard month must use two-digit values within the range of 01 through 12\. The rule for a valid month is 20 October 2017 becomes 2017-10-20 or +2017-10-20\. The rule for valid month without a day is October 2017 becomes 2017-10 or +2017-10. Warning You cannot use YYYYMM, so 201710 is not valid. This standard avoids confusion with the truncated representation YYMMDD, which is still often used in source systems. 201011 could be 2010-11 or 1920-10-11. Open your IPython editor and investigate the following date format: print('Date:',str(now_utc.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) print('Month:',str(now_utc.strftime("%m"))) You should get back Date: 2001-02-03 04:05:06 (UTC) (+0000) Month: 02 Now you can get the month. This follows the standard English names for months.

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"></colgroup> 
| 数字 | 名字 |
| --- | --- |
| one | 一月 |
| Two | 二月 |
| three | 三月 |
| four | 四月 |
| five | 五月 |
| six | 六月 |
| seven | 七月 |
| eight | 八月 |
| nine | 九月 |
| Ten | 十月 |
| Eleven | 十一月 |
| Twelve | 十二月 |

Open your IPython editor and investigate the following date format: print('Date:',str(now_utc.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) print('MonthName:', str(now_utc.strftime("%B"))) This gives you the months in words. Date: 2001-02-03 04:05:06 (UTC) (+0000) MonthName: February

#### 一天

The standard day of the month must use a two-digit value with possible values within the range of 01 through 31, as per an agreed format for the specific month and year. The rule for a valid month is 20 October 2017 becomes 2017-10-20 or +2017-10-20. Open your IPython editor and investigate the following date format: print('Date:',str(now_utc.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) print('Day:',str(now_utc.strftime("%d"))) This returns the day of the date. Date: 2001-02-03 04:05:06 (UTC) (+0000) Day: 03 There are two standards that apply to validate dates: non-leap year and leap year. Owing to the length of the solar year being marginally less than 365¼ days—by about 11 minutes—every 4 years, the calendar indicates a leap year, unless the year is divisible by 400\. This fixes the 11 minutes gained over 400 years. Open your IPython editor and investigate the following date format. I will show you a way to find the leap years . import datetime for year in range(1960,2025):     month=2     day=29     hour=0     correctDate = None     try:         newDate = datetime.datetime(year=year,month=month,day=day,hour=hour)         correctDate = True     except ValueError:         correctDate = False     if correctDate == True:         if year%400 == 0:             print(year, 'Leap Year (400)')         else:                         print(year, 'Leap Year')     else:         print(year,'Non Leap Year') The result shows that the leap years are as follows: 1960,1964,1968,1972,1976,1980,1984,1988,1992,1996,2000(This was a 400) 2004,2008,2012,2016,2020,2024. Now, we can look at the time component of the date time.

#### 小时

The standard hour must use a two-digit value within the range of 00 through 24\. The valid format is hhmmss or hh:mm:ss. The shortened format hhmm or hh:mm is accepted but not advised. The format hh is not supported. The use of 00:00:00 is the beginning of the calendar day. The use of 24:00:00 is only to indicate the end of the calendar day. Open your IPython editor and investigate the following date format: print('Date:',str(now_utc.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) print('Hour:',str(now_utc.strftime("%H"))) The results are: Date: 2001-02-03 04:05:06 (UTC) (+0000) Hour: 04

#### 分钟

The standard minute must use two-digit values within the range of 00 through 59\. The valid format is hhmmss or hh:mm:ss. The shortened format hhmm or hh:mm is accepted but not advised. Open your IPython editor and investigate the following date format: print('Date:',str(now_utc.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) print('Minute:',str(now_utc.strftime("%M"))) The results are Date: 2001-02-03 04:05:06 (UTC) (+0000) Minute: 05

#### 第二

The standard second must use two-digit values within the range of 00 through 59\. The valid format is hhmmss or hh:mm:ss. Open your IPython editor and investigate the following date format: print('Date:',str(now_utc.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) print('Second:',str(now_utc.strftime("%S"))) This returns the seconds of the date time. Date: 2001-02-03 04:05:06 (UTC) (+0000) Second: 06 Note This is normally where most systems stop recording time, but I have a few clients that require a more precise time-recording level. So, now we venture into sub-second times. This is a fast world with interesting dynamics.

#### 几分之一秒

The fraction of a second is only defined as a format: hhmmss,sss or hh:mm:ss,sss or hhmmss.sss or hh:mm:ss.sss. I prefer the hh:mm:ss.sss format, as the use of the comma causes issues with exports to CSV file formats, which are common in the data science ecosystem. The current commonly used formats are the following:

*   hh:mm:ss:十分之一秒
*   hh:mm:ss:百分之一秒
*   hh:mm:ss . ss:千分之一秒

Data science works best if you can record the data at the lowest possible levels. I suggest that you record your time at the hh:mm:ss.sss level. Open your IPython editor and investigate the following date format: print('Date:',str(now_utc.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) print('Millionth of Second:',str(now_utc.strftime("%f"))) This returns the fraction of a second in one-millionth of a second, or vicrosecond, as follows:

*   日期:2001-02-03 04:05:06 (UTC) (+0000)
*   百万分之一秒:000007

There are some smaller units, such as the following:

*   纳秒:十亿分之一秒
    *   标度比较:一纳秒等于一秒，正如一秒等于 31.71 年。
*   皮秒:万亿分之一或百万分之一秒
    *   尺度比较:一皮秒等于一秒，就像一秒等于 31，710 年一样。

I have a customer who owns a scientific device called a streak, or intensified CCD (ICCD), camera. These are able to picture the motion of light, wherein +/-3.3 picoseconds is the time it takes for light to travel 1 millimeter. Note In 2011, MIT researchers announced they had a trillion-frame-per-second video camera. This camera’s ability is exceptional, as the standard for film is 24 frames per second (fps), and 30 fps for video, with some 48 fps models. In the measurement of high-performance computing, it takes +/-330 picoseconds for a common 3.0 GHz computer CPU to add two integers. The world of a data scientists is a world of extreme measures. Remember the movie-to-frame example you performed earlier in Chapter [5](05.html)? This camera would produce enough frames in a second for 1,056,269,593.8 years of video playback at 30 fps.

#### 本地时间和协调世界时(UTC)

The date and time we use is bound to a specific location on the earth and the specific time of the calendar year. The agreed local time is approved by the specific country. I am discussing time zones because, as a data scientist, you will have to amalgamate data from multiple data sources across the globe. The issue is the impact of time zones and the irregular application of rules by customers.

##### 当地时间

The approved local time is agreed by the specific country, by approving the use of a specific time zone for that country. Daylight Saving Time can also be used, which gives a specific country a shift in its local time during specific periods of the year, if they use it. The general format for local time is hh:mm:ss. Open your IPython editor and investigate the following date format: from datetime import datetime now_date = datetime.now() print('Date:',str(now_date.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) The results is 2017-10-01 10:45:58 () () The reason for the two empty brackets is that the date time is in a local time setting. Warning Most data systems record the local time in the local time setting for the specific server, PC, or tablet. As a data scientist, it is required that you understand that not all systems in the ecosystem record time in a reliable manner. Tip There is a Simple Network Time Protocol (SNTP) that you can use automatically to synchronize your system’s time with that of a remote server. The SNTP can be used to update the clock on a machine with a remote server. This keeps your machine’s time accurate, by synchronizing with servers that are known to have accurate times. I normally synchronize my computers that record data at a customer with the same NTP server the customer is using, to ensure that I get the correct date and time, as per their system. Now you can start investigating the world of international date and time settings.

##### 协调世界时(UTC)

Coordinated Universal Time is a measure of time within about 1 second of mean solar time at 0° longitude. The valid format is hh:mm:ss±hh:mm or hh:mm:ss±hh. For example, on September 11, 2017, nine a.m. in London, UK, is 09:00:00 local time and UTC 10:00:00+01:00\. In New York, US, it would be 06:00:00 local time and UTC 10:00:00-04:00\. In New Delhi, India, it would be 15:30:00 local time and UTC 10:00:00+05:30. The more precise method would be to record it as 2017-09-11T10:00:00Z. I will explain over the next part of this chapter, why the date is important when you work with multi-time zones as a data scientist. Open your IPython editor and investigate the following date format: from datetime import datetime now_date = datetime.now() print('Local Date Time:',str(now_date.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) now_utc=now_date.replace(tzinfo=timezone('UTC')) print('UTC Date Time:',str(now_utc.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) Warning I am in working in the London, Edinburgh time zone, so my results will be based on that fact. My time zone is Europe/London. from datetime import datetime now_date = datetime.now() print('Local Date Time:',str(now_date.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) now_utc=now_date.replace(tzinfo=timezone('UTC')) print('UTC Date Time:',str(now_utc.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) now_london=now_date.replace(tzinfo=timezone('Europe/London')) print('London Date Time:',str(now_london.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) So, my reference results are Local Date Time: 2017-10-03 16:14:43 () () UTC Date Time: 2017-10-03 16:14:43 (UTC) (+0000) London Date Time: 2017-10-03 16:14:43 (LMT) (-0001) Now, with that knowledge noted, let’s look at your results again. from datetime import datetime now_date = datetime.now() print('Local Date Time:',str(now_date.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) now_utc=now_date.replace(tzinfo=timezone('UTC')) print('UTC Date Time:',str(now_utc.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) My results are Local Date Time: 2017-10-03 16:14:43 () () UTC Date Time: 2017-10-03 16:14:43 (UTC) (+0000) Must companies now have different time zones? Yes! VKHCG is no different. Open your Python editor, and let’s investigate VKHCG’s time zones. from datetime import datetime from pytz import timezone, all_timezones Get the current time. now_date_local=datetime.now() Change the local time to 'Europe/London' local time by tagging it as 'Europe/London' time. now_date=now_date_local.replace(tzinfo=timezone('Europe/London')) print('Local Date Time:',str(now_date.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) Now you have a time-zone-enabled data time value that you can use to calculate other time zones. The first conversion you perform is to the UTC time zone, as follows: now_utc=now_date.astimezone(timezone('UTC')) print('UTC Date Time:',str(now_utc.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) What is the time in London, UK? now_eu_london=now_date.astimezone(timezone('Europe/London')) print('London Date Time:',str(now_eu_london.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) What is the time in Berlin, Germany? now_eu_berlin=now_date.astimezone(timezone('Europe/Berlin')) print('Berlin Date Time:',str(now_eu_berlin.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) What is the time in the Jersey Islands, UK? now_eu_jersey=now_date.astimezone(timezone('Europe/Jersey')) print('Jersey Date Time:',str(now_eu_jersey.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) What is the time in New York, US? now_us_eastern=now_date.astimezone(timezone('US/Eastern')) print('USA Easten Date Time:',str(now_us_eastern.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) What is the time in Arizona, US? now_arizona=now_date.astimezone(timezone('US/Arizona')) print('USA Arizona Date Time:',str(now_arizona.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) What is the time in Auckland , Australia? now_auckland=now_date.astimezone(timezone('Pacific/Auckland')) print('Auckland Date Time:',str(now_auckland.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) What is the time in Yukon, Canada? now_yukon=now_date.astimezone(timezone('Canada/Yukon')) print('Canada Yukon Date Time:',str(now_yukon.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) What is the time in Reykjavik, Iceland? now_reyk=now_date.astimezone(timezone('Atlantic/Reykjavik')) print('Reykjavik Date Time:',str(now_reyk.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) What is the time in Mumbai, India? now_india=now_date.astimezone(timezone('Etc/GMT-7')) print('India Date Time:',str(now_india.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) What time zones does Vermeulen use? print('Vermeulen Companies') print('Local Date Time:',str(now_date_local.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) print('HQ Edinburgh:',str(now_utc.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) print('Iceland Thor Computers:',str(now_reyk.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) print('USA Arizona Computers:',str(now_arizona.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) Here are the Vermeulen results: Vermeulen Companies Local Date Time: 2017-10-03 16:14:43 () () HQ Edinburgh: 2017-10-03 16:15:43 (UTC) (+0000) Iceland Thor Computers: 2017-10-03 16:15:43 (GMT) (+0000) USA Arizona Computers: 2017-10-03 09:15:43 (MST) (-0700) What about Krennwallner? print('Krennwallner Companies') print('Local Date Time:',str(now_date_local.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) print('HQ Berlin:',str(now_eu_berlin.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) print('HQ USA:',str(now_us_eastern.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) Following are Krennwallner’s results: Krennwallner Companies Local Date Time: 2017-10-03 16:14:43 () () HQ Berlin: 2017-10-03 18:15:43 (CEST) (+0200) HQ USA: 2017-10-03 12:15:43 (EDT) (-0400) What about Hillman ? print('Hillman Companies') print('HQ London:',str(now_london.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) print('HQ USA:',str(now_arizona.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) print('HQ Canada:',str(now_yukon.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) print('HQ Australia:',str(now_auckland.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) print('HQ India:',str(now_india.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) Hillman’s results are only for headquarters (HQs). Hillman Companies HQ London: 2017-10-03 16:14:43 (LMT) (-0001) HQ USA: 2017-10-03 09:15:43 (MST) (-0700) HQ Canada: 2017-10-03 09:15:43 (PDT) (-0700) HQ Australia: 2017-10-04 05:15:43 (NZDT) (+1300) HQ India: 2017-10-03 23:15:43 (+07) (+0700) What about Clark? print('Clark Companies') print('HQ Jersey:',str(now_eu_jersey.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) print('HQ Berlin:',str(now_eu_berlin.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) print('HQ USA:',str(now_us_eastern.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) What about Hillman? HQ Jersey: 2017-10-03 17:15:43 (BST) (+0100) HQ Berlin: 2017-10-03 18:15:43 (CEST) (+0200) HQ USA: 2017-10-03 12:15:43 (EDT) (-0400) This shows how our date and time is investigated by data scientists. As humans, we have separated the concepts of date and time for thousands of years. The date was calculated from the stars. Time was once determined by an hourglass. Later, the date was on a calendar, and time is now on a pocket watch. But in the modern international world there is another date-time data item. Mine is a smartphone—period!

#### 组合日期和时间

When using date and time in one field, ISO supports the format YYYY-MM-DDThh:mm:ss.sss for local time and YYYY-MM-DDThh:mm:ss.sssZ for Coordinated Universal Time. These date and time combinations are regularly found in international companies’ financials or logistics shipping and on my smartphone.

#### 时区

Time zones enable the real world to create a calendar of days and hours that people can use to run their lives. The date and time is normally expressed, for example, as October 20, 2017, and 9:00\. In the world of the data scientist, this is inadequate information. The data scientist requires a date, time, and time zone. YYYY-MM-DDThh:mm:ss.sssZ is the format for a special time zone for the data scientist’s data processing needs. Note This format was discussed earlier in this chapter, as an alternative to UTC. In international data , I have also seen time zones set in other formats and must discuss with you how to handle these formats in a practical manner. Open your IPython editor and investigate the following date format: from pytz import all_timezones for zone in all_timezones:     print(zone) There are 593 zones. Yes, read correctly! The issue is that companies do not even adhere to these standard zones, due to lack of understanding. For example one of my customers uses YYYY-DDThh:mm:ss.sssY for UTC-12:00, or Yankee Time Zone. Warning These zones are not valid formats of ISO! But they are generally used by source systems around the world. I will offer you the list that I have seen before, if you have to convert back to UTC format or local time. Tip I normally add a UTC time zone with YYYY-DDThh:mm:ss.sssZ to all my data science, as it ensures that you always have a common variable for date and time processing between data from multi-time zones. The point of data source recording is normally the time zone you use. Warning The time zone is dependent on your source system’s longitude and latitude or the clock settings on the hardware. The data below is generalized. I suggest you investigate the source of your data first, before picking the correct time zone. Once you have found the correct original time zone, I suggest you do the following: now_date_local=datetime.now() now_date=now_date_local.replace(tzinfo=timezone('Europe/London')) print('Local Date Time:',str(now_date_local.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) print('Local Date Time:',str(now_date.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) The result is Local Date Time: 2017-10-01 12:54:03 () () Local Date Time: 2017-10-01 12:54:03 (LMT) (-0001) 'Europe/London' applies the correct time zone from the list of 593 zones. Now convert to UTC. now_utc=now_date.astimezone(timezone('UTC')) print('UTC Date Time:',str(now_utc.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)"))) The result is UTC Date Time: 2017-10-01 12:55:03 (UTC) (+0000) I advise that you only use this with caution, as your preferred conversion. You will note that there is a more or less inaccurate meaning to YYYY-DDDThh:mmM, which converts to UTC+12:00, UTC+12:45, UTC+13:00, and UTC+14:00. UTC+14:00 is UTC+14 and stretches as far as 30° east of the 180° longitude line, creating a large fold in the International Date Line and interesting data issues for the data scientist.

##### 有趣的事实

The Republic of Kiribati introduced a change of date and time for its eastern half on January 1, 1995, from time zones -11 and -10 to +13 and +14\. Before this, the time zones UTC+13 and UTC+14 did not exist. The reason was that Kiritimati Island started the year 2000 before any other country on Earth, a feature the Kiribati government capitalized on as a prospective tourist asset. It was successful, but it also caused major issues with the credit card transactions of the tourists who took up the offer. The local times for the Phoenix and Line Islands government offices are on opposite sides of the International Date Line and can only communicate by radio or telephone on the four days of the week when both sides experience weekdays concurrently. The problems with changes always appear in the data system. The GPS and time zone validation checks on encryption engines do not support this nonstandard change. Tonga used UTC+14 for daylight saving time from 1999 to 2002 and from 2016 to the present, to enable the celebration of New Year 2000 at the same time as the Line Islands in Kiribati and returned to the changed time zone in 2016 to get the New Year tourist business back. It became active on November 5, 2017, and inactive on January 21, 2018. On December 29, 2011 (UTC-10), Samoa changed its standard time from UTC-11 to UTC+13 and its daylight-saving time from UTC-10 to UTC+14, to move the International Date Line to the other side of the country. Wonder why? Eureka, New Year’s tourist business. Alaska (formerly Russian America) used local times from GMT+11:30 to GMT+15:10 until 1867 (UTC was introduced in 1960). The Mexican state of Quintana Roo changed permanently on February 1, 2015, to UTC-5, to boost the tourism sector by creating longer, lighter evenings. Namibia changed on September 3, 2017, from UTC+1 and daylight saving of UTC+2 to permanent UTC+2. UTC+14 was used as a daylight-saving time before 1982 in parts of eastern Russia (Chukotka). UTC-12:00 is the last local time zone to start a new day . The 180° meridian was nominated as the International Date Line, because it generally passes through the sparsely populated central Pacific Ocean, and approved in 1884\. Now, we have many research probes, cruise liners, and airplanes regularly crossing this line and recording data via the source systems. The data scientist must take note of these exceptions when dealing with data from different time zones, as a minor shift in the data from one year to another may hide a seasonal trend. I was paid by a hotel group in Cancún, Quintana Roo, Mexico, to investigate if a change in time zone yielded growth in their own income and what potential additional income would accrue if they actively promoted the additional open times of their hotel bars.

#### 由开始和结束日期和时间标识的间隔

There is a standard format for supporting a date and time interval in a single field: YYYYMMDDThhmmss/YYYYMMDDThhmmss or YYYY-MM-DDThh:mm:ss/YYYY-MM-DDThh:mm:ss. So, October 20, 2017, can be recorded as 2017-10-20T000000/2017-10-20T235959 and Holiday December 2017 as 2017-12-22T120001/2017-12-27T085959, in the same data table. Note The preceding example was found in one of my source systems. Complex data issues are common in data science. Spot them quickly, and deal with them quickly! As a general note of caution, I advise that when you discover these complex structures, convert them to more simple structures as soon as you can. I suggest that you add an assess step, to split these into four fields: Start Date Time, End Date Time, Original Date Time, and Business Term. Using the previously mentioned format, October 20, 2017, recorded as 2017-10-20T000000/2017-10-20T235959, becomes the following:

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 开始日期时间 | 结束日期时间 | 原始日期时间 | 营业期 |
| --- | --- | --- | --- |
| 2017-10-2000000 | 2017-10-20T235959 | 2017-10-20t 000000/2017-10-20t 235959 | 2017 年 10 月 20 |

Holiday December 2017 can be recorded as 2017-12-22T120001/2017-12-27T085959 in the same data table , as follows:

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 开始日期时间 | 结束日期时间 | 原始日期时间 | 营业期 |
| --- | --- | --- | --- |
| 2017-12-22T120001 | 2017-12-22T235959 | 2017-12-22t 120001/2017-12-27t 085959 | 2017 年 12 月假期 |
| 2017-12-23T000000 | 2017-12-23T235959 | 2017-12-22t 120001/2017-12-27t 085959 | 2017 年 12 月假期 |
| 2017-12-24T000000 | 2017-12-24T235959 | 2017-12-22t 120001/2017-12-27t 085959 | 2017 年 12 月假期 |
| 2017-12-25T000000 | 2017-12-25T235959 | 2017-12-22t 120001/2017-12-27t 085959 | 2017 年 12 月假期 |
| 2017-12-26T000000 | 2017-12-26T235959 | 2017-12-22t 120001/2017-12-27t 085959 | 2017 年 12 月假期 |
| 2017-12-27T000000 | 2017-12-27T085959 | 2017-12-22t 120001/2017-12-27t 085959 | 2017 年 12 月假期 |

Now you have same day time periods that are easier to convert into date time formats.

#### 特殊日期格式

The following are special date formats.

##### 星期几

ISO 8601-2004 sets the following standard:

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"></colgroup> 
| 数字 | 名字 |
| --- | --- |
| one | 星期一 |
| Two | 星期二 |
| three | 星期三 |
| four | 星期四 |
| five | 星期五 |
| six | 星期六 |
| seven | 在星期日 |

Open your IPython editor and investigate the following date format: now_date_local=datetime.now() now_date=now_date_local.replace(tzinfo=timezone('Europe/London')) print('Weekday:',str(now_date.strftime("%w"))) The result is (It is Sunday) Weekday: 0 Caution I have seen variations! Sunday = 0, Sunday = 1, and Monday = 0\. Make sure you have used the correct format before joining data.

##### 星期日期

The use of week dates in many source systems causes some complications when working with dates. Week dates are denoted by the use of a capital W and W with D in the format. The following are all valid formats: YYYY-Www or YYYYWww or YYYY-Www-D or YYYYWwwD. For examples, October 22, 2017, is 2017-W42, if you use ISO 8601(European), but 2017-W43, if you use ISO 8601(US). Warning ISO 8601(European) is an official format. The standard is that the first week of a year is a week that includes the first Thursday of the year or that contains January 4th. By this method, January 1st, 2nd, and 3rd can be included in the last week of the previous year, or December 29th, 30th, and 31st can be included in the first week of next year. ISO 8601(US) is widely used, and according to this standard, the first week begins on January1st. The next week begins on the next Sunday. Other options are that the first week of the year begins on January 1st, and the next week begins on the following Monday, and the first week of the year begins on January 1st, and the next week begins on January 8th. now_date_local=datetime.now() now_date=now_date_local.replace(tzinfo=timezone('Europe/London')) print('Week of Year:',str(now_date.strftime("%YW%WD%w"))) The result is Week of Year: 2017W39D0

##### 一年中的某一天

The day of the year is calculated simply by counting the amount of days from January 1st of the given year. For example: October 20, 2017 is 2017/293. Caution Change this to the ISO standard as soon as possible! I found this format stored as 2017293 in a birth date field. The reason was to hide a staff member’s age. This successfully broke the pension planner’s projection, which read the value as the US date for March 29, 2017, hence, 2017/29/3\. This staff member would potentially have received two birthday cards and cake from his management in 2017\. I fixed the problem in September 2017. So, let’s look at this day of the year as it was intended to be used by the format. The day of the year is useful to determine seasonality in the data when comparing data between years. Check if the same thing happens on the same day every year. Not really that simple, but close! Here is the format for the day of year: now_date_local=datetime.now() now_date=now_date_local.replace(tzinfo=timezone('Europe/London')) print('Day of year:',str(now_date.strftime("%Y/%j"))) The result is Day of year: 2017/274 Well done. You have now completed the basic introduction to date and time in the Process step. In the practical part of this chapter, I will show how important the YYYY-MM-DDThh:mm:ss.sssZ value for data and time is to the data science. This completes the basic information on date and time. Next, you will start to build the time hub, links, and satellites. Open your Python editor and create a file named Process_Time.py. Save it into directory .. \VKHCG\01-Vermeulen\03-Process. The first part consists of standard imports and environment setups. ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os from datetime import datetime from datetime import timedelta from pytz import timezone, all_timezones import pandas as pd import sqlite3 as sq from pandas.io import sql import uuid pd.options.mode.chained_assignment = None ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ Company='03-Hillman' InputDir='00-RawData' InputFileName='VehicleData.csv' ################################################################ sDataBaseDir=Base + '/' + Company + '/03-Process/SQLite' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataBaseDir + '/Hillman.db' conn1 = sq.connect(sDatabaseName) ################################################################ sDataVaultDir=Base + '/88-DV' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataVaultDir + '/datavault.db' conn2 = sq.connect(sDatabaseName) You will set up a time hub for a period of ten years before January 1, 2018\. If you want to experiment with different periods, simply change the parameters. ################################################################ base = datetime(2018,1,1,0,0,0) numUnits=10*365*24 ################################################################ date_list = [base - timedelta(hours=x) for x in range(0, numUnits)] t=0 for i in date_list:     now_utc=i.replace(tzinfo=timezone('UTC'))     sDateTime=now_utc.strftime("%Y-%m-%d %H:%M:%S")       sDateTimeKey=sDateTime.replace(' ','-').replace(':','-')     t+=1     IDNumber=str(uuid.uuid4())     TimeLine=[('ZoneBaseKey', ['UTC']),              ('IDNumber', [IDNumber]),              ('nDateTimeValue', [now_utc]),              ('DateTimeValue', [sDateTime]),              ('DateTimeKey', [sDateTimeKey])]     if t==1:        TimeFrame = pd.DataFrame.from_items(TimeLine)     else:         TimeRow = pd.DataFrame.from_items(TimeLine)         TimeFrame = TimeFrame.append(TimeRow) ################################################################ TimeHub=TimeFrame[['IDNumber','ZoneBaseKey','DateTimeKey','DateTimeValue']] TimeHubIndex=TimeHub.set_index(['IDNumber'],inplace=False) ################################################################ TimeFrame.set_index(['IDNumber'],inplace=True) ################################################################ sTable = 'Process-Time' print('Storing :',sDatabaseName,' Table:',sTable) TimeHubIndex.to_sql(sTable, conn1, if_exists="replace") ################################################################ sTable = 'Hub-Time' print('Storing :',sDatabaseName,' Table:',sTable) TimeHubIndex.to_sql(sTable, conn2, if_exists="replace") ################################################################ You have successfully built the hub for time in the data vault. Now, we will build satellites for each of the time zones. So, let’s create a loop to perform these tasks: active_timezones=all_timezones z=0 for zone in active_timezones:         t=0     for j in range(TimeFrame.shape[0]):         now_date=TimeFrame['nDateTimeValue'][j]         DateTimeKey=TimeFrame['DateTimeKey'][j]         now_utc=now_date.replace(tzinfo=timezone('UTC'))         sDateTime=now_utc.strftime("%Y-%m-%d %H:%M:%S")         now_zone = now_utc.astimezone(timezone(zone))         sZoneDateTime=now_zone.strftime("%Y-%m-%d %H:%M:%S")           t+=1         z+=1         IDZoneNumber=str(uuid.uuid4())         TimeZoneLine=[('ZoneBaseKey', ['UTC']),                       ('IDZoneNumber', [IDZoneNumber]),                       ('DateTimeKey', [DateTimeKey]),                       ('UTCDateTimeValue', [sDateTime]),                       ('Zone', [zone]),                       ('DateTimeValue', [sZoneDateTime])]         if t==1:            TimeZoneFrame = pd.DataFrame.from_items(TimeZoneLine)         else:             TimeZoneRow = pd.DataFrame.from_items(TimeZoneLine)             TimeZoneFrame = TimeZoneFrame.append(TimeZoneRow)     TimeZoneFrameIndex=TimeZoneFrame.set_index(['IDZoneNumber'],inplace=False)     sZone=zone.replace('/','-').replace(' ','')     #############################################################       sTable = 'Process-Time-'+sZone     print('Storing :',sDatabaseName,' Table:',sTable)     TimeZoneFrameIndex.to_sql(sTable, conn1, if_exists="replace") #################################################################     #############################################################       sTable = 'Satellite-Time-'+sZone     print('Storing :',sDatabaseName,' Table:',sTable)     TimeZoneFrameIndex.to_sql(sTable, conn2, if_exists="replace") ################################################################# You will note that your databases are growing at an alarming rate. I suggest that we introduce a minor database maintenance step that compacts the database to take up less space on the disk. The required command is vacuum. It performs an action similar to that of a vacuum-packing system, by compressing the data into the smallest possible disk footprint. print('################') print('Vacuum Databases') sSQL="VACUUM;" sql.execute(sSQL,conn1) sql.execute(sSQL,conn2) print('################') ################################################################# print('### Done!! ############################################') ################################################################# Congratulations! You have built your first hub and satellites for time in the data vault. The data vault has been built in directory ..\ VKHCG\88-DV\datavault.db. You can access it with your SQLite tools, if you want to investigate what you achieved. In Figure [9-8](#Fig8), you should see a structure that contains these data items.![A435693_1_En_9_Fig8_HTML.jpg](Images/A435693_1_En_9_Fig8_HTML.jpg) Figure 9-8Structure with three data items.

### 人

This structure records the person(s) involved within the data sources. A person is determined as a set of data items that, combined, describes a single “real” person in the world. Remember Angus MacGyver, the baker down the street, is a person. Angus, my dog, is an object. Angus MacGyver, a fictional character on TV played by Richard Dean Anderson, is an object. Richard Dean Anderson, the actor, is a person. You would be surprised how many fictional characters data scientists find in databases around the world. Warning Beware the use of unwanted superfluous data entries in your data science! Twice in the last year, a public safety system dispatched numerous emergency vehicles and staff because a data scientist assumed it would be tolerable to use “Angus MacGyver” to name a terror suspect who plants bombs in high-rise buildings at random. The data scientist defended his actions by reason that he tests his data science code by hard-coding it into his data science algorithms. Not the hallmark of a good data scientist! Tip Remove self-generated data errors from production systems immediately. They represent bad science! And, yes, they can damage property and hurt people.

#### 黄金名义

A golden nominal record is a single person’s record, with distinctive references for use by all systems. This gives the system a single view of the person . I use first name, other names, last name, and birth date as my golden nominal. The data we have in the assess directory requires a birth date to become a golden nominal. I will explain how to generate a golden nominal using our sample data set. Open your Python editor and create a file called Process-People.py in the ..\VKHCG\04-Clark\03-Process directory. ################################################################ import sys import os import sqlite3 as sq import pandas as pd from datetime import datetime, timedelta from pytz import timezone, all_timezones from random import randint ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ Company='04-Clark' sInputFileName='02-Assess/01-EDS/02-Python/Assess_People.csv' ################################################################ sDataBaseDir=Base + '/' + Company + '/03-Process/SQLite' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataBaseDir + '/clark.db' conn = sq.connect(sDatabaseName) ################################################################ ### Import Female Data ################################################################ sFileName=Base + '/' + Company + '/' + sInputFileName print('################################') print('Loading :',sFileName) print('################################') print(sFileName) Now load the previous assess data into memory to process. RawData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") RawData.drop_duplicates(subset=None, keep="first", inplace=True) Let’s assume nobody was born before January 1, 1900. start_date = datetime(1900,1,1,0,0,0) start_date_utc=start_date.replace(tzinfo=timezone('UTC')) Let’s assume, too, that the people who were born spread by random allocation over a period of 100 years. HoursBirth=100*365*24 RawData['BirthDateUTC']=RawData.apply(lambda row:                 (start_date_utc + timedelta(hours=randint(0, HoursBirth)))                 ,axis=1) zonemax=len(all_timezones)-1 RawData['TimeZone']=RawData.apply(lambda row:                 (all_timezones[randint(0, zonemax)])                 ,axis=1) RawData['BirthDateISO']=RawData.apply(lambda row:                 row["BirthDateUTC"].astimezone(timezone(row['TimeZone']))                 ,axis=1) RawData['BirthDateKey']=RawData.apply(lambda row:                 row["BirthDateUTC"].strftime("%Y-%m-%d %H:%M:%S")                 ,axis=1) RawData['BirthDate']=RawData.apply(lambda row:                 row["BirthDateISO"].strftime("%Y-%m-%d %H:%M:%S")                 ,axis=1) ################################################################ Data=RawData.copy() You have to drop to two ISO complex date time structures, as the code does not translate into SQLite’s data types. Data.drop('BirthDateUTC', axis=1,inplace=True)   Data.drop('BirthDateISO', axis=1,inplace=True) print('################################') You now save your new golden nominal to SQLite. ################################################################# print('################')   sTable='Process_Person' print('Storing :',sDatabaseName,' Table:',sTable) Data.to_sql(sTable, conn, if_exists="replace") print('################')   Now save your new golden nominal to a CSV file. ################################################################ sFileDir=Base + '/' + Company + '/03-Process/01-EDS/02-Python' if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ sOutputFileName = sTable + '.csv' sFileName=sFileDir + '/' + sOutputFileName print('################################') print('Storing :', sFileName) print('################################') RawData.to_csv(sFileName, index = False) print('################################') ################################################################# print('### Done!! ############################################') ################################################################# A real-world example is Ada, countess of Lovelace, who was born Augusta Ada King-Noel. She was an English mathematician and writer, known principally for her work on Charles Babbage’s proposed mechanical general-purpose computer, the Analytical Engine. She was born on December 10, 1815.

*   名字=奥古斯塔
*   其他名字=艾达·金-诺埃尔
*   姓氏=爱拉斯
*   出生日期= 1815-10-10 00:00:00

You use an official name , as on her birth record, not her common name. You could add a satellite called CommonKnownAs and put “Ada Lovelace” in that table. Note The next section covers some interesting concepts that I discovered while working all over the world. It gives you some insight into the diversity of requirements data scientists can be presented with in executing their work.

#### 一个人名字的含义

We all have names, and our names supply a larger grouping, or even hidden meanings, to data scientists. If people share the same last name, there is a probability they may be related. I will discuss this again later. Around the world, there are also many other relationships connected to people’s names. Let’s look at a few to which I was exposed over the years that I processed data.

#### [名、母名和父名](https://www.w3.org/International/questions/qa-personal-names%23patronymic)的含义

There can be a specific relationship or meaning embedded in people’s names. The Icelandic name Björk Guðmundsdóttir expresses a complex relationship between the individual and her family members. Björk is the person’s name. Guðmundsdóttir is a relationship: daughter (-dóttir) of Guðmundur. This makes the person female. Björk’s father is Guðmundur Gunnarsson. This is the most common practice. Björk could also have the following matronymic name: Hildardottir, or even matro- and patronymic names: Hildar- og Guðmundsdóttir, because Björk’s mother is Hildur R’una Hauksdottir. Gunnarsson denotes the relationship “son of Gunnar.” Therefore, Ólafur Guðmundsson is Björk’s brother. Ólafur’s son is Jón Ólafurson. I’ve just contradicted my own earlier assumption. Different last names can equally denote members of the same family! Caution Do not think that algorithms and assumptions that work in your country work in all other countries with similar success. The Malay name Isa bin Osman also indicates a hidden relationship. Isa is the given name. bin Osman denotes the relationship “son of Osman.” Isa’s sister would be Mira binti Osman , as binti means “daughter of.” Encik Isa would mean “Mr. Isa bin Osman,” but the person writing knows Isa personally. So, do not address Isa as Encik Isa, if you do not know him personally. Here are a few more naming conventions to take note of.

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 语言 | 后缀“最后” | 后缀“女儿” |
| --- | --- | --- |
| 古挪威语。参见 OＬＤ IＣＥＬＡＮＤIＣ | -儿子 | -多蒂尔 |
| 丹麦的 | -你呢 | -女儿 |
| 法罗群岛的 | -儿子 | -多蒂尔 |
| 芬兰人的 | -男孩 | -女儿 |
| 冰岛的 | -森松 | -多蒂尔 |
| 挪威的 | -然后是儿子 | 女儿/女儿 |
| 瑞典的 | -儿子 | -多特 |

#### [姓名各部分的不同顺序](https://www.w3.org/International/questions/qa-personal-names%23partorder)

A full name is not always in the order first name, middle name, and last name. In the Chinese name 毛泽东 (Mao Zedong), for example, the family name (surname) is 毛 (Mao). The middle character, 泽 (Ze), is a generational name and is common to all siblings of the same family. According to the pīnyīn system governing the transliteration of Chinese into English, the last two characters, or elements of a Chinese name are generally combined as one word, e.g., Zedong, rather than Ze Dong. Mao’s brothers and sister are 毛泽民 (Mao Zemin), 毛泽覃 (Mao Zetan), and 毛泽紅 (Mao Zehong). Note “Ze Dong” is rarely, if ever, used in transliteration. Among people with whom he was not on familiar terms, Mao may have been referred to as 毛泽东先生 (Mao Zedong xiānshēng) or 毛先生 (Mao xiānshēng) (xiānshēng being the equivalent of “Mr.”). If you are on familiar terms with someone called 毛泽东, you would normally refer to them using 泽东 (Zedong), not just 东 (Dong). In Japan, Korea, and Hungary, the order of names is family name, followed by given name(s). Note Chinese people who deal with Westerners will often adopt an additional given name that is easier for Westerners to use. Yao Ming (family name Yao, given name Ming) also renders his as Fred Yao Ming or Fred Ming Yao, for Westerners. With Spanish-speaking people, it is common to have two family names. For example, María-Jose Carreño Quiñones refers to the daughter of Antonio Carreño Rodríguez and María Quiñones Marqués. You would address her as Señorita Carreño, not Señorita Quiñones. Brazilians have similar customs and may have three or four family names, chaining the names of ancestors, such as, for example, José Eduardo Santos Tavares Melo Silva. Typically, two Spanish family names have the order paternal+maternal, whereas Portuguese names in Brazil have the order maternal+paternal. However, this order may change, just to confuse the data scientist. Note Some names consist of particles, such as de or e, between family names, for example, Carreño de Quiñones or Tavares e Silva. Russians use patronymics as their middle name but also use family names, in the order given name+patronymic+family name. The endings of the patronymic and family names indicate whether the person is male or female. For example, the wife of Бopиc Hикoлaeвич Eльцин (Boris Nikolayevich Yeltsin) is Haинa Иocифoвнa Eльцинa (Naina Iosifovna Yeltsina). Note According to Russian naming conventions, a male’s name ends in a consonant, while a female’s name (even the patronymic) ends in a. This russian naming convention applies to all males and females people, not only married individuals.

#### [姓名的继承](https://www.w3.org/International/questions/qa-personal-names%23inheritance)

It would be wrong to assume that members of the same immediate family share the same last name. There is a growing trend in the West for married women to keep their maiden names after marriage. There are, however, other cultures, such as the Chinese, in which this is the standard practice. In some countries, a married woman may or may not take the name of her husband. If, for example, the Malay girl Zaiton married Isa, mentioned previously, she might remain Mrs. Zaiton, or she might choose to become Zaiton Isa, in which case you could refer to her as Mrs. Isa. These data characteristics can make grouping data complex for the data scientist.

#### 其他关系

The Dutch culture has its own naming conventions. The rules governing it are the following:

*   第一个出生的儿子以祖父的名字命名。
*   头胎女儿以外祖母的名字命名。
*   次子是以外祖父的名字命名的。
*   第二个女儿以祖母的名字命名。
*   随后的孩子往往以叔叔阿姨的名字命名。

However, if a son has died before his next brother is born, the younger brother is usually given the same name as his deceased brother. The same goes for a daughter. When a father dies before the birth of a son, the son is named after him. When a mother dies upon the birth of a daughter, the daughter is named after the mother. I found out the hard way, from a data set , that there is this structure to Dutch names, when we flagged a person deceased after finding a death certificate. It is not common in non-dutch families to re-use a dead person’s name in the same generation and our data science always marked it as an error. So we had to remove this error rule when we moved system from america into Europe to resolve these cultural nonconformity. Dutch names also have a historical and cultural relationship to places of birth or family occupations. For example, Vermeulen is a Belgian variant of the Dutch surname ver Meulen, which indicates that my family was “from the far mills” or were “traveling millers” by trade. Hence the windmill on my family crest. In line with Dutch tradition, marriage used to require a woman to precede her [maiden name](https://en.wikipedia.org/wiki/Maiden_name%23Maiden%20name) with her husband’s name, linked by a hyphen. An Anna Pietersen who married a Jan Jansen became Anna Jansen-Pietersen. The current law in the Netherlands gives people more freedom. Upon marriage within the Netherlands, both partners default to keeping their own surnames, but both are given the choice of legally using their partner’s surname or a combination of the two. In simple terms, Anna Pietersen, Jan Pietersen, Anna Jansen, Jan Jansen, Anna Jansen-Pietersen, and Jan Jansen-Pietersen could be the names of the same two people over a period of time. These variances and additional meanings to names can enable the data scientist to train his or her algorithms to find relationships between people, by simply using their names. I have shared with you some of the interesting discoveries I have made during my years of working in many countries. For the purposes of this book, I will keep the person names simple and follow the Western convention of first name and last name. Now, we load the person into the data vault. Open your Python editor and create a new file named Process-People.py in directory ..\VKHCG\04-Clark\03-Process. Now copy the following Python code into the file: ################################################################ import sys import os import sqlite3 as sq import pandas as pd from pandas.io import sql from datetime import datetime, timedelta from pytz import timezone, all_timezones from random import randint ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ Company='04-Clark' sInputFileName='02-Assess/01-EDS/02-Python/Assess_People.csv' ################################################################ sDataBaseDir=Base + '/' + Company + '/03-Process/SQLite' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataBaseDir + '/clark.db' conn1 = sq.connect(sDatabaseName) ################################################################ sDataVaultDir=Base + '/88-DV' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataVaultDir + '/datavault.db' conn2 = sq.connect(sDatabaseName) ################################################################ ### Import Female Data ################################################################ sFileName=Base + '/' + Company + '/' + sInputFileName print('################################') print('Loading :',sFileName) print('################################') print(sFileName) RawData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") RawData.drop_duplicates(subset=None, keep="first", inplace=True) start_date = datetime(1900,1,1,0,0,0) start_date_utc=start_date.replace(tzinfo=timezone('UTC')) HoursBirth=100*365*24 RawData['BirthDateUTC']=RawData.apply(lambda row:                 (start_date_utc + timedelta(hours=randint(0, HoursBirth)))                 ,axis=1) zonemax=len(all_timezones)-1 RawData['TimeZone']=RawData.apply(lambda row:                 (all_timezones[randint(0, zonemax)])                 ,axis=1) RawData['BirthDateISO']=RawData.apply(lambda row:                 row["BirthDateUTC"].astimezone(timezone(row['TimeZone']))                 ,axis=1) RawData['BirthDateKey']=RawData.apply(lambda row:                 row["BirthDateUTC"].strftime("%Y-%m-%d %H:%M:%S")                 ,axis=1) RawData['BirthDate']=RawData.apply(lambda row:                 row["BirthDateISO"].strftime("%Y-%m-%d %H:%M:%S")                 ,axis=1) ################################################################ Data=RawData.copy() Data.drop('BirthDateUTC', axis=1,inplace=True)   Data.drop('BirthDateISO', axis=1,inplace=True) indexed_data = Data.set_index(['PersonID']) print('################################') ################################################################# print('################')   sTable='Process_Person' print('Storing :',sDatabaseName,' Table:',sTable) indexed_data.to_sql(sTable, conn1, if_exists="replace") print('################')   ################################################################ PersonHubRaw=Data[['PersonID','FirstName','SecondName','LastName','BirthDateKey']] PersonHubRaw['PersonHubID']=RawData.apply(lambda row:                 str(uuid.uuid4())                 ,axis=1) PersonHub=PersonHubRaw.drop_duplicates(subset=None, \                                                  keep='first',\                                                  inplace=False) indexed_PersonHub = PersonHub.set_index(['PersonHubID']) sTable = 'Hub-Person' print('Storing :',sDatabaseName,' Table:',sTable) indexed_PersonHub.to_sql(sTable, conn2, if_exists="replace")   ################################################################ PersonSatelliteGenderRaw=Data[['PersonID','FirstName','SecondName','LastName'\                            ,'BirthDateKey','Gender']] PersonSatelliteGenderRaw['PersonSatelliteID']=RawData.apply(lambda row:                 str(uuid.uuid4())                 ,axis=1) PersonSatelliteGender=PersonSatelliteGenderRaw.drop_duplicates(subset=None, \                                                            keep='first', \                                                            inplace=False) indexed_PersonSatelliteGender = PersonSatelliteGender.set_index(['PersonSatelliteID']) sTable = 'Satellite-Person-Gender' print('Storing :',sDatabaseName,' Table:',sTable) indexed_PersonSatelliteGender.to_sql(sTable, conn2, if_exists="replace") ################################################################ PersonSatelliteBirthdayRaw=Data[['PersonID','FirstName','SecondName','LastName',\                              'BirthDateKey','TimeZone','BirthDate']] PersonSatelliteBirthdayRaw['PersonSatelliteID']=RawData.apply(lambda row:                 str(uuid.uuid4())                 ,axis=1) PersonSatelliteBirthday=PersonSatelliteBirthdayRaw.drop_duplicates(subset=None, \                                                             keep='first',\                                                             inplace=False) indexed_PersonSatelliteBirthday = PersonSatelliteBirthday.set_index(['PersonSatelliteID']) sTable = 'Satellite-Person-Names' print('Storing :',sDatabaseName,' Table:',sTable) indexed_PersonSatelliteBirthday.to_sql(sTable, conn2, if_exists="replace") ################################################################ sFileDir=Base + '/' + Company + '/03-Process/01-EDS/02-Python' if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ sOutputFileName = sTable + '.csv' sFileName=sFileDir + '/' + sOutputFileName print('################################') print('Storing :', sFileName) print('################################') RawData.to_csv(sFileName, index = False) print('################################') ################################################################# print('################') print('Vacuum Databases') sSQL="VACUUM;" sql.execute(sSQL,conn1) sql.execute(sSQL,conn2) print('################') ################################################################# print('### Done!! ############################################') #################################################################

### 目标

This structure records the other objects and nonhuman entities that are involved in the data sources. The object must have three basic fields: Object Type, Object Group, and Object Code to be valid. I also add a short and long description for information purposes. The data science allows for the data vault’s object hub to be a simple combination of Object Type, Object Group, and Object Code, to formulate a full description of any nonhuman entities in the real world. I must note that as you start working with objects, you will discover that most Object Types have standards to support the specific object’s processing. Let’s look at a few common objects I’ve had to handle by data science.

#### 动物

Animals belong to many different groups, starting with the Animal Kingdom.

##### 王国

All living organisms are first placed into different kingdoms . There are five different kingdoms that classify life on Earth: Animals, Plants, Fungi, Bacteria, and Protists (single-celled organisms).

##### 门

The Animal Kingdom is divided into 40 smaller groups, known as phyla . Accordingly, animals are grouped by their main features. Animals usually fall into one of five different phyla: Cnidaria (invertebrates), Chordata (vertebrates), Arthropods, Molluscs, and Echinoderms.

##### 班级

A phylum is divided into even smaller groups, known as classes . The Chordata (vertebrates) phylum splits into Mammalia (mammals), Actinopterygii (bony fish), Chondrichthyes (cartilaginous fish), Aves (birds), Amphibia (amphibians), and Reptilia (reptiles).

##### 命令

Each class is divided into even smaller groups, known as orders. The class Mammalia (mammals) splits into different groups, including Carnivora, Primate, Artiodactyla, and Rodentia.

##### 家庭的

In every order, there are different families of animals, which all have very similar features. The Carnivora order breaks into families that include Felidae (cats), Canidae (dogs), Ursidae (bears), and Mustelidae (weasels).

##### 种

Every animal family is then divided into groups known as genera . Each genus contains animals that have very similar features and are closely related. For example, the Felidae (cat) family contains the genera Felis (small cats and domestic cats), Panthera (tigers, leopards, jaguars, and lions), and Puma (panthers and cougars).

##### 种类

Each individual species within a genus is named after its individual features and characteristics. The names of animals are in Latin, as are genera, so that they can be understood worldwide, and consist of two words. The first part in the name of an animal will be the genus, and the second part indicates the specific species. Let’s classify Tigger, my toy tiger.

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"></colgroup> 
| 王国 | 动物(动物) |
| --- | --- |
| 门 | 脊索动物(脊椎动物) |
| 班级 | 哺乳动物 |
| 命令 | 食肉动物 |
| 家庭的 | 猫科（猫） |
| 种 | 潘瑟拉 |
| 种类 | 底格里斯(老虎) |

Now let’s classify my dog, Angus .![A435693_1_En_9_Figa_HTML.jpg](Images/A435693_1_En_9_Figa_HTML.jpg)

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"></colgroup> 
| 王国 | 动物界 |
| 班级 | 哺乳动物 |
| 命令 | 食肉目 |
| 家庭的 | 犬科 |
| 种 | [脊椎]犬属 |
| 硬币 | 家族狼 |

He is classified as Canis Lupus familiaris. You must look at an object as revealed by recently discovered knowledge. Now, I will guide you through a Python code session, to convert the flat file into a graph with knowledge. The only information you have are in file Animals.csv in directory .. VKHCG\03-Hillman\00-RawData. The format is

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 项目级别 | 父 ID | ItemID | ItemName |
| --- | --- | --- | --- |
| Zero | Zero | Fifty | 细菌 |
| Zero | Zero | Two hundred and two thousand four hundred and twenty-two | 植物界 |
| one | Fifty | Nine hundred and fifty-six thousand and ninety-six | negibactoria |
| one | Fifty | Nine hundred and fifty-six thousand and ninety-seven | posi 细菌 |

The field has the following meanings:

*   ItemLevel 是特定项目离分类中的顶层节点有多远。
*   ParentID 是所列项目的父项的 ItemID。
*   ItemID 是该项目的唯一标识符。
*   ItemName 是项目的全名。

The data fits together as a considerable tree of classifications. You must create a graph that gives you the following: Bacteria-> Negibacteria and Bacteria-> Posibacteria Following is the code to transform it. You will perform a few sections of data preparation, data storage for the retrieve, Assess supersteps, and then we will complete the Process step into the data vault. You start with the standard framework, so please transfer the code to your Python editor. First, let’s set up the data: ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import pandas as pd import networkx as nx import sqlite3 as sq import numpy as np ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ ReaderCode='SuperDataScientist' Please replace the 'Practical Data Scientist' in the next line with your name. ReaderName='Practical Data Scientist' You now set up the locations of all the deliverables of the code. ################################################################ Company='03-Hillman' InputRawFileName='Animals.csv' EDSRetrieveDir='01-Retrieve/01-EDS' InputRetrieveDir=EDSRetrieveDir + '/02-Python' InputRetrieveFileName='Retrieve_All_Animals.csv' EDSAssessDir='02-Assess/01-EDS' InputAssessDir=EDSAssessDir + '/02-Python' InputAssessFileName='Assess_All_Animals.csv' InputAssessGraphName='Assess_All_Animals.gml' You now create the locations of all the deliverables of the code. ################################################################ sFileRetrieveDir=Base + '/' + Company + '/' + InputRetrieveDir if not os.path.exists(sFileRetrieveDir):     os.makedirs(sFileRetrieveDir) ################################################################ sFileAssessDir=Base + '/' + Company + '/' + InputAssessDir if not os.path.exists(sFileAssessDir):     os.makedirs(sFileAssessDir) ################################################################ sDataBaseDir=Base + '/' + Company + '/03-Process/SQLite' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataBaseDir + '/Hillman.db' conn = sq.connect(sDatabaseName) ################################################################ # Raw to Retrieve ################################################################ You upload the CSV file with the flat structure. sFileName=Base + '/' + Company + '/00-RawData/' + InputRawFileName print('###########') print('Loading :',sFileName) AnimalRaw=pd.read_csv(sFileName,header=0,low_memory=False, encoding = "ISO-8859-1") AnimalRetrieve=AnimalRaw.copy() print(AnimalRetrieve.shape) ################################################################ You store the Retrieve steps data now. sFileName=sFileRetrieveDir + '/' + InputRetrieveFileName print('###########') print('Storing Retrieve :',sFileName) AnimalRetrieve.to_csv(sFileName, index = False) You store the Assess steps data now. ################################################################ # Retrieve to Assess ################################################################ AnimalGood1 = AnimalRetrieve.fillna('0', inplace=False) AnimalGood2=AnimalGood1[AnimalGood1.ItemName!=0] AnimalGood2[['ItemID','ParentID']]=AnimalGood2[['ItemID','ParentID']].astype(np.int32) AnimalAssess=AnimalGood2 print(AnimalAssess.shape) ################################################################ sFileName=sFileAssessDir + '/' + InputAssessFileName print('###########') print('Storing Assess :',sFileName) AnimalAssess.to_csv(sFileName, index = False) ################################################################ print('################')   sTable='All_Animals' print('Storing :',sDatabaseName,' Table:',sTable) AnimalAssess.to_sql(sTable, conn, if_exists="replace") print('################')   You start with the Process steps, to process the flat data into a graph. You can now extract the nodes, as follows: ################################################################ print('################')   sTable='All_Animals' print('Loading Nodes :',sDatabaseName,' Table:',sTable) sSQL=" SELECT DISTINCT" sSQL=sSQL+ " CAST(ItemName AS VARCHAR(200)) AS NodeName," sSQL=sSQL+ " CAST(ItemLevel AS INT) AS NodeLevel" sSQL=sSQL+ " FROM" sSQL=sSQL+ " " + sTable + ";" AnimalNodeData=pd.read_sql_query(sSQL, conn) print(AnimalNodeData.shape) You have now successfully extracted the nodes. Well done. You can now extract the edges. You will start with the Process step, to convert the data into an appropriate graph structure. ################################################################ print('################')   sTable='All_Animals' print('Loading Edges :',sDatabaseName,' Table:',sTable) sSQL=" SELECT DISTINCT" sSQL=sSQL+ " CAST(A1.ItemName AS VARCHAR(200)) AS Node1," sSQL=sSQL+ " CAST(A2.ItemName AS VARCHAR(200)) AS Node2" sSQL=sSQL+ " FROM" sSQL=sSQL+ " " + sTable + " AS A1" sSQL=sSQL+ " JOIN" sSQL=sSQL+ " " + sTable + " AS A2" sSQL=sSQL+ " ON" sSQL=sSQL+ " A1.ItemID=A2.ParentID;" AnimalEdgeData=pd.read_sql_query(sSQL, conn) print(AnimalEdgeData.shape) You have now extracted the edges. So, let’s build a graph . ################################################################ G=nx.Graph() t=0 G.add_node('world', NodeName="World") ################################################################ You add the nodes first. GraphData=AnimalNodeData print(GraphData) ################################################################ m=GraphData.shape[0] for i in range(m):     t+=1     sNode0Name=str(GraphData['NodeName'][i]).strip()     print('Node :',t,' of ',m,sNode0Name)     sNode0=sNode0Name.replace(' ', '-').lower()     G.add_node(sNode0, NodeName=sNode0Name)     if GraphData['NodeLevel'][i] == 0:         G.add_edge(sNode0,'world')          ################################################################ You add the edges second. GraphData=AnimalEdgeData t=0 ################################################################ m=GraphData.shape[0] for i in range(m):     t+=1     sNode0Name=str(GraphData['Node1'][i]).strip()     sNode1Name=str(GraphData['Node2'][i]).strip()     print('Link :',t,' of ',m,sNode0Name,' to ',sNode1Name)     sNode0=sNode0Name.replace(' ', '-').lower()     sNode1=sNode1Name.replace(' ', '-').lower()     G.add_edge(sNode0,sNode1) You have nodes and edges . Now add yourself as a node. ################################################################ RCode=ReaderCode.replace(' ', '-').lower() G.add_node(RCode,NodeName=ReaderName)   G.add_edge('homo-sapiens',RCode) ################################################################ Now store the graph. sFileName= sFileAssessDir + '/' + InputAssessGraphName print('################################') print('Storing :', sFileName) print('################################') nx.write_gml(G,sFileName) sFileName=sFileName +'.gz' nx.write_gml(G,sFileName) Let’s consider some of the knowledge we already can extract. Look at how the following objects relate to each other:

*   你自己？
*   作者？
*   安格斯，我的狗？
*   跳跳虎，我的玩具老虎？
*   克里斯·希尔曼，我的技术顾问和朋友？

Enter the following Python file into your editor, and you will discover new knowledge. ################################################################ # Find Lists of Objects ################################################################ TargetNodes=[ReaderCode,'Andre Vermeulen','Angus', 'Tigger', 'Chris Hillman'] for j in range(len(TargetNodes))  :     TargetNodes[j]=TargetNodes[j].replace(' ', '-').lower() ################################################################     for TargetNode in TargetNodes:     if TargetNode in nx.nodes(G):         print('=============================')         print('Path:','World',' to ',G.node[TargetNode]['NodeName'])         print('=============================')         for nodecode in nx.shortest_path(G,source='world',target=TargetNode):             print(G.node[nodecode]['NodeName'])                 print('=============================')     else:         print('=============================')         print('No data - ', TargetNode, ' is missing!')         print('=============================')     If you are classed as a Homo sapiens, well done! Now, let’s look at what it would take via DNA for Angus to become Tigger (other than an 87 kilogram weight loss and allowing him on the bed)? ################################################################ print('=============================') print(' How do we turn Angus into Tigger?') print('=============================') for nodecode in nx.shortest_path(G,source='angus',target='tigger'):     print(G.node[nodecode]['NodeName']) ################################################################ Let’s look at what it would take for Chris to become Dr. Chris. print('=============================')   print('How do you make Chris a Doctor?') print('=============================')      for nodecode in nx.shortest_path(G,source='chris-hillman',target='dr-chris'):     print(G.node[nodecode]['NodeName'])        print('=============================')    ################################################################ print('### Done!! ############################################') ################################################################ You have now successfully created a new graph, which you will use to populate the animal object part of the data vault. Create a new file in your Python editor named Process-Animal-Graph.py in directory ..\VKHCG\03-Hillman\03-Process. Copy the following Python code into the file. First, we will load some of the libraries you need and create a set of basic file names and directories, to enable the ecosystem. ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import pandas as pd import networkx as nx import sqlite3 as sq from pandas.io import sql import uuid ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ Company='03-Hillman' Here is the path to the graph you created earlier in this chapter: InputAssessGraphName='Assess_All_Animals.gml' EDSAssessDir='02-Assess/01-EDS' InputAssessDir=EDSAssessDir + '/02-Python' ################################################################ sFileAssessDir=Base + '/' + Company + '/' + InputAssessDir if not os.path.exists(sFileAssessDir):     os.makedirs(sFileAssessDir) ################################################################ sDataBaseDir=Base + '/' + Company + '/03-Process/SQLite' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataBaseDir + '/Hillman.db' conn1 = sq.connect(sDatabaseName) ################################################################ sDataVaultDir=Base + '/88-DV' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataVaultDir + '/datavault.db' conn2 = sq.connect(sDatabaseName) ################################################################ You now load the graph you created earlier in this chapter. sFileName= sFileAssessDir + '/' + InputAssessGraphName print('################################') print('Loading Graph :', sFileName) print('################################') G=nx.read_gml(sFileName) print('Nodes: ', G.number_of_nodes()) print('Edges: ', G.number_of_edges()) You will now read the nodes of the graph and add them to the data vault as objects. ################################################################ t=0 tMax=G.number_of_nodes() for node in nx.nodes_iter(G):     t+=1     IDNumber=str(uuid.uuid4())     NodeName=G.node[node]['NodeName']     print('Extract:',t,' of ',tMax,':',NodeName)     ObjectLine=[('ObjectBaseKey', ['Species']),              ('IDNumber', [IDNumber]),              ('ObjectNumber', [str(t)]),              ('ObjectValue', [NodeName])]     if t==1:        ObjectFrame = pd.DataFrame.from_items(ObjectLine)     else:         ObjectRow = pd.DataFrame.from_items(ObjectLine)         ObjectFrame = ObjectFrame.append(ObjectRow) ################################################################ ObjectHubIndex=ObjectFrame.set_index(['IDNumber'],inplace=False) ################################################################ sTable = 'Process-Object-Species' print('Storing :',sDatabaseName,' Table:',sTable) ObjectHubIndex.to_sql(sTable, conn1, if_exists="replace") ################################################################# sTable = 'Hub-Object-Species' print('Storing :',sDatabaseName,' Table:',sTable) ObjectHubIndex.to_sql(sTable, conn2, if_exists="replace") ################################################################# print('################') print('Vacuum Databases') sSQL="VACUUM;" sql.execute(sSQL,conn1) sql.execute(sSQL,conn2) print('################') ################################################################ print('### Done!! ############################################') ################################################################ Well done. You now have an additional object hub in the data vault for species. Have a look at these two pieces of code . Loop through all nodes and return the shortest path from 'World' node. for node1 in nx.nodes_iter(G):     p=nx.shortest_path(G, source="world", target=node1, weight=None)     print('Path from World to ',node1,':',p) Loop through all nodes and return the shortest path from the entire node set. for node1 in nx.nodes_iter(G):     for node2 in nx.nodes_iter(G):         p=nx.shortest_path(G, source=node1, target=node1, weight=None)         print('Path:',p) Let’s quickly discuss a new algorithm. Dijkstra’s algorithm is an algorithm for discovery of the shortest paths between nodes in a graph, designed by computer scientist Edsger Wybe Dijkstra in 1956\. Figure [9-9](#Fig9) shows an example.![A435693_1_En_9_Fig9_HTML.jpg](Images/A435693_1_En_9_Fig9_HTML.jpg) Figure 9-9Example of Dijkstra’s algorithm import networkx as nx G = nx.Graph() G.add_edge('a', 'c', weight=1) G.add_edge('a', 'd', weight=2) G.add_edge('b', 'c', weight=2) G.add_edge('c', 'd', weight=1) G.add_edge('b', 'f', weight=3) G.add_edge('c', 'e', weight=3) G.add_edge('e', 'f', weight=2) G.add_edge('d', 'g', weight=1) G.add_edge('g', 'f', weight=1) d=nx.dijkstra_path(G, source="a", target="f", weight=None) print(d) I calculate that ['a', 'c', 'b', 'f'] is the shortest path. Yes, this algorithm was conceived in 1956 and published three years later. Data science is not as new as many people think. We deploy countless well-established algorithms in our work. Loop through all nodes and return the shortest path from the entire node set, using Dijkstra’s algorithm: for node1 in nx.nodes_iter(G):     for node2 in nx.nodes_iter(G):         d=nx.dijkstra_path(G, source=node1, target=node1, weight=None)         print('Path:',d) Question: Could you create a link table that uses this path as the link? Here’s the table you must populate: CREATE TABLE [Link-Object-Species] (     LinkID     VARCHAR (100),     NodeFrom   VARCHAR (200),     NodeTo     VARCHAR (200),     ParentNode VARCHAR (200),     Node       VARCHAR (200),     Step       INTEGER ); Try your new skills against the graph. Can you get it to work? Let’s look at the next object type.

#### 车辆

The international classification of vehicles is a complex process. There are standards, but these are not universally applied or similar between groups or countries. For example, my Cadillac Escalade ESV 4WD PLATINUM is a

*   细分市场中的“全尺寸 SUV 皮卡”(美式英语)
*   细分市场中的“大 4×4”(英式英语)
*   细分市场中的“高级大型 SUV ”(澳大利亚英语)
*   美国 EPA 尺寸等级中的“标准运动型多功能车”
*   欧洲 NCAP 结构类别中的“越野车”
*   欧洲 NCAP 级别的“大型越野 4×4”(1997–2009)
*   欧洲市场中的“J 级运动型多功能车”
*   美国公路损失数据研究所分类中的“豪华旅行车”

To specifically categorize my vehicle, I require a vehicle identification number (VIN) . A single VIN supplies a data scientist with massive amounts of classification details. There are two standards for vehicles: ISO 3779:2009 (road vehicles—VIN—content and structure) and ISO 3780:2009 (road vehicles—world manufacturer identifier [WMI] code). I have found that these two combined can categorize the vehicle in good detail, i.e., I can decode the VIN. VIN: 1HGBH41JXMN109186

*   型号:前奏
*   类型:乘用车
*   品牌:本田
*   年款:1991 年
*   制造商:美国本田制造公司。公司。
*   制造地:美国(北美)
*   序列号:109186
*   车身类型:双门轿跑车
*   车辆类型:乘用车

Warning A vehicle’s number plate or tags is not unique to that vehicle. There are private plate numbers that belong to a person, not a vehicle. This results in several different vehicles having the same plate number over time. When a car is resold, the new owner can also receive new plate numbers. So, do not use the plate number as a unique key. As a data scientist, you must understand the data you can infer from simple codes and classifications of the object been recorded. Tip Vehicles are systematically coded, and you will appreciate the longer-term rewards of this, if you take the time to understand the coding standards first. Let’s load the vehicle data for Hillman Ltd into the data vault, as we will need it later. Create a new file named Process-Vehicle-Logistics.py in the Python editor in directory ..\VKHCG\03-Hillman\03-Process. Copy the code into your new file, as I guide you through it. Start with the standard structure that creates the ecosystem. ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import pandas as pd import sqlite3 as sq from pandas.io import sql import uuid pd.options.mode.chained_assignment = None ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ Company='03-Hillman' InputDir='00-RawData' InputFileName='VehicleData.csv' ################################################################ sDataBaseDir=Base + '/' + Company + '/03-Process/SQLite' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataBaseDir + '/Hillman.db' conn1 = sq.connect(sDatabaseName) ################################################################ sDataVaultDir=Base + '/88-DV' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataVaultDir + '/datavault.db' conn2 = sq.connect(sDatabaseName) ################################################################ Now load the vehicle to be added to the data vault. sFileName=Base + '/' + Company + '/' + InputDir + '/' + InputFileName print('###########') print('Loading :',sFileName) VehicleRaw=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") ################################################################ sTable='Process_Vehicles' print('Storing :',sDatabaseName,' Table:',sTable) VehicleRaw.to_sql(sTable, conn1, if_exists="replace") ################################################################ Now subset the vehicle characteristics that you need to ready for the data vault to Make and Model only. VehicleRawKey=VehicleRaw[['Make','Model']].copy() Remove any duplicates. VehicleKey=VehicleRawKey.drop_duplicates() Create a vehicle key for the hub. You can clean up the VehicleKey by removing unwanted characters. VehicleKey['ObjectKey']=VehicleKey.apply(lambda row:     str('('+ str(row['Make']).strip().replace(' ', '-').replace('/', '-').lower() +      ')-(' + (str(row['Model']).strip().replace(' ', '-').replace(' ', '-').lower())      +')')        ,axis=1) Set all records a been 'vehicle' as a objectType. VehicleKey['ObjectType']=VehicleKey.apply(lambda row:     'vehicle'        ,axis=1) Set every record with unique id. VehicleKey['ObjectUUID']=VehicleKey.apply(lambda row:     str(uuid.uuid4())        ,axis=1) VehicleHub=VehicleKey[['ObjectType','ObjectKey','ObjectUUID']].copy() VehicleHub.index.name='ObjectHubID' sTable = 'Hub-Object-Vehicle' print('Storing :',sDatabaseName,' Table:',sTable) VehicleHub.to_sql(sTable, conn2, if_exists="replace") Create a vehicle satellite for the hub, to store any extra vehicle characteristics. ################################################################ ### Vehicle Satellite ################################################################ # VehicleSatellite=VehicleKey[['ObjectType','ObjectKey','ObjectUUID','Make','Model']].copy() VehicleSatellite.index.name='ObjectSatelliteID' sTable = 'Satellite-Object-Make-Model' print('Storing :',sDatabaseName,' Table:',sTable) VehicleSatellite.to_sql(sTable, conn2, if_exists="replace") Create a vehicle dimension that joins the hub to the satellite. ################################################################ ### Vehicle Dimension ################################################################ sView='Dim-Object' print('Storing :',sDatabaseName,' View:',sView) sSQL="CREATE VIEW IF NOT EXISTS [" + sView + "] AS" sSQL=sSQL+ " SELECT DISTINCT" sSQL=sSQL+ "   H.ObjectType," sSQL=sSQL+ "   H.ObjectKey AS VehicleKey," sSQL=sSQL+ "   TRIM(S.Make) AS VehicleMake," sSQL=sSQL+ "   TRIM(S.Model) AS VehicleModel" sSQL=sSQL+ " FROM" sSQL=sSQL+ "   [Hub-Object-Vehicle] AS H" sSQL=sSQL+ " JOIN" sSQL=sSQL+ "   [Satellite-Object-Make-Model] AS S" sSQL=sSQL+ " ON" sSQL=sSQL+ "     H.ObjectType=S.ObjectType" sSQL=sSQL+ " AND" sSQL=sSQL+ " H.ObjectUUID=S.ObjectUUID;" sql.execute(sSQL,conn2) Let’s test the vehicle dimension that joins the hub to the satellite. print('################')   print('Loading :',sDatabaseName,' Table:',sView) sSQL=" SELECT DISTINCT" sSQL=sSQL+ " VehicleMake," sSQL=sSQL+ " VehicleModel" sSQL=sSQL+ " FROM" sSQL=sSQL+ " [" + sView + "]" sSQL=sSQL+ " ORDER BY" sSQL=sSQL+ " VehicleMake" sSQL=sSQL+ " AND" sSQL=sSQL+ " VehicleMake;" DimObjectData=pd.read_sql_query(sSQL, conn2) DimObjectData.index.name='ObjectDimID' DimObjectData.sort_values(['VehicleMake','VehicleModel'],inplace=True, ascending=True) print('################')   print(DimObjectData) ################################################################# print('################') print('Vacuum Databases') sSQL="VACUUM;" sql.execute(sSQL,conn1) sql.execute(sSQL,conn2) print('################') ################################################################# conn1.close() conn2.close() ################################################################# #print('### Done!! ############################################') ################################################################# Congratulations! You now have two object hubs created. I will cover several other objects that you can investigate.

#### 化学化合物

Objects consist of numerous chemicals, and these chemicals can inform you about the categorization of any object. There is a related library named pyEQL (pip3 install pyEQL). Open your Python editor and try the following: from pyEQL import chemical_formula as cf print(cf.hill_order('CH2(CH3)4COOH')) This will help you resolve chemical formulas. Let’s look at a few examples.

*   食盐(氯化钠)是氯化钠。它含有一份钠和一份氯。print(cf . get _ element _ mole _ ratio(' NaCl '，' Na '))print(cf . get _ element _ mole _ ratio(' NaCl '，' Cl '))
*   排水清洁剂(硫酸)是 H2SO4。它含有两份氢、一份硫和四份氧。
*   小苏打(碳酸氢钠)就是 NaHCO3。有哪些比例？
*   甘油是 C3H8O3。有哪些比例？物质是由什么组成的？print(参见 get_element_names('C3H8O3 '))

Check if a formula is valid. print(cf.is_valid_formula('Fe2(SO4)3')) print(cf.is_valid_formula('Fe2(SO4)3+Bad')) You can also investigate other options. See library periodictable at [https://pypi.python.org/pypi/periodictable](https://pypi.python.org/pypi/periodictable) . from periodictable import * for el in elements:         print("%s %s"%(el.symbol, el.name)) You should now have a list of elements. You can also investigate biopython (see [http://biopython.org](http://biopython.org) ). You can access this by installing conda install -c anaconda biopython. This provides insights into the world of computational molecular biology. Can you combine it with your knowledge, to create extra object hubs? Tip You can extract various objects from the world around you and then start analyzing how they change the art of data science. The object data structure will change as your customers’ data changes from business to business. I advise that you start with the analysis for any nonperson entities’ classifications as soon as possible in your project. Talk to your customers’ subject-matter experts, get their classification model, and understand them 100%. Also include all angles of the object’s description. A retail store might sell table salt, but the manufacturer produces sodium chloride. By connecting these within your data vault, your algorithms will understand these relationships. Algorithms such as deep learning, logistic regression, and random forests, which I will cover in Chapter [10](10.html), work with improved net results, if you have already created the basic relationships, by linking different areas of insight. Tip Start by Object Type and Object Group first when identifying your business requirements. If you have the two fields sorted, you are 80+% of the way to completing the object structure. This is just a basic start to the processing of objects in data science. This part of the data vault will always be the largest portion of the data science work in the Process superstep. I have also enriched my customers’ data by getting external experts to categorize objects for which particular customers have never identified the common characteristics. Warning It is better to load in large amounts of overlapping details than to have inadequate and incomplete details. Do not over-summarize your Object Type and Object Group groupings. For the purpose of the practical examples, I will cover the algorithms and techniques that will assist you to perform the categorization of the objects in data sources.

### 位置

This structure records any location where an interaction between data entities occurred in the data sources. The location of a data action is a highly valuable piece of data for any business. It enables geospatial insight of the complete data sets. Geospatial refers to information that categorizes where defined structures are on the earth’s surface. Let’s discuss a few basics concerning location.

#### 纬度、经度和高度

Latitude, longitude, and altitude are some of the oldest official methods of describing the location on the earth, elaborated by Hipparchus, a Greek astronomer (190–120 BC). Lines of longitude are perpendicular to and lines of latitude are parallel to the equator. Longitude is a range of values between -° to 180°, with the Greenwich Prime Meridian set at 0°. Latitude is a range of values between -90° to 90°, with the equator set at 0°. Altitude is the distance above sea level at any given latitude and longitude. The range is -11,033 meters (Mariana Trench) to +8,850 meters (Mt. Everest, in Nepal). The three deepest trenches are the Mariana Trench (11,033 meters), Tonga Trench (10,882 meters), and Kuril-Kamchatka Trench (10,500 meters). The three highest mountains are Mt. Everest (+8,850 meters), K2/Qogir (+8,611 meters), and Kangchenjunga (+8,586 meters). Locations on Earth have the potential to be anywhere on a 510.1 trillion square meter grid. Tip I always reduce any location to a latitude, longitude, and altitude, with a short location description. When allocating locations of equipment, I use latitude, longitude, and altitude, plus a cabinet and rack location. If I have servers at Thor Data Center, I record their location as latitude=64.0496287 and longitude=-21.9958472, with a description field of “Thor Data Center,” plus a cabinet number and rack number.

#### 全球定位系统(GPS)和全球导航卫星系统(GLONASS)

The Global Positioning System and Global Navigation Satellite System are space-based radio navigation systems that supply data to any GPS/GLONASS receiver, to enable the receiver to calculate its own position on the earth’s surface. This system still uses latitude, longitude, and altitude but has an advanced technique that uses a series of overlapping distant mappings from a minimum of four satellites, to map the receiver’s location. Hence, this system is highly accurate.

#### 位置特征

Any location has more than one type of geographic characteristic. I will discuss some of the more commonly used location classifications and data entries.

##### 绝对位置

Absolute location provides a definite reference to locate a specific place. The absolute reference latitude, longitude, and attitude in the data I process are the core data I record. Using a GPS data source is always advisable. Examples: Big Ben, Westminster, London, SW1A 0AA, UK is at latitude: 51.510357 and longitude: -0.116773\. The London Eye, London, is at latitude: 51.503399 and longitude: -0.119519\. The Statue of Liberty National Monument, New York, NY 10004, USA, is at latitude: 40.689247 and longitude: -74.044502.

##### 相对位置

Relative location describes a place with respect to its environment and its connection to other places. The data is dependent on other data being correct. I have seen a few relative location data sources. I will use Big Ben , Westminster, London, SW1A 0AA, UK, at latitude: 51.510357 and longitude: -0.116773, and the Statue of Liberty National Monument, New York, NY 10004, USA, at latitude: 40.689247 and longitude: -74.044502, to show the process you can expect.

###### 描述性路线

These data sources are useful for getting an approximate relative location between absolute locations. Example: From Big Ben to the London Eye is a nine-minute (0.5-mile) walk, via A302 and The Queen’s Walk, or six minutes, via A302 with a bicycle, or five minutes via taxi. From Big Ben to the Statue of Liberty National Monument is 7.5 hours by plane.

###### 纬度/经度点之间的距离，方位角

The distance, bearing between latitude/longitude points is a calculation used in the Assess step with the logistics calculations. It gives you a more accurate relative relationship between absolute locations. Big Ben to London Eye results:

*   距离:0.7737 公里
*   初始方位:179° 45 ' 55 "
*   最终方位:179° 45 ' 56 "
*   中点:北纬 51 度 30 分 25 秒，东经 000 度 00 分 07 秒

Big Ben to Statue of Liberty results:

*   距离:5583 公里
*   初始方位:071° 35 ' 41 "
*   最终方位:128° 50 ' 54 "
*   中点:北纬 52 度 23 分 15 秒，东经 041 度 16 分 05 秒

This method supplies data scientists with more precise information between two absolute locations. Warning The method only works if the absolute locations are 100% correct!

###### 文森特直接方位角和距离公式

If you start at Big Ben and travel at bearing 289 for a distance of 5583 kilometers using the WGS84 ellipsoid, you end at

*   纬度： 41°06′19"N 41.10539784
*   经度:74° 22′37′w-74.3794307
*   最终轴承:231° 23′36″53686 . 46868868661
*   后轴承:51 英寸 23 英尺 36 英寸 53867.66868686667

We are in New York, but not at the Statue of Liberty. Tip With more precise bearing and distance values, it is possible to get to the Statue of Liberty. The relative location of a location is useful when you want to find the closest shop or calculate the best method of transport between locations.

##### 自然特征

A [topographical map](https://www.thoughtco.com/topographic-maps-overview-1435657) is a good format for the physical characteristics of a location. I suggest that you collect data on a specific location, to enhance your customer’s data. I also suggest that you look at data sources for weather measurements, temperature, land types, soil types, underground minerals, water levels, plant life, and animal life for specific locations. Note Any natural information that can enhance the location data is good to collect. Tip Collect the information over a period. That way, you get a time series that you can use to check for changes in the natural characteristics of the location. These natural characteristics are a good source of data for testing the correlations between business behavior and the natural location. Such natural characteristics as a forest, desert, or lake region can indicate to businesses like a chain store when it will or will not sell, say, a motorboat or walking boots. An area covered in red mud 90% of the year may not be selling white trousers . The lack or excess of wind in an area may determine the success of energy-saving solutions, such as wind power generation. Or sinkholes will increase the cost of homeowner’s insurance.

##### 人类特征

I suggest that you collect data on any human-designed or cultural features of a location. These features include the type of government, land use, architectural styles, forms of livelihood, religious practices, political systems, common foods, local folklore, transportation, languages spoken, money availability, and methods of communication. These natural characteristics are a good source of data to test for correlations between business behavior and the natural location. Human characteristics can also assist with categorization of the location.

##### 人-环境相互作用

The interaction of humans with their environment is a major relationship that guides people’s behavior and the characteristics of the location. Activities such as mining and other industries, roads, and landscaping at a location create both positive and negative effects on the environment, but also on humans. A location earmarked as a green belt, to assist in reducing the carbon footprint, or a new interstate change its current and future characteristics. The location is a main data source for the data science, and, normally, we find unknown or unexpected effects on the data insights. In the Python editor, open a new file named Process_Location.py in directory ..\VKHCG\01-Vermeulen\03-Process. Start with the standard imports and setup of the ecosystem. ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import pandas as pd import sqlite3 as sq from pandas.io import sql import uuid ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ Company='01-Vermeulen' InputAssessGraphName='Assess_All_Animals.gml' EDSAssessDir='02-Assess/01-EDS' InputAssessDir=EDSAssessDir + '/02-Python' ################################################################ sFileAssessDir=Base + '/' + Company + '/' + InputAssessDir if not os.path.exists(sFileAssessDir):     os.makedirs(sFileAssessDir) ################################################################ sDataBaseDir=Base + '/' + Company + '/03-Process/SQLite' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataBaseDir + '/Vermeulen.db' conn1 = sq.connect(sDatabaseName) ################################################################ sDataVaultDir=Base + '/88-DV' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataVaultDir + '/datavault.db' conn2 = sq.connect(sDatabaseName) ################################################################ tMax=360*180 ################################################################ You will now loop through longitude and latitude. Warning I am using a step value of 1 in the following sample code. In real-life systems, I use a step value as small as 0.000001\. This will result in a long-running loop . To do this, generate the pandas data frame. for Longitude in range(-180,180,1):     for Latitude in range(-90,90,1):         t+=1         IDNumber=str(uuid.uuid4())         LocationName='L'+format(round(Longitude,3)*1000, '+07d') +\                                 '-'+format(round(Latitude,3)*1000, '+07d')         print('Create:',t,' of ',tMax,':',LocationName)         LocationLine=[('ObjectBaseKey', ['GPS']),                      ('IDNumber', [IDNumber]),                      ('LocationNumber', [str(t)]),                      ('LocationName', [LocationName]),                      ('Longitude', [Longitude]),                      ('Latitude', [Latitude])]         if t==1:             LocationFrame = pd.DataFrame.from_items(LocationLine)         else:             LocationRow = pd.DataFrame.from_items(LocationLine)             LocationFrame = LocationFrame.append(LocationRow) ################################################################ LocationHubIndex=LocationFrame.set_index(['IDNumber'],inplace=False) ################################################################ sTable = 'Process-Location' print('Storing :',sDatabaseName,' Table:',sTable) LocationHubIndex.to_sql(sTable, conn1, if_exists="replace") ################################################################# sTable = 'Hub-Location' print('Storing :',sDatabaseName,' Table:',sTable) LocationHubIndex.to_sql(sTable, conn2, if_exists="replace") ################################################################# print('################') print('Vacuum Databases') sSQL="VACUUM;" sql.execute(sSQL,conn1) sql.execute(sSQL,conn2) print('################') ################################################################ print('### Done!! ############################################') ################################################################ Congratulations! You have created the hub for location . You can expand the knowledge by adding other satellites, for example, post code, common names, regional information, and other business-specific descriptions. You should investigate libraries such as geopandas (see [http://geopandas.org](http://geopandas.org) ), to enhance your location with extra knowledge. You can install this by using  install -c conda-forge geopandas. Example: import geopandas as gpd from matplotlib import pyplot as pp world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres')) southern_world = world.cx[:, :0] southern_world.plot(figsize=(10, 3)); world.plot(figsize=(10, 6)); pp.show()import geopandas as gpd from matplotlib import pyplot as pp world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres')) southern_world = world.cx[:, :0] southern_world.plot(figsize=(10, 3)); world.plot(figsize=(10, 6)); pp.show() The library supports an easy technique to handle locations. Now that we have completed location structure successfully, we have only to handle events, to complete the data vault.

### 事件

This structure records any specific event or action that is discovered in the data sources. An event is any action that occurs within the data sources. Events are recorded using three main data entities : Event Type, Event Group, and Event Code. The details of each event are recorded as a set of details against the event code. There are two main types of events.

#### 显性事件

This type of event is stated in the data source clearly and with full details. There is clear data to show that the specific action was performed. Following are examples of explicit events :

*   编号为 1234 的安全卡被用来打开 a 门。
*   你正在阅读《实用数据科学》的第 9 章。
*   我买了十罐咖喱牛肉。

Explicit events are the events that the source systems supply, as these have direct data that proves that the specific action was performed.

#### 隐含事件

This type of event is formulated from characteristics of the data in the source systems plus a series of insights on the data relationships. The following are examples of implicit events:

*   编号为 8887 的安全卡被用来打开 x 门。
*   给维穆伦先生发了一张号码为 8887 的安全卡。
*   302 房间安装了标记为 x 门的安全阅读器。

These three events would imply that Mr. Vermeulen entered room 302 as an event. Not true! Warning Beware the natural nature of humans to assume that a set of actions always causes a specific known action. Record as events only proven fact! If I discover that all visitor passes are issued at random and not recovered on exit by the visitor’s desk, there is a probability that there is more than one security card with the number 1234\. As a data scientist, you must ensure that you do not create implicit events from assumptions that can impact the decisions of the business. You can only record facts as explicit events. The event is the last of the core structures of the Process step. I will now assist you to convert the basic knowledge of the time, person, object, location, and event structures into a data source that will assist the data scientist to perform all the algorithms and techniques required to convert the data into business insights. These five main pillars (time, person, object, location, and event), as supported by the data extracted from the data sources, will be used by the data scientist to construct the data vault. I will return to the data vault later in this chapter. I want to introduce a few data science concepts that will assist you in converting the assess data into a data vault. I suggest you open your Python editor and see if you can create the event hub on your own. ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import pandas as pd import sqlite3 as sq from pandas.io import sql import uuid ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ Company='01-Vermeulen' InputFileName='Action_Plan.csv' ################################################################ sDataBaseDir=Base + '/' + Company + '/03-Process/SQLite' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataBaseDir + '/Vermeulen.db' conn1 = sq.connect(sDatabaseName) ################################################################ sDataVaultDir=Base + '/88-DV' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataVaultDir + '/datavault.db' conn2 = sq.connect(sDatabaseName) ################################################################ sFileName=Base + '/' + Company + '/00-RawData/' + InputFileName print('Loading :',sFileName) EventRawData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") EventRawData.index.names=['EventID'] EventHubIndex=EventRawData ################################################################ sTable = 'Process-Event' print('Storing :',sDatabaseName,' Table:',sTable) EventHubIndex.to_sql(sTable, conn1, if_exists="replace") ################################################################# sTable = 'Hub-Event' print('Storing :',sDatabaseName,' Table:',sTable) EventHubIndex.to_sql(sTable, conn2, if_exists="replace") ################################################################# print('################') print('Vacuum Databases') sSQL="VACUUM;" sql.execute(sSQL,conn1) sql.execute(sSQL,conn2) print('################') ################################################################ print('### Done!! ############################################') ################################################################ Well done. You just completed the next hub .

## 数据科学过程

I want to discuss a basic data science process at this point, before I progress to the rest of the Process step. This basic process will guide the process forward and formulate the basic investigation techniques that I will teach you as we progress through the further data discovery.

### 数据科学的根源

Data science is at its core about curiosity and inquisitiveness. This core is rooted in the 5 Whys. The 5 Whys is a technique used in the analysis phase of data science.

#### 五个为什么的好处

The 5 Whys assist the data scientist to identify the root cause of a problem and determine the relationship between different root causes of the same problem. It is one of the simplest investigative tools—easy to complete without intense statistical analysis.

#### 什么时候 5 个为什么最有用？

The 5 Whys are most useful for finding solutions to problems that involve human factors or interactions that generate multilayered data problems. In day-to-day business life, they can be used in real-world businesses to find the root causes of issues.

#### 如何完成 5 个为什么

Write down the specific problem. This will help you to formalize the problem and describe it completely. It also helps the data science team to focus on the same problem. Ask why the problem occurred and write the answer below the problem. If the answer you provided doesn’t identify the root cause of the problem that you wrote down first, ask why again, and write down that answer. Loop back to the preceding step until you and your customer are in agreement that the problem’s root cause is identified. Again, this may require fewer or more than the 5 Whys. Tip I have found that most questions take only four whys, but if you require seven, I recommend that you go back over the process, as it is not solving the root cause. As a data scientist, I have found that it helps to perform the 5 Whys when dealing with data and processing the data into the core data vault. Seek explicit reasons for loading that specific data entry into the specific part of the data vault. Caution The data-mapping decisions you take will affect the quality of the data science and the value of the insights you derive from the data vault.

#### 鱼骨图

I have found this simple diagram is a useful tool to guide the questions that I need to resolve related to where each data source’s data fits into the data vault. The diagram (Figure [9-10](#Fig10)) is drawn up as you complete the 5 Whys process, as you will discover that there are normally many causes for why specific facts have been recorded.![A435693_1_En_9_Fig10_HTML.jpg](Images/A435693_1_En_9_Fig10_HTML.jpg) Figure 9-10Fishbone diagram Let’s return to my earlier event, the one in which I bought ten cans of beef curry. The ten cans are the effect (Y), but the four root causes of the purchase are

*   我饿了，所以我买了十罐。我不喜欢咖喱的品牌，我上周买了 10 罐。
*   我的邻居需要五罐，因为她已经不能走路了，她要了我买的牌子，因为这是最便宜的，1.10 英镑。
*   我给狗喂了两罐，因为我觉得狗粮没有营养，但我不准备给狗买更贵品牌的咖喱牛肉罐头。
*   我在当地学校外面的慈善垃圾箱里放了三罐，因为它想要那个品牌的咖喱用于其施粥场，并以每罐 2.50 英镑的价格为其“资助你们学校的新笔记本电脑”活动提供代币(代金券)。

I had a customer’s data scientist (an ex-employee now) categorize my ten cans plus the 9,000 others sold that week as a major success for their new brand of curry. The customer’s buyer signed a prepaid six-month purchase order as a result of this success. Some 100,000 cans showed up at the retailer and were sold within eight days. The sad part is that the reason the product sold so well was the £2.50-per-tin token (voucher) that obscured a 25-cents (US) cash-back offer that the local wholesaler erroneously covered with a £2.50 “Fund your School’s new laptops” sticker that was meant for the more exclusive £25.00 schoolbags he was also supplying. The wholesaler’s buyer was illegally shipping black market expired products into the country. The £2.50 sticker also covered the expiry date. The data system advised by the wholesaler’s data scientist used an implicit event, whereby the system marked the unknown expiry date as the scan date into the wholesaler’s warehouses. Caution If it sounds too good to be true, it is probably wrong! This trivial sales disaster cost two data scientists and a buyer their jobs; a wholesaler was declared bankrupt; six people contracted food poisoning at the soup kitchen; and my dog had to be taken to the vet. The formula is simple: Dog + Curry = Sick puppy! I still place the liability on the shop; my wife disagrees. The “Fund your School’s new laptops” campaign was a big success, with nearly 90% of the tokens now redeemed, sadly, without an increase in sales of the £25.00 schoolbags. My neighbor received five tins of our better-brand curry, because the dog got sick. I got more work. All was good. I’m still not sure why only 2,000 unopened cans were returned by customers to the retailer, and only 1 in 200 still had the token attached. Tip Always ask why, and never assume the answer is correct until you’ve proven it. So, let’s look at another 5 Whys example.

#### 5 个为什么的例子

So, let’s look at the data science I performed with the retailer after the trouble with the cans of curry. Problem Statement: Customers are unhappy because they are being shipped products that don’t meet their specifications.

1.  1.为什么客户会收到劣质产品？
    *   因为制造部门制造产品的规格与客户和销售人员同意的规格不同。 
2.  2.为什么制造部门生产的产品规格不同于销售部门的规格？
    *   因为销售人员通过直接给生产部门主管打电话开始工作来加快车间的工作。传达或记录规格时出错。 
3.  3.为什么销售人员直接打电话给制造负责人开始工作，而不是按照公司制定的程序来做？
    *   因为“开始工作”表单需要销售主管的批准才能开始工作，并且会减慢生产过程(或者在主管不在办公室时停止生产)。 
4.  4.为什么表单包含对销售总监的批准？
    *   因为销售总监必须不断更新销售情况，以便与首席执行官讨论，因为我的零售商客户是前十大关键客户。 

In this case, only four whys were required to determine that a non-value-added signature authority helped to cause a process breakdown in the quality assurance for a key account! The rest was just criminal. The external buyer at the wholesaler knew this process was regularly bypassed and started buying the bad tins to act as an unofficial backfill for the failing process in the quality-assurance process in manufacturing, to make up the shortfalls in sales demand. The wholesaler simply relabeled the product and did not change how it was manufactured. The reason? Big savings lead to big bonuses. A key client’s orders had to be filled. . . . Sales are important! Tip It takes just a quick why—times five—to verify! The next technique I employ is useful in 90% of my data science. It usually offers multifaceted solutions but requires a further level of analysis .

### 蒙特 卡罗模拟

I want to introduce you to a powerful data science tool called a Monte Carlo simulation. This technique performs analysis by building models of possible results, by substituting a range of values—a probability distribution—for parameters that have inherent uncertainty. It then calculates results over and over, each time using a different set of random values from the probability functions. Depending on the number of uncertainties and the ranges specified for them, a Monte Carlo simulation can involve thousands or tens of thousands of recalculations before it is complete. Monte Carlo simulation produces distributions of possible outcome values. As a data scientist, this gives you an indication of how your model will react under real-life situations. It also gives the data scientist a tool to check complex systems, wherein the input parameters are high-volume or complex. I will show you a practical use of this tool in the next section.

### 因果循环图

A causal loop diagram (CLD) is a causal diagram that aids in visualizing how a number of variables in a system are interrelated and drive cause-and-effect processes. The diagram consists of a set of nodes and edges. Nodes represent the variables, and edges are the links that represent a connection or a relation between the two variables. I normally use my graph theory, covered in Chapter [8](08.html), to model the paths through the system. I simply create a directed graph and then step through it one step at a time. Example: The challenge is to keep the “Number of Employees Available to Work and Productivity” as high as possible. I modeled the following as a basic diagram, while I investigated the tricky process (Figure [9-11](#Fig11)).![A435693_1_En_9_Fig11_HTML.jpg](Images/A435693_1_En_9_Fig11_HTML.jpg) Figure 9-11Causal loop diagram Our first conclusion was “We need more staff.” I then simulated the process, using a hybrid data science technique formulated from two other techniques—Monte Carlo simulation and Deep Reinforcement Learning—against a graph data model. The result was the discovery of several other additional pieces of the process that affected the outcome I was investigating. This was the result (Figure [9-12](#Fig12)).![A435693_1_En_9_Fig12_HTML.jpg](Images/A435693_1_En_9_Fig12_HTML.jpg) Figure 9-12Monte Carlo result The Monte Carlo simulation cycled through a cause-and-effect model by changing the values at points R1 through R4 for three hours, as a training set. The simulation was then run for three days with a reinforced deep learning model that adapted the key drivers in the system. The result was “Managers need to manage not work.” The R2—percentage of manage doing employees’ duties—was the biggest cause and impact driver in the system. Tip Do not deliver results early, if you are not sure of the true data science results. Work effectively and efficiently not quickly! I routinely have numerous projects going through analysis in parallel. This way, I can process parallel data science tasks while still supporting excellent, effective, and efficient data science.

### 帕累托图

Pareto charts are a technique I use to perform a rapid processing plan for the data science. Pareto charts can be constructed by segmenting the range of the data into groups (also called segments, bins, or categories). (See Figure [9-13](#Fig13).)![A435693_1_En_9_Fig13_HTML.jpg](Images/A435693_1_En_9_Fig13_HTML.jpg) Figure 9-13Pareto chart Questions the Pareto chart answers:

*   我们团队或我客户的业务面临的最大问题是什么？
*   哪 20%的来源导致了 80%的问题(80/20 法则)？
*   我们应该集中精力在哪些方面实现最大的改进？

I perform a rapid assessment of the data science processes and determine what it will take to do 80% of the data science effectively and efficiently in the most rapid time frame. It is a maximum-gain technique. Tip If you are saving your customer 80% of its proven loses, it is stress-free to identify resources to complete the remaining 20% savings .

### 相关分析

The most common analysis I perform at this step is the correlation analysis of all the data in the data vault. Feature development is performed between data items, to find relationships between data values. import pandas as pd a = [ [1, 2, 3], [5, 6, 9], [7, 3, 13], [5, 3, 19], [5, 3, 12], [5, 6, 11], [5, 6, 13], [5, 3, 4]] df = pd.DataFrame(data=a) cr=df.corr() print(cr)

### 预测

Forecasting is the ability to project a possible future, by looking at historical data. The data vault enables these types of investigations, owing to the complete history it collects as it processes the source’s systems data. You will perform many forecasting projects during your career as a data scientist and supply answers to such questions as the following:

*   我们应该买什么？
*   我们应该卖什么？
*   我们的下一个业务将来自哪里？

People want to know what you calculate to determine what is about to happen. I will cover an example on forecasting now. VKHCG financed a number of other companies to generate additional capital. Check in \VKHCG\04-Clark\ 00-RawData for a file named VKHCG_Shares.csv. Here is our investment portfolio:

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 分享 | 稳定的 | 单位 |
| --- | --- | --- |
| WIKI/GOOGL | WIKI_Google | One million |
| 维基/MSFT | WIKI_Microsoft | One million |
| WIKI/UPS | WIKI_UPS | One million |
| 维基/AMZN | WIKI_Amazon | One million |
| 本地 BTC/美元 | LOCALBTC_USD(本地 _ 美元) | One million |
| 珀斯/AUD_USD_M | 珀斯 _AUD_USD_M | One thousand |
| 珀斯/AUD_USD_D | 珀斯 _AUD_USD_D | One thousand |
| 佩尔斯/奥德 _SLVR_M | perth _ slvr _ 美元 _M | One thousand |
| S7-1200 可编程控制器 | perth _ slvr _ 美元 _D | One thousand |
| BTER/AURBTC | bter _ aurbtc 函数 | One million |
| 联邦/美国纽约英国 | 美国联邦航空公司 | One thousand |
| 美联储/RXI_N_A_CA | FED_RXI_N_A_CA | One thousand |

I will guide you through a technique to determine if we can forecast what our shares will do in the near-future. Open a new file in your Python editor and save it as Process-Shares-Data.py in directory \VKHCG\04-Clark\03-Process. I will guide you through this process. You will require a library called quandl (conda install -c anaconda quandl), which enables you to use online share numbers for the examples. As before, start the ecosystem by setting up the libraries and the basic directories of the environment. ################################################################ import sys import os import sqlite3 as sq import quandl import pandas as pd ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base :',Base, ' using ', sys.platform) print('################################') ################################################################ Company='04-Clark' sInputFileName='00-RawData/VKHCG_Shares.csv' sOutputFileName='Shares.csv' ################################################################ sDataBaseDir=Base + '/' + Company + '/03-Process/SQLite' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sFileDir1=Base + '/' + Company + '/01-Retrieve/01-EDS/02-Python' if not os.path.exists(sFileDir1):     os.makedirs(sFileDir1) ################################################################ sFileDir2=Base + '/' + Company + '/02-Assess/01-EDS/02-Python' if not os.path.exists(sFileDir2):     os.makedirs(sFileDir2) ################################################################ sFileDir3=Base + '/' + Company + '/03-Process/01-EDS/02-Python' if not os.path.exists(sFileDir3):     os.makedirs(sFileDir3) ################################################################ sDatabaseName=sDataBaseDir + '/clark.db' conn = sq.connect(sDatabaseName) ################################################################ You will now import the list of shares we own to create the forecasts . ################################################################ ### Import Share Names Data ################################################################ sFileName=Base + '/' + Company + '/' + sInputFileName print('################################') print('Loading :',sFileName) print('################################') RawData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") RawData.drop_duplicates(subset=None, keep="first", inplace=True) print('Rows   :',RawData.shape[0]) print('Columns:',RawData.shape[1]) print('################')    ################################################################ sFileName=sFileDir1 + '/Retrieve_' + sOutputFileName print('################################') print('Storing :', sFileName) print('################################') RawData.to_csv(sFileName, index = False) print('################################')   ################################################################ sFileName=sFileDir2 + '/Assess_' + sOutputFileName print('################################') print('Storing :', sFileName) print('################################') RawData.to_csv(sFileName, index = False) print('################################')   ################################################################ sFileName=sFileDir3 + '/Process_' + sOutputFileName print('################################') print('Storing :', sFileName) print('################################') RawData.to_csv(sFileName, index = False) print('################################') You will now loop through each of the shares and perform a forecast for each share. ################################################################ ### Import Shares Data Details ################################################################ for sShare in range(RawData.shape[0]):     sShareName=str(RawData['Shares'][sShare])     ShareData = quandl.get(sShareName)     UnitsOwn=RawData['Shares'][sShare]     ShareData['UnitsOwn']=ShareData.apply(lambda row:(UnitsOwn),axis=1)       print('################')     print('Share  :',sShareName)     print('Rows   :',ShareData.shape[0])     print('Columns:',ShareData.shape[1])     print('################')       #################################################################     print('################')       sTable=str(RawData['sTable'][sShare])     print('Storing :',sDatabaseName,' Table:',sTable)     ShareData.to_sql(sTable, conn, if_exists="replace")     print('################')       ################################################################     sOutputFileName = sTable + '.csv'     sFileName=sFileDir1 + '/Retrieve_' + sOutputFileName     print('################################')     print('Storing :', sFileName)     print('################################')     RawData.to_csv(sFileName, index = False)     print('################################')     ################################################################     sOutputFileName = sTable + '.csv'     sFileName=sFileDir2 + '/Assess_' + sOutputFileName     print('################################')     print('Storing :', sFileName)     print('################################')     RawData.to_csv(sFileName, index = False)     print('################################')     ################################################################     sOutputFileName = sTable + '.csv'     sFileName=sFileDir3 + '/Process_' + sOutputFileName     print('################################')     print('Storing :', sFileName)     print('################################')     RawData.to_csv(sFileName, index = False)     print('################################') ################################################################ ################################################################ print('### Done!! ############################################') ################################################################ What do you think? Are we doing well, or are we heading for bankruptcy? Would you advise us to buy more shares? The shares will be stored in the data vault as objects, and the specific shares, i.e., share codes, will be stored in the object hub. Store the real daily share prices in the object-share-price satellite, to track the daily changes. Store your daily projections or forecasts in an object-share-price-projected hub. That way, you can compare your projections with real pricing, and you can also spot trends in the share price over time. To track share price over time, you will also require a link to the time hub, to enable investigation of the share price against any other activities in the data vault. So, as I stated before, you can model any data you get from your data lake or process from the insights into the data within the data vault.

## 数据科学

You must understand that data science works best when you follow approved algorithms and techniques. You can experiment and wrangle the data, but in the end, you must verify and support your results. Applying good data science ensures that you can support your work with acceptable proofs and statistical tests. I believe that data science that works follows these basic steps :

1.  1.从一个问题开始。确保你已经完全回答了 5 个为什么。
2.  2.遵循一个好的模式来制定一个模型。制定一个模型，猜测数据的原型，并开始对真实世界的参数进行虚拟模拟。如果你有运筹学或计量经济学技能，这是你会发现他们的用处。将一些数学和统计学融入到解决方案中，你就有了一个数据科学模型的开端。请记住:所有问题都必须与客户的业务相关，并且必须提供对问题的见解，从而产生可操作的结果，并且通常是您计划部署的数据科学流程的应用的可量化投资回报。
3.  3.收集观察结果，并利用它们提出假设。根据您的模型，通过收集所需的观察值来开始调查。根据观察结果处理你的模型，并证明你的假设是对还是错。
4.  4.用现实世界的证据来判断假设。将调查结果与现实世界联系起来，通过讲故事，将结果转化为现实生活中的商业建议和见解。小心你可以得到最好的结果，但是如果你不能解释为什么它们是重要的，你就没有达到你的目标。
5.  5.尽早并经常与客户和主题专家合作。你还必须尽早并经常与你的相关专家沟通，以确保你在探索之旅中带着他们。商人希望成为他们问题的解决方案的一部分。你的责任是提供好的科学成果来支持业务。

## 摘要

You have successfully completed the Process step of this chapter. You should now have a solid data vault with processed data ready for extraction by the Transform step, to turn your new knowledge into wisdom and understanding. At this point, the Retrieve, Assess and Process steps would have enabled you to change the unknown data in the data lake into a classified series of hub, i.e., T-P-O-L-E structures. You should have the following in the data vault:

*   带有卫星和链接的时间中心
*   带有卫星和链接的个人中心
*   带有卫星和链接的对象中枢
*   具有卫星和链路的位置中枢
*   有卫星和链接的活动中心

The rest of the data science is now performed against the standard structure. This will help you to develop solid and insightful data science techniques and methods against the T-P-O-L-E structure. Note If you have processed the features from the data lake into the data vault, you should now have completed most of the data engineering or data transformation tasks. You have achieved a major milestone by building the data vault. Note The data vault creates a natural history, as it stores all of the preceding processing runs as snapshots in the data vault. You can reprocess the complete Transform superstep and recover a full history of every step in the processing. The next step is for you to start performing more pure data science tasks, to investigate what data you have found and how you are going to formulate the data from the data vault into a query-able solution for the business.