© Andreas François Vermeulen 2018 Andreas François VermeulenPractical Data Science[https://doi.org/10.1007/978-1-4842-3054-1_1](01.html)

# 1.数据科学技术栈

Andreas François Vermeulen<sup class="calibre7">1 </sup> (1)West Kilbride North Ayrshire, UK   The Data Science Technology Stack covers the data processing requirements in the Rapid Information Factory ecosystem. Throughout the book, I will discuss the stack as the guiding pattern. In this chapter, I will help you to recognize the basics of data science tools and their influence on modern data lake development. You will discover the techniques for transforming a data vault into a data warehouse bus matrix. I will explain the use of Spark, Mesos, Akka, Cassandra, and Kafka, to tame your data science requirements. I will guide you in the use of elastic search and MQTT (MQ Telemetry Transport) , to enhance your data science solutions. I will help you to recognize the influence of R as a creative visualization solution. I will also introduce the impact and influence on the data science ecosystem of such programming languages as R, Python, and Scala.

## 快速信息工厂生态系统

The Rapid Information Factory ecosystem is a convention of techniques I use for my individual processing developments. The processing route of the book will be formulated on this basis, but you are not bound to use it exclusively. The tools I discuss in this chapter are available to you without constraint. The tools can be used in any configuration or permutation that is suitable to your specific ecosystem. I recommend that you begin to formulate an ecosystem of your own or simply adopt mine. As a prerequisite, you must become accustomed to a set of tools you know well and can deploy proficiently. Note Remember: Your data lake will have its own properties and features, so adopt your tools to those particular characteristics.

## 数据科学存储工具

This data science ecosystem has a series of tools that you use to build your solutions. This environment is undergoing a rapid advancement in capabilities, and new developments are occurring every day. I will explain the tools I use in my daily work to perform practical data science. Next, I will discuss the following basic data methodologies.

### 写模式和读模式

There are two basic methodologies that are supported by the data processing tools. Following is a brief outline of each methodology and its advantages and drawbacks.

#### 写模式生态系统

A traditional relational database management system (RDBMS) requires a schema before you can load the data. To retrieve data from my structured data schemas, you may have been running standard SQL queries for a number of years. Benefits include the following:

*   在传统的数据生态系统中，工具采用模式，并且只有在模式被描述后才能工作，因此数据只有一个视图。
*   这种方法对于阐明数据点之间的关系非常有价值，因此已经配置了一些关系。
*   这是存储“密集”数据的有效方式。
*   所有数据都在同一个数据存储中。

On the other hand, schema-on-write isn’t the answer to every data science problem. Among the downsides of this approach are that

*   它的模式通常是专门构建的，这使得它们很难更改和维护。
*   它通常会丢失作为未来分析来源的原始/原子数据。
*   在能够处理数据之前，需要大量的建模/实现工作。
*   如果特定类型的数据不能存储在模式中，您就不能从模式中有效地处理它。

At present, schema-on-write is a widely adopted methodology to store data.

#### 读取模式生态系统

This alternative data storage methodology does not require a schema before you can load the data. Fundamentally, you store the data with minimum structure. The essential schema is applied during the query phase. Benefits include the following:

*   它提供了存储非结构化、半结构化和无组织数据的灵活性。
*   当从结构中查询数据时，它允许无限的灵活性。
*   叶级数据保持完整，不被转换，以供将来参考和使用。
*   这种方法鼓励实验和探索。
*   它加快了生成新的可操作知识的速度。
*   它缩短了从数据生成到可操作知识的可用性之间的周期时间。

Schema-on-read methodology is expanded on in Chapter [6](06.html). I recommend a hybrid between schema-on-read and schema-on-write ecosystems for effective data science and engineering. I will discuss in detail why this specific ecosystem is the optimal solution when I cover the functional layer’s purpose in data science processing.

## 数据湖

A data lake is a storage repository for a massive amount of raw data. It stores data in native format, in anticipation of future requirements. You will acquire insights from this book on why this is extremely important for practical data science and engineering solutions. While a schema-on-write data warehouse stores data in predefined databases, tables, and records structures, a data lake uses a less restricted schema-on-read-based architecture to store data. Each data element in the data lake is assigned a distinctive identifier and tagged with a set of comprehensive metadata tags. A data lake is typically deployed using distributed data object storage, to enable the schema-on-read structure. This means that business analytics and data mining tools access the data without a complex schema. Using a schema-on-read methodology enables you to load your data as is and start to get value from it instantaneously. I will discuss and provide more details on the reasons for using a schema-on-read storage methodology in Chapters [6](06.html)–[11](11.html). For deployment onto the cloud, it is a cost-effective solution to use Amazon’s Simple Storage Service (Amazon S3) to store the base data for the data lake. I will demonstrate the feasibility of using cloud technologies to provision your data science work. It is, however, not necessary to access the cloud to follow the examples in this book, as they can easily be processed using a laptop.

## 数据库

Data vault modeling, designed by Dan Linstedt, is a database modeling method that is intentionally structured to be in control of long-term historical storage of data from multiple operational systems. The data vaulting processes transform the schema-on-read data lake into a schema-on-write data vault. The data vault is designed into the schema-on-read query request and then executed against the data lake. I have also seen the results stored in a schema-on-write format, to persist the results for future queries. The techniques for both methods are discussed in Chapter [9](09.html). At this point, I expect you to understand only the rudimentary structures required to formulate a data vault. The structure is built from three basic data structures: hubs, inks, and satellites. Let’s examine the specific data structures, to clarify why they are compulsory.

### 中心

Hubs contain a list of unique business keys with low propensity to change. They contain a surrogate key for each hub item and metadata classification of the origin of the business key. The hub is the core backbone of your data vault, and in Chapter [9](09.html), I will discuss in more detail how and why you use this structure.

### 链接

Associations or transactions between business keys are modeled using link tables. These tables are essentially many-to-many join tables, with specific additional metadata. The link is a singular relationship between hubs to ensure the business relationships are accurately recorded to complete the data model for the real-life business. In Chapter [9](09.html), I will explain how and why you would require specific relationships.

### 卫星

Hubs and links form the structure of the model but store no chronological characteristics or descriptive characteristics of the data. These characteristics are stored in appropriated tables identified as satellites. Satellites are the structures that store comprehensive levels of the information on business characteristics and are normally the largest volume of the complete data vault data structure. In Chapter [9](09.html), I will explain how and why these structures work so well to model real-life business characteristics. The appropriate combination of hubs, links, and satellites helps the data scientist to construct and store prerequisite business relationships. This is a highly in-demand skill for a data modeler. The transformation to this schema-on-write data structure is discussed in detail in Chapter [9](09.html), to point out why a particular structure supports the processing methodology. I will explain in that chapter why you require particular hubs, links, and satellites.

## 数据仓库总线矩阵

The Enterprise Bus Matrix is a data warehouse planning tool and model created by Ralph Kimball and used by numerous people worldwide over the last 40+ years. The bus matrix and architecture builds upon the concept of conformed dimensions that are interlinked by facts. The data warehouse is a major component of the solution required to transform data into actionable knowledge. This schema-on-write methodology supports business intelligence against the actionable knowledge. In Chapter [10](10.html), I provide more details on this data tool and give guidance on its use.

## 数据科学处理工具

Now that I have introduced data storage, the next step involves processing tools to transform your data lakes into data vaults and then into data warehouses. These tools are the workhorses of the data science and engineering ecosystem. Following are the recommended foundations for the data tools I use.

## 火花

Apache Spark is an open source cluster computing framework. Originally developed at the AMP Lab of the University of California, Berkeley, the Spark code base was donated to the Apache Software Foundation, which now maintains it as an open source project. This tool is evolving at an incredible rate. IBM is committing more than 3,500 developers and researchers to work on Spark-related projects and formed a dedicated Spark technology center in San Francisco to pursue Spark-based innovations. SAP, Tableau, and Talend now support Spark as part of their core software stack. Cloudera, Hortonworks, and MapR distributions support Spark as a native interface. Spark offers an interface for programming distributed clusters with implicit data parallelism and fault-tolerance. Spark is a technology that is becoming a de-facto standard for numerous enterprise-scale processing applications. I discovered the following modules using this tool as part of my technology toolkit.

### 火花核心

Spark Core is the foundation of the overall development. It provides distributed task dispatching, scheduling, and basic I/O functionalities. This enables you to offload the comprehensive and complex running environment to the Spark Core. This safeguards that the tasks you submit are accomplished as anticipated. The distributed nature of the Spark ecosystem enables you to use the same processing request on a small Spark cluster, then on a cluster of thousands of nodes, without any code changes. In Chapter [10](10.html), I will discuss how you accomplish this.

### Spark SQL

Spark SQL is a component on top of the Spark Core that presents a data abstraction called Data Frames. Spark SQL makes accessible a domain-specific language (DSL) to manipulate data frames. This feature of Spark enables ease of transition from your traditional SQL environments into the Spark environment. I have recognized its advantage when you want to enable legacy applications to offload the data from their traditional relational-only data storage to the data lake ecosystem.

### 火花流

Spark Streaming leverages Spark Core’s fast scheduling capability to perform streaming analytics. Spark Streaming has built-in support to consume from Kafka, Flume, Twitter, ZeroMQ, Kinesis, and TCP/IP sockets. The process of streaming is the primary technique for importing data from the data source to the data lake. Streaming is becoming the leading technique to load from multiple data sources. I have found that there are connectors available for many data sources. There is a major drive to build even more improvements on connectors, and this will improve the ecosystem even further in the future. In Chapters [7](07.html) and [11](11.html), I will discuss the use of streaming technology to move data through the processing layers.

### 机器学习库

Spark MLlib is a distributed machine learning framework used on top of the Spark Core by means of the distributed memory-based Spark architecture. In Spark 2.0, a new library, spark.mk, was introduced to replace the RDD-based data processing with a DataFrame-based model. It is planned that by the introduction of Spark 3.0, only DataFrame-based models will exist. Common machine learning and statistical algorithms have been implemented and are shipped with MLlib, which simplifies large-scale machine learning pipelines, including

*   降维技术，如奇异值分解(SVD)和主成分分析(PCA)
*   汇总统计、相关性、分层抽样、假设检验和随机数据生成
*   协同过滤技术，包括交替最小二乘法(ALS)
*   分类和回归:支持向量机、逻辑回归、线性回归、决策树和朴素贝叶斯分类
*   聚类分析方法，包括 k-均值和潜在狄利克雷分配(LDA)
*   优化算法，如随机梯度下降和有限记忆 BFGS (L-BFGS)
*   特征提取和变换函数

In Chapter [10](10.html), I will discuss the use of machine learning proficiency to support the automatic processing through the layers.

### 图形 x

GraphX is a powerful graph-processing application programming interface (API) for the Apache Spark analytics engine that can draw insights from large data sets. GraphX provides outstanding speed and capacity for running massively parallel and machine-learning algorithms. The introduction of the graph-processing capability enables the processing of relationships between data entries with ease. In Chapters [9](09.html) and [10](10.html), I will discuss the use of a graph database to support the interactions of the processing through the layers.

## 梅索斯

Apache Mesos is an open source cluster manager that was developed at the University of California, Berkeley. It delivers efficient resource isolation and sharing across distributed applications. The software enables resource sharing in a fine-grained manner, improving cluster utilization. The Enterprise version of Mesos is Mesosphere Enterprise DC/OS. This runs containers elastically, and data services support Kafka, Cassandra, Spark, and Akka. In microservices architecture, I aim to construct a service that spawns granularity, processing units and lightweight protocols through the layers. In Chapter [6](06.html), I will discuss the use of fine-grained microservices know-how to support data processing through the framework.

## 阿卡

The toolkit and runtime methods shorten development of large-scale data-centric applications for processing. Akka is an actor-based message-driven runtime for running concurrency, elasticity, and resilience processes. The use of high-level abstractions such as actors, streams, and futures facilitates the data science and engineering granularity processing units. The use of actors enables the data scientist to spawn a series of concurrent processes by using a simple processing model that employs a messaging technique and specific predefined actions/behaviors for each actor. This way, the actor can be controlled and limited to perform the intended tasks only. In Chapter [7](07.html)-[11](11.html), I will discuss the use of different fine-grained granularity processes to support data processing throughout the framework.

## 卡桑德拉

Apache Cassandra is a large-scale distributed database supporting multi–data center replication for availability, durability, and performance. I use DataStax Enterprise (DSE) mainly to accelerate my own ability to deliver real-time value at epic scale, by providing a comprehensive and operationally simple data management layer with a unique always-on architecture built in [Apache Cassandra](http://www.planetcassandra.org/%23_blank). The standard Apache Cassandra open source version works just as well, minus some extra management it does not offer as standard. I will just note that, for graph databases, as an alternative to GraphX, I am currently also using DataStax Enterprise Graph. In Chapter [7](07.html)-[11](11.html), I will discuss, the use of these large-scale distributed database models to process data through data science structures.

## 卡夫卡

This is a high-scale messaging backbone that enables communication between data processing entities. The Apache Kafka streaming platform, consisting of Kafka Core, Kafka Streams, and Kafka Connect, is the foundation of the Confluent Platform. The Confluent Platform is the main commercial supporter for Kafka (see [www.confluent.io/](http://www.confluent.io/) ). Most of the Kafka projects I am involved with now use this platform. Kafka components empower the capture, transfer, processing, and storage of data streams in a distributed, fault-tolerant manner throughout an organization in real time.

### 卡夫卡核心

At the core of the Confluent Platform is Apache Kafka. Confluent extends that core to make configuring, deploying, and managing Kafka less complex.

### 卡夫卡溪流

Kafka Streams is an open source solution that you can integrate into your application to build and execute powerful stream-processing functions.

### 卡夫卡连接

This ensures Confluent-tested and secure connectors for numerous standard data systems. Connectors make it quick and stress-free to start setting up consistent data pipelines. These connectors are completely integrated with the platform, via the schema registry. Kafka Connect enables the data processing capabilities that accomplish the movement of data into the core of the data solution from the edge of the business ecosystem. In Chapter [7](07.html)-[11](11.html), I will discuss the use of this messaging pipeline to stream data through the configuration.

## 弹性搜索

Elastic search is a distributed, open source search and analytics engine designed for horizontal scalability, reliability, and stress-free management. It combines the speed of search with the power of analytics, via a sophisticated, developer-friendly query language covering structured, unstructured, and time-series data. In Chapter [11](11.html), I will discuss, the use of this elastic search to categorize data within the framework.

## 稀有

R is a programming language and software environment for statistical computing and graphics. The R language is widely used by data scientists, statisticians, data miners, and data engineers for developing statistical software and performing data analysis. The capabilities of R are extended through user-created packages using specialized statistical techniques and graphical procedures. A core set of packages is contained within the core installation of R, with additional packages accessible from the Comprehensive R Archive Network (CRAN). Knowledge of the following packages is a must:

*   sqldf(使用 sql 的数据帧):这个函数在用 SQL 语句过滤数据时将文件读入 R。R 只处理经过过滤的部分，所以比 R 可以本地导入的文件大的文件可以用作数据源。
*   forecast(时间序列的预测):这个包提供了时间序列和线性模型的预测功能。
*   dplyr(数据聚合):在 R 中拆分、应用和组合数据的工具
*   stringr(字符串操作):简单、一致的通用字符串操作包装器
*   RODBC、RSQLite 和 RCassandra 数据库连接包:这些包用于连接数据库，在 R 之外操作数据，并支持与源系统的交互。
*   lubridate(时间和日期操作):使在 R
*   ggplot2(数据可视化):使用图形语法创建优雅的数据可视化。这是一种超级可视化能力。
*   reshape2(数据重构):灵活地重构和聚集数据，只使用两个函数:melt 和 dcast(或 acast)。
*   randomForest(随机森林预测模型):Leo Breiman 和 Adele Cutler 的分类和回归随机森林
*   gbm(广义增强回归模型):Yoav Freund 和 Robert Schapire 的 AdaBoost 算法和 Jerome Friedman 的梯度增强机

I will discuss each of these packages as I guide you through the book. In Chapter [6](06.html), I will discuss, the use of R to process the sample data within the sample framework. I will provide examples that demonstrate the basic ideas and engineering behind the framework and the tools. Please note that there are many other packages in CRAN, which is growing on a daily basis. Investigating the different packages to improve your capabilities in the R environment is time well spent.

## 斯卡拉

Scala is a general-purpose programming language. Scala supports functional programming and a strong static type system. Many high-performance data science frameworks are constructed using Scala, because of its amazing concurrency capabilities. Parallelizing masses of processing is a key requirement for large data sets from a data lake. Scala is emerging as the de-facto programming language used by data-processing tools. I provide guidance on how to use it, in the course of this book. Scala is also the native language for Spark, and it is useful to master this language.

## 计算机编程语言

Python is a high-level, general-purpose programming language created by Guido van Rossum and released in 1991\. It is important to note that it is an interpreted language: Python has a design philosophy that emphasizes code readability. Python uses a dynamic type system and automatic memory management and supports multiple programming paradigms (object-oriented, imperative, functional programming, and procedural). Thanks to its worldwide success, it has a large and comprehensive standard library. The Python Package Index (PyPI) ( [https://pypi.python.org/pypi](https://pypi.python.org/pypi) ) supplies thousands of third-party modules ready for use for your data science projects. I provide guidance on how to use it, in the course of this book. I suggest that you also install Anaconda. It is an open source distribution of Python that simplifies package management and deployment of features (see [www.continuum.io/downloads](http://www.continuum.io/downloads) ).

## 遥测传输

MQTT stands for MQ Telemetry Transport. The protocol uses publish and subscribe, extremely simple and lightweight messaging protocols. It was intended for constrained devices and low-bandwidth, high-latency, or unreliable networks. This protocol is perfect for machine-to-machine- (M2M) or Internet-of-things-connected devices. MQTT-enabled devices include handheld scanners, advertising boards, footfall counters, and other machines. In Chapter [7](07.html), I will discuss how and where you can use MQTT technology and how to make use of the essential benefits it generates. The apt use of this protocol is critical in the present and future data science environments. In Chapter [11](11.html), will discuss the use of MQTT for data collection and distribution back to the business.

## 下一步是什么？

As things change daily in the current ecosphere of ever-evolving and -increasing collections of tools and technological improvements to support data scientists, feel free to investigate technologies not included in the preceding lists. I have acquainted you with my toolbox. This Data Science Technology Stack has served me well, and I will show you how to use it to achieve success. Note My hard-earned advice is to practice with your tools. Make them your own! Spend time with them, cultivate your expertise.