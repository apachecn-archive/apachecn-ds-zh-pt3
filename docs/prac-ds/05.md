© Andreas François Vermeulen 2018 Andreas François VermeulenPractical Data Science[https://doi.org/10.1007/978-1-4842-3054-1_5](05.html)

# 5.效用层

Andreas François Vermeulen<sup class="calibre7">1 </sup> (1)West Kilbride North Ayrshire, UK   The utility layer is used to store repeatable practical methods of data science. The objective of this chapter is to define how the utility layer is used in the ecosystem. Utilities are the common and verified workhorses of the data science ecosystem. The utility layer is a central storehouse for keeping all one’s solutions utilities in one place. Having a central store for all utilities ensures that you do not use out-of-date or duplicate algorithms in your solutions. The most important benefit is that you can use stable algorithms across your solutions. Tip Collect all your utilities (including source code) in one central place. Keep records on all versions for future reference. If you use algorithms , I suggest that you keep any proof and credentials that show that the process is a high-quality, industry-accepted algorithm. Hard experience has taught me that you are likely to be tested, making it essential to prove that your science is 100% valid. The additional value is the capability of larger teams to work on a similar project and know that each data scientist or engineer is working to the identical standards. In several industries, it is a regulated requirement to use only sanctioned algorithms. On May 25, 2018, a new European Union General Data Protection Regulation (GDPR) goes into effect. The GDPR has the following rules:

*   您必须有有效的同意书作为处理的法律依据。对于您使用的任何实用程序，测试同意是至关重要的。在第[7](07.html)–[11](11.html)章，我将讨论如何在过程的每一步测试同意。
*   您必须确保透明度，提供关于收集哪些数据以及如何处理这些数据的清晰信息。公用事业公司必须生成其所有活动的完整审计跟踪。我将在第 [6](06.html) 章详细介绍这一点，关于审计追踪的要求。
*   您必须支持获得准确个人数据的权利。电力公司必须只使用最新的准确数据。我将在第 [8](08.html) 章讨论评估数据处理的技术，涵盖评估超级步骤。
*   您必须支持删除个人数据的权利。实用程序必须支持删除特定个人的所有信息。我还将讨论如果要求“被遗忘的权利”会发生什么。警告“被遗忘权”是一项要求您立即从所有系统中删除某人的请求。不遵守此类要求将导致罚款。这听起来很容易，但是从我的经历中吸取教训，确保这个请求被小心地实现。在第[7](07.html)–[11](11.html)章中，我将讨论如何在过程的每一步处理这个复杂的请求。
*   您必须获得批准才能在服务提供商之间移动数据。我建议你确保你有 100%的许可在数据提供商之间移动数据。如果您在没有明确批准的情况下将数据从客户的系统转移到您自己的系统，您和您的客户都可能会陷入法律纠纷。
*   您必须支持不受制于仅基于自动化处理的决定的权利。这个项目是我参加的许多会议辩论的主题。根据我们作为数据科学家的工作性质，我们或多或少是在进行某种形式的剖析。我们公用事业公司的行动支持我们客户的决策。在我们的实用程序中使用认可的算法使合规性更容易。

Warning Noncompliance with GDPR might incur fines of 4% of global turnover. Demonstrate compliance by maintaining a record of all data processing activities. This will be discussed in detail in Chapter [6](06.html), in the section “Audit.” In France, you must use only approved health processing rules from the National Health Data Institute. Processing of any personal health data is prohibited without the provision of an individual’s explicit consent. In the United States, the Health Insurance Portability and Accountability Act of 1996 (HIPAA) guides any processing of health data. Warning Noncompliance with HIPAA could incur fines of up to $50,000 per violation. I suggest you investigate the rules and conditions for processing any data you handle. In addition, I advise you to get your utilities certified, to show compliance. Discuss with your chief data officer what procedures are used and which prohibited procedures require checking.

## 基本公用设施设计

The basic utility must have a common layout to enable future reuse and enhancements. This standard makes the utilities more flexible and effective to deploy in a large-scale ecosystem. I use a basic design (Figure [5-1](#Fig1)) for a processing utility, by building it a three-stage process.

1.  1.根据输入协议加载数据。
2.  2.应用实用程序的处理规则。
3.  3.根据输出协议保存数据。

![A435693_1_En_5_Fig1_HTML.jpg](img/A435693_1_En_5_Fig1_HTML.jpg) Figure 5-1Basic utility design The main advantage of this methodology in the data science ecosystem is that you can build a rich set of utilities that all your data science algorithms require. That way, you have a basic pre-validated set of tools to use to perform the common processing and then spend time only on the custom portions of the project. You can also enhance the processing capability of your entire project collection with one single new utility update. Note I spend more than 80% of my non-project work time designing new utilities and algorithms to improve my delivery capability. I suggest that you start your utility layer with a small utility set that you know works well and build the set out as you go along. In this chapter, I will guide you through utilities I have found to be useful over my years of performing data science. I have split the utilities across various layers in the ecosystem, to assist you in connecting the specific utility to specific parts of the other chapters. There are three types of utilities

*   数据处理实用程序
*   维护工具
*   处理实用程序

I will discuss these types in detail over the next few pages.

### 数据处理实用程序

Data processing utilities are grouped for the reason that they perform some form of data transformation within the solutions.

#### 检索实用程序

Utilities for this superstep contain the processing chains for retrieving data out of the raw data lake into a new structured format. I will give specific implementations of these utilities in Chapter [7](07.html). I suggest that you build all your retrieve utilities to transform the external raw data lake format into the Homogeneous Ontology for Recursive Uniform Schema (HORUS) data format that I have been using in my projects. HORUS is my core data format . It is used by my data science framework, to enable the reduction of development work required to achieve a complete solution that handles all data formats. I will provide details on format in the remaining chapters. At this point, just take note that HORUS exists. Note HORUS is my foundation and the solution to my core format requirements. If you prefer, create your own format, but feel free to use mine. For demonstration purposes, I have selected the HORUS format to be CSV-based. I would normally use a JSON-based Hadoop ecosystem, or a distributed database such as Cassandra, to hold the core HORUS format. Tip Check the sample directory C:\VKHCG\05-DS\9999-Data\ for the subsequent code and data. I recommend the following retrieve utilities as a good start.

##### 文本分隔到荷鲁斯

These utilities enable your solution to import text-based data from your raw data sources. I will demonstrate this utility in Chapter [7](07.html), with sample data. Example: This utility imports a list of countries in CSV file format into HORUS format. # Utility Start CSV to HORUS ================================= # Standard Tools #============================================================= import pandas as pd # Input Agreement ============================================ sInputFileName='C:/VKHCG/05-DS/9999-Data/Country_Code.csv' InputData=pd.read_csv(sInputFileName,encoding="latin-1") print('Input Data Values ===================================') print(InputData) print('=====================================================') # Processing Rules =========================================== ProcessData=InputData # Remove columns ISO-2-Code and ISO-3-CODE ProcessData.drop('ISO-2-CODE', axis=1,inplace=True) ProcessData.drop('ISO-3-Code', axis=1,inplace=True) # Rename Country and ISO-M49 ProcessData.rename(columns={'Country': 'CountryName'}, inplace=True) ProcessData.rename(columns={'ISO-M49': 'CountryNumber'}, inplace=True) # Set new Index ProcessData.set_index('CountryNumber', inplace=True) # Sort data by CurrencyNumber ProcessData.sort_values('CountryName', axis=0, ascending=False, inplace=True) print('Process Data Values =================================') print(ProcessData) print('=====================================================') # Output Agreement =========================================== OutputData=ProcessData sOutputFileName='C:/VKHCG/05-DS/9999-Data/HORUS-CSV-Country.csv' OutputData.to_csv(sOutputFileName, index = False) print('CSV to HORUS - Done') # Utility done ===============================================

##### XML 到 HORUS

These utilities enable your solution to import XML-based data from your raw data sources. I will demonstrate this utility in Chapter [7](07.html), with sample data. Example: This utility imports a list of countries in XML file format into HORUS format. # Utility Start XML to HORUS ================================= # Standard Tools #============================================================= import pandas as pd import xml.etree.ElementTree as ET #============================================================= def df2xml(data):     header = data.columns     root = ET.Element('root')     for row in range(data.shape[0]):         entry = ET.SubElement(root,'entry')         for index in range(data.shape[1]):             schild=str(header[index])             child = ET.SubElement(entry, schild)             if str(data[schild][row]) != 'nan':                 child.text = str(data[schild][row])             else:                 child.text = 'n/a'             entry.append(child)     result = ET.tostring(root)     return result #============================================================= def xml2df(xml:data):     root = ET.XML(xml:data)     all_records = []     for i, child in enumerate(root):         record = {}         for subchild in child:             record[subchild.tag] = subchild.text         all_records.append(record)     return pd.DataFrame(all_records) #============================================================= # Input Agreement ============================================ #============================================================= sInputFileName='C:/VKHCG/05-DS/9999-Data/Country_Code.xml' InputData = open(sInputFileName).read() print('=====================================================') print('Input Data Values ===================================') print('=====================================================') print(InputData) print('=====================================================') #============================================================= # Processing Rules =========================================== #============================================================= ProcessDataXML=InputData # XML to Data Frame ProcessData=xml2df(ProcessDataXML) # Remove columns ISO-2-Code and ISO-3-CODE ProcessData.drop('ISO-2-CODE', axis=1,inplace=True) ProcessData.drop('ISO-3-Code', axis=1,inplace=True) # Rename Country and ISO-M49 ProcessData.rename(columns={'Country': 'CountryName'}, inplace=True) ProcessData.rename(columns={'ISO-M49': 'CountryNumber'}, inplace=True) # Set new Index ProcessData.set_index('CountryNumber', inplace=True) # Sort data by CurrencyNumber ProcessData.sort_values('CountryName', axis=0, ascending=False, inplace=True) print('=====================================================') print('Process Data Values =================================') print('=====================================================') print(ProcessData) print('=====================================================') #============================================================= # Output Agreement =========================================== #============================================================= OutputData=ProcessData sOutputFileName='C:/VKHCG/05-DS/9999-Data/HORUS-XML-Country.csv' OutputData.to_csv(sOutputFileName, index = False) print('=====================================================') print('XML to HORUS - Done') print('=====================================================') # Utility done ===============================================

##### JSON 到 HORUS

These utilities enable your solution to import XML-based data from your raw data sources. I will demonstrate this utility in Chapter [7](07.html), with sample data. Example: This utility imports a list of countries in JSON file format into HORUS format. # Utility Start JSON to HORUS ================================= # Standard Tools #============================================================= import pandas as pd # Input Agreement ============================================ sInputFileName='C:/VKHCG/05-DS/9999-Data/Country_Code.json' InputData=pd.read_json(sInputFileName,                        orient='index',                        encoding="latin-1") print('Input Data Values ===================================') print(InputData) print('=====================================================') # Processing Rules =========================================== ProcessData=InputData # Remove columns ISO-2-Code and ISO-3-CODE ProcessData.drop('ISO-2-CODE', axis=1,inplace=True) ProcessData.drop('ISO-3-Code', axis=1,inplace=True) # Rename Country and ISO-M49 ProcessData.rename(columns={'Country': 'CountryName'}, inplace=True) ProcessData.rename(columns={'ISO-M49': 'CountryNumber'}, inplace=True) # Set new Index ProcessData.set_index('CountryNumber', inplace=True) # Sort data by CurrencyNumber ProcessData.sort_values('CountryName', axis=0, ascending=False, inplace=True) print('Process Data Values =================================') print(ProcessData) print('=====================================================') # Output Agreement =========================================== OutputData=ProcessData sOutputFileName='C:/VKHCG/05-DS/9999-Data/HORUS-JSON-Country.csv' OutputData.to_csv(sOutputFileName, index = False) print('JSON to HORUS - Done') # Utility done ===============================================

##### 荷鲁斯的数据库

These utilities enable your solution to import data from existing database sources. I will demonstrate this utility in Chapter [7](07.html), with sample data. Example: This utility imports a list of countries in SQLite data format into HORUS format. # Utility Start Database to HORUS ================================= # Standard Tools #============================================================= import pandas as pd import sqlite3 as sq # Input Agreement ============================================ sInputFileName='C:/VKHCG/05-DS/9999-Data/utility.db' sInputTable='Country_Code' conn = sq.connect(sInputFileName) sSQL='select * FROM ' + sInputTable + ';' InputData=pd.read_sql_query(sSQL, conn) print('Input Data Values ===================================') print(InputData) print('=====================================================') # Processing Rules =========================================== ProcessData=InputData # Remove columns ISO-2-Code and ISO-3-CODE ProcessData.drop('ISO-2-CODE', axis=1,inplace=True) ProcessData.drop('ISO-3-Code', axis=1,inplace=True) # Rename Country and ISO-M49 ProcessData.rename(columns={'Country': 'CountryName'}, inplace=True) ProcessData.rename(columns={'ISO-M49': 'CountryNumber'}, inplace=True) # Set new Index ProcessData.set_index('CountryNumber', inplace=True) # Sort data by CurrencyNumber ProcessData.sort_values('CountryName', axis=0, ascending=False, inplace=True) print('Process Data Values =================================') print(ProcessData) print('=====================================================') # Output Agreement =========================================== OutputData=ProcessData sOutputFileName='C:/VKHCG/05-DS/9999-Data/HORUS-CSV-Country.csv' OutputData.to_csv(sOutputFileName, index = False) print('Database to HORUS - Done') # Utility done =============================================== There are also additional expert utilities that you may want.

##### 图片给荷鲁斯

These expert utilities enable your solution to convert a picture into extra data. These utilities identify objects in the picture, such as people, types of objects, locations, and many more complex data features. I will discuss this in more detail in Chapter [7](07.html). Example: This utility imports a picture of a dog called Angus in JPG format into HORUS format. # Utility Start Picture to HORUS ================================= # Standard Tools #============================================================= from scipy.misc import imread import pandas as pd import matplotlib.pyplot as plt import numpy as np # Input Agreement ============================================ sInputFileName='C:/VKHCG/05-DS/9999-Data/Angus.jpg' InputData = imread(sInputFileName, flatten=False, mode="RGBA") print('Input Data Values ===================================') print('X: ',InputData.shape[0]) print('Y: ',InputData.shape[1]) print('RGBA: ', InputData.shape[2]) print('=====================================================') # Processing Rules =========================================== ProcessRawData=InputData.flatten() y=InputData.shape[2] + 2 x=int(ProcessRawData.shape[0]/y) ProcessData=pd.DataFrame(np.reshape(ProcessRawData, (x, y))) sColumns= ['XAxis','YAxis','Red', 'Green', 'Blue','Alpha'] ProcessData.columns=sColumns ProcessData.index.names =['ID'] print('Rows: ',ProcessData.shape[0]) print('Columns :',ProcessData.shape[1]) print('=====================================================') print('Process Data Values =================================') print('=====================================================') plt.imshow(InputData) plt.show() print('=====================================================') # Output Agreement =========================================== OutputData=ProcessData print('Storing File') sOutputFileName='C:/VKHCG/05-DS/9999-Data/HORUS-Picture.csv' OutputData.to_csv(sOutputFileName, index = False) print('=====================================================') print('Picture to HORUS - Done') print('=====================================================') # Utility done ===============================================

##### 视频给荷鲁斯

These expert utilities enable your solution to convert a video into extra data. These utilities identify objects in the video frames, such as people, types of objects, locations, and many more complex data features. I will provide more detail on these in Chapter [7](07.html). Example: This utility imports a movie in MP4 format into HORUS format. The process is performed in two stages.

###### 电影到帧

# Utility Start Movie to HORUS (Part 1) ====================== # Standard Tools #============================================================= import os import shutil import cv2 #============================================================= sInputFileName='C:/VKHCG/05-DS/9999-Data/dog.mp4' sDataBaseDir='C:/VKHCG/05-DS/9999-Data/temp' if os.path.exists(sDataBaseDir):     shutil.rmtree(sDataBaseDir) if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) print('=====================================================') print('Start Movie to Frames') print('=====================================================')     vidcap = cv2.VideoCapture(sInputFileName) success,image = vidcap.read() count = 0 while success:     success,image = vidcap.read()     sFrame=sDataBaseDir + str('/dog-frame-' + str(format(count, '04d')) + '.jpg')     print('Extracted: ', sFrame)     cv2.imwrite(sFrame, image)       if os.path.getsize(sFrame) == 0:         count += -1         os.remove(sFrame)         print('Removed: ', sFrame)     if cv2.waitKey(10) == 27: # exit if Escape is hit         break     count += 1 print('=====================================================') print('Generated : ', count, ' Frames') print('=====================================================') print('Movie to Frames HORUS - Done') print('=====================================================') # Utility done =============================================== I have now created frames and need to load them into HORUS.

###### 框架到荷鲁斯

# Utility Start Movie to HORUS (Part 2) ====================== # Standard Tools #============================================================= from scipy.misc import imread import pandas as pd import matplotlib.pyplot as plt import numpy as np import os # Input Agreement ============================================ sDataBaseDir='C:/VKHCG/05-DS/9999-Data/temp' f=0 for file in os.listdir(sDataBaseDir):     if file.endswith(".jpg"):         f += 1         sInputFileName=os.path.join(sDataBaseDir, file)         print('Process : ', sInputFileName)         InputData = imread(sInputFileName, flatten=False, mode="RGBA")         print('Input Data Values ===================================')         print('X: ',InputData.shape[0])         print('Y: ',InputData.shape[1])         print('RGBA: ', InputData.shape[2])         print('=====================================================')         # Processing Rules ===========================================         ProcessRawData=InputData.flatten()         y=InputData.shape[2] + 2         x=int(ProcessRawData.shape[0]/y)         ProcessFrameData=pd.DataFrame(np.reshape(ProcessRawData, (x, y)))         ProcessFrameData['Frame']=file         print('=====================================================')         print('Process Data Values =================================')         print('=====================================================')         plt.imshow(InputData)         plt.show()         if f == 1:             ProcessData=ProcessFrameData         else:             ProcessData=ProcessData.append(ProcessFrameData) if f > 0:     sColumns= ['XAxis','YAxis','Red', 'Green', 'Blue','Alpha','FrameName']     ProcessData.columns=sColumns     print('=====================================================')     ProcessFrameData.index.names =['ID']     print('Rows: ',ProcessData.shape[0])     print('Columns :',ProcessData.shape[1])     print('=====================================================')     # Output Agreement ===========================================     OutputData=ProcessData     print('Storing File')     sOutputFileName='C:/VKHCG/05-DS/9999-Data/HORUS-Movie-Frame.csv'     OutputData.to_csv(sOutputFileName, index = False) print('=====================================================') print('Processed ; ', f,' frames') print('=====================================================') print('Movie to HORUS - Done') print('=====================================================') # Utility done ===============================================

##### HORUS 的音频

These expert utilities enable your solution to convert an audio into extra data. These utilities identify objects in the video frames, such as people, types of objects, locations, and many more complex data features. I will discuss these in more detail in Chapter [7](07.html). Example: This utility imports a set of audio files in WAV format into HORUS format. # Utility Start Audio to HORUS =============================== # Standard Tools #============================================================= from scipy.io import wavfile import pandas as pd import matplotlib.pyplot as plt import numpy as np #============================================================= def show_info(aname, a,r):     print ('----------------')     print ("Audio:", aname)     print ('----------------')     print ("Rate:", r)     print ('----------------')     print ("shape:", a.shape)     print ("dtype:", a.dtype)     print ("min, max:", a.min(), a.max())     print ('----------------')     plot_info(aname, a,r) #============================================================= def plot_info(aname, a,r):             sTitle= 'Signal Wave - '+ aname + ' at ' + str(r) + 'hz'     plt.title(sTitle)     sLegend=[]     for c in range(a.shape[1]):         sLabel = 'Ch' + str(c+1)         sLegend=sLegend+[str(c+1)]         plt.plot(a[:,c], label=sLabel)     plt.legend(sLegend)     plt.show() #============================================================= sInputFileName='C:/VKHCG/05-DS/9999-Data/2ch-sound.wav' print('=====================================================') print('Processing : ', sInputFileName) print('=====================================================') InputRate, InputData = wavfile.read(sInputFileName) show_info("2 channel", InputData,InputRate) ProcessData=pd.DataFrame(InputData) sColumns= ['Ch1','Ch2'] ProcessData.columns=sColumns OutputData=ProcessData sOutputFileName='C:/VKHCG/05-DS/9999-Data/HORUS-Audio-2ch.csv' OutputData.to_csv(sOutputFileName, index = False) #============================================================= sInputFileName='C:/VKHCG/05-DS/9999-Data/4ch-sound.wav' print('=====================================================') print('Processing : ', sInputFileName) print('=====================================================') InputRate, InputData = wavfile.read(sInputFileName) show_info("4 channel", InputData,InputRate) ProcessData=pd.DataFrame(InputData) sColumns= ['Ch1','Ch2','Ch3', 'Ch4'] ProcessData.columns=sColumns OutputData=ProcessData sOutputFileName='C:/VKHCG/05-DS/9999-Data/HORUS-Audio-4ch.csv' OutputData.to_csv(sOutputFileName, index = False) #============================================================= sInputFileName='C:/VKHCG/05-DS/9999-Data/6ch-sound.wav' print('=====================================================') print('Processing : ', sInputFileName) print('=====================================================') InputRate, InputData = wavfile.read(sInputFileName) show_info("6 channel", InputData,InputRate) ProcessData=pd.DataFrame(InputData) sColumns= ['Ch1','Ch2','Ch3', 'Ch4', 'Ch5','Ch6'] ProcessData.columns=sColumns OutputData=ProcessData sOutputFileName='C:/VKHCG/05-DS/9999-Data/HORUS-Audio-6ch.csv' OutputData.to_csv(sOutputFileName, index = False) #============================================================= sInputFileName='C:/VKHCG/05-DS/9999-Data/8ch-sound.wav' print('=====================================================') print('Processing : ', sInputFileName) print('=====================================================') InputRate, InputData = wavfile.read(sInputFileName) show_info("8 channel", InputData,InputRate) ProcessData=pd.DataFrame(InputData) sColumns= ['Ch1','Ch2','Ch3', 'Ch4', 'Ch5','Ch6','Ch7','Ch8'] ProcessData.columns=sColumns OutputData=ProcessData sOutputFileName='C:/VKHCG/05-DS/9999-Data/HORUS-Audio-8ch.csv' OutputData.to_csv(sOutputFileName, index = False) print('=====================================================') print('Audio to HORUS - Done') print('=====================================================') #============================================================= # Utility done =============================================== #=============================================================

##### 流向荷鲁斯的数据流

These expert utilities enable your solution to handle data streams. Data streams are evolving as the fastest-growing data collecting interface at the edge of the data lake. I will offer extended discussions and advice later in the book on the use of data streaming in the data science ecosystem. Tip I use a package called python-confluent-kafka for my Kafka streaming requirements. I have also used PyKafka with success. In the Retrieve superstep of the functional layer (Chapter [7](07.html)), I dedicate more text to clarifying how to use and generate full processing chains to retrieve data from your data lake, using optimum techniques.

#### 评估公用事业

Utilities for this superstep contain all the processing chains for quality assurance and additional data enhancements. I will provide specific implementations for these utilities in Chapter [8](08.html). The assess utilities ensure that the data imported via the Retrieve superstep are of a good quality, to ensure it conforms to the prerequisite standards of your solution. I perform feature engineering at this level, to improve the data for better processing success in the later stages of the data processing. There are two types of assess utilities:

##### 特征工程

Feature engineering is the process by which you enhance or extract data sources, to enable better extraction of characteristics you are investigating in the data sets. Following is a small subset of the utilities you may use. I will cover many of these in Chapter [8](08.html).

###### 修理工具

Fixers enable your solution to take your existing data and fix a specific quality issue. Examples include

*   从数据条目中删除前导或滞后空格 Python 中的示例:baddata = "空格太多的数据科学不好！！! "print(' > '，baddata，'【T0]'，cleandata，'
*   从数据条目中移除不可打印字符 Python 中的示例:import string printable = set(string . printable)baddata = " Data \ x00 带有\x02 滑稽字符的科学是\x10bad！！!"cleandata=“”。join(filter(lambda x:x in string . printable，baddata)) print(cleandata)
*   重新格式化数据条目以匹配特定的格式标准。将 2017/01/31 转换为 2017 年 1 月 31 日 Python 示例:将日期时间作为 dt baddate = dt.date(2017，1，31) baddata=format(baddate，' % Y-% m-% d ')print(baddata)gooddate = dt . datetime . strptime(baddata，' % Y-% m-% d ')gooddata = format(gooddate，' %d %B %Y') print(gooddata)

In Chapter [8](08.html), I will consider different complementary utilities.

###### 加法器实用程序

Adders use existing data entries and then add additional data entries to enhance your data. Examples include

*   根据解决方案中的现有数据条目查找额外数据的实用程序。实用程序可以使用国家列表的联合国 ISO M49 来查找 826，以将国家名称设置为英国。另一个实用程序使用 ISO alpha-2 查找 GB 将国家名称返回到英国。
*   基于测试通过额外数据条目添加的分区数据。该实用程序可以指示数据条目是有效的，即您在查找中找到了代码。实用程序可以指示您的银行余额数据条目是黑色还是红色。

I will discuss many of these utilities in Chapter [8](08.html).

###### 过程实用程序

Utilities for this superstep contain all the processing chains for building the data vault. In Chapter [9](09.html), I will provide specific implementations for these utilities. I will discuss the data vault’s (Time, Person, Object, Location, Event) design, model, and inner workings in detail during the Process supersstep of the functional layer (Chapter [9](09.html)). For the purposes of this chapter, I will at this point introduce the data vault as a data structure that uses well-structured design to store data with full history. The basic elements of the data vault are hubs, satellites, and links. Full details on the structure and how to build the data vault is explained in Chapter [9](09.html), covering the Process superstep. In this chapter, I will note only some basic concepts. There are three basic process utilities.

#### 数据仓库实用程序

The data vault is a highly specialist data storage technique that was designed by Dan Linstedt. The data vault is a detail-oriented, historical-tracking, and uniquely linked set of normalized tables that support one or more functional areas of business. It is a hybrid approach encompassing the best of breed between 3rd normal form (3NF) and star schema. I will discuss the use of this configuration in detail in Chapter [9](09.html). A basic example is shown in Figure [5-2](#Fig2).![A435693_1_En_5_Fig2_HTML.jpg](img/A435693_1_En_5_Fig2_HTML.jpg) Figure 5-2 Simple Data Vault

##### 集线器实用程序

Hub utilities ensure that the integrity of the data vault’s (Time, Person, Object, Location, Event) hubs is 100% correct, to verify that the vault is working as designed.

##### 卫星应用

Satellite utilities ensure the integrity of the specific satellite and its associated hub.

##### 链接实用程序

Link utilities ensure the integrity of the specific link and its associated hubs. As the data vault is a highly structured data model, the utilities in the Process superstep of the functional layer (see Chapter [9](09.html)) will assist you in building your own solution.

#### 转换实用程序

Utilities for this superstep contain all the processing chains for building the data warehouse from the results of your practical data science. In Chapter [10](10.html), I will provide specific implementations for these utilities. I will discuss the data science and data warehouse design, model, and inner workings in detail in Chapter [10](10.html). I will, therefore, only note some basic data warehouse concepts in this chapter. In the Transform superstep, the system builds dimensions and facts to prepare a data warehouse, via a structured data configuration, for the algorithms in data science to use to produce data science discoveries. There are two basic transform utilities.

##### 维度实用程序

The dimensions use several utilities to ensure the integrity of the dimension structure. Concepts such as conformed dimension, degenerate dimension, role-playing dimension, mini-dimension, outrigger dimension, slowly changing dimension, late-arriving dimension, and dimension types (0, 1, 2, 3) will be discussed in detail in Chapter [10](10.html). They all require specific utilities to ensure 100% integrity of the dimension structure. Pure data science algorithms are the most used at this point in your solution. I will discuss extensively in Chapter [10](10.html) what data science algorithms are required to perform practical data science. I will ratify that the most advanced of these are standard algorithms, which will result in common utilities.

##### 事实实用程序

These consist of a number of utilities that ensure the integrity of the dimensions structure and the facts. There are various statistical and data science algorithms that can be applied to the facts that will result in additional utilities. Note The most important utilities for your data science will be transform utilities, as they hold the accredited data science you need for your solution to be successful.

#### 数据科学实用程序

There are several data science–specific utilities that are required for you to achieve success in the data processing ecosystem.

##### 数据宁滨或分桶

Binning is a data preprocessing technique used to reduce the effects of minor observation errors. Statistical data binning is a way to group a number of more or less continuous values into a smaller number of “bins.” Example: Open your Python editor and create a file called DU-Histogram.py in the directory C:\VKHCG\05-DS\4000-UL\0200-DU. Now copy this code into the file, as follows: import numpy as np import matplotlib.mlab as mlab import matplotlib.pyplot as plt np.random.seed(0) # example data mu = 90  # mean of distribution sigma = 25  # standard deviation of distribution x = mu + sigma * np.random.randn(5000) num_bins = 25 fig, ax = plt.subplots() # the histogram of the data n, bins, patches = ax.hist(x, num_bins, normed=1) # add a 'best fit' line y = mlab.normpdf(bins, mu, sigma) ax.plot(bins, y, '--') ax.set_xlabel('Example Data') ax.set_ylabel('Probability density') sTitle=r'Histogram ' + str(len(x)) + ' entries into ' + str(num_bins) + ' Bins: $\mu=' + str(mu) + '$, $\sigma=' + str(sigma) + '$' ax.set_title(sTitle) fig.tight_layout() plt.show() As you can see in Figure [5-3](#Fig3), the binning reduces the 5000 data entries to only 25, with close-to-reality values. ![A435693_1_En_5_Fig3_HTML.jpg](img/A435693_1_En_5_Fig3_HTML.jpg) Figure 5-3Histogram showing reduced minor observation errors

##### 数据平均

The use of averaging of features value enables the reduction of data volumes in a control fashion to improve effective data processing. Example: Open your Python editor and create a file called DU-Mean.py in the directory C:\VKHCG\05-DS\4000-UL\0200-DU. The following code reduces the data volume from 3562 to 3 data entries, which is a 99.91% reduction. ################################################################ import pandas as pd ################################################################ InputFileName='IP_DATA_CORE.csv' OutputFileName='Retrieve_Router_Location.csv' ################################################################ Base='C:/VKHCG' sFileName=Base + '/01-Vermeulen/00-RawData/' + InputFileName print('Loading :',sFileName) IP_DATA_ALL=pd.read_csv(sFileName,header=0,low_memory=False,   usecols=['Country','Place Name','Latitude','Longitude'], encoding="latin-1") IP_DATA_ALL.rename(columns={'Place Name': 'Place_Name'}, inplace=True) AllData=IP_DATA_ALL[['Country', 'Place_Name','Latitude']] print(AllData) MeanData=AllData.groupby(['Country', 'Place_Name'])['Latitude'].mean() print(MeanData) ################################################################ This technique also enables the data science to prevent a common issue called overfitting the model.; I will discuss this issue in detail in Chapter [10](10.html).

##### 离群点检测

Outliers are data that is so different from the rest of the data in the data set that it may be caused by an error in the data source. There is a technique called outlier detection that, with good data science, will identify these outliers. Example: Open your Python editor and create a file DU-Outliers.py in the directory C:\VKHCG\05-DS\4000-UL\0200-DU. ################################################################ import pandas as pd ################################################################ InputFileName='IP_DATA_CORE.csv' OutputFileName='Retrieve_Router_Location.csv' ################################################################ Base='C:/VKHCG' sFileName=Base + '/01-Vermeulen/00-RawData/' + InputFileName print('Loading :',sFileName) IP_DATA_ALL=pd.read_csv(sFileName,header=0,low_memory=False,   usecols=['Country','Place Name','Latitude','Longitude'], encoding="latin-1") IP_DATA_ALL.rename(columns={'Place Name': 'Place_Name'}, inplace=True) LondonData=IP_DATA_ALL.loc[IP_DATA_ALL['Place_Name']=='London'] AllData=LondonData[['Country', 'Place_Name','Latitude']] print('All Data') print(AllData) MeanData=AllData.groupby(['Country', 'Place_Name'])['Latitude'].mean() StdData=AllData.groupby(['Country', 'Place_Name'])['Latitude'].std() print('Outliers') UpperBound=float(MeanData+StdData) print('Higher than ', UpperBound) OutliersHigher=AllData[AllData.Latitude>UpperBound] print(OutliersHigher) LowerBound=float(MeanData-StdData) print('Lower than ', LowerBound) OutliersLower=AllData[AllData.Latitude<LowerBound] print(OutliersLower) print('Not Outliers') OutliersNot=AllData[(AllData.Latitude>=LowerBound) & (AllData.Latitude<=UpperBound)] print(OutliersNot) ################################################################

#### 组织实用程序

Utilities for this superstep contain all the processing chains for building the data marts. The organize utilities are mostly used to create data marts against the data science results stored in the data warehouse dimensions and facts. Details on these are provided in Chapter [11](11.html).

#### 报告实用程序

Utilities for this superstep contain all the processing chains for building virtualization and reporting of the actionable knowledge. The report utilities are mostly used to create data virtualization against the data science results stored in the data marts. Details on these are provided in Chapter [11](11.html).

### 维护工具

The data science solutions you are building are a standard data system and, consequently, require maintenance utilities, as with any other system. Data engineers and data scientists must work together to ensure that the ecosystem works at its most efficient level at all times. Utilities cover several areas.

#### 备份和恢复实用程序

These perform different types of database backups and restores for the solution. They are standard for any computer system. For the specific utilities, I suggest you have an in-depth discussion with your own systems manager or the systems manager of your client. I normally provide a wrapper for the specific utility that I can call in my data science ecosystem, without direct exposure to the custom requirements at each customer.

#### 检查数据完整性实用程序

These utilities check the allocation and structural integrity of database objects and indexes across the ecosystem, to ensure the accurate processing of the data into knowledge. I will discuss specific utilities in Chapters [7](07.html)–[11](11.html).

#### 历史清理实用程序

These utilities archive and remove entries in the history tables in the databases. I will discuss specific cleanup utilities in Chapters [7](07.html)–[11](11.html). Note The “right-to-be-forgotten” statute in various countries around the world imposes multifaceted requirements in this area of data science to be able to implement selective data processing. Warning I suggest you look at your information protection laws in detail, because the processing of data now via data science is becoming highly exposed, and in a lot of countries, fines are imposed if you get these processing rules wrong.

#### 维护清理实用程序

These utilities remove artifacts related to maintenance plans and database backup files. I will discuss these utilities in detail in Chapter [6](06.html).

#### 通知操作员实用程序

Utilities that send notification messages to the operations team about the status of the system are crucial to any data science factory. These utilities are discussed in detail in Chapter [6](06.html).

#### 重建数据结构实用程序

These utilities rebuild database tables and views to ensure that all the development is as designed. In Chapters [6](06.html)–[11](11.html), I will discuss the specific rebuild utilities.

#### 重组索引实用程序

These utilities reorganize indexes in database tables and views, which is a major operational process when your data lake grows at a massive volume and velocity. The variety of data types also complicates the application of indexes to complex data structures. In Chapters [6](06.html)–[11](11.html), I will discuss the specific rebuild utilities. As a data scientist, you must understand when and how your data sources will change. An unclear indexing strategy could slow down algorithms without your taking note, and you could lose data, owing to your not handling the velocity of the data flow.

#### 收缩/移动数据结构实用程序

These reduce the footprint size of your database data and associated log artifacts, to ensure an optimum solution is executing. I will discuss specific rebuild utilities in Chapters [6](06.html)–[11](11.html).

#### 解决方案统计实用程序

These utilities update information about the data science artifacts, to ensure that your data science structures are recorded. Call it data science on your data science. These utilities will be discussed in detail in Chapter [6](06.html). The preceding list is a comprehensive, but not all-inclusive. I suggest that you speak to your development and operations organization staff, to ensure that your data science solution fits into the overall data processing structures of your organization.

### 处理实用程序

The data science solutions you are building require processing utilities to perform standard system processing. The data science environment requires two basic processing utility types.

#### 调度实用程序

The scheduling utilities I use are based on the basic agile scheduling principles.

##### 积压实用程序

Backlog utilities accept new processing requests into the system and are ready to be processed in future processing cycles.

##### 待办事项实用程序

The to-do utilities take a subset of backlog requests for processing during the next processing cycle. They use classification labels, such as priority and parent-child relationships, to decide what process runs during the next cycle.

##### 做公用事业

The doing utilities execute the current cycle’s requests.

##### 完成实用程序

The done utilities confirm that the completed requests performed the expected processing.

#### 监控实用程序

The monitoring utilities ensure that the complete system is working as expected.

## 设计一个实用的应用层

Any source code or other supplementary material referenced by me in this book is available to readers on GitHub, via this book’s product page, located at [www.apress.com/9781484230534](http://www.apress.com/9781484230534) . Note See the following source code from Chapter [2](02.html): ./VKHCG/05-DS/4000-UL. The utility layer holds all the utilities you share across the data science environment. I suggest that you create three sublayers to help the utility layer support better future use of the utilities.

### 维护实用程序

Collect all the maintenance utilities in this single directory, to enable the environment to handle the utilities as a collection. Note See the following source code from Chapter [2](02.html): ./VKHCG/05-DS/4000-UL /4000-UL/0100-MU. I suggest that you keep a maintenance utilities registry, to enable your entire team to use the common utilities. Include enough documentation for each maintenance utility, to explain its complete workings and requirements.

### 数据实用程序

Collect all the data utilities in this single directory, to enable the environment to handle the utilities as a collection. Note See the following source code from Chapter [2](02.html): ./VKHCG/05-DS/4000-UL /4000-UL/0200-DU. I suggest that you keep a data utilities registry to enable your entire team to use the common utilities. Include enough documentation for each data utility to explain its complete workings and requirements.

### 处理实用程序

Collect all the processing utilities in this single directory to enable the environment to handle the utilities as a collection. Note See the following source code from Chapter [2](02.html): ./VKHCG/05-DS/4000-UL /4000-UL/0300-PU. I suggest that you keep a processing utilities registry, to enable your entire team to use the common utilities. Include sufficient documentation for each processing utility to explain its complete workings and requirements. Warning Ensure that you support your company’s processing environment and that the suggested environment supports an agile processing methodology. This may not always match your own environment. Caution Remember: These utilities are used by your wider team, if you interrupt them, you will pause other current working processing. Take extra care with this layer’s artifacts.

## 摘要

I have completed the utility layer that supports the common utilities in the data science environment. This layer will evolve as your data science improves. In the beginning of your environment, you will add numerous extra utilities, but in time, you will reuse the artifacts you already own and trust. You must be at ease with the utility layer now and understand that growing it in keeping with your requirements means that you have successfully completed this chapter. Remember A good utility layer will continuously evolve. That is normal. In next chapter, I will move on to the three management layers.