© Andreas François Vermeulen 2018 Andreas François VermeulenPractical Data Science[https://doi.org/10.1007/978-1-4842-3054-1_4](04.html)

# 4.业务层

Andreas François Vermeulen<sup class="calibre7">1 </sup> (1)West Kilbride North Ayrshire, UK   In this chapter, I define the business layer in detail, clarifying why, where, and what functional and nonfunctional requirements are presented in the data science solution. With the aid of examples, I will help you to engineer a practical business layer and advise you, as I explain the layer in detail and discuss methods to assist you in performing good data science. The business layer is the transition point between the nontechnical business requirements and desires and the practical data science, where, I suspect, most readers of this book will have a tendency to want to spend their careers, doing the perceived more interesting data science. The business layer does not belong to the data scientist 100%, and normally, its success represents a joint effort among such professionals as business subject matter experts, business analysts, hardware architects, and data scientists.

## 业务层

The business layer is where we record the interactions with the business. This is where we convert business requirements into data science requirements. If you want to process data and wrangle with your impressive data science skills, this chapter may not be the start of a book about practical data science that you would expect. I suggest, however, that you read this chapter, if you want to work in a successful data science group. As a data scientist, you are not in control of all aspects of a business, but you have a responsibility to ensure that you identify the true requirements. Warning I have seen too many data scientists blamed for bad science when the issue was bad requirement gathering.

### 功能要求

Functional requirements record the detailed criteria that must be followed to realize the business’s aspirations from its real-world environment when interacting with the data science ecosystem. These requirements are the business’s view of the system, which can also be described as the “Will of the Business.” Tip Record all the business’s aspirations. Make everyone supply their input. You do not want to miss anything, as later additions are expensive and painful for all involved. I use the MoSCoW method (Table [4-1](#Tab1)) as a prioritization technique, to indicate how important each requirement is to the business. I revisit all outstanding requirements before each development cycle, to ensure that I concentrate on the requirements that are of maximum impact to the business at present, as businesses evolve, and you must be aware of their true requirements.Table 4-1 MoSCoW Options

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"></colgroup> 
| 必须有 | 优先级为“必须拥有”的需求对于当前的交付周期至关重要。 |
| --- | --- |
| 应该有 | 优先级为“应该拥有”的需求很重要，但是对于当前的交付周期来说并不是必需的。 |
| 本能够 | 优先考虑为“可能有”的需求是那些期望的但不是必需的，也就是说，在当前的交付周期中必须改善用户体验。 |
| 不会有 | 具有“不会”优先级的需求是那些被涉众确定为最不重要的、回报最低的需求，或者在交付周期的那个时候不合适的需求。 |

#### 一般功能要求

As a [user role] I want [goal] so that [business value] is achieved.

#### 特定功能要求

The following requirements specific to data science environments will assist you in creating requirements that enable you to transform a business’s aspirations into technical descriptive requirements. I have found these techniques highly productive in aligning requirements with my business customers, while I can easily convert or extend them for highly technical development requirements.

##### 数据映射矩阵

The data mapping matrix is one of the core functional requirement recording techniques used in data science. It tracks every data item that is available in the data sources. I advise that you keep this useful matrix up to date as you progress through the processing layers.

##### 太阳模型

The base sun models process was developed by Professor Mark Whitehorn ( [www.penguinsoft.co.uk/mark.html](http://www.penguinsoft.co.uk/mark.html) ) and was taught to me in a master’s degree course. (Thanks, Mark.) I have added changes and evolved the process for my own consultancy process, mixing it with Dr. Ralph Kimball’s wisdoms, Bill Inmon’s instructions, and the great insights of several other people I have met in the course of my career. These techniques will continue to evolve with fluctuations in data science, and we will have to adapt. So, here is version 2.0 sun models. Tip Learn to evolve continually to survive! The sun models is a requirement mapping technique that assists you in recording requirements at a level that allows your nontechnical users to understand the intent of your analysis, while providing you with an easy transition to the detailed technical modeling of your data scientist and data engineer. Note Over the next few pages, I will introduce several new concepts. Please read on, as the section will help in explaining the complete process. I will draft a sun model set as part of an example and explain how it guides your development and knowledge insights. First, I must guide you through the following basic knowledge , to ensure that you have the background you require to navigate subsequent chapters. This sun model is for fact LivesAt and supports three dimensions: person, location, and date. Figure [4-1](#Fig1) shows a typical sun model.![A435693_1_En_4_Fig1_HTML.jpg](Images/A435693_1_En_4_Fig1_HTML.jpg) Figure 4-1 Typical sun model This can be used to store the fact that a specific person lived at a specific location since a specific date. So, if you want to record that Dr Jacob Roggeveen lived on Rapa Nui, Easter Island, since April 5, 1722, your data would fit into the sun model, as seen in Figure [4-2](#Fig2).![A435693_1_En_4_Fig2_HTML.jpg](Images/A435693_1_En_4_Fig2_HTML.jpg) Figure 4-2 Dr Jacob Roggeveen record in sun model See if you can record your own information in the same sun model. Next, I’ll discuss the components of the sun model.

###### 规模

A dimension is a structure that categorizes facts and measures, to enable you to respond to business questions. A slowly changing dimension is a data structure that stores the complete history of the data loads in the dimension structure over the life cycle of the data lake. There are several types of Slowly Changing Dimensions (SCDs) in the data warehousing design toolkit that enable different recording rules of the history of the dimension. SCD Type 1—Only Update This dimension contains only the latest value for specific business information. Example: Dr Jacob Roggeveen lived on Rapa Nui, Easter Island, but now lives in Middelburg, Netherlands.

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 负荷 | 西方人名的第一个字 | 姓 | 地址 |
| --- | --- | --- | --- |
| one | 雅各 | 罗格芬 | 复活节岛拉帕努伊岛 |
| Two | 雅各 | 罗格芬 | 荷兰米德尔堡 |

The SCD Type 1 records only the first place Dr Jacob Roggeveen lived and ignores the second load’s changes.

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 人 |   | 位置 |
| --- | --- | --- |
| 西方人名的第一个字 | 姓 | 地址 |
| 雅各 | 罗格芬 | 复活节岛拉帕努伊岛 |

I find this useful for storing the first address you register for a user, or the first product he or she has bought. SCD Type 2—Keeps Complete History This dimension contains the complete history of the specific business information. There are three ways to construct a SCD Type 2.

1.  1.版本化:SCD 保存了版本号的日志，允许您按照进行更改的顺序读回数据。
2.  2.标记:SCD 将业务数据行的最新版本标记为 1，其余标记为 0。这样，您总是可以获得最新的行。
3.  3.生效日期:SCD 记录特定值对业务数据行有效的时间段。

Hint The best SCD for data science is SCD Type 2 with Effective Date. Example: Dr Jacob Roggeveen lived in Middelburg, Netherlands, and then in Rapa Nui, Easter Island, but now lives in Middelburg, Netherlands.

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 负荷 | 西方人名的第一个字 | 姓 | 地址 |
| --- | --- | --- | --- |
| one | 雅各 | 罗格芬 | 荷兰米德尔堡 |
| Two | 雅各 | 罗格芬 | 复活节岛拉帕努伊岛 |
| three | 雅各 | 罗格芬 | 荷兰米德尔堡 |

SCD Type 2 records only changes in the location Dr Jacob Roggeveen lived, as it loads the three data loads, using versioning.

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 人 |   |   | 位置 |   |
| --- | --- | --- | --- | --- |
| 西方人名的第一个字 | 姓 | 版本 | 地址 | 版本 |
| 雅各 | 罗格芬 | one | 荷兰米德尔堡 | one |
|   |   |   | 复活节岛拉帕努伊岛 | Two |
|   |   |   | 荷兰米德尔堡 | three |

I find this useful for storing all the addresses at which your customer has lived, without knowing from what date to what date he or she was at each address. SCD Type 2 records only changes to the places Dr Jacob Roggeveen has lived, each time it loads the three data loads, using flagging.

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 人 |   |   | 位置 |   |
| --- | --- | --- | --- | --- |
| 西方人名的第一个字 | 姓 | 旗 | 地址 | 旗 |
| 雅各 | 罗格芬 | one | 荷兰米德尔堡 | Zero |
|   |   |   | 复活节岛拉帕努伊岛 | Zero |
|   |   |   | 荷兰米德尔堡 | one |

I find this useful when I want to store all the addresses at which a customer has lived, without knowing from what date till what date he or she was at each address, or the order he or she lived at each address. The flag only shows what the latest address is. SCD Type 2 records only changes in the location Dr Jacob Roggeveen lived, as it loads the three data loads, using effective date.

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 人 |   |   | 位置 |   |
| --- | --- | --- | --- | --- |
| 西方人名的第一个字 | 姓 | 生效期 | 地址 | 生效期 |
| 雅各 | 罗格芬 | 1722 年 4 月 5 日 | 荷兰米德尔堡 | 1659 年 2 月 1 日 |
|   |   |   | 复活节岛拉帕努伊岛 | 1722 年 4 月 5 日 |
|   |   |   | 荷兰米德尔堡 | 1729 年 1 月 31 日 |

I find this useful when I want to store all the addresses at which a customer lived, in the order they lived at those locations, plus when they moved to that address. SCD Type 3—Transition Dimension This dimension records only the transition for the specific business information. Example: Dr Jacob Roggeveen lived in Middelburg, Netherlands, then in Rapa Nui, Easter Island, but now lives in Middelburg, Netherlands.

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 负荷 | 西方人名的第一个字 | 姓 | 地址 |
| --- | --- | --- | --- |
| one | 雅各 | 罗格芬 | 荷兰米德尔堡 |
| Two | 雅各 | 罗格芬 | 复活节岛拉帕努伊岛 |
| three | 雅各 | 罗格芬 | 荷兰米德尔堡 |

The transition dimension records only the last address change, by keeping one record with the last change in value and the current value. The three load steps are as follows: Step 1: Lives in Middelburg, Netherlands

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"></colgroup> 
| 位置 |   |
| --- | --- |
| 地址上一个 | 地址 |
|   | 荷兰米德尔堡 |

Step 2: Moves from Middelburg, Netherlands, to Rapa Nui, Easter Island

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"></colgroup> 
| 位置 |   |
| --- | --- |
| 地址上一个 | 地址 |
| 荷兰米德尔堡 | 复活节岛拉帕努伊岛 |

Step 3: Moves from Rapa Nui, Easter Island, to Middelburg, Netherlands

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"></colgroup> 
| 位置 |   |
| --- | --- |
| 地址上一个 | 地址 |
| 复活节岛拉帕努伊岛 | 荷兰米德尔堡 |

I use SCD Type 3 to record the transitions to the current values of the customer. SCD Type 4—Fast-Growing Dimension. This dimension handles specific business information with a high change rate. This enables the data to track the fast changes, without overwhelming the storage requirements. In Chapter [9](09.html), I will show you how to design and deploy these dimensions for your data science.

###### 事实

A fact is a measurement that symbolizes a fact about the managed entity in the real world. In Chapter [9](09.html), I will show you how to design and deploy facts into the data science ecosystem.

###### Sun 内部模型合并矩阵

The intra-sun model consolidation matrix is a tool that helps you to identify common dimensions between sun models. Let’s assume our complete set of sun models is only three models. Tip When drafting sun models, it is better to draft too many than too few. Use them to record your requirements in detail. Sun Model One The first sun model (Figure [4-3](#Fig3)) records the relationship for Birth, with its direct dependence on the selectors Event, Date, and Time. ![A435693_1_En_4_Fig3_HTML.jpg](Images/A435693_1_En_4_Fig3_HTML.jpg) Figure 4-3Sun Model One Sun Model Two The second sun model (Figure [4-4](#Fig4)) records the relationship for LivesAt, with its direct dependence on the selectors Person, Date, Time, and Location. ![A435693_1_En_4_Fig4_HTML.jpg](Images/A435693_1_En_4_Fig4_HTML.jpg) Figure 4-4Sun Model Two Sun Model Three The third sun model (Figure [4-5](#Fig4)) records the relationship for Owns, with its direct dependence on the selectors Object, Person, Date, and Time .![A435693_1_En_4_Fig5_HTML.jpg](Images/A435693_1_En_4_Fig5_HTML.jpg) Figure 4-5Sun Model Three I then consolidate the different dimensions and facts onto a single matrix (Figure [4-6](#Fig6)).![A435693_1_En_4_Fig6_HTML.jpg](Images/A435693_1_En_4_Fig6_HTML.jpg) Figure 4-6 Intra-Sun model Linkage matrix The matrix will be used in Chapter [10](10.html), to design and implement the transform process for the data science. So, now that you know how to use the sun models to capture the functional requirements, it is also that you understand how to achieve the collection of the nonfunctional requirements.

### 非功能性需求

Nonfunctional requirements record the precise criteria that must be used to appraise the operation of a data science ecosystem.

#### 可访问性要求

Accessibility can be viewed as the “ability to access” and benefit from some system or entity. The concept focuses on enabling access for people with disabilities, or special needs, or enabling access through assistive technology. Assistive technology covers the following:

*   盲人支持水平:必须能够增加字体大小或类型，以帮助受影响的人阅读
*   色盲支持级别:必须能够改变调色板，以满足个人需求
*   使用声控命令帮助残疾人:必须能够为无法正常键入命令或使用鼠标的人使用声控命令

#### 审计和控制要求

Audit is the ability to investigate the use of the system and report any violations of the system’s data and processing rules. Control is making sure the system is used in the manner and by whom it is pre-approved to be used. An approach called role-based access control (RBAC) is the most commonly used approach to restricting system access to authorized users of your system. RBAC is an access-control mechanism formulated around roles and privileges. The components of RBAC are role-permissions—user-role and role-role relationships that together describe the system’s access policy. These audit and control requirements are also compulsory, by regulations on privacy and processing. Please check with your local information officer which precise rules apply. I will discuss some of the laws and regulations I have seen over the last years in Chapter [6](06.html), which will cover these in detail.

#### 可用性要求

Availability is as a ratio of the expected uptime of a system to the aggregate of the downtime of the system. For example, if your business hours are between 9h00 and 17h00, and you cannot have more than 1 hour of downtime during your business hours, you require 87.5% availability. Take note that you specify precisely at what point you expect the availability. If you are measuring at the edge of the data lake, it is highly possible that you will sustain 99.99999% availability with ease. The distributed and fault-tolerant nature of the data lake technology would ensure a highly available data lake. But if you measure at critical points in the business, you will find that at these critical business points, the requirements are more specific for availability. Record your requirements in the following format: Component C will be entirely operational for P% of the time over an uninterrupted measured period of D days. Your customers will understand this better than the general “24/7” or “business hours” terminology that I have seen used by some of my previous customers. No system can achieve these general requirement statements. The business will also have periods of high availability at specific periods during the day, week, month, or year. An example would be every Monday morning the data science results for the weekly meeting has to be available. This could be recorded as the following: Weekly reports must be entirely operational for 100% of the time between 06h00 and 10h00 every Monday for each office. Note Think what this means to a customer that has worldwide offices over several time zones. Be sure to understand every requirement fully! The correct requirements are

*   伦敦的周报必须在每周一 06h00 到 10h00(格林威治标准时间或英国夏令时)之间的 100%时间内完全运行。
*   纽约的周报必须在每周一 06h00 到 10h00(东部标准时间或东部夏令时)之间的 100%时间内完全运行。

Note You can clearly see that these requirements are now more precise than the simple general requirement. Identify single points of failure (SPOFs) in the data science solution. Ensure that you record this clearly, as SPOFs can impact many of your availability requirements indirectly. Highlight that those dependencies between components that may not be available at the same time must be recorded and requirements specified, to reflect this availability requirement fully. Note Be aware that the different availability requirements for different components in the same solution are the optimum requirement recording option.

#### 备份要求

A backup, or the process of backing up, refers to the archiving of the data lake and all the data science programming code, programming libraries, algorithms, and data models, with the sole purpose of restoring these to a known good state of the system, after a data loss or corruption event. Remember: Even with the best distribution and self-healing capability of the data lake, you have to ensure that you have a regular and appropriate backup to restore. Remember a backup is only valid if you can restore it. The merit of any system is its ability to return to a good state. This is a critical requirement. For example, suppose that your data scientist modifies the system with a new algorithm that erroneously updates an unknown amount of the data in the data lake. Oh, yes, that silent moment before every alarm in your business goes mad! You want to be able at all times to return to a known good state via a backup. Warning Please ensure that you can restore your backups in an effective and efficient manner. The process is backup-and-restore. Just generating backups does not ensure survival. Understand the impact it has to the business if it goes back two hours or what happens while you restore.

#### 产能、当前和预测

Capacity is the ability to load, process, and store a specific quantity of data by the data science processing solution. You must track the current and forecast the future requirements, because as a data scientist, you will design and deploy many complex models that will require additional capacity to complete the processing pipelines you create during your processing cycles. Warning I have inadvertently created models that generate several terabytes of workspace requirements, simply by setting the parameters marginally too in-depth than optimal. Suddenly, my model was demanding disk space at an alarming rate!

##### 容量

Capacity is measured per the component’s ability to consistently maintain specific levels of performance as data load demands vary in the solution. The correct way to record the requirement is Component C will provide P% capacity for U users, each with M MB of data during a time frame of T seconds. Example: The data hard drive will provide 95% capacity for 1000 users, each with 10MB of data during a time frame of 10 minutes. Warning Investigate the capacity required to perform a full rebuild in one process. I advise researching new cloud on-demand capacity, for disaster recovery or capacity top-ups. I have been consulted after major incidents that crippled a company for weeks, owing to a lack of proper capacity top-up plans.

##### 并发

Concurrency is the measure of a component to maintain a specific level of performance when under multiple simultaneous loads conditions. The correct way to record the requirement is Component C will support a concurrent group of U users running predefined acceptance script S simultaneously. Example: The memory will support a concurrent group of 100 users running a sort algorithm of 1000 records simultaneously. Note Concurrency is the ability to handle a subset of the total user base effectively. I have found that numerous solutions can handle substantial volumes of users with as little as 10% of the users’ running concurrently. Concurrency is an important requirement to ensure an effective solution at the start. Capacity can be increased by adding extra processing resources, while concurrency normally involves complete replacements of components. Design Tip If on average you have short-running data science algorithms, you can support high concurrency to maximum capacity ratio. But if your average running time is higher, your concurrency must be higher too. This way, you will maintain an effective throughput performance.

##### 物料通过量

This is how many transactions at peak time the system requires to handle specific conditions.

##### 存储(内存)

This is the volume of data the system will persist in memory at runtime to sustain an effective processing solution. Tip Remember: You can never have too much or too slow memory.

##### 存储(磁盘)

This is the volume of data the system stores on disk to sustain an effective processing solution. Tip Make sure that you have a proper mix of disks, to ensure that your solutions are effective. You will need short-term storage on fast solid-state drives to handle the while-processing capacity requirements. Warning There are data science algorithms that produce larger data volumes during data processing than the input or output data. The next requirement is your long-term storage. The basic rule is to plan for bigger but slower storage. Investigate using clustered storage, whereby two or more storage servers work together to increase performance, capacity, and reliability. Clustering distributes workloads to each server and manages the transfer of workloads between servers, while ensuring availability. The use of clustered storage will benefit you in the long term, during periods of higher demand, to scale out vertically with extra equipment. Tip Ensure that the server network is more than capable of handling any data science load. Remember: The typical data science algorithm requires massive data sets to work effectively. The big data evolution is now bringing massive amounts of data into the processing ecosystem. So, make sure you have enough space to store any data you need. Warning If you have a choice, do not share disk storage or networks with a transactional system. The data science will consume any spare capacity on the shared resources. It is better to have a lower performance dedicated set of resources than to share a volatile process.

##### 存储(GPU)

This is the volume of data the system will persist in GPU memory at runtime to sustain an effective parallel processing solution, using the graphical processing capacity of the solution. A CPU consists of a limited amount of cores that are optimized for sequential serial processing, while a GPU has a massively parallel architecture consisting of thousands of smaller, more efficient cores intended for handling massive amounts of multiple tasks simultaneously. The big advantage is to connect an effective quantity of very high-speed memory as closely as possible to these thousands of processing units, to use this increased capacity. I am currently deploying systems such as Kinetic DB and MapD, which are GPU-based data base engines. This improves the processing of my solutions by factors of a hundred in speed. I suspect that we will see key enhancements in the capacity of these systems over the next years. Tip Investigate a GPU processing grid for your high-performance processing. It is an effective solution with the latest technology.

##### 同比增长要求

The biggest growth in capacity will be for long-term storage. These requirements are specified as how much capacity increases over a period. The correct way to record the requirement is Component C will be responsible for necessary growth capacity to handle additional M MB of data within a period of T.

#### 结构管理

Configuration management (CM) is a systems engineering process for establishing and maintaining consistency of a product’s performance, functional, and physical attributes against requirements, design, and operational information throughout its life.

#### 部署

A methodical procedure of introducing data science to all areas of an organization is required. Investigate how to achieve a practical continuous deployment of the data science models. These skills are much in demand, as the processes model changes more frequently as the business adopts new processing techniques.

#### 文件

Data science requires a set of documentation to support the story behind the algorithms. I will explain the documentation required at each stage of the processing pipe, as I guide you through Chapters [5](05.html)–[11](11.html).

#### 灾难恢复

Disaster recovery (DR) involves a set of policies and procedures to enable the recovery or continuation of vital technology infrastructure and systems following a natural or human-induced disaster.

#### 效率(给定负载的资源消耗)

Efficiency is the ability to accomplish a job with a minimum expenditure of time and effort. As a data scientist, you are required to understand the efficiency curve of each of your modeling techniques and algorithms. As I suggested before, you must practice with your tools at different scales. Tip If it works at a sample 100,000 data points, try 200,000 data points or 500,000 data points. Make sure you understand the scaling dynamics of your tools.

#### 有效性(与努力相关的最终绩效)

Effectiveness is the ability to accomplish a purpose; producing the precise intended or expected result from the ecosystem. As a data scientist, you are required to understand the effectiveness curve of each of your modeling techniques and algorithms. You must ensure that the process is performing only the desired processing and has no negative side effects.

#### 展开性

The ability to add extra features and carry forward customizations at next-version upgrades within the data science ecosystem. The data science must always be capable of being extended to support new requirements.

#### 故障管理

Failure management is the ability to identify the root cause of a failure and then successfully record all the relevant details for future analysis and reporting. I have found that most of the tools I would include in my ecosystem have adequate fault management and reporting capability already built in to their native internal processing. Tip I found it takes a simple but well-structured set of data science processes to wrap the individual failure logs into a proper failure-management system. Apply normal data science to it, as if it is just one more data source. I always stipulate the precise expected process steps required when a failure of any component of the ecosystem is experienced during data science processing. Acceptance script S completes and reports every one of the X faults it generates. As a data scientist , you are required to log any failures of the system, to ensure that no unwanted side effects are generated that may cause a detrimental impact to your customers.

#### 容错

Fault tolerance is the ability of the data science ecosystem to handle faults in the system’s processing. In simple terms, no single event must be able to stop the ecosystem from continuing the data science processing. Here, I normally stipulate the precise operational system monitoring, measuring, and management requirements within the ecosystem, when faults are recorded. Acceptance script S withstands the X faults it generates. As a data scientist, you are required to ensure that your data science algorithms can handle faults and recover from them in an orderly manner.

#### 潜伏

Latency is the time it takes to get the data from one part of the system to another. This is highly relevant in the distributed environment of the data science ecosystems. Acceptance script S completes within T seconds on an unloaded system and within T2 seconds on a system running at maximum capacity, as defined in the concurrency requirement. Tip Remember: There is also internal latency between components that make up the ecosystem that is not directly accessible to users. Make sure you also note these in your requirements.

#### 互用性

Insist on a precise ability to share data between different computer systems under this section. Explain in detail what system must interact with what other systems. I normally investigate areas such as communication protocols, locations of servers, operating systems for different subcomponents, and the now-important end user’s Internet access criteria. Warning Be precise with requirements, as open-ended interoperability can cause unexpected complications later in the development cycle.

#### 可维护性

Insist on a precise period during which a specific component is kept in a specified state. Describe precisely how changes to functionalities, repairs, and enhancements are applied while keeping the ecosystem in a known good state.

#### 可变性

Stipulate the exact amount of change the ecosystem must support for each layer of the solution. Tip State what the limits are for specific layers of the solution. If the database can only support 2024 fields to a table, share that information!

#### 网络拓扑结构

Stipulate and describe the detailed network communication requirements within the ecosystem for processing. Also, state the expected communication to the outside world, to drive successful data science. Note Owing to the high impact on network traffic from several distributed data science algorithms processing, it is required that you understand and record the network necessary for the ecosystem to operate at an acceptable level.

#### 隐私

I suggest listing the exact privacy laws and regulations that apply to this ecosystem. Make sure you record the specific laws and regulations that apply. Seek legal advice if you are unsure. This is a hot topic worldwide, as you will process and store other people’s data and execute algorithms against this data. As a data scientist, you are responsible for your actions. Warning Remember: A privacy violation will result in a fine! Tip I hold liability insurance against legal responsibility claims for the data I process.

#### 质量

Specify the rigorous faults discovered, faults delivered, and fault removal efficiency at all levels of the ecosystem. Remember: Data quality is a functional requirement. This is a nonfunctional requirement that states the quality of the ecosystem, not the data flowing through it.

#### 恢复/可恢复性

The ecosystem must have a clear-cut mean time to recovery (MTTR) specified. The MTTR for specific layers and components in the ecosystem must be separately specified. I typically measure in hours, but for other extra-complex systems, I measure in minutes or even seconds.

#### 可靠性

The ecosystem must have a precise mean time between failures (MTBF) . This measurement of availability is specified in a pre-agreed unit of time. I normally measure in hours, but there are extra sensitive systems that are best measured in years.

#### 弹性

Resilience is the capability to deliver and preserve a tolerable level of service when faults and issues to normal operations generate complications for the processing. The ecosystem must have a defined ability to return to the original form and position in time, regardless of the issues it has to deal with during processing.

#### 资源限制

Resource constraints are the physical requirements of all the components of the ecosystem. The areas of interest are processor speed, memory, disk space, and network bandwidth, plus, normally, several other factors specified by the tools that you deploy into the ecosystem. Tip Discuss these requirements with your system’s engineers. This is not normally the area in which data scientists work.

#### 复用性

Reusability is the use of pre-built processing solutions in the data science ecosystem development process. The reuse of preapproved processing modules and algorithms is highly advised in the general processing of data for the data scientists. The requirement here is that you use approved and accepted standards to validate your own results. Warning I always advise that you use methodologies and algorithms that have proven lineage. An approved algorithm will guarantee acceptance by the business. Do not use unproven ideas!

#### 可量测性

Scalability is how you get the data science ecosystem to adapt to your requirements. I use three scalability models in my ecosystem: horizontal, vertical, and dynamic (on-demand). Horizontal scalability increases capacity in the data science ecosystem through more separate resources, to improve performance and provide high availability (HA). The ecosystem grows by scale out, by adding more servers to the data science cluster of resources. Tip Horizontal scalability is the proven way to handle full-scale data science ecosystems. Warning Not all models and algorithms can scale horizontally. Test them first. I would counsel against making assumptions. Vertical scalability increases capacity by adding more resources (more memory or an additional CPU) to an individual machine. Warning Make sure that you size your data science building blocks correctly at the start, as vertical scaling of clusters can get expensive and complex to swap at later stages. Dynamic (on-demand) scalability increases capacity by adding more resources, using either public or private cloud capability, which can be increased and decreased on a pay-as-you-go model. This is a hybrid model using a core set of resources that is the minimum footprint of the system, with additional burst agreements to cover any planned or even unplanned extra scalability increases in capacity that the system requires. I’d like to discuss scalability for your power users. Traditionally, I would have suggested high-specification workstations, but I have found that you will serve them better by providing them access to a flexible horizontal scalability on-demand environment. This way, they use what they need during peak periods of processing but share the capacity with others when they do not require the extra processing power.

#### 安全性

One of the most important nonfunctional requirements is security. I specify security requirements at three levels.

##### 隐私

I would specifically note requirements that specify protection for sensitive information within the ecosystem. Types of privacy requirements to note include data encryption for database tables and policies for the transmission of data to third parties. Tip Sources for privacy requirements are legislative or corporate. Please consult your legal experts.

##### 物理的

I would specifically note requirements for the physical protection of the system. Include physical requirements such as power, elevated floors, extra server cooling, fire prevention systems, and cabinet locks. Warning Some of the high-performance workstations required to process data science have stringent power requirements, so ensure that your data scientists are in a preapproved environment, to avoid overloading the power grid.

##### 接近

I purposely specify detailed access requirements with defined account types/groups and their precise access rights. Tip I use role-based access control (RBAC) to regulate access to data science resources, based on the roles of individual users within the ecosystem and not by their separate names. This way, I simply move the role to a new person, without any changes to the security profile.

#### 易测性

International standard IEEE 1233-1998 states that testability is the “degree to which a requirement is stated in terms that permit establishment of test criteria and performance of tests to determine whether those criteria have been met.” In simple terms, if your requirements are not testable, do not accept them. Remember A lower degree of testability results in increased test effort. I have spent too many nights creating tests for requirements that are unclear. Following is a series of suggestions, based on my experience.

##### 可控制性

Knowing the precise degree to which I can control the state of the code under test, as required for testing, is essential. The algorithms used by data science are not always controllable, as they include random start points to speed the process. Running distributed algorithms is not easy to deal with, as the distribution of the workload is not under your control.

##### 隔离能力

The specific degree to which I can isolate the code under test will drive most of the possible testing. A process such as deep learning includes non-isolation, so do not accept requirements that you cannot test, owing to not being able to isolate them.

##### 易懂

I have found that most algorithms have undocumented “extra features” or, in simple terms, “got-you” states. The degree to which the algorithms under test are documented directly impacts the testability of requirements.

##### 自动化

I have found the degree to which I can automate testing of the code directly impacts the effective and efficient testing of the algorithms in the ecosystem. I am an enthusiast of known result inline testing. I add code to my algorithms that test specific sub-sessions, to ensure that the new code has not altered the previously verified code.

### 需求的常见陷阱

I just want to list a sample of common pitfalls I have noted while performing data science for my customer base. If you are already aware of these pitfalls, well done! Many seem obvious; however I regularly work on projects in which these pitfalls have cost my clients millions of dollars before I was hired. So, let’s look at some of the more common pitfalls I encounter regularly.

#### 无力的话语

Weak words are subjective or lack a common or precise definition. The following are examples in which weak words are included and identified:

*   用户必须能够方便地访问系统。什么是“轻松”？
*   使用可靠的技术。什么叫“靠谱”？
*   最先进的设备什么是最先进的？
*   报告必须经常运行。什么是“经常”？
*   用户友好的报告布局什么是“用户友好”？
*   安全访问系统。“安全是什么？
*   所有数据必须立即上报。什么是“全部”？什么是“马上”？

I could add many more examples. But you get the common theme. Make sure the wording of your requirements is precise and specific. I have lamentably come to understand that various projects end in disaster, owing to weak words.

#### 无界列表

An unbounded list is an incomplete list of items. Examples include the following:

*   至少可以从伦敦和纽约到达。我只连接伦敦和纽约吗？其他 20 家分店呢？
*   包括但不限于伦敦和纽约办事处必须能够访问。那么，新德里办事处不是解决方案的一部分吗？

Make sure your lists are complete and precise. This prevents later issues caused by requirements being misunderstood.

#### 隐式集合

When collections of objects within requirements are not explicitly defined, you or your team will assume an incorrect meaning. See the following example: The solution must support TCP/IP and other network protocols supported by existing users with Linux.

*   “现有用户”是什么意思？
*   什么属于「其他网络协议」的集合？
*   包括哪些具体的 TCP/IP 协议？
*   “Linux”是来自多个厂商的操作系统的集合，有很多版本甚至修订版。你支持所有不同的版本还是只支持其中一个？

Make sure your collections are explicit and precise. This prevents later issues from requirements being misunderstood.

#### 含糊

Ambiguity occurs when a word within the requirement has multiple meanings. Examples are listed following.

##### 含糊

The system must pass between 96–100% of the test cases using current standards for data science. What are the “current standards”? This is an example of an unclear requirement!

##### 主观性

The report must easily and seamlessly integrate with the web sites. “Easily” and “seamlessly” are highly subjective terms where testing is concerned.

##### 可选性:

The solution should be tested under as many hardware conditions as possible. “As possible” makes this requirement optional. What if it fails testing on every hardware setup? Is that okay with your customer?

##### 不符合规格

The solution must support Hive 2.1 and other database versions. Do other database versions only include other Hive databases, or also others such as HBase version 1.0 and Oracle version 10i?

##### 参考不足

Users must be able to complete all previously defined reports in less than three minutes 90% of the day. What are these “previously defined” reports? This is an example of an unclear requirement.

## 设计一个实用的业务层

Any source code or other supplementary material referenced by me in this book is available to readers on GitHub, via this book’s product page, located at [www.apress.com/9781484230534](http://www.apress.com/9781484230534) . The business layer follows general business analysis and project management principals. I suggest a practical business layer consist of a minimum of three primary structures. Note See the following source code from Chapter [2](02.html): ./VKHCG/05-DS/5000-BL/. For the business layer, I suggest using a directory structure, such as ./VKHCG/05-DS/5000-BL. This enables you to keep your solutions clean and tidy for a successful interaction with a standard version-control system.

### 要求

Note See the following source code from Chapter [2](02.html): ./VKHCG/05-DS/5000-BL/0300-Requirements. Every requirement must be recorded with full version control, in a requirement-per-file manner. I suggest a numbering scheme of 000000-00, which supports up to a million requirements with up to a hundred versions of each requirement.

### 需求登记处

Note See the following source code from Chapter [2](02.html): ./VKHCG/05-DS/5000-BL/0100-Requirements-Registry. Keep a summary registry of all requirements in one single file, to assist with searching for specific requirements. I suggest you have a column with the requirement number, MoSCoW, a short description, date created, date last version, and status. I normally use the following status values:

*   发展中
*   介绍
*   退休的

The register acts as a control for the data science environment’s requirements. An example of a template is shown in Figure [4-7](#Fig6).![A435693_1_En_4_Fig7_HTML.jpg](Images/A435693_1_En_4_Fig7_HTML.jpg) Figure 4-7Requirements registry template

### 追溯矩阵

Note See the following source code from Chapter [2](02.html):./VKHCG/05-DS/5000-BL/0200-Traceability-Matrix. Create a traceability matrix against each requirement and the data science process you developed, to ensure that you know what data science process supports which requirement. This ensures that you have complete control of the environment. Changes are easy if you know how everything interconnects. An example of a template is shown in Figure [4-8](#Fig7).![A435693_1_En_4_Fig8_HTML.jpg](Images/A435693_1_En_4_Fig8_HTML.jpg) Figure 4-8Traceability matrix template

## 摘要

Well done! You now have a business layer, and you know in detail what is expected of your data science environment. Remember: The business layer must support the comprehensive collection of entire sets of requirements, to be used successfully by the data scientists. In the next chapter, on the utility layer, I will continue to steer you on the path to practical data science, to enable your data science environment to utilize a reusable processing ecosystem.