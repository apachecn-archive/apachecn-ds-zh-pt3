© Andreas François Vermeulen 2018 Andreas François VermeulenPractical Data Science[https://doi.org/10.1007/978-1-4842-3054-1_8](#)

# 8.评估超级步骤

Andreas François Vermeulen<sup class="calibre7">1 </sup> (1)West Kilbride North Ayrshire, UK   The objectives of this chapter are to show you how to assess your data science data for invalid or erroneous data values. Caution I have found that in 90% of my projects, I spend 70% of the processing work on this step, to improve the quality of the data used. This step will definitely improve the quality of your data science. I urge that you spend the time to “clean up” the data before you progress to the data science, as the incorrect data entries will cause a major impact on the later steps in the process. Perform a data science project on “erroneous” data also, to understand why the upstream processes are producing erroneous data. I have uncovered numerous unknown problems in my customers’ ecosystems by investigating this erroneous data. I will now introduce you to a selection of assessment techniques and algorithms that I scientifically deploy against the data imported during the retrieve phase of the project. Note Your specific data sources will have additional requirements. Explore additional quality techniques and algorithms that you need for your particular data.

## 评估超级步骤

Data quality refers to the condition of a set of qualitative or quantitative variables. Data quality is a multidimensional measurement of the acceptability of specific data sets. In business, data quality is measured to determine whether data can be used as a basis for reliable intelligence extraction for supporting organizational decisions. Data profiling involves observing in your data sources all the viewpoints that the information offers. The main goal is to determine if individual viewpoints are accurate and complete. The Assess superstep determines what additional processing to apply to the entries that are noncompliant. I should point out that performing this task is not as straightforward as most customers think. Minor pieces of incorrect data can have major impacts in later data processing steps and can impact the quality of the data science. Note You will always have errors. It is the quality techniques and algorithms that resolve these errors, to guide you down the path of successful data science.

## 错误

Did you find errors or issues? Typically, I can do one of four things to the data.

### 接受错误

If it falls within an acceptable standard (i.e., West Street instead of West St.), I can decide to accept it and move on to the next data entry. Take note that if you accept the error, you will affect data science techniques and algorithms that perform classification, such as binning, regression, clustering, and decision trees, because these processes assume that the values in this example are not the same. This option is the easy option, but not always the best option.

### 拒绝错误

Occasionally, predominantly with first-time data imports, the information is so severely damaged that it is better to simply delete the data entry methodically and not try to correct it. Take note: Removing data is a last resort. I normally add a quality flag and use this flag to avoid this erroneous data being used in data science techniques and algorithms that it will negatively affect. I will discuss specific data science techniques and algorithms in the rest of this book, and at each stage, I will explain how to deal with erroneous data.

### 纠正错误

This is the option that a major part of the assess step is dedicated to. Spelling mistakes in customer names, addresses, and locations are a common source of errors, which are methodically corrected. If there are variations on a name, I recommend that you set one data source as the “master” and keep the data consolidated and correct across all the databases using that master as your primary source. I also suggest that you store the original error in a separate value, as it is useful for discovering patterns for data sources that consistently produce errors.

### 创建默认值

This is an option that I commonly see used in my consulting work with companies. Most system developers assume that if the business doesn’t enter the value, they should enter a default value. Common values that I have seen are “unknown” or “n/a.” Unfortunately, I have also seen many undesirable choices, such as birthdays for dates or pets’ names for first name and last name, parents’ addresses . . . This address choice goes awry, of course, when more than 300 marketing letters with sample products are sent to parents’ addresses by several companies that are using the same service to distribute their marketing work. I suggest that you discuss default values with your customer in detail and agree on an official “missing data” value.

## 数据分析

I always generate a health report for all data imports. I suggest the following six data quality dimensions.

### 完全

I calculate the number of incorrect entries on each data source’s fields as a percentage of the total data. If the data source holds specific importance because of critical data (customer names, phone numbers, e-mail addresses, etc.), I start the analysis of these first, to ensure that the data source is fit to progress to the next phase of analysis for completeness on noncritical data. For example, for personal data to be unique, you need, as a minimum, a first name, last name, and date of birth. If any of this information is not part of the data, it is an incomplete personal data entry. Note that completeness is specific to the business area of the data you are processing.

### 独特性

I evaluate how unique the specific value is, in comparison to the rest of the data in that field. Also, test the value against other known sources of the same data sets. The last test for uniqueness is to show where the same field is in many data sources. You will report the uniqueness normally, as a histogram across all unique values in each data source.

### 及时

Record the impact of the date and time on the data source. Are there periods of stability or instability? This check is useful when scheduling extracts from source systems. I have seen countless month-end snapshot extracts performed before the month-end completed. These extracts are of no value. I suggest you work closely with your customer’s operational people, to ensure that your data extracts are performed at the correct point in the business cycle.

### 有效期

Validity is tested against known and approved standards. It is recorded as a percentage of nonconformance against the standard. I have found that most data entries are covered by a standard. For example, country code uses ISO 3166-1; currencies use ISO 4217. I also suggest that you look at customer-specific standards, for example, International Classification of Diseases (ICD) standards ICD-10\. Take note: Standards change over time. For example, ICD-10 is the tenth version of the standard. ICD-7 took effect in 1958, ICD-8A in 1968, ICD-9 in 1979, and ICD-10 in 1999\. So, when you validate data, make sure that you apply the correct standard on the correct data period.

### 准确

Accuracy is a measure of the data against the real-world person or object that is recorded in the data source. There are regulations, such as the European Union’s General Data Protection Regulation (GDPR) , that require data to be compliant for accuracy. I recommend that you investigate what standards and regulations you must comply with for accuracy.

### 一致性

This measure is recorded as the shift in the patterns in the data. Measure how data changes load after load. I suggest that you measure patterns and checksums for data sources. In Chapter [9](09.html), I will introduce a data structure called a data vault that will assist you with this process. In Chapter [10](10.html), I will demonstrate how you use slowly changing dimensions to track these consistencies. Caution The study of the errors in the system is just as important as the study of the valid data. Numerous real business issues are due to data errors. I will now proceed with the practical implementation of the knowledge you have just acquired.

## 实际行动

In Chapter [7](07.html), I introduced you to the Python package pandas. The package enables several automatic error-management features.

### 熊猫身上缺失的价值观

I will guide you first through some basic error-processing concepts, and then we will apply what you have learned to the sample data. I will use Python with the pandas package, but the basic concepts can be used in many other tools. Following are four basic processing concepts.

#### 删除所有元素都缺少值的列

Open your Python editor and create this in a file named Assess-Good-Bad-01.py in directory C:\VKHCG\01-Vermeulen\02-Assess. Now copy the following code into the file: ################################################################ # -*- coding: utf-8 -*- ################################################################ Import sys import os import pandas as pd ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base:',Base,'using',sys.platform) print('################################') sInputFileName='Good-or-Bad.csv' sOutputFileName='Good-or-Bad-01.csv' Company='01-Vermeulen' ################################################################ ################################################################ sFileDir=Base + '/' + Company + '/02-Assess/01-EDS/02-Python' if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ ### Import Warehouse ################################################################ sFileName=Base + '/' + Company + '/00-RawData/' + sInputFileName print('Loading:',sFileName) RawData=pd.read_csv(sFileName,header=0) print('################################')   print('## Raw Data Values')   print('################################')   print(RawData) print('################################')    print('## Data Profile') print('################################') print('Rows:',RawData.shape[0]) print('Columns:',RawData.shape[1]) print('################################') ################################################################ sFileName=sFileDir + '/' + sInputFileName RawData.to_csv(sFileName, index = False) ################################################################ ## This is the important action! The rest of this code snippet ## Only supports this action. ################################################################ TestData=RawData.dropna(axis=1, how="all") ################################################################ print('################################')   print('## Test Data Values')   print('################################')   print(TestData) print('################################')    print('## Data Profile') print('################################') print('Rows:',TestData.shape[0]) print('Columns:',TestData.shape[1]) print('################################') ################################################################ sFileName=sFileDir + '/' + sOutputFileName TestData.to_csv(sFileName, index = False) ################################################################ print('################################') print('### Done!! #####################') print('################################') ################################################################ Save the file Assess-Good-Bad-01.py, then compile and execute with your Python compiler. This will produce a set of displayed values plus two files named Good-or-Bad.csv and Good-or-Bad-01.csv in directory C:/VKHCG/01-Vermeulen/02-Assess/01-EDS/02-Python. The code loads the following raw data:

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 身份 | 费姿 | 费姿 | 田地（复数）；场；域；字段 | FieldD | 领域 | 田地（复数）；场；域；字段 | 费姿 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| one | 好的 | 较好的 | 最好的 | One thousand and twenty-four |   | Ten thousand two hundred and forty-one | one |
| Two | 好的 |   | 最好的 | Five hundred and twelve |   | Five thousand one hundred and twenty-one | Two |
| three | 好的 | 较好的 |   | Two hundred and fifty-six |   | Two hundred and fifty-six | three |
| four | 好的 | 较好的 | 最好的 |   |   | Two hundred and eleven | four |
| five | 好的 | 较好的 |   | Sixty-four |   | Six thousand four hundred and eleven | five |
| six | 好的 |   | 最好的 | Thirty-two |   | Thirty-two | six |
| seven |   | 较好的 | 最好的 | Sixteen |   | One thousand six hundred and eleven | seven |
| eight |   |   | 最好的 | eight |   | Eight thousand one hundred and eleven | eight |
| nine |   |   |   | four |   | Forty-one | nine |
| Ten | A | B | C | Two |   | Twenty-one thousand one hundred and eleven | Ten |
|   |   |   |   |   |   |   | Eleven |
| Ten | 好的 | 较好的 | 最好的 | One thousand and twenty-four |   | One hundred and two thousand four hundred and eleven | Twelve |
| Ten | 好的 |   | 最好的 | Five hundred and twelve |   | Five hundred and twelve | Thirteen |
| Ten | 好的 | 较好的 |   | Two hundred and fifty-six |   | One thousand two hundred and fifty-six | Fourteen |
| Ten | 好的 | 较好的 | 最好的 |   |   |   | Fifteen |
| Ten | 好的 | 较好的 |   | Sixty-four |   | One hundred and sixty-four | Sixteen |
| Ten | 好的 |   | 最好的 | Thirty-two |   | Three hundred and twenty-two | Seventeen |
| Ten |   | 较好的 | 最好的 | Sixteen |   | One hundred and sixty-three | Eighteen |
| Ten |   |   | 最好的 | eight |   | Eight hundred and forty-four | Nineteen |
| Ten |   |   |   | four |   | Four thousand five hundred and fifty-five | Twenty |
| Ten | A | B | C | Two |   | One hundred and eleven | Twenty-one |

The TestData=RawData.dropna(axis=1, how="all") code results in the following error correction:

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 身份 | 费姿 | 费姿 | 田地（复数）；场；域；字段 | FieldD | 田地（复数）；场；域；字段 | 费姿 |
| --- | --- | --- | --- | --- | --- | --- |
| one | 好的 | 较好的 | 最好的 | One thousand and twenty-four | Ten thousand two hundred and forty-one | one |
| Two | 好的 |   | 最好的 | Five hundred and twelve | Five thousand one hundred and twenty-one | Two |
| three | 好的 | 较好的 |   | Two hundred and fifty-six | Two hundred and fifty-six | three |
| four | 好的 | 较好的 | 最好的 |   | Two hundred and eleven | four |
| five | 好的 | 较好的 |   | Sixty-four | Six thousand four hundred and eleven | five |
| six | 好的 |   | 最好的 | Thirty-two | Thirty-two | six |
| seven |   | 较好的 | 最好的 | Sixteen | One thousand six hundred and eleven | seven |
| eight |   |   | 最好的 | eight | Eight thousand one hundred and eleven | eight |
| nine |   |   |   | four | Forty-one | nine |
| Ten | A | B | C | Two | Twenty-one thousand one hundred and eleven | Ten |
|   |   |   |   |   |   | Eleven |
| Ten | 好的 | 较好的 | 最好的 | One thousand and twenty-four | One hundred and two thousand four hundred and eleven | Twelve |
| Ten | 好的 |   | 最好的 | Five hundred and twelve | Five hundred and twelve | Thirteen |
| Ten | 好的 | 较好的 |   | Two hundred and fifty-six | One thousand two hundred and fifty-six | Fourteen |
| Ten | 好的 | 较好的 | 最好的 |   |   | Fifteen |
| Ten | 好的 | 较好的 |   | Sixty-four | One hundred and sixty-four | Sixteen |
| Ten | 好的 |   | 最好的 | Thirty-two | Three hundred and twenty-two | Seventeen |
| Ten |   | 较好的 | 最好的 | Sixteen | One hundred and sixty-three | Eighteen |
| Ten |   |   | 最好的 | eight | Eight hundred and forty-four | Nineteen |
| Ten |   |   |   | four | Four thousand five hundred and fifty-five | Twenty |
| Ten | A | B | C | Two | One hundred and eleven | Twenty-one |

All of column E has been deleted, owing to the fact that all values in that column were missing values/errors.

#### 删除任何元素缺少值的列

Open your Python editor and create a file named Assess-Good-Bad-02.py in directory C:\VKHCG\01-Vermeulen\02-Assess. Now copy the following code into the file: ################################################################ # -*- coding: utf-8 -*- ################################################################ Import sys import os import pandas as pd ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base:',Base,'using',sys.platform) print('################################') sInputFileName='Good-or-Bad.csv' sOutputFileName='Good-or-Bad-02.csv' Company='01-Vermeulen' ################################################################ Base='C:/VKHCG' ################################################################ sFileDir=Base + '/' + Company + '/02-Assess/01-EDS/02-Python' if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ ### Import Warehouse ################################################################ sFileName=Base + '/' + Company + '/00-RawData/' + sInputFileName print('Loading:',sFileName) RawData=pd.read_csv(sFileName,header=0) print('################################')   print('## Raw Data Values')   print('################################')   print(RawData) print('################################')    print('## Data Profile') print('################################') print('Rows:',RawData.shape[0]) print('Columns:',RawData.shape[1]) print('################################') ################################################################ sFileName=sFileDir + '/' + sInputFileName RawData.to_csv(sFileName, index = False) ################################################################ ## This is the important action! The rest of this code snippet ## Only supports this action. ################################################################ TestData=RawData.dropna(axis=1, how="any") ################################################################ print('################################')   print('## Test Data Values')   print('################################')   print(TestData) print('################################')    print('## Data Profile') print('################################') print('Rows:',TestData.shape[0]) print('Columns:',TestData.shape[1]) print('################################') ################################################################ sFileName=sFileDir + '/' + sOutputFileName TestData.to_csv(sFileName, index = False) ################################################################ print('################################') print('### Done!! #####################') print('################################') ################################################################ Save the Assess-Good-Bad-02.py file, then compile and execute with your Python compiler. This will produce a set of displayed values plus two files named Good-or-Bad.csv and Good-or-Bad-02.csv in directory C:/VKHCG/01-Vermeulen/02-Assess/01-EDS/02-Python. The code loads the following raw data:

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 身份 | 费姿 | 费姿 | 田地（复数）；场；域；字段 | FieldD | 领域 | 田地（复数）；场；域；字段 | 费姿 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| one | 好的 | 较好的 | 最好的 | One thousand and twenty-four |   | Ten thousand two hundred and forty-one | one |
| Two | 好的 |   | 最好的 | Five hundred and twelve |   | Five thousand one hundred and twenty-one | Two |
| three | 好的 | 较好的 |   | Two hundred and fifty-six |   | Two hundred and fifty-six | three |
| four | 好的 | 较好的 | 最好的 |   |   | Two hundred and eleven | four |
| five | 好的 | 较好的 |   | Sixty-four |   | Six thousand four hundred and eleven | five |
| six | 好的 |   | 最好的 | Thirty-two |   | Thirty-two | six |
| seven |   | 较好的 | 最好的 | Sixteen |   | One thousand six hundred and eleven | seven |
| eight |   |   | 最好的 | eight |   | Eight thousand one hundred and eleven | eight |
| nine |   |   |   | four |   | Forty-one | nine |
| Ten | A | B | C | Two |   | Twenty-one thousand one hundred and eleven | Ten |
|   |   |   |   |   |   |   | Eleven |
| Ten | 好的 | 较好的 | 最好的 | One thousand and twenty-four |   | One hundred and two thousand four hundred and eleven | Twelve |
| Ten | 好的 |   | 最好的 | Five hundred and twelve |   | Five hundred and twelve | Thirteen |
| Ten | 好的 | 较好的 |   | Two hundred and fifty-six |   | One thousand two hundred and fifty-six | Fourteen |
| Ten | 好的 | 较好的 | 最好的 |   |   |   | Fifteen |
| Ten | 好的 | 较好的 |   | Sixty-four |   | One hundred and sixty-four | Sixteen |
| Ten | 好的 |   | 最好的 | Thirty-two |   | Three hundred and twenty-two | Seventeen |
| Ten |   | 较好的 | 最好的 | Sixteen |   | One hundred and sixty-three | Eighteen |
| Ten |   |   | 最好的 | eight |   | Eight hundred and forty-four | Nineteen |
| Ten |   |   |   | four |   | Four thousand five hundred and fifty-five | Twenty |
| Ten | A | B | C | Two |   | One hundred and eleven | Twenty-one |

The TestData=RawData.dropna(axis=1, how="any") code results in the following error correction:

<colgroup class="calibre11"><col class="calibre12"></colgroup> 
| 费姿 |
| --- |
| one |
| Two |
| three |
| four |
| five |
| six |
| seven |
| eight |
| nine |
| Ten |
| Eleven |
| Twelve |
| Thirteen |
| Fourteen |
| Fifteen |
| Sixteen |
| Seventeen |
| Eighteen |
| Nineteen |
| Twenty |
| Twenty-one |

Columns A, B, C, D, E, and F are removed, owing to the fact that some of their values were missing values/errors. The removal of an entire column is useful when you want to isolate the complete and correct columns.

#### 仅保留最多包含两个缺失值的行

Open your Python editor and create a file named Assess-Good-Bad-03.py in directory C:\VKHCG\01-Vermeulen\02-Assess. Now copy the following code into the file: ################################################################ # -*- coding: utf-8 -*- ################################################################ Import sys import os import pandas as pd ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base:',Base,'using',sys.platform) print('################################') ################################################################ sInputFileName='Good-or-Bad.csv' sOutputFileName='Good-or-Bad-03.csv' Company='01-Vermeulen' ################################################################ Base='C:/VKHCG' ################################################################ sFileDir=Base + '/' + Company + '/02-Assess/01-EDS/02-Python' if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ ### Import Warehouse ################################################################ sFileName=Base + '/' + Company + '/00-RawData/' + sInputFileName print('Loading:',sFileName) RawData=pd.read_csv(sFileName,header=0) print('################################')   print('## Raw Data Values')   print('################################')   print(RawData) print('################################')    print('## Data Profile') print('################################') print('Rows:',RawData.shape[0]) print('Columns:',RawData.shape[1]) print('################################') ################################################################ sFileName=sFileDir + '/' + sInputFileName RawData.to_csv(sFileName, index = False) ################################################################ ## This is the important action! The rest of this code snippet ## Only supports this action. ################################################################ TestData=RawData.dropna(thresh=2) ################################################################ print('################################')   print('## Test Data Values')   print('################################')   print(TestData) print('################################')    print('## Data Profile') print('################################') print('Rows:',TestData.shape[0]) print('Columns:',TestData.shape[1]) print('################################') ################################################################ sFileName=sFileDir + '/' + sOutputFileName TestData.to_csv(sFileName, index = False) ################################################################ print('################################') print('### Done!! #####################') print('################################') ################################################################ Save the Assess-Good-Bad-03.py file, then compile and execute with your Python compiler. This will produce a set of displayed values plus two files named Good-or-Bad.csv and Good-or-Bad-03.csv in directory C:/VKHCG/01-Vermeulen/02-Assess/01-EDS/02-Python. The code loads the following raw data:

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 身份 | 费姿 | 费姿 | 田地（复数）；场；域；字段 | FieldD | 领域 | 田地（复数）；场；域；字段 | 费姿 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| one | 好的 | 较好的 | 最好的 | One thousand and twenty-four |   | Ten thousand two hundred and forty-one | one |
| Two | 好的 |   | 最好的 | Five hundred and twelve |   | Five thousand one hundred and twenty-one | Two |
| three | 好的 | 较好的 |   | Two hundred and fifty-six |   | Two hundred and fifty-six | three |
| four | 好的 | 较好的 | 最好的 |   |   | Two hundred and eleven | four |
| five | 好的 | 较好的 |   | Sixty-four |   | Six thousand four hundred and eleven | five |
| six | 好的 |   | 最好的 | Thirty-two |   | Thirty-two | six |
| seven |   | 较好的 | 最好的 | Sixteen |   | One thousand six hundred and eleven | seven |
| eight |   |   | 最好的 | eight |   | Eight thousand one hundred and eleven | eight |
| nine |   |   |   | four |   | Forty-one | nine |
| Ten | A | B | C | Two |   | Twenty-one thousand one hundred and eleven | Ten |
|   |   |   |   |   |   |   | Eleven |
| Ten | 好的 | 较好的 | 最好的 | One thousand and twenty-four |   | One hundred and two thousand four hundred and eleven | Twelve |
| Ten | 好的 |   | 最好的 | Five hundred and twelve |   | Five hundred and twelve | Thirteen |
| Ten | 好的 | 较好的 |   | Two hundred and fifty-six |   | One thousand two hundred and fifty-six | Fourteen |
| Ten | 好的 | 较好的 | 最好的 |   |   |   | Fifteen |
| Ten | 好的 | 较好的 |   | Sixty-four |   | One hundred and sixty-four | Sixteen |
| Ten | 好的 |   | 最好的 | Thirty-two |   | Three hundred and twenty-two | Seventeen |
| Ten |   | 较好的 | 最好的 | Sixteen |   | One hundred and sixty-three | Eighteen |
| Ten |   |   | 最好的 | eight |   | Eight hundred and forty-four | Nineteen |
| Ten |   |   |   | four |   | Four thousand five hundred and fifty-five | Twenty |
| Ten | A | B | C | Two |   | One hundred and eleven | Twenty-one |

The TestData=RawData.dropna(thresh=2) results in the following error correction:

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 身份 | 费姿 | 费姿 | 田地（复数）；场；域；字段 | FieldD | 领域 | 田地（复数）；场；域；字段 | 费姿 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| one | 好的 | 较好的 | 最好的 | One thousand and twenty-four |   | Ten thousand two hundred and forty-one | one |
| Two | 好的 |   | 最好的 | Five hundred and twelve |   | Five thousand one hundred and twenty-one | Two |
| three | 好的 | 较好的 |   | Two hundred and fifty-six |   | Two hundred and fifty-six | three |
| four | 好的 | 较好的 | 最好的 |   |   | Two hundred and eleven | four |
| five | 好的 | 较好的 |   | Sixty-four |   | Six thousand four hundred and eleven | five |
| six | 好的 |   | 最好的 | Thirty-two |   | Thirty-two | six |
| seven |   | 较好的 | 最好的 | Sixteen |   | One thousand six hundred and eleven | seven |
| eight |   |   | 最好的 | eight |   | Eight thousand one hundred and eleven | eight |
| nine |   |   |   | four |   | Forty-one | nine |
| Ten | A | B | C | Two |   | Twenty-one thousand one hundred and eleven | Ten |
| Ten | 好的 | 较好的 | 最好的 | One thousand and twenty-four |   | One hundred and two thousand four hundred and eleven | Twelve |
| Ten | 好的 |   | 最好的 | Five hundred and twelve |   | Five hundred and twelve | Thirteen |
| Ten | 好的 | 较好的 |   | Two hundred and fifty-six |   | One thousand two hundred and fifty-six | Fourteen |
| Ten | 好的 | 较好的 | 最好的 |   |   |   | Fifteen |
| Ten | 好的 | 较好的 |   | Sixty-four |   | One hundred and sixty-four | Sixteen |
| Ten | 好的 |   | 最好的 | Thirty-two |   | Three hundred and twenty-two | Seventeen |
| Ten |   | 较好的 | 最好的 | Sixteen |   | One hundred and sixty-three | Eighteen |
| Ten |   |   | 最好的 | eight |   | Eight hundred and forty-four | Nineteen |
| Ten |   |   |   | four |   | Four thousand five hundred and fifty-five | Twenty |
| Ten | A | B | C | Two |   | One hundred and eleven | Twenty-one |

Row 11 has been removed, owing to the fact that more than two of its values are missing values/errors.

#### 用特定数字列的平均值、中值、众数、最小值和最大值填充所有缺失值

Open your Python editor and create a file named Assess-Good-Bad-04.py in directory C:\VKHCG\01-Vermeulen\02-Assess. Now copy the following code into the file: ################################################################ # -*- coding: utf-8 -*- ################################################################ Import sys import os import pandas as pd ################################################################ Base='C:/VKHCG' sInputFileName='Good-or-Bad.csv' sOutputFileNameA='Good-or-Bad-04-A.csv' sOutputFileNameB='Good-or-Bad-04-B.csv' sOutputFileNameC='Good-or-Bad-04-C.csv' sOutputFileNameD='Good-or-Bad-04-D.csv' sOutputFileNameE='Good-or-Bad-04-E.csv' Company='01-Vermeulen' ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base:',Base,'using',sys.platform) print('################################')################################################################ sFileDir=Base + '/' + Company + '/02-Assess/01-EDS/02-Python' if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ ### Import Warehouse ################################################################ sFileName=Base + '/' + Company + '/00-RawData/' + sInputFileName print('Loading:',sFileName) RawData=pd.read_csv(sFileName,header=0) print('################################')   print('## Raw Data Values')   print('################################')   print(RawData) print('################################')    print('## Data Profile') print('################################') print('Rows:',RawData.shape[0]) print('Columns:',RawData.shape[1]) print('################################') ################################################################ sFileName=sFileDir + '/' + sInputFileName RawData.to_csv(sFileName, index = False) ################################################################ TestData=RawData.fillna(RawData.mean()) ################################################################ print('################################')   print('## Test Data Values- Mean')   print('################################')   print(TestData) print('################################')    print('## Data Profile') print('################################') print('Rows:',TestData.shape[0]) print('Columns:',TestData.shape[1]) print('################################') ################################################################ sFileName=sFileDir + '/' + sOutputFileNameA TestData.to_csv(sFileName, index = False) ################################################################ ## This is the important action! The rest of this code snippet ## Only supports this action. ################################################################ TestData=RawData.fillna(RawData.median()) ################################################################ print('################################')   print('## Test Data Values - Median')   print('################################')   print(TestData) print('################################')    print('## Data Profile') print('################################') print('Rows:',TestData.shape[0]) print('Columns:',TestData.shape[1]) print('################################') ################################################################ sFileName=sFileDir + '/' + sOutputFileNameB TestData.to_csv(sFileName, index = False) ################################################################ ################################################################ TestData=RawData.fillna(RawData.mode()) ################################################################ print('################################')   print('## Test Data Values - Mode')   print('################################')   print(TestData) print('################################')    print('## Data Profile') print('################################') print('Rows:',TestData.shape[0]) print('Columns:',TestData.shape[1]) print('################################') ################################################################ sFileName=sFileDir + '/' + sOutputFileNameC TestData.to_csv(sFileName, index = False) ################################################################ ################################################################ TestData=RawData.fillna(RawData.min()) ################################################################ print('################################')   print('## Test Data Values - Minumum')   print('################################')   print(TestData) print('################################')    print('## Data Profile') print('################################') print('Rows:',TestData.shape[0]) print('Columns:',TestData.shape[1]) print('################################') ################################################################ sFileName=sFileDir + '/' + sOutputFileNameD TestData.to_csv(sFileName, index = False) ################################################################ ################################################################ TestData=RawData.fillna(RawData.max()) ################################################################ print('################################')   print('## Test Data Values - Maximum')   print('################################')   print(TestData) print('################################')    print('## Data Profile') print('################################') print('Rows:',TestData.shape[0]) print('Columns:',TestData.shape[1]) print('################################') ################################################################ sFileName=sFileDir + '/' + sOutputFileNameE TestData.to_csv(sFileName, index = False) ################################################################ ################################################################ print('################################') print('### Done!! #####################') print('################################') ################################################################ Save the Assess-Good-Bad-04.py file, then compile and execute with your Python compiler. This will produce a set of displayed values plus six files named Good-or-Bad.csv, Good-or-Bad-04-A.csv, Good-or-Bad-04-B.csv, Good-or-Bad-04-C.csv, Good-or-Bad-04-D.csv, and Good-or-Bad-04-E.csv in directory C:/VKHCG/01-Vermeulen/02-Assess/01-EDS/02-Python. I suggest you investigate each result and understand how it was transformed by the code.

## 设计一个实用的评估超级步骤

Any source code or other supplementary material referenced by me in this book is available to readers on GitHub, via this book’s product page, located at [www.apress.com/9781484230534](http://www.apress.com/9781484230534) . Please note that this source code assumes that you have completed the source code setup outlined in Chapter [2](02.html), as it creates all the directories you will need to complete the examples. Now that I have explained the various aspects of the Assess superstep, I will explain how to help our company process its data.

### vermeulen plc 公司

As I guide you through the Assess superstep for the company, the coding will become increasingly multilayered. I will assist you end-to-end through these phases of discovery. Vermeulen PLC has two primary processes that I will lead you through: network routing and job scheduling.

#### 创建网络路由图

The next step along the route is to generate a full network routing solution for the company, to resolve the data issues in the retrieve data. Note In the next example, I switch off the SettingWithCopyWarning check, as I am using the data manipulation against a copy of the same data: pd.options.mode.chained_assignment = None. Open your Python editor and create a file named Assess-Network-Routing-Company.py in directory C:\VKHCG\01-Vermeulen\02-Assess. Now copy the following code into the file: ################################################################ import sys import os import pandas as pd ################################################################ pd.options.mode.chained_assignment = None ################################################################ ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + '/VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base:',Base,'using',sys.platform) print('################################') print('################################') print('Working Base:',Base,'using',sys.platform) print('################################') ################################################################ sInputFileName1='01-Retrieve/01-EDS/01-R/Retrieve_Country_Code.csv' sInputFileName2='01-Retrieve/01-EDS/02-Python/Retrieve_Router_Location.csv' ################################################################ sOutputFileName='Assess-Network-Routing-Company.csv' Company='01-Vermeulen' ################################################################ ################################################################ ### Import Country Data ################################################################ sFileName=Base + '/' + Company + '/' + sInputFileName1 print('################################') print('Loading:',sFileName) print('################################') CountryData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") print('Loaded Country:',CountryData.columns.values) print('################################') ################################################################ ## Assess Country Data ################################################################ print('################################') print('Changed:',CountryData.columns.values) Tip I use many of pandas’s built-in functions to manipulate data in my data science projects. It will be to your future advantage to investigate the capabilities of this pandas library. See [https://pandas.pydata.org/pandas-docs/stable/api.html](https://pandas.pydata.org/pandas-docs/stable/api.html) , for reference. CountryData.rename(columns={'Country': 'Country_Name'}, inplace=True) CountryData.rename(columns={'ISO-2-CODE': 'Country_Code'}, inplace=True) CountryData.drop('ISO-M49', axis=1, inplace=True) CountryData.drop('ISO-3-Code', axis=1, inplace=True) CountryData.drop('RowID', axis=1, inplace=True) print('To:',CountryData.columns.values) print('################################') ################################################################ ################################################################ ### Import Company Data ################################################################ sFileName=Base + '/' + Company + '/' + sInputFileName2 print('################################') print('Loading:',sFileName) print('################################') CompanyData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") print('Loaded Company:',CompanyData.columns.values) print('################################') ################################################################ ## Assess Company Data ################################################################ print('################################') print('Changed:',CompanyData.columns.values) CompanyData.rename(columns={'Country':'Country_Code'}, inplace=True) print('To:',CompanyData.columns.values) print('################################') ################################################################ ################################################################ ### Import Customer Data ################################################################ sFileName=Base + '/' + Company + '/' + sInputFileName3 print('################################') print('Loading:',sFileName) print('################################') CustomerRawData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") print('################################') print('Loaded Customer:',CustomerRawData.columns.values) print('################################') ################################################################ CustomerData=CustomerRawData.dropna(axis=0, how="any") print('################################') print('Remove Blank Country Code') print('Reduce Rows from',CustomerRawData.shape[0],'to',CustomerData.shape[0]) print('################################') ################################################################ print('################################') print('Changed:',CustomerData.columns.values) CustomerData.rename(columns={'Country':'Country_Code'},inplace=True) print('To:',CustomerData.columns.values) print('################################') ################################################################ print('################################') print('Merge Company and Country Data') print('################################') CompanyNetworkData=pd.merge(         CompanyData,         CountryData,         how='inner',         on='Country_Code'         ) ################################################################ print('################################') print('Change',CompanyNetworkData.columns.values) for i in CompanyNetworkData.columns.values:     j='Company_'+i     CompanyNetworkData.rename(columns={i:j},inplace=True) print('To',CompanyNetworkData.columns.values) print('################################') ################################################################ ################################################################ sFileDir=Base + '/' + Company + '/02-Assess/01-EDS/02-Python' if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ sFileName=sFileDir + '/' + sOutputFileName print('################################') print('Storing:',sFileName) print('################################') CompanyNetworkData.to_csv(sFileName, index = False, encoding="latin-1") ################################################################ ################################################################ print('################################') print('### Done!! #####################') print('################################') ################################################################ Save the Assess-Network-Routing-Company.py file, then compile and execute with your Python compiler. This will produce a set of demonstrated values onscreen, plus a file named Assess-Network-Routing-Company.csv.

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 公司 _ 国家 _ 代码 | 公司地点名称 | 公司 _ 纬度 | 公司 _ 经度 | 公司国家名称 |
| --- | --- | --- | --- | --- |
| 美国 | 纽约 | 40.7528 | -73.9725 | 美利坚合众国 |
| 美国 | 纽约 | 40.7214 | -74.0052 | 美利坚合众国 |
| 美国 | 纽约 | 40.7662 | -73.9862 | 美利坚合众国 |

Open your Python editor and create a file called Assess-Network-Routing-Customer.py in directory C:\VKHCG\01-Vermeulen\02-Assess. Next, create a new file, and move on to the following example. Here is the code for the example: ################################################################ import sys import os import pandas as pd ################################################################ pd.options.mode.chained_assignment = None ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + 'VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base:',Base,'using',sys.platform) print('################################') ################################################################ sInputFileName1='01-Retrieve/01-EDS/01-R/Retrieve_Country_Code.csv' sInputFileName2='01-Retrieve/01-EDS/02-Python/Retrieve_All_Router_Location.csv' ################################################################ sOutputFileName='Assess-Network-Routing-Customer.csv' Company='01-Vermeulen' ################################################################ ################################################################ ### Import Country Data ################################################################ sFileName=Base + '/' + Company + '/' + sInputFileName1 print('################################') print('Loading:',sFileName) print('################################') CountryData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") print('Loaded Country:',CountryData.columns.values) print('################################') ################################################################ ## Assess Country Data ################################################################ print('################################') print('Changed:',CountryData.columns.values) CountryData.rename(columns={'Country': 'Country_Name'}, inplace=True) CountryData.rename(columns={'ISO-2-CODE': 'Country_Code'}, inplace=True) CountryData.drop('ISO-M49', axis=1, inplace=True) CountryData.drop('ISO-3-Code', axis=1, inplace=True) CountryData.drop('RowID', axis=1, inplace=True) print('To:',CountryData.columns.values) print('################################') ################################################################ ### Import Customer Data ################################################################ sFileName=Base + '/' + Company + '/' + sInputFileName2 print('################################') print('Loading:',sFileName) print('################################') CustomerRawData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") print('################################') print('Loaded Customer:',CustomerRawData.columns.values) print('################################') ################################################################ CustomerData=CustomerRawData.dropna(axis=0, how="any") print('################################') print('Remove Blank Country Code') print('Reduce Rows from',CustomerRawData.shape[0],'to',CustomerData.shape[0]) print('################################') ################################################################ print('################################') print('Changed:',CustomerData.columns.values) CustomerData.rename(columns={'Country': 'Country_Code'}, inplace=True) print('To:',CustomerData.columns.values) print('################################') ################################################################ print('################################') print('Merge Customer and Country Data') print('################################') CustomerNetworkData=pd.merge(         CustomerData,         CountryData,         how='inner',         on='Country_Code'         ) ################################################################ print('################################') print('Change',CustomerNetworkData.columns.values) for i in CustomerNetworkData.columns.values:     j='Customer_'+i     CustomerNetworkData.rename(columns={i:j}, inplace=True) print('To', CustomerNetworkData.columns.values) print('################################') ################################################################ sFileDir=Base + '/' + Company + '/02-Assess/01-EDS/02-Python' if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ sFileName=sFileDir + '/' + sOutputFileName print('################################') print('Storing:', sFileName) print('################################') CustomerNetworkData.to_csv(sFileName, index = False, encoding="latin-1") ################################################################ print('################################') print('### Done!! #####################') print('################################') ################################################################ Save the file Assess-Network-Routing-Customer.py, then compile and execute with your Python compiler. This will produce a set of demonstrated values onscreen plus a file named Assess-Network-Routing-Customer.csv.

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 客户 _ 国家 _ 代码 | 客户地点名称 | 客户 _ 纬度 | 客户 _ 经度 | 客户 _ 国家 _ 名称 |
| --- | --- | --- | --- | --- |
| 频带宽度(Band Width) | 哈博罗内 | -24.6464 | 25.9119 | 博茨瓦纳 |
| 频带宽度(Band Width) | 弗朗西斯敦 | -21.1667 | 27.5167 | 博茨瓦纳 |
| 频带宽度(Band Width) | 必须 | -19.9833 | 23.4167 | 博茨瓦纳 |

Open your Python editor and create a file named Assess-Network-Routing-Node.py in directory C:\VKHCG\01-Vermeulen\02-Assess. Now copy the following code into the file: ################################################################ import sys import os import pandas as pd ################################################################ pd.options.mode.chained_assignment = None ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + 'VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base:',Base,'using',sys.platform) print('################################') ################################################################ sInputFileName='01-Retrieve/01-EDS/02-Python/Retrieve_IP_DATA.csv' ################################################################ sOutputFileName='Assess-Network-Routing-Node.csv' Company='01-Vermeulen' ################################################################ ### Import IP Data ################################################################ sFileName=Base + '/' + Company + '/' + sInputFileName print('################################') print('Loading:',sFileName) print('################################') IPData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") print('Loaded IP:', IPData.columns.values) print('################################') ################################################################ print('################################') print('Changed:',IPData.columns.values) IPData.drop('RowID', axis=1, inplace=True) IPData.drop('ID', axis=1, inplace=True) IPData.rename(columns={'Country': 'Country_Code'}, inplace=True) IPData.rename(columns={'Place.Name': 'Place_Name'}, inplace=True) IPData.rename(columns={'Post.Code': 'Post_Code'}, inplace=True) IPData.rename(columns={'First.IP.Number': 'First_IP_Number'}, inplace=True) IPData.rename(columns={'Last.IP.Number': 'Last_IP_Number'}, inplace=True) print('To:',IPData.columns.values) print('################################') ################################################################ print('################################') print('Change',IPData.columns.values) for i in IPData.columns.values:     j='Node_'+i     IPData.rename(columns={i:j}, inplace=True) print('To', IPData.columns.values) print('################################') ################################################################ sFileDir=Base + '/' + Company + '/02-Assess/01-EDS/02-Python' if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ sFileName=sFileDir + '/' + sOutputFileName print('################################') print('Storing:', sFileName) print('################################') IPData.to_csv(sFileName, index = False, encoding="latin-1") ################################################################ print('################################') print('### Done!! #####################') print('################################') ################################################################ Save the Assess-Network-Routing-Node.py file, then compile and execute with your Python compiler. This will produce a set of demonstrated values onscreen, plus a file named Assess-Network-Routing-Node.csv.

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 节点国家代码 | 节点地点名称 | 节点 _ 发布 _ 代码 | 节点纬度 | 节点经度 | 节点名 IP 号 | 节点最后一个 IP 号码 |
| --- | --- | --- | --- | --- | --- | --- |
| 千兆字节 | 勒威克 | ZE1 | Sixty point one five | -1.15 | Five hundred and twenty-three million four hundred and seventy-eight thousand and sixteen | Five hundred and twenty-three million four hundred and seventy-nine thousand and thirty-nine |
| 千兆字节 | 勒威克 | ZE1 | Sixty point one five | -1.15 | One billion five hundred and forty-five million three hundred and sixty-nine thousand nine hundred and eighty-four | One billion five hundred and forty-five million three hundred and seventy thousand one hundred and eleven |
| 千兆字节 | 塞尔比 | 我 8 岁 | 53.7833 | -1.0667 | Thirty-five million two hundred and thirty-one thousand two hundred and thirty-two | Thirty-five million two hundred and thirty-one thousand three hundred and fifty-nine |

#### 图论

I provide a simple introduction to graph theory , before we progress to the examples. Graphs are useful for indicating relationships between entities in the real world. The basic building blocks are two distinct graph components, as follows.

##### 结节

The node is any specific single entity. For example, in “Andre kiss Zhaan,” there are two nodes: Andre and Zhaan (Figure [8-1](#Fig1)).![A435693_1_En_8_Fig1_HTML.jpg](Images/A435693_1_En_8_Fig1_HTML.jpg) Figure 8-1Nodes Andre and Zhaan

##### 边缘

The edge is any specific relationship between two nodes. For example, in “Andre kiss Zhaan,” there are two nodes, i.e., Andre and Zhaan. The edge is “kiss.” The edge can be recorded as non-directed. This will record “kiss” as “kiss each other” (Figure [8-2](#Fig2)).![A435693_1_En_8_Fig2_HTML.jpg](Images/A435693_1_En_8_Fig2_HTML.jpg) Figure 8-2Nodes Andre and Zhaan kiss each other The edge can be recorded as directed. This will record the “kiss” as “kiss toward” (Figure [8-3](#Fig3)).![A435693_1_En_8_Fig3_HTML.jpg](Images/A435693_1_En_8_Fig3_HTML.jpg) Figure 8-3Nodes Andre kiss toward Zhaan This enforces the direction of the edge. This concept of direction is useful when dealing with actions that must occur in a specific order, or when you have to follow a specific route. Figure [8-4](#Fig4) shows an example of this.![A435693_1_En_8_Fig4_HTML.jpg](Images/A435693_1_En_8_Fig4_HTML.jpg) Figure 8-4Direction of edges You can only travel from A to C via B, A to D via B, A to B, B to C, and B to D. You cannot travel from C to B, D to B, B to A, C to A, and D to A. The directed edge prevents this action.

##### 有向无环图

A directed acyclic graph is a specific graph that only has one path through the graph. Figure [8-5](#Fig5) shows a graph that is a DAG, and Figure [8-6](#Fig6) shows a graph that is not a DAG.![A435693_1_En_8_Fig5_HTML.jpg](Images/A435693_1_En_8_Fig5_HTML.jpg) Figure 8-5This graph is a DAG ![A435693_1_En_8_Fig6_HTML.jpg](Images/A435693_1_En_8_Fig6_HTML.jpg) Figure 8-6This graph is not a DAG The reason Figure [8-6](#Fig6) is not a DAG is because there are two paths between A and C: A to C and A to B to C. Now that you have the basics, let me show you how to use this in your data science.

#### 构建调度作业的 DAG

The following example explains how to schedule networking equipment maintenance jobs to perform the required repairs during the month. To create a DAG, I will define what it is first: A DAG is a data structure that enables you to generate a relationship between data entries that can only be performed in a specific order. The DAG is an important structure in the core data science environments, as it is the fundamental structure that enables tools such as Spark, Pig, and Tez in Hadoop to work. It is also used for recording task scheduling and process interdependencies in processing data. I will introduce a new Python library called networkx . Open your python editor and create a file named Assess-DAG-Location.py in directory ..\VKHCG\01-Vermeulen\02-Assess. Now copy the following code into the file: ################################################################ import networkx as nx import matplotlib.pyplot as plt import sys import os import pandas as pd ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + 'VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base:',Base,'using',sys.platform) print('################################') ################################################################ sInputFileName='01-Retrieve/01-EDS/02-Python/Retrieve_Router_Location.csv' sOutputFileName1='Assess-DAG-Company-Country.png' sOutputFileName2='Assess-DAG-Company-Country-Place.png' Company='01-Vermeulen' ################################################################ ### Import Company Data ################################################################ sFileName=Base + '/' + Company + '/' + sInputFileName print('################################') print('Loading:',sFileName) print('################################') CompanyData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") print('Loaded Company:',CompanyData.columns.values) print('################################') ################################################################ print(CompanyData) print('################################') print('Rows:',CompanyData.shape[0]) print('################################') We will now create two directed graphs: G1 and G2 using the G=DiGraph(), if you just wanted a graph you would use G=Graph(). ################################################################ G1=nx.DiGraph() G2=nx.DiGraph() ################################################################ We will now create a node for each of the places in our data. For this, use the G.add_node() function. Note If you execute G.add_node("A") and then G.add_node("A") again, you only get one node: "A". The graph automatically resolves any duplicate node requests. We will now loop through all the country and place-name-country records. for i in range(CompanyData.shape[0]):     G1.add_node(CompanyData['Country'][i])     sPlaceName= CompanyData['Place_Name'][i] + '-' + CompanyData['Country'][i]     G2.add_node(sPlaceName) We will now interconnect all the nodes (G.nodes()), by looping though them in pairs. print('################################') for n1 in G1.nodes():     for n2 in G1.nodes():         if n1 != n2:             print('Link:',n1,' to ', n2)             G1.add_edge(n1,n2) print('################################') Now, we can see the nodes and edges you created. print('################################') print("Nodes of graph:") print(G1.nodes()) print("Edges of graph:") print(G1.edges()) print('################################') We can now save our graph, as follows: ################################################################ sFileDir=Base + '/' + Company + '/02-Assess/01-EDS/02-Python' if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ sFileName=sFileDir + '/' + sOutputFileName1 print('################################') print('Storing:', sFileName) print('################################') Now, you can display the graph as a picture. nx.draw(G1,pos=nx.spectral_layout(G1),         nodecolor='r',edge_color='b',         with_labels=True,node_size=8000,         font_size=12) plt.savefig(sFileName) # save as png plt.show() # display ################################################################ Now, you can complete the second graph. print('################################') for n1 in G2.nodes():     for n2 in G2.nodes():         if n1 != n2:             print('Link:',n1,'to',n2)             G2.add_edge(n1,n2) print('################################') print('################################') print("Nodes of graph:") print(G2.nodes()) print("Edges of graph:") print(G2.edges()) print('################################') ################################################################ sFileDir=Base + '/' + Company + '/02-Assess/01-EDS/02-Python' if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ sFileName=sFileDir + '/' + sOutputFileName2 print('################################') print('Storing:', sFileName) print('################################') nx.draw(G2,pos=nx.spectral_layout(G2),         nodecolor='r',edge_color='b',         with_labels=True,node_size=8000,         font_size=12) plt.savefig(sFileName) # save as png plt.show() # display ################################################################ Save the Assess-DAG-Location.py file, then compile and execute with your Python compiler. This will produce a set of demonstrated values onscreen, plus two graphical files named Assess-DAG-Company-Country.png (Figure [8-7](#Fig7)) and Assess-DAG-Company-Country-Place.png (Figure [8-8](#Fig8)).![A435693_1_En_8_Fig7_HTML.jpg](Images/A435693_1_En_8_Fig7_HTML.jpg) Figure 8-7 Assess-DAG-Company-Country.png file ![A435693_1_En_8_Fig8_HTML.jpg](Images/A435693_1_En_8_Fig8_HTML.jpg) Figure 8-8 Assess-DAG-Company-Country-Place.png file Well done. You just completed a simple but valid graph. Let’s try a bigger graph. Remember There are only two entities. Size has no direct impact on the way to build. Caution The networkx library works well with fairly large graphs, but it is limited by your memory. My two 32-core AMD CPUs with 1TB DRAM5 hold 100 million nodes and edges easily. But when I cross into a billion nodes and edges, I move onto a distributed graph engine, using my 12-node DataStax Enterprise data platform, with DSE Graph to handle the graphs, via the dse-graph library. I can confirm that the examples in this book run on my Quad-Core 4MB RAM laptop easily. Open your Python editor and create a file named Assess-DAG-GPS.py in directory C:\VKHCG\01-Vermeulen\02-Assess. Now copy the following code into the file: ################################################################ import networkx as nx import matplotlib.pyplot as plt import sys import os import pandas as pd ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + 'VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base:',Base,'using',sys.platform) print('################################') ################################################################ sInputFileName='01-Retrieve/01-EDS/02-Python/Retrieve_Router_Location.csv' sOutputFileName='Assess-DAG-Company-GPS.png' Company='01-Vermeulen' ################################################################ ### Import Company Data ################################################################ sFileName=Base + '/' + Company + '/' + sInputFileName print('################################') print('Loading:',sFileName) print('################################') CompanyData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") print('Loaded Company:',CompanyData.columns.values) print('################################') ################################################################ print(CompanyData) print('################################') print('Rows:',CompanyData.shape[0]) print('################################') ################################################################ G=nx.Graph() ################################################################ Tip I used a simple data science technique in the following code, by adding the round(x,1) on the latitude and longitude values. This reduces the amount of nodes required from 150 to 16, a 90% savings in node processing. Edges reduces the nodes from 11,175 to 120, a 99% savings in edge processing. If you use round(x,2), you get 117 nodes and 6786 edges, a 22% and 39% savings, respectively. This is a useful technique to use when you only want to test your findings at an estimated level. for i in range(CompanyData.shape[0]):     nLatitude=round(CompanyData['Latitude'][i],1)     nLongitude=round(CompanyData['Longitude'][i],1)     if nLatitude < 0:         sLatitude = str(nLatitude*-1) + 'S'     else:         sLatitude = str(nLatitude) + 'N'     if nLongitude < 0:         sLongitude = str(nLongitude*-1) + 'W'     else:         sLongitude = str(nLongitude) + 'E'     sGPS= sLatitude + '-' + sLongitude     G.add_node(sGPS) print('################################') for n1 in G.nodes():     for n2 in G.nodes():         if n1 != n2:             print('Link:',n1,'to',n2)             G.add_edge(n1,n2) print('################################') print('################################') print("Nodes of graph:") print(G.nodes()) print("Edges of graph:") print(G.edges()) print('################################') ################################################################ sFileDir=Base + '/' + Company + '/02-Assess/01-EDS/02-Python' if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ sFileName=sFileDir + '/' + sOutputFileName print('################################') print('Storing:', sFileName) print('################################') pos=nx.circular_layout(G,dim=1, scale=2) nx.draw(G,pos=pos,         nodecolor='r',edge_color='b',         with_labels=True,node_size=4000,         font_size=9) plt.savefig(sFileName) # save as png plt.show() # display ################################################################ Save the file Assess-DAG-GPS.py, then compile and execute with your Python compiler. This will produce a set of demonstrated values onscreen, plus a graphical file named Assess-DAG-Company-GPS.png (Figure [8-9](#Fig9)).![A435693_1_En_8_Fig9_HTML.jpg](Images/A435693_1_En_8_Fig9_HTML.jpg) Figure 8-9 Assess-DAG-Company-GPS.png file I have guided you through the basic concepts of using graph theory data structures to assess that you have the correct relationship between data entities. I will now demonstrate how to convert the IP data we retrieved into a valid routing network (Figure [8-10](#Fig10)) .![A435693_1_En_8_Fig10_HTML.jpg](Images/A435693_1_En_8_Fig10_HTML.jpg) Figure 8-10 Valid routing network The basic validation is as follows:

*   各国互联互通，形成网状网络。
*   国家内的地点连接到所有国内节点，即国家内所有网络连接集中在一起的一个点，称为“地点集中器”
*   Post 代码只能通过 post 代码集中器(类似于位置集中器的设备，但每个 post 代码有一个)连接到位置集中器。
*   位置只能连接到它们的邮政编码集中器。

Note There is more than one set of rules to interconnect the nodes. However, the result is still only one graph, when all the rules are applied. If there are overlapping connections between the rules, the graph will resolve only one relationship. This saves you from solving the overlaps. Nice to remember! Now test your skills against this example. I will guide you through the assess step for Retrieve_IP_DATA_CORE.csv. Open your Python editor and create a file called Assess-DAG-Schedule.py in directory C:\VKHCG\01-Vermeulen\02-Assess. Now copy this code into the following file: ################################################################ import networkx as nx import matplotlib.pyplot as plt import sys import os import pandas as pd ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + 'VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base:',Base,'using',sys.platform) print('################################') ################################################################ sInputFileName='01-Retrieve/01-EDS/01-R/Retrieve_IP_DATA_CORE.csv' sOutputFileName='Assess-DAG-Schedule.gml' Company='01-Vermeulen' ################################################################ ### Import Core Company Data ################################################################ sFileName=Base + '/' + Company + '/' + sInputFileName print('################################') print('Loading:',sFileName) print('################################') CompanyData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") print('Loaded Company:',CompanyData.columns.values) print('################################') ################################################################ print(CompanyData) print('################################') print('Rows:',CompanyData.shape[0]) print('################################') ################################################################ G=nx.Graph() ################################################################ for i in range(CompanyData.shape[0]):         sGroupName0= str(CompanyData['Country'][i])     sGroupName1= str(CompanyData['Place.Name'][i])     sGroupName2= str(CompanyData['Post.Code'][i])     nLatitude=round(CompanyData['Latitude'][i],6)     nLongitude=round(CompanyData['Longitude'][i],6)   Note You can see here that you can store extra information on the nodes and edges, to enhance the data stored in the graph, while still not causing issues with the basic node-to-edge relationship. You can add additional information by using the extension of the G.add_node() features.     CountryName=sGroupName0     print('Add Node:',sGroupName0)     G.add_node(CountryName,                routertype='CountryName',                group0=sGroupName0)     sPlaceName= sGroupName1 + '-' + sGroupName0     G.add_node(sPlaceName,                routertype='PlaceName',                group0=sGroupName0,                group1=sGroupName1)     sPostCodeName= sGroupName1 + '-' + sGroupName2 + '-' + sGroupName0     print('Add Node:',sPostCodeName)     G.add_node(sPostCodeName,                routertype='PostCode',                group0=sGroupName0,                group1=sGroupName1,                group2=sGroupName2)     if nLatitude < 0:         sLatitude = str(nLatitude*-1) + 'S'     else:         sLatitude = str(nLatitude) + 'N'        if nLongitude < 0:         sLongitude = str(nLongitude*-1) + 'W'     else:         sLongitude = str(nLongitude) + 'E'          sGPS= sLatitude + '-' + sLongitude     print('Add Node:',sGPS)     G.add_node(sGPS,routertype='GPS',                group0=sGroupName0,                group1=sGroupName1,                group2=sGroupName2,                sLatitude=sLatitude,                sLongitude=sLongitude,                nLatitude=nLatitude,                nLongitude=nLongitude) ################################################################ You use the extra information by using the extension of the G.node[][] features. Note, too, nodes_iter(G) , which gives you access to all the nodes. We will now add the rules between the nodes. print('################################') print('Link Country to Country') print('################################') for n1 in nx.nodes_iter(G):     if G.node[n1]['routertype'] == 'CountryName':         for n2 in nx.nodes_iter(G):             if G.node[n2]['routertype'] == 'CountryName':                 if n1 != n2:                     print('Link:',n1,'to',n2)                     G.add_edge(n1,n2) print('################################') print('################################') print('Link Country to Place') print('################################') for n1 in nx.nodes_iter(G):     if G.node[n1]['routertype'] == 'CountryName':         for n2 in nx.nodes_iter(G):             if G.node[n2]['routertype'] == 'PlaceName':                 if G.node[n1]['group0'] == G.node[n2]['group0']:                     if n1 != n2:                         print('Link:',n1,'to',n2)                         G.add_edge(n1,n2) print('################################') print('################################') print('Link Place to Post Code') print('################################') for n1 in nx.nodes_iter(G):     if G.node[n1]['routertype'] == 'PlaceName':         for n2 in nx.nodes_iter(G):             if G.node[n2]['routertype'] == 'PostCode':                 if G.node[n1]['group0'] == G.node[n2]['group0']:                     if G.node[n1]['group1'] == G.node[n2]['group1']:                         if n1 != n2:                             print('Link:',n1,'to',n2)                             G.add_edge(n1,n2) print('################################') print('################################') print('Link Post Code to GPS') print('################################') for n1 in nx.nodes_iter(G):     if G.node[n1]['routertype'] == 'PostCode':         for n2 in nx.nodes_iter(G):             if G.node[n2]['routertype'] == 'GPS':                 if G.node[n1]['group0'] == G.node[n2]['group0']:                     if G.node[n1]['group1'] == G.node[n2]['group1']:                         if G.node[n1]['group2'] == G.node[n2]['group2']:                             if n1 != n2:                                 print('Link:',n1,'to',n2)                                 G.add_edge(n1,n2) print('################################') print('################################') print("Nodes of graph:",nx.number_of_nodes(G)) print("Edges of graph:",nx.number_of_edges(G)) print('################################') ################################################################ sFileDir=Base + '/' + Company + '/02-Assess/01-EDS/02-Python' if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ sFileName=sFileDir + '/' + sOutputFileName print('################################') print('Storing:', sFileName) print('################################') nx.write_gml(G,sFileName) sFileName=sFileName +'.gz' nx.write_gml(G,sFileName) ################################################################ Save the Assess-DAG-Schedule.py file, then compile and execute with your Python compiler. This will produce a set of demonstrated values onscreen, plus a graph data file named Assess-DAG-Schedule.gml. Tip You can use a text editor to view the GML format. I use it many times to fault find graphs. If you open the file with a text editor, you can see the format it produces. The format is simple but effective. This is a post code node: node [     id 327     label "Munich-80331-DE"     routertype "PostCode"     group0 "DE"     group1 "Munich"     group2 "80331"   ] This is a GPS node: node [     id 328     label "48.1345 N-11.571 E"     routertype "GPS"     group0 "DE"     group1 "Munich"     group2 "80331"     sLatitude "48.1345 N"     sLongitude "11.571 E"     nLatitude 48.134500000000003     nLongitude 11.571   ] This is an edge connecting the two nodes:   edge [     source 327     target 328   ] This conversion to a graph data structure has now validated the comprehensive IP structure, by eliminating any duplicate node or edges that were created by the different rules or any duplicate entries in the base data. This natural de-duplication is highly useful when you have more than one system loading requests into the same system. For example: Two transport agents can request a truck to transport two different boxes from location A to location B. Before the truck leaves, the logistics program will create a shipping graph and then send one truck with the two boxes. I have proved that this works on the smaller IP core data. Now we can apply it to the larger IP data set called Retrieve_IP_DATA.csv. With the additional scope of the larger data set, I suggest that we process the data, by altering the input and export source design to include the SQLite database , to assist with the resolution of the data processing. As a data scientist, you should constantly explore the accessible tools to develop the effectiveness and efficiency of your solutions. I will guide you through the following processing pattern, to enable you to understand where I am using the strengths of the database engine to resolve the complexity the larger data set requires to complete some of the relationship. We will match the power of the graph data structure with the processing capability of the SQLite database. Warning If you have not yet used SQLite on your system, you must install it. See [www.sqlite.org](http://www.sqlite.org) for details on how to achieve this installation. You can check the installation by using the sqlite3 version. If you get a result of 3.20.x, you are ready to proceed. Make sure that you also have the python SQLite library installed. Perform an import SQLite in your IPython console. If this does not work, perform a library installation. Open your Python editor and create a file named Retrieve-IP_2_SQLite_2_DAG.py in directory C:\VKHCG\01-Vermeulen\02-Assess. Now copy the following code into the file: ################################################################ # -*- coding: utf-8 -*- ################################################################ import networkx as nx import sys import os import sqlite3 as sq import pandas as pd ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + 'VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base:',Base,'using',sys.platform) print('################################') ################################################################ sDatabaseName=Base + '/01-Vermeulen/00-RawData/SQLite/vermeulen.db' conn = sq.connect(sDatabaseName) ################################################################ sFileName=Base + '/01-Vermeulen/01-Retrieve/01-EDS/02-Python/Retrieve_IP_DATA.csv' sOutputFileName='Assess-DAG-Schedule-All.csv' Company='01-Vermeulen' Now we load the base data into memory as a pandas data structure , and then store it into a SQLite database using the df.tosql() command . ################################################################ print('Loading :',sFileName) IP_DATA_ALL=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") IP_DATA_ALL.index.names = ['RowIDCSV'] IP_DATA_ALL.rename(columns={'Place.Name': 'PlaceName'}, inplace=True) IP_DATA_ALL.rename(columns={'Post.Code': 'PostCode'}, inplace=True) #print(IP_DATA_ALL) print('################')   sTable='Assess_IP_DATA' print('Storing:',sDatabaseName,'Table:',sTable) IP_DATA_ALL.to_sql(sTable, conn, if_exists="replace") print('################')   We create a non-directed graph, using G=nx.Graph() . ################################################################ G=nx.Graph() ################################################################ We now create a subset of the loaded data from SQLite, to match the formats you require. Tip This offloading from the processing to a third-party processing engine enables you to connect to core resources, such as databases to then perform the complex joins and heavier processing. I normally also offload my graph processing in the same way, via DSE Graph, to enhance more complex graphs. To be able to successfully offload complex data processing is a good skill for a data scientist to have. Note I can use a small computer, such as a Raspberry Pi 3, to drive my clusters to perform massive processing tasks with ease. So, let’s restart our example. You must return the distinct list of countries from a data set that has duplicates. I suggest that you use the SQL DISTINCT command to perform the task, via the SQLite data engine. print('################')   sTable = 'Assess_IP_DATA' print('Loading:',sDatabaseName,'Table:',sTable) sSQL="select distinct" sSQL=sSQL+ "A.Country," sSQL=sSQL+ "A.Country AS NodeName," sSQL=sSQL+ "A.Country AS GroupName0," sSQL=sSQL+ "'Country-Router' AS RouterType" sSQL=sSQL+ "from" sSQL=sSQL+ "Assess_IP_DATA as A" sSQL=sSQL+ "ORDER BY A.Country;" CompanyData=pd.read_sql_query(sSQL, conn) print('################')   You now have the distinct list of countries in CompanyData . Well done. You now need to add these nodes to the graph you already created earlier. You have completed this task before, so just follow this code: for i in range(CompanyData.shape[0]):     sNode=str(CompanyData['NodeName'][i])     sRouterType=str(CompanyData['RouterType'][i])     sGroupName0=str(CompanyData['GroupName0'][i])     G.add_node(sNode,                routertype=sRouterType,                group0=sGroupName0) print('################################') print("Nodes of graph:",nx.number_of_nodes(G)) print("Edges of graph:",nx.number_of_edges(G)) print('################################') Well done, you are making progress here. You have all the country nodes. I suggest we store the list of countries for later as table Assess_IP_Country. print('################')   sTable='Assess_IP_Country' print('Storing:',sDatabaseName,'Table:',sTable) CompanyData.to_sql(sTable, conn, if_exists="replace") Now that you have the basic process, you should find this easy to repeat. Next, extract the distinct country and place, add the nodes, and store this as table Assess_IP_PlaceName. print('################') ################################################################ print('################')    sTable = 'Assess_IP_DATA' print('Loading:',sDatabaseName,'Table:',sTable) sSQL="select distinct" sSQL=sSQL+ "A.Country," sSQL=sSQL+ "A.PlaceName," sSQL=sSQL+ "A.PlaceName || '-' || A.Country AS NodeName," sSQL=sSQL+ "A.Country AS GroupName0," sSQL=sSQL+ "A.PlaceName AS GroupName1," sSQL=sSQL+ "'Place-Router' AS RouterType" sSQL=sSQL+ "from" sSQL=sSQL+ "Assess_IP_DATA as A" sSQL=sSQL+ "ORDER BY A.Country AND" sSQL=sSQL+ "A.PlaceName;" CompanyData=pd.read_sql_query(sSQL, conn) print('################')   for i in range(CompanyData.shape[0]):     sNode=str(CompanyData['NodeName'][i])     sRouterType=str(CompanyData['RouterType'][i])     sGroupName0= str(CompanyData['Country'][i])     sGroupName1= str(CompanyData['PlaceName'][i])     G.add_node(sNode,                routertype=sRouterType,                group0=sGroupName0,                group1=sGroupName1) print('################################') print("Nodes of graph:",nx.number_of_nodes(G)) print("Edges of graph:",nx.number_of_edges(G)) print('################################') print('################')   sTable='Assess_IP_PlaceName' print('Storing:',sDatabaseName,'Table:',sTable) CompanyData.to_sql(sTable, conn, if_exists="replace") print('################') ################################################################ You’ve now realized more success with the process. Can you now see the advantage of using the database to perform the work? Next, repeat for country, place-name, and node name data selected. Add nodes and store table as Assess_IP_PostCode. print('################')    sTable = 'Assess_IP_DATA' print('Loading:',sDatabaseName,'Table:',sTable) sSQL="select distinct" sSQL=sSQL+ "A.Country," sSQL=sSQL+ "A.PlaceName," sSQL=sSQL+ "A.PostCode," sSQL=sSQL+ "A.PlaceName || '-' || A.PostCode || '-' || A.Country AS NodeName," sSQL=sSQL+ "A.Country AS GroupName0," sSQL=sSQL+ "A.PlaceName AS GroupName1," sSQL=sSQL+ "A.PostCode AS GroupName2," sSQL=sSQL+ "'Place-Router' AS RouterType" sSQL=sSQL+ "from" sSQL=sSQL+ "Assess_IP_DATA as A" sSQL=sSQL+ "ORDER BY A.Country AND" sSQL=sSQL+ "A.PlaceName AND" sSQL=sSQL+ "A.PostCode;" CompanyData=pd.read_sql_query(sSQL, conn) print('################')   for i in range(CompanyData.shape[0]):     sNode=str(CompanyData['NodeName'][i])     sRouterType=str(CompanyData['RouterType'][i])     sGroupName0= str(CompanyData['GroupName0'][i])     sGroupName1= str(CompanyData['GroupName1'][i])     sGroupName2= str(CompanyData['GroupName2'][i])     G.add_node(sNode,                routertype=sRouterType,                group0=sGroupName0,                group1=sGroupName1,                group2=sGroupName2) print('################################') print("Nodes of graph:",nx.number_of_nodes(G)) print("Edges of graph:",nx.number_of_edges(G)) print('################################') print('################')   sTable='Assess_IP_PostCode' print('Storing:',sDatabaseName,'Table:',sTable) CompanyData.to_sql(sTable, conn, if_exists="replace") print('################') ################################################################ Next, repeat for country, place-name, post code, latitude, and longitude data selected. Add nodes and store table as Assess_IP_GPS. print('################')    sTable = 'Assess_IP_DATA' print('Loading:',sDatabaseName,'Table:',sTable) sSQL="select distinct" sSQL=sSQL+ "A.Country," sSQL=sSQL+ "A.PlaceName," sSQL=sSQL+ "A.PostCode," sSQL=sSQL+ "A.Latitude," sSQL=sSQL+ "A.Longitude," sSQL=sSQL+ "(CASE WHEN A.Latitude < 0 THEN" sSQL=sSQL+ "'S' || ABS(A.Latitude)" sSQL=sSQL+ "ELSE " sSQL=sSQL+ "'N' || ABS(A.Latitude)" sSQL=sSQL+ "END ) AS sLatitude," sSQL=sSQL+ "(CASE WHEN A.Longitude < 0 THEN" sSQL=sSQL+ "'W' || ABS(A.Longitude)" sSQL=sSQL+ "ELSE" sSQL=sSQL+ "'E' || ABS(A.Longitude)" sSQL=sSQL+ "END ) AS sLongitude" sSQL=sSQL+ "from" sSQL=sSQL+ "Assess_IP_DATA as A;" CompanyData=pd.read_sql_query(sSQL, conn) print('################')   sTable='Assess_IP_GPS' CompanyData.to_sql(sTable, conn, if_exists="replace") sSQL="select distinct" sSQL=sSQL+ "A.Country," sSQL=sSQL+ "A.PlaceName," sSQL=sSQL+ "A.PostCode," sSQL=sSQL+ "A.Latitude," sSQL=sSQL+ "A.Longitude," sSQL=sSQL+ "A.sLatitude," sSQL=sSQL+ "A.sLongitude," sSQL=sSQL+ "A.sLatitude || '-' || A.sLongitude AS NodeName," sSQL=sSQL+ "A.Country AS GroupName0," sSQL=sSQL+ "A.PlaceName AS GroupName1," sSQL=sSQL+ "A.PostCode AS GroupName2," sSQL=sSQL+ "'GPS-Client' AS RouterType" sSQL=sSQL+ "from" sSQL=sSQL+ "Assess_IP_GPS as A" sSQL=sSQL+ "ORDER BY A.Country AND" sSQL=sSQL+ "A.PlaceName AND" sSQL=sSQL+ "A.PostCode AND" sSQL=sSQL+ "A.Latitude AND" sSQL=sSQL+ "A.Longitude;" CompanyData=pd.read_sql_query(sSQL, conn) for i in range(CompanyData.shape[0]):     sNode=str(CompanyData['NodeName'][i])     sRouterType=str(CompanyData['RouterType'][i])     sGroupName0= str(CompanyData['GroupName0'][i])     sGroupName1= str(CompanyData['GroupName1'][i])     sGroupName2= str(CompanyData['GroupName2'][i])     nLatitude=round(CompanyData['Latitude'][i],6)     nLongitude=round(CompanyData['Longitude'][i],6)     sLatitude= str(CompanyData['sLatitude'][i])     sLongitude= str(CompanyData['sLongitude'][i])     G.add_node(sNode,                routertype=sRouterType,                group0=sGroupName0,                group1=sGroupName1,                group2=sGroupName2,                sLatitude=sLatitude,                sLongitude=sLongitude,                nLatitude=nLatitude,                nLongitude=nLongitude) print('################################') print("Nodes of graph:",nx.number_of_nodes(G)) print("Edges of graph:",nx.number_of_edges(G)) print('################################') print('################')   sTable='Assess_IP_GPS' print('Storing:',sDatabaseName,'Table:',sTable) CompanyData.to_sql(sTable, conn, if_exists="replace") print('################') ############################################################### Well done. You have created all the nodes. Now you will see the true power of using the database. We must link every country with every other country but never connect the country back to itself. This task would have been complex work without the tables we created while loading the nodes. Now we can perform a join between Assess_IP_Country twice and add an enhancement in the selection, performing only one direction edge creation, by adding the extra requirement of one country being smaller than the other country. The use of the Graph() type graph supplies the add_edge() link in the opposite direction for free. Hence, we have a +/- 50% saving in processing effort. So, proceed with the example to link the different types of nodes . print('################################') print('Link Country to Country') print('################################') print('################')   print('Loading:',sDatabaseName,'Table:',sTable) sSQL="select distinct" sSQL=sSQL+ "A.NodeName as N1," sSQL=sSQL+ "B.NodeName as N2" sSQL=sSQL+ "from" sSQL=sSQL+ "Assess_IP_Country as A," sSQL=sSQL+ "Assess_IP_Country as B" sSQL=sSQL+ "WHERE " sSQL=sSQL+ "A.Country < B.Country AND" sSQL=sSQL+ "A.NodeName <> B.NodeName;" CompanyData=pd.read_sql_query(sSQL, conn) print('################')       n1= str(CompanyData['N1'][i])         n2= str(CompanyData['N2'][i])     print('Link Country:',n1,'to Country:',n2)     G.add_edge(n1,n2) print('################################') print('################################') print("Nodes of graph:",nx.number_of_nodes(G)) print("Edges of graph:",nx.number_of_edges(G)) print('################################') ############################################################### print('################################') print('Link Country to Place') print('################################') print('################')   print('Loading:',sDatabaseName,'Table:',sTable) sSQL="select distinct" sSQL=sSQL+ "A.NodeName as N1," sSQL=sSQL+ "B.NodeName as N2" sSQL=sSQL+ "from" sSQL=sSQL+ "Assess_IP_Country as A," sSQL=sSQL+ "Assess_IP_PlaceName as B" sSQL=sSQL+ "WHERE " sSQL=sSQL+ "A.Country = B.Country AND" sSQL=sSQL+ "A.NodeName <> B.NodeName;" CompanyData=pd.read_sql_query(sSQL, conn) print('################')   for i in range(CompanyData.shape[0]):         n1= str(CompanyData['N2'][i])     n2= str(CompanyData['N2'][i])     print('Link Country:',n1,'to Place:',n2)     G.add_edge(n1,n2) print('################################') print('################################') print("Nodes of graph:",nx.number_of_nodes(G)) print("Edges of graph:",nx.number_of_edges(G)) print('################################') ############################################################### print('################################') print('Link Place to Post Code') print('################################') print('################')   print('Loading:',sDatabaseName,'Table:',sTable) sSQL="select distinct" sSQL=sSQL+ "A.NodeName as N1," sSQL=sSQL+ "B.NodeName as N2" sSQL=sSQL+ "from" sSQL=sSQL+ "Assess_IP_PlaceName as A," sSQL=sSQL+ "Assess_IP_PostCode as B" sSQL=sSQL+ "WHERE " sSQL=sSQL+ "A.Country = B.Country AND" sSQL=sSQL+ "A.PlaceName = B.PlaceName AND" sSQL=sSQL+ "A.NodeName <> B.NodeName;" CompanyData=pd.read_sql_query(sSQL, conn) print('################')   for i in range(CompanyData.shape[0]):         n1= str(CompanyData['N1'][i])         n2= str(CompanyData['N2'][i])     print('Link Place:',n1,'to Post Code:',n2)     G.add_edge(n1,n2) print('################################') print('################################') print("Nodes of graph:",nx.number_of_nodes(G)) print("Edges of graph:",nx.number_of_edges(G)) print('################################') ############################################################### print('################################') print('Link Post Code to GPS') print('################################') print('################')   print('Loading:',sDatabaseName,'Table:',sTable) sSQL="select distinct" sSQL=sSQL+ "A.NodeName as N1," sSQL=sSQL+ "B.NodeName as N2" sSQL=sSQL+ "from" sSQL=sSQL+ "Assess_IP_PostCode as A," sSQL=sSQL+ "Assess_IP_GPS as B" sSQL=sSQL+ "WHERE " sSQL=sSQL+ "A.Country = B.Country AND" sSQL=sSQL+ "A.PlaceName = B.PlaceName AND" sSQL=sSQL+ "A.PostCode = B.PostCode AND" sSQL=sSQL+ "A.NodeName <> B.NodeName;" CompanyData=pd.read_sql_query(sSQL, conn) print('################')   for i in range(CompanyData.shape[0]):         n1= str(CompanyData['N1'][i])         n2= str(CompanyData['N2'][i])     print('Link Post Code:',n1,'to GPS:',n2)     G.add_edge(n1,n2) print('################################') print('################################') print("Nodes of graph:",nx.number_of_nodes(G)) print("Edges of graph:",nx.number_of_edges(G)) print('################################') Congratulations! You have mastered the graph and database consolidation techniques. Now, save your graph into the Assess-DAG-Schedule-All.gml file. ################################################################ sFileDir=Base + '/' + Company + '/02-Assess/01-EDS/02-Python' if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ sFileName=sFileDir + '/' + sOutputFileName print('################################') print('Storing:',sFileName) print('################################') nx.write_gml(G,sFileName) sFileName=sFileName +'.gz' nx.write_gml(G,sFileName) ################################################################ ################################################################ print('### Done!! ############################################') ################################################################ Save the file Retrieve-IP_2_SQLite_2_DAG.py, then compile and execute with your Python compiler. This will produce a set of demonstrated values onscreen, plus a graph data file named Assess-DAG-Schedule-All.gml. Note You can view the GML file with a text editor. This produces nodes and edges. This type of analysis is possible in a graph environment, as it is a close-to-reality match with the tangible business problem it is modeling. Graph theory is always a useful tool to use when relationships between business entities require analyzing. I have now completed the assess step for Vermeulen PLC. At this point, you should be comfortable loading data from files and databases. You can perform SQL requests that offload the work to the database engine. You must, however, understand the format of a graph and how to construct the rules into the graph your customers require. Tip If you can draw it on a whiteboard, you can make it a graph. Debugging Tip You can open the GML format and use it to draw the nodes on a whiteboard. This is because there are only nodes and edges in the format. The following two commands will remove nodes (G.remove_node()) and edges (remove_edge(,)) from an existing graph. So, if you want to enforce rules that state you cannot attach two specific nodes, you can simply remove those by performing a remove action. You can also use graphs to perform what-if scenarios, such as the following: What if we sell the New York office; what will be the impact on our network schedules? The simple action required is to load the graph, remove the node, and save it, as follows: G=nx.read_gml(<location of saved graph>) G.remove_node('New York') nx.write_gml(G,(<location of new saved graph>) Any schedules attached to New York are now removed. If you understand the basic concepts graphs and databases, I suggest that you get some refreshment, and then we can progress to the next set of examples.

### Krennwallner 股份有限公司

Krennwallner AG is the German company that handles all our media business. The company will be used to demonstrate three basic assessment solutions.

#### 为广告牌挑选内容

Krennwallner is responsible for publicizing the customer’s advertisements on a set of billboards that the group owns. I will explain the concepts that are required to assess the information relating to billboards that we must process. You will use the sqlite3 and pandas packages to create a solution. You performed similar actions related to Vermeulen, so the following should be an example that you can handle. The basic process required is to combine two sets of data and then calculate the number of visitors per day from the range of IP addresses that access the billboards in Germany. Open your Python editor and create a file named Assess-DE-Billboard.py in directory C:\VKHCG\02-Krennwallner\02-Assess. Now copy the following code into the file: ################################################################ import sys import os import sqlite3 as sq import pandas as pd ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + 'VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base:',Base,'using',sys.platform) print('################################') ################################################################ The two data sources we must combine are listed following: sInputFileName1='01-Retrieve/01-EDS/02-Python/Retrieve_DE_Billboard_Locations.csv' sInputFileName2='01-Retrieve/01-EDS/02-Python/Retrieve_Online_Visitor.csv' The results will be stored in this output file: sOutputFileName='Assess-DE-Billboard-Visitor.csv' Company='02-Krennwallner' ################################################################ sDataBaseDir=Base + '/' + Company + '/02-Assess/SQLite' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ The database that will support the processing for this example follows: sDatabaseName=sDataBaseDir + '/krennwallner.db' conn = sq.connect(sDatabaseName) ################################################################ ### Import Billboard Data ################################################################ sFileName=Base + '/' + Company + '/' + sInputFileName1 print('################################') print('Loading:',sFileName) print('################################') Load the first file and clean up the duplicates. BillboardRawData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") BillboardRawData.drop_duplicates(subset=None, keep="first", inplace=True) BillboardData=BillboardRawData print('Loaded Company:',BillboardData.columns.values) print('################################') ################################################################ Store the cleanly loaded data into SQLite. print('################')   sTable='Assess_BillboardData' print('Storing:',sDatabaseName,'Table:',sTable) BillboardData.to_sql(sTable, conn, if_exists="replace") print('################')   ################################################################ print(BillboardData.head()) print('################################') print('Rows:',BillboardData.shape[0]) print('################################') Load the second file and clean up the duplicates. ################################################################ ### Import Billboard Data ################################################################ sFileName=Base + '/' + Company + '/' + sInputFileName2 print('################################') print('Loading:',sFileName) print('################################') VisitorRawData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") VisitorRawData.drop_duplicates(subset=None, keep="first", inplace=True) With the country=DE result, you can now limit the data to be Germany only. This early discarding of superfluous data is a big contributor to effective data science. Tip Keep the data set under investigation to the minimum size required. This saves you and your customers time and money. VisitorData=VisitorRawData[VisitorRawData.Country=='DE'] print('Loaded Company:',VisitorData.columns.values) print('################################') ################################################################ Store the second set of cleanly loaded data into SQLite. print('################')   sTable='Assess_VisitorData' print('Storing:',sDatabaseName,'Table:',sTable) VisitorData.to_sql(sTable, conn, if_exists="replace") print('################')   ################################################################ print(VisitorData.head()) print('################################') print('Rows:',VisitorData.shape[0]) print('################################') ################################################################ Retrieve an indirectly cleaned data set from SQLite, because you stored the clean data. The reduction in the second data set now also improves the process, with increased processing efficiencies. print('################')   sTable='Assess_BillboardVisitorData' print('Loading:',sDatabaseName,'Table:',sTable) sSQL="select distinct" sSQL=sSQL+ "A.Country AS BillboardCountry," sSQL=sSQL+ "A.Place_Name AS BillboardPlaceName," sSQL=sSQL+ "A.Latitude AS BillboardLatitude," sSQL=sSQL+ "A.Longitude AS BillboardLongitude," sSQL=sSQL+ "B.Country AS VisitorCountry," sSQL=sSQL+ "B.Place_Name AS VisitorPlaceName," sSQL=sSQL+ "B.Latitude AS VisitorLatitude," sSQL=sSQL+ "B.Longitude AS VisitorLongitude," sSQL=sSQL+ "(B.Last_IP_Number - B.First_IP_Number) * 365.25 * 24 * 12 AS VisitorYearRate" sSQL=sSQL+ "from" sSQL=sSQL+ "Assess_BillboardData as A" sSQL=sSQL+ "JOIN" sSQL=sSQL+ "Assess_VisitorData as B" sSQL=sSQL+ "ON" sSQL=sSQL+ "A.Country = B.Country" sSQL=sSQL+ "AND" sSQL=sSQL+ "A.Place_Name = B.Place_Name;" BillboardVistorsData=pd.read_sql_query(sSQL, conn) print('################')   ################################################################ Store the combined set of data back into SQLite. print('################')   sTable='Assess_BillboardVistorsData' print('Storing:',sDatabaseName,'Table:',sTable) BillboardVistorsData.to_sql(sTable, conn, if_exists="replace") print('################')   ################################################################ print(BillboardVistorsData.head()) print('################################') print('Rows:',BillboardVistorsData.shape[0]) print('################################') ################################################################ sFileDir=Base + '/' + Company + '/02-Assess/01-EDS/02-Python' if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ print('################################') print('Storing:', sFileName) print('################################') sFileName=sFileDir + '/' + sOutputFileName BillboardVistorsData.to_csv(sFileName, index = False) print('################################') ################################################################ print('### Done!! ############################################') ################################################################ Save the Assess-DE-Billboard.py, then compile and execute with your Python compiler. This will produce a set of demonstrated values onscreen, plus a data file named Assess-DE-Billboard-Visitor.csv.

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 广告牌国家 | 广告牌地名 | 广告牌纬度 | 告示牌经度 | 游客国家 | 游客地名 | 游客纬度 | 游客经度 | 游客年率 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 特拉华州 | 湖 | 51.7833 | 8.5667 | 特拉华州 | 湖 | 51.7833 | 8.5667 | Twenty-six million eight hundred and twenty-three thousand nine hundred and sixty |
| 特拉华州 | 霍布 | 48.4333 | 8.6833 | 特拉华州 | 霍布 | 48.4333 | 8.6833 | Twenty-six million eight hundred and twenty-three thousand nine hundred and sixty |
| 特拉华州 | 霍布 | 48.4333 | 8.6833 | 特拉华州 | 霍布 | 48.4333 | 8.6833 | Fifty-three million seven hundred and fifty-three thousand one hundred and twelve |

You should now understand that you can offload the SQL to the SQLite database, to gain easy access to the data processing power of that the SQL system enables. We will now investigate the next example.

#### 了解您的在线访问者数据

Online visitors have to be mapped to their closest billboard, to ensure we understand where and what they can access. To achieve this, I will guide you through a graph data processing example, to link the different entities involved in a graph of the visitor activity. The data that we have, however, is stored with some issues.

*   广告牌的名字不见了。通过一些特征工程，我们可以从经度和纬度值中推断出这些值。
*   广告牌和游客之间的距离是未知的。通过一些特征工程，通过文森特公式，这是可以找到的。
*   经度和纬度需要平滑，以符合广告牌命名格式协议。

What are Vincenty’s formulae? Thaddeus Vincenty’s formulae are two related iterative methods used in geodesy to calculate the distance between two points on the surface of a spheroid. They assume that the true shape of Earth is an oblate spheroid and, therefore, are more accurate than methods that assume a spherical Earth. The distance is called the great-circle distance. (See Figure [8-11](#Fig11))![A435693_1_En_8_Fig11_HTML.jpg](Images/A435693_1_En_8_Fig11_HTML.jpg) Figure 8-11 Thaddeus Vincenty’s formulae in graphic form Tip In data science, it is always sensible to use proven techniques that have the backing of other experts. Thaddeus Vincenty is an expert in calculating distances on Earth. The geopy library also has a proven Vincenty calculator. The wise data scientist uses these proven formulae, as they give your work additional credibility. You will use the networkx, sqlite3, pandas, and geopy packages to create a solution. Open your Python editor and create a file called Assess-Billboard_2_Visitor.py in directory C:\VKHCG\ 02-Krennwallner\02-Assess. Now copy the following code into the file: ################################################################ # -*- coding: utf-8 -*- ################################################################ import networkx as nx import sys import os import sqlite3 as sq import pandas as pd from geopy.distance import vincenty ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + 'VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base:',Base,'using',sys.platform) print('################################') ################################################################ Company='02-Krennwallner' sTable='Assess_BillboardVisitorData' sOutputFileName='Assess-DE-Billboard-Visitor.gml' ################################################################ sDataBaseDir=Base + '/' + Company + '/02-Assess/SQLite' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataBaseDir + '/krennwallner.db' conn = sq.connect(sDatabaseName) ################################################################ print('################')   print('Loading:',sDatabaseName,'Table:',sTable) sSQL="select" sSQL=sSQL+ "A.BillboardCountry," sSQL=sSQL+ "A.BillboardPlaceName," We are smoothing the data using a round(x, 3) formula. This ensures that there is no billboard that has nonstandard numbers. sSQL=sSQL+ "ROUND(A.BillboardLatitude,3) AS BillboardLatitude," sSQL=sSQL+ "ROUND(A.BillboardLongitude,3) AS BillboardLongitude," sSQL=sSQL+ "(CASE WHEN A.BillboardLatitude < 0 THEN" sSQL=sSQL+ "'S' || ROUND(ABS(A.BillboardLatitude),3)" sSQL=sSQL+ "ELSE " sSQL=sSQL+ "'N' || ROUND(ABS(A.BillboardLatitude),3)" sSQL=sSQL+ "END ) AS sBillboardLatitude," sSQL=sSQL+ "(CASE WHEN A.BillboardLongitude < 0 THEN" sSQL=sSQL+ "'W' || ROUND(ABS(A.BillboardLongitude),3)" sSQL=sSQL+ "ELSE " sSQL=sSQL+ "'E' || ROUND(ABS(A.BillboardLongitude),3)" sSQL=sSQL+ "END ) AS sBillboardLongitude," sSQL=sSQL+ "A.VisitorCountry," sSQL=sSQL+ "A.VisitorPlaceName," sSQL=sSQL+ "ROUND(A.VisitorLatitude,3) AS VisitorLatitude," sSQL=sSQL+ "ROUND(A.VisitorLongitude,3) AS VisitorLongitude," sSQL=sSQL+ "(CASE WHEN A.VisitorLatitude < 0 THEN" sSQL=sSQL+ "'S' || ROUND(ABS(A.VisitorLatitude),3)" sSQL=sSQL+ "ELSE " sSQL=sSQL+ "'N' ||ROUND(ABS(A.VisitorLatitude),3)" sSQL=sSQL+ "END ) AS sVisitorLatitude," sSQL=sSQL+ "(CASE WHEN A.VisitorLongitude < 0 THEN" sSQL=sSQL+ "'W' || ROUND(ABS(A.VisitorLongitude),3)" sSQL=sSQL+ "ELSE " sSQL=sSQL+ "'E' || ROUND(ABS(A.VisitorLongitude),3)" sSQL=sSQL+ "END ) AS sVisitorLongitude," sSQL=sSQL+ "A.VisitorYearRate" sSQL=sSQL+ "from" sSQL=sSQL+ "Assess_BillboardVistorsData AS A;" BillboardVistorsData=pd.read_sql_query(sSQL, conn) print('################')   ################################################################ With this single command, we apply Vincenty’s formulae, and we are sure we can support the result in miles. BillboardVistorsData['Distance']=BillboardVistorsData.apply(lambda row:     round(         vincenty((row['BillboardLatitude'],row['BillboardLongitude']),                  (row['VisitorLatitude'],row['VisitorLongitude'])).miles              ,4)        ,axis=1) We now use the newly calculated distance, plus the billboard naming fix, to generate a graph of the billboards and visitors. ################################################################ G=nx.Graph() ################################################################ for i in range(BillboardVistorsData.shape[0]):     sNode0='MediaHub-' + BillboardVistorsData['BillboardCountry'][i]     sNode1='B-'+ BillboardVistorsData['sBillboardLatitude'][i] + '-'     sNode1=sNode1 + BillboardVistorsData['sBillboardLongitude'][i]     G.add_node(sNode1,                Nodetype='Billboard',                Country=BillboardVistorsData['BillboardCountry'][i],                PlaceName=BillboardVistorsData['BillboardPlaceName'][i],                Latitude=round(BillboardVistorsData['BillboardLatitude'][i],3),                Longitude=round(BillboardVistorsData['BillboardLongitude'][i],3))     sNode2='M-'+ BillboardVistorsData['sVisitorLatitude'][i] + '-'     sNode2=sNode2 + BillboardVistorsData['sVisitorLongitude'][i]     G.add_node(sNode2,                Nodetype='Mobile',                Country=BillboardVistorsData['VisitorCountry'][i],                PlaceName=BillboardVistorsData['VisitorPlaceName'][i],                Latitude=round(BillboardVistorsData['VisitorLatitude'][i],3),                Longitude=round(BillboardVistorsData['VisitorLongitude'][i],3))     print('Link Media Hub:',sNode0,'to Billboard:',sNode1)     G.add_edge(sNode0,sNode1)     print('Link Post Code:',sNode1,'to GPS:',sNode2)     G.add_edge(sNode1,sNode2,distance=round(BillboardVistorsData['Distance'][i])) ################################################################             print('################################') print("Nodes of graph:",nx.number_of_nodes(G)) print("Edges of graph:",nx.number_of_edges(G)) print('################################') Well done. You have a good graph that you can now store for future queries. ################################################################ sFileDir=Base + '/' + Company + '/02-Assess/01-EDS/02-Python' if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ sFileName=sFileDir + '/' + sOutputFileName print('################################') print('Storing:', sFileName) print('################################') nx.write_gml(G,sFileName) sFileName=sFileName +'.gz' nx.write_gml(G,sFileName) ################################################################ ################################################################ print('### Done!! ############################################') ################################################################ Save the file Assess-Billboard_2_Visitor.py, then compile and execute with your Python compiler. This will produce a set of demonstrated values onscreen, plus a graph data file named Assess-DE-Billboard-Visitor.gml. Note You can view the GML file with a text editor.

#### 你的成就总结

*   您可以使用自己的公式来提取特征，例如广告牌名称。
*   你可以成功地使用其他人的公式来提取特征，例如广告牌和游客之间的距离。
*   您可以使用 df.apply(lambda row)技术将单个公式应用于内存中的数据集。

#### 为前十名客户策划活动

It is necessary to generate on a regular basis a preapproved view of the data you are assessing. I will guide you through a process called processing offloading, to find the top-ten customers . I will also show you how to use SQLite to offload processing, using views. The result is that the heavy processing is performed by the database engine. This is also useful when running long-running processes. Tip This is the way you would offload processing to systems such as Spark, Hadoop, or Cassandra. The next examples will demonstrate how you can permanently offload processing to the database engine, by creating views. Once created, you can, in future, use these views for other data science projects. You will use the sqlite3 and pandas packages to create a solution. Open your Python editor and create a file named Assess-Visitors.py in directory C:\VKHCG\ 02-Krennwallner\02-Assess. Now copy the following code into the file: ################################################################ import sys import os import sqlite3 as sq import pandas as pd from pandas.io import sql ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + 'VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base:',Base,'using',sys.platform) print('################################') ################################################################ Company='02-Krennwallner' sInputFileName='01-Retrieve/01-EDS/02-Python/Retrieve_Online_Visitor.csv' ################################################################ sDataBaseDir=Base + '/' + Company + '/02-Assess/SQLite' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataBaseDir + '/krennwallner.db' conn = sq.connect(sDatabaseName) ################################################################ ### Import Country Data ################################################################ At this point, we load the data into memory. sFileName=Base + '/' + Company + '/' + sInputFileName print('################################') print('Loading:',sFileName) print('################################') VisitorRawData=pd.read_csv(sFileName,                            header=0,                            low_memory=False,                            encoding="latin-1",                            skip_blank_lines=True) VisitorRawData.drop_duplicates(subset=None, keep="first", inplace=True) VisitorData=VisitorRawData print('Loaded Company:',VisitorData.columns.values) print('################################') ################################################################ At this point. we store the data from memory into a database. print('################')   sTable='Assess_Visitor' print('Storing:',sDatabaseName,'Table:',sTable) VisitorData.to_sql(sTable, conn, if_exists="replace") print('################')   ################################################################ print(VisitorData.head()) print('################################') print('Rows:',VisitorData.shape[0]) print('################################') ################################################################ We store the first set of data rules into the database at this point. print('################')   sView='Assess_Visitor_UseIt' print('Creating:',sDatabaseName,'View:',sView) sSQL="DROP VIEW IF EXISTS"+ sView + ";" sql.execute(sSQL,conn) sSQL="CREATE VIEW "+ sView + "AS" sSQL=sSQL+ "SELECT" sSQL=sSQL+ "A.Country," sSQL=sSQL+ "A.Place_Name," sSQL=sSQL+ "A.Latitude," sSQL=sSQL+ "A.Longitude," sSQL=sSQL+ "(A.Last_IP_Number - A.First_IP_Number) AS UsesIt" sSQL=sSQL+ "FROM" sSQL=sSQL+ "Assess_Visitor as A" sSQL=sSQL+ "WHERE" sSQL=sSQL+ "Country is not null" sSQL=sSQL+ "AND" sSQL=sSQL+ "Place_Name is not null;" sql.execute(sSQL,conn) ################################################################# At this point, we store the second set of data rules into the database. print('################')   sView='Assess_Total_Visitors_Location' print('Creating:',sDatabaseName,'View:',sView) sSQL="DROP VIEW IF EXISTS"+ sView + ";" sql.execute(sSQL,conn) sSQL="CREATE VIEW"+ sView + "AS" sSQL=sSQL+ "SELECT" sSQL=sSQL+ "Country," sSQL=sSQL+ "Place_Name," sSQL=sSQL+ "SUM(UsesIt) AS TotalUsesIt" sSQL=sSQL+ "FROM" sSQL=sSQL+ "Assess_Visitor_UseIt" sSQL=sSQL+ "GROUP BY" sSQL=sSQL+ "Country," sSQL=sSQL+ "Place_Name" sSQL=sSQL+ "ORDER BY" sSQL=sSQL+ "TotalUsesIt DESC" sSQL=sSQL+ "LIMIT 10;" sql.execute(sSQL,conn) ################################################################# At this point, we store the third set of data rules into the database. print('################')   sView='Assess_Total_Visitors_GPS' print('Creating:',sDatabaseName,'View:',sView) sSQL="DROP VIEW IF EXISTS "+ sView + ";" sql.execute(sSQL,conn) sSQL="CREATE VIEW"+ sView + "AS" sSQL=sSQL+ "SELECT" sSQL=sSQL+ "Latitude," sSQL=sSQL+ "Longitude," sSQL=sSQL+ "SUM(UsesIt) AS TotalUsesIt" sSQL=sSQL+ "FROM" sSQL=sSQL+ "Assess_Visitor_UseIt" sSQL=sSQL+ "GROUP BY" sSQL=sSQL+ "Latitude," sSQL=sSQL+ "Longitude" sSQL=sSQL+ "ORDER BY" sSQL=sSQL+ "TotalUsesIt DESC" sSQL=sSQL+ "LIMIT 10;" sql.execute(sSQL,conn) ################################################################# We now simply loop through the business rules views and extract the data we need into text files. sTables=['Assess_Total_Visitors_Location', 'Assess_Total_Visitors_GPS'] for sTable in sTables:     print('################')       print('Loading:',sDatabaseName,'Table:',sTable)     sSQL="SELECT"     sSQL=sSQL+ "*"     sSQL=sSQL+ "FROM"     sSQL=sSQL+ " "+ sTable + ";"     TopData=pd.read_sql_query(sSQL, conn)     print('################')       print(TopData)     print('################')       print('################################')     print('Rows:',TopData.shape[0])     print('################################') ################################################################ print('### Done!! ############################################') ################################################################ Save the file Assess-Visitors.py, then compile and execute with your Python compiler. You should see two summary results like the following:

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 评估 _ 总计 _ 访客 _ 位置 |
| --- |
|   | 国家 | 地点名称 | TotalUsesIt |
| --- | --- | --- | --- |
| Zero | 通信网络（Communicating Net 的缩写） | 北京 | Fifty-three million one hundred and thirty-nine thousand four hundred and seventy-five |
| one | 美国 | 高杆 | Thirty-three million six hundred and eighty-two thousand three hundred and forty-one |
| Two | 美国 | 华丘堡 | Thirty-three million four hundred and seventy-two thousand four hundred and twenty-seven |
| three | 地方官 | 东京 | Thirty-one million four hundred and four thousand seven hundred and ninety-nine |
| four | 美国 | 剑桥 | Twenty-five million five hundred and ninety-eight thousand eight hundred and fifty-one |
| five | 美国 | 圣迭戈 | Seventeen million seven hundred and fifty-one thousand three hundred and sixty-seven |
| six | 通信网络（Communicating Net 的缩写） | 广州。亦称 CＡＮＴＯＮ | Seventeen million five hundred and sixty-three thousand seven hundred and forty-four |
| seven | 美国 | 纽瓦克 | Seventeen million two hundred and seventy thousand six hundred and four |
| eight | 美国 | 罗利 | Seventeen million one hundred and sixty-seven thousand four hundred and eighty-four |
| nine | 美国 | 达勒姆 | Sixteen million nine hundred and fourteen thousand and thirty-three |

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 评估 _ 总计 _ 访客 _ 全球定位系统 |   |
| --- | --- |
|   | 纬度 | 经度 | TotalUsesIt |
| --- | --- | --- | --- |
| Zero | 39.9289 | 116.3883 | Fifty-three million one hundred and thirty-nine thousand seven hundred and thirty-two |
| one | 37.3762 | -122.1826 | Thirty-three million five hundred and fifty-one thousand four hundred and four |
| Two | 31.5273 | -110.3607 | Thirty-three million four hundred and seventy-two thousand four hundred and twenty-seven |
| three | 35.6427 | 139.7677 | Thirty-one million four hundred and thirty-nine thousand seven hundred and seventy-two |
| four | 23.1167 | One hundred and thirteen point two five | Seventeen million five hundred and seventy-seven thousand and fifty-three |
| five | 42.3646 | -71.1028 | Sixteen million eight hundred and ninety thousand six hundred and ninety-eight |
| six | 40.7355 | -74.1741 | Sixteen million eight hundred and thirteen thousand three hundred and seventy-three |
| seven | 42.3223 | -83.1763 | Sixteen million seven hundred and seventy-seven thousand two hundred and twelve |
| eight | 35.7977 | -78.6253 | Sixteen million seven hundred and sixty-one thousand and eighty-four |
| nine | 32.8072 | -117.1649 | Sixteen million seven hundred and forty-seven thousand six hundred and eighty |

This is the concluding assess process requirement for Krennwallner.

#### Krennwallner AG 的成就总结

*   您已经成功地使用 pandas 处理了内存和磁盘数据的生成。
*   您已经成功地使用 networkx 来处理图形数据。
*   您已经成功地使用 sqlite3 来访问 sqlite 表，以帮助进行数据处理。
*   您已经成功地使用 pandas 设置了 SQLite 视图，以在数据库中持久化 SQL 规则，并将处理任务卸载到 SQLite 数据库引擎。

### 希尔曼有限公司

Hillman Ltd is our logistics company and works with data related to locations and the distances between these locations.

#### 规划仓库的位置

Planning the location of the warehouses requires the assessment of the GPS locations of these warehouses against the requirements for Hillman’s logistics needs. Warning The online services provider that supplies the information has a Nominatim API limit of a maximum of one request per second. See [https://operations.osmfoundation.org/policies/nominatim/](https://operations.osmfoundation.org/policies/nominatim/) . I have limited the requests by limiting the amount of requests per session. You can install your own services: see [https://wiki.openstreetmap.org/wiki/Nominatim/Installation](https://wiki.openstreetmap.org/wiki/Nominatim/Installation) . For the purposes of this book, the limited usage is achieving what I intended. You will use the geopy and pandas packages to create a solution. Open your Python editor and create a file named Assess-Warehouse-Address.py in directory C:\VKHCG\03-Hillman\02-Assess. Now copy the following code into the file: ################################################################ ################################################################ # -*- coding: utf-8 -*- ################################################################ import os import pandas as pd from geopy.geocoders import Nominatim geolocator = Nominatim() ################################################################ InputDir='01-Retrieve/01-EDS/01-R' InputFileName='Retrieve_GB_Postcode_Warehouse.csv' EDSDir='02-Assess/01-EDS' OutputDir=EDSDir + '/02-Python' OutputFileName='Assess_GB_Warehouse_Address.csv' Company='03-Hillman' ################################################################ Base='C:/VKHCG' ################################################################ sFileDir=Base + '/' + Company + '/' + EDSDir if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ sFileDir=Base + '/' + Company + '/' + OutputDir if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ sFileName=Base + '/' + Company + '/' + InputDir + '/' + InputFileName print('###########') print('Loading:',sFileName) Warehouse=pd.read_csv(sFileName,header=0,low_memory=False) Warehouse.sort_values(by='postcode', ascending=1) ################################################################ ## Limited to 10 due to service limit on Address Service. ################################################################ WarehouseGoodHead=Warehouse[Warehouse.latitude != 0].head(5) WarehouseGoodTail=Warehouse[Warehouse.latitude != 0].tail(5) The next section takes the ten warehouse locations and uses the online service to obtain a full address for the longitude and latitude supplied to the API call. The queries are performed in two cycles of five records each. First, we fix the parameter that is required for each row in memory. ################################################################ WarehouseGoodHead['Warehouse_Point']=WarehouseGoodHead.apply(lambda row:             (str(row['latitude'])+','+str(row['longitude']))             ,axis=1) Second, we use the new parameter to call the address locator services. WarehouseGoodHead['Warehouse_Address']=WarehouseGoodHead.apply(lambda row:             geolocator.reverse(row['Warehouse_Point']).address            ,axis=1) Third, we remove the unwanted columns from the data in memory. WarehouseGoodHead.drop('Warehouse_Point', axis=1, inplace=True) WarehouseGoodHead.drop('id', axis=1, inplace=True) WarehouseGoodHead.drop('postcode', axis=1, inplace=True) Here, we simply repeat the previous three steps for a second cycle of five queries. ################################################################ WarehouseGoodTail['Warehouse_Point']=WarehouseGoodTail.apply(lambda row:             (str(row['latitude'])+','+str(row['longitude']))             ,axis=1) WarehouseGoodTail['Warehouse_Address']=WarehouseGoodTail.apply(lambda row:             geolocator.reverse(row['Warehouse_Point']).address            ,axis=1) WarehouseGoodTail.drop('Warehouse_Point', axis=1, inplace=True) WarehouseGoodTail.drop('id', axis=1, inplace=True) WarehouseGoodTail.drop('postcode', axis=1, inplace=True) ################################################################ WarehouseGood=WarehouseGoodHead.append(WarehouseGoodTail, ignore_index=True) print(WarehouseGood) ################################################################ Here we store our now enhanced data set. sFileName=sFileDir + '/' + OutputFileName WarehouseGood.to_csv(sFileName, index = False) ################################################################# print('### Done!! ############################################') ################################################################# ################################################################ Save the Assess-Warehouse-Address.py file , then compile and execute with your Python compiler. This will produce a set of demonstrated values onscreen, plus a graph data file named Assess_GB_Warehouse_Address.csv.

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 纬度 | 经度 | 仓库 _ 地址 |
| --- | --- | --- |
| 57.13514 | -2.11731 | 英国苏格兰阿伯丁市阿伯丁凯姆希尔布鲁姆希尔路 35 号 |
| 57.13875 | -2.09089 | 英国苏格兰阿伯丁市阿伯丁托里南滨海艺术中心西 |
| Fifty-seven point one zero one | -2.1106 | 英国苏格兰阿伯丁市，阿伯丁，金科尔斯，A92 |

##### 成就摘要

*   现在，您可以通过 API 将处理任务卸载到在线服务上。
*   您可以调整内部参数以符合外部要求。
*   您可以将这种模式扩展到许多其他服务提供商。(参见[https://geopy.readthedocs.io/en/1.11.0/](https://geopy.readthedocs.io/en/1.11.0/)

#### 全球新仓库

Hillman wants to add extra global warehouses, and you are required to assess where they should be located. We only have to collect the possible locations for warehouses. In Chapters [9](09.html) and [10](10.html), we will use data science, via clustering, to determine a set of new warehouses for the company. The following example will show you how to modify the data columns you read in that are totally ambiguous. Open your Python editor and create a file named Assess-Warehouse-Global.py in directory C:\VKHCG\03-Hillman\02-Assess. Now copy the following code into the file: ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import pandas as pd from geopy.geocoders import Nominatim geolocator = Nominatim() ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + 'VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base:',Base,' using',sys.platform) print('################################') ################################################################ Company='03-Hillman' InputDir='01-Retrieve/01-EDS/01-R' InputFileName='Retrieve_All_Countries.csv' EDSDir='02-Assess/01-EDS' OutputDir=EDSDir + '/02-Python' OutputFileName='Assess_All_Warehouse.csv' ################################################################ sFileDir=Base + '/' + Company + '/' + EDSDir if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ sFileDir=Base + '/' + Company + '/' + OutputDir if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ sFileName=Base + '/' + Company + '/' + InputDir + '/' + InputFileName print('###########') print('Loading:',sFileName) Warehouse=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") The columns are totally ambiguous and require new names, to be useful to any future processing. ################################################################ sColumns={'X1': 'Country',           'X2': 'PostCode',           'X3': 'PlaceName',           'X4': 'AreaName',           'X5': 'AreaCode',           'X10': 'Latitude',           'X11': 'Longitude'} Warehouse.rename(columns=sColumns,inplace=True) WarehouseGood=Warehouse ################################################################ sFileName=sFileDir + '/' + OutputFileName WarehouseGood.to_csv(sFileName, index = False) ################################################################# print('### Done!! ############################################') ################################################################ Save the file Assess-Warehouse-Global.py file, then compile and execute with your Python compiler. This will produce a set of demonstrated values onscreen, plus a graph data file named Assess_All_Warehouse.csv.

<colgroup class="calibre11"><col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"> <col class="calibre12"></colgroup> 
| 国家 | 邮政编码 | 安慰剂 Name | 所属地区 | 伫列区域 | 纬度 | 经度 |
| --- | --- | --- | --- | --- | --- | --- |
| 胡（汉语拼音） | Eight thousand three hundred and thirteen | 巴拉通约洛克 | 扎拉 | 为 | Forty-six point seven five | 17.3667 |
| 胡（汉语拼音） | Eight thousand three hundred and fourteen | 冯亚奇 | 扎拉 | 为 | Forty-six point seven five | 17.3167 |
| 胡（汉语拼音） | Eight thousand three hundred and fifteen | 他是个天才 | 扎拉 | 为 | 46.7667 | 17.2833 |

#### 为最适合的国际物流计划运输规则

Hillman requires an international logistics solution to support all the required shipping routes. Tip I have performed the shipping route solution for about every customer I have worked for over the last ten years. Companies have to move their products according to an effective route and schedule. So, please pay attention to the techniques and the business logic, as they are used by numerous companies. This example will combine all the skills you mastered in this chapter. Open your Python editor and create a file named Assess-Best-Fit-Logistics.py in directory C:\VKHCG\03-Hillman\02-Assess. Now copy the following code into the file: ################################################################ # -*- coding: utf-8 -*- ################################################################ We will use many libraries in the course of this example, as follows: import sys import os import pandas as pd import networkx as nx from geopy.distance import vincenty import sqlite3 as sq from pandas.io import sql Here we go again—with basic well-known parameters to run the process. ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + 'VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base:',Base,'using',sys.platform) print('################################') ################################################################ Company='03-Hillman' InputDir='01-Retrieve/01-EDS/01-R' InputFileName='Retrieve_All_Countries.csv' EDSDir='02-Assess/01-EDS' OutputDir=EDSDir + '/02-Python' OutputFileName='Assess_Best_Logistics.gml' ################################################################ sFileDir=Base + '/' + Company + '/' + EDSDir if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ sFileDir=Base + '/' + Company + '/' + OutputDir if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ sDataBaseDir=Base + '/' + Company + '/02-Assess/SQLite' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataBaseDir + '/Hillman.db' conn = sq.connect(sDatabaseName) ################################################################ We must load the data from the data source and fix the columns. sFileName=Base + '/' + Company + '/' + InputDir + '/' + InputFileName print('###########') print('Loading:',sFileName) Warehouse=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") ################################################################ sColumns={'X1': 'Country',           'X2': 'PostCode',           'X3': 'PlaceName',           'X4': 'AreaName',           'X5': 'AreaCode',           'X10': 'Latitude',           'X11': 'Longitude'} Warehouse.rename(columns=sColumns,inplace=True) WarehouseGood=Warehouse #print(WarehouseGood.head()) You now have your data in memory. Now we start to manipulate it to fit our requirements. You need the average latitude and longitude for each country in the data source. So, to extract this feature, you must calculate the mean() for each country. ################################################################ RoutePointsCountry=pd.DataFrame(WarehouseGood.groupby(['Country'])[['Latitude','Longitude']].mean()) print(RoutePointsCountry.head()) You will now have a list of countries with a single average point for all the known countries. Now store this list as Assess_RoutePointsCountry in the SQLite database. print('################')   sTable='Assess_RoutePointsCountry' print('Storing:',sDatabaseName,'Table:',sTable) RoutePointsCountry.to_sql(sTable, conn, if_exists="replace") print('################')   You need the average latitude and longitude for each country and post code combined in the data source. So, to extract this feature, you must calculate the mean() for each country and post code and store this list as Assess_RoutePointsPostCode in the SQLite database. ################################################################ RoutePointsPostCode=pd.DataFrame(WarehouseGood.groupby(['Country', 'PostCode'])[['Latitude','Longitude']].mean()) #print(RoutePointsPostCode.head()) print('################')   sTable='Assess_RoutePointsPostCode' print('Storing :',sDatabaseName,'Table:',sTable) RoutePointsPostCode.to_sql(sTable, conn, if_exists="replace") print('################')   You need the average latitude and longitude for each country, post code, and place-name combination in the data source. So, to extract this feature, you must calculate the mean() for each country post code and place-name and store this list as Assess_RoutePointsPlaceName in the SQLite database. ################################################################ RoutePointsPlaceName=pd.DataFrame(WarehouseGood.groupby(['Country', 'PostCode','PlaceName'])[['Latitude','Longitude']].mean()) #print(RoutePointsPlaceName.head()) print('################')   sTable='Assess_RoutePointsPlaceName' print('Storing:',sDatabaseName,'Table:',sTable) RoutePointsPlaceName.to_sql(sTable, conn, if_exists="replace") print('################')   You will now match the country-to-country routes. I have limited the data to process only country codes GB, DE, BE, AU, US, and IN, to reduce the data processing time. I have processed the complete data set on my cluster in about 20 minutes, but on my not-so-powerful laptop, it took two hours! Hence the limit. ################################################################ ### Fit Country to Country ################################################################ print('################')   sView='Assess_RouteCountries' print('Creating:',sDatabaseName,'View:',sView) sSQL="DROP VIEW IF EXISTS" + sView + ";" sql.execute(sSQL,conn) sSQL="CREATE VIEW" + sView + "AS" sSQL=sSQL+ "SELECT DISTINCT" sSQL=sSQL+ "S.Country AS SourceCountry," sSQL=sSQL+ "S.Latitude AS SourceLatitude," sSQL=sSQL+ "S.Longitude AS SourceLongitude," sSQL=sSQL+ "T.Country AS TargetCountry," sSQL=sSQL+ "T.Latitude AS TargetLatitude," sSQL=sSQL+ "T.Longitude AS TargetLongitude" sSQL=sSQL+ "FROM" sSQL=sSQL+ "Assess_RoutePointsCountry AS S" sSQL=sSQL+ "," sSQL=sSQL+ "Assess_RoutePointsCountry AS T" sSQL=sSQL+ "WHERE S.Country <> T.Country" sSQL=sSQL+ "AND" sSQL=sSQL+ "S.Country in('GB','DE','BE','AU','US','IN')" sSQL=sSQL+ "AND" sSQL=sSQL+ "T.Country in('GB','DE','BE','AU','US','IN');" sql.execute(sSQL,conn) print('################')   print('Loading:',sDatabaseName,'Table:',sView) sSQL="SELECT" sSQL=sSQL+ "*" sSQL=sSQL+ "FROM" sSQL=sSQL+ " " + sView + ";" RouteCountries=pd.read_sql_query(sSQL, conn) You then apply Vincenty’s formulae to get the distance in miles between each combination pair. RouteCountries['Distance']=RouteCountries.apply(lambda row:     round(         vincenty((row['SourceLatitude'],row['SourceLongitude']),                  (row['TargetLatitude'],row['TargetLongitude'])).miles              ,4)        ,axis=1) print(RouteCountries.head(5)) You will now match the country to post code routes and apply Vincenty’s formulae, as previously. ################################################################ ### Fit Country to Post Code ################################################################ print('################')   sView='Assess_RoutePostCode' print('Creating:',sDatabaseName,'View:',sView) sSQL="DROP VIEW IF EXISTS" + sView + ";" sql.execute(sSQL,conn) sSQL="CREATE VIEW" + sView + "AS" sSQL=sSQL+ "SELECT DISTINCT" sSQL=sSQL+ "S.Country AS SourceCountry," sSQL=sSQL+ "S.Latitude AS SourceLatitude," sSQL=sSQL+ "S.Longitude AS SourceLongitude," sSQL=sSQL+ "T.Country AS TargetCountry," sSQL=sSQL+ "T.PostCode AS TargetPostCode," sSQL=sSQL+ "T.Latitude AS TargetLatitude," sSQL=sSQL+ "T.Longitude AS TargetLongitude" sSQL=sSQL+ "FROM" sSQL=sSQL+ "Assess_RoutePointsCountry AS S" sSQL=sSQL+ "," sSQL=sSQL+ "Assess_RoutePointsPostCode AS T" sSQL=sSQL+ "WHERE S.Country = T.Country" sSQL=sSQL+ "AND" sSQL=sSQL+ "S.Country in ('GB','DE','BE','AU','US','IN')" sSQL=sSQL+ "AND" sSQL=sSQL+ "T.Country in ('GB','DE','BE','AU','US','IN');" sql.execute(sSQL,conn) print('################')   print('Loading:',sDatabaseName,'Table:',sView) sSQL="SELECT" sSQL=sSQL+ "*" sSQL=sSQL+ "FROM" sSQL=sSQL+ " " + sView + ";" RoutePostCode=pd.read_sql_query(sSQL, conn) RoutePostCode['Distance']=RoutePostCode.apply(lambda row:     round(         vincenty((row['SourceLatitude'],row['SourceLongitude']),                  (row['TargetLatitude'],row['TargetLongitude'])).miles              ,4)        ,axis=1) print(RoutePostCode.head(5)) You will now match the post code to place-name routes and apply Vincenty’s formulae, as previously. ################################################################ ### Fit Post Code to Place Name ################################################################ print('################')   sView='Assess_RoutePlaceName' print('Creating:',sDatabaseName,'View:',sView) sSQL="DROP VIEW IF EXISTS" + sView + ";" sql.execute(sSQL,conn) sSQL="CREATE VIEW" + sView + "AS" sSQL=sSQL+ "SELECT DISTINCT" sSQL=sSQL+ "S.Country AS SourceCountry," sSQL=sSQL+ "S.PostCode AS SourcePostCode," sSQL=sSQL+ "S.Latitude AS SourceLatitude," sSQL=sSQL+ "S.Longitude AS SourceLongitude," sSQL=sSQL+ "T.Country AS TargetCountry," sSQL=sSQL+ "T.PostCode AS TargetPostCode," sSQL=sSQL+ "T.PlaceName AS TargetPlaceName," sSQL=sSQL+ "T.Latitude AS TargetLatitude," sSQL=sSQL+ "T.Longitude AS TargetLongitude" sSQL=sSQL+ "FROM" sSQL=sSQL+ "Assess_RoutePointsPostCode AS S" sSQL=sSQL+ "," sSQL=sSQL+ "Assess_RoutePointsPLaceName AS T" sSQL=sSQL+ "WHERE" sSQL=sSQL+ "S.Country = T.Country" sSQL=sSQL+ "AND" sSQL=sSQL+ "S.PostCode = T.PostCode" sSQL=sSQL+ "AND" sSQL=sSQL+ "S.Country in('GB','DE','BE','AU','US','IN')" sSQL=sSQL+ "AND" sSQL=sSQL+ "T.Country in('GB','DE','BE','AU','US','IN');" sql.execute(sSQL,conn) print('################')   print('Loading:',sDatabaseName,'Table:',sView) sSQL="SELECT" sSQL=sSQL+ "*" sSQL=sSQL+ "FROM" sSQL=sSQL+ " " + sView + ";" RoutePlaceName=pd.read_sql_query(sSQL, conn) RoutePlaceName['Distance']=RoutePlaceName.apply(lambda row:     round(         vincenty((row['SourceLatitude'],row['SourceLongitude']),                  (row['TargetLatitude'],row['TargetLongitude'])).miles              ,4)        ,axis=1) print(RoutePlaceName.head(5)) You will now map your new locations onto a graph. First the countries. ################################################################ G=nx.Graph() ################################################################ print('Countries:',RouteCountries.shape) for i in range(RouteCountries.shape[0]):     sNode0='C-' + RouteCountries['SourceCountry'][i]         G.add_node(sNode0,                Nodetype='Country',                Country=RouteCountries['SourceCountry'][i],                Latitude=round(RouteCountries['SourceLatitude'][i],4),                Longitude=round(RouteCountries['SourceLongitude'][i],4))     sNode1='C-' + RouteCountries['TargetCountry'][i]         G.add_node(sNode1,                Nodetype='Country',                Country=RouteCountries['TargetCountry'][i],                Latitude=round(RouteCountries['TargetLatitude'][i],4),                Longitude=round(RouteCountries['TargetLongitude'][i],4))     G.add_edge(sNode0,sNode1,distance=round(RouteCountries['Distance'][i],3))     #print(sNode0,sNode1) Then the postcodes. ################################################################ print('Post Code:',RoutePostCode.shape) for i in range(RoutePostCode.shape[0]):     sNode0='C-' + RoutePostCode['SourceCountry'][i]         G.add_node(sNode0,                Nodetype='Country',                Country=RoutePostCode['SourceCountry'][i],                Latitude=round(RoutePostCode['SourceLatitude'][i],4),                Longitude=round(RoutePostCode['SourceLongitude'][i],4))     sNode1='P-' + RoutePostCode['TargetPostCode'][i]  + '-' + RoutePostCode['TargetCountry'][i]        G.add_node(sNode1,                Nodetype='PostCode',                Country=RoutePostCode['TargetCountry'][i],                PostCode=RoutePostCode['TargetPostCode'][i],                Latitude=round(RoutePostCode['TargetLatitude'][i],4),                Longitude=round(RoutePostCode['TargetLongitude'][i],4))     G.add_edge(sNode0,sNode1,distance=round(RoutePostCode['Distance'][i],3))     #print(sNode0,sNode1) Then the place-names. ################################################################ print('Place Name:',RoutePlaceName.shape) for i in range(RoutePlaceName.shape[0]):     sNode0='P-' + RoutePlaceName['TargetPostCode'][i]  + '-'     sNode0=sNode0 + RoutePlaceName['TargetCountry'][i]         G.add_node(sNode0,                Nodetype='PostCode',                Country=RoutePlaceName['SourceCountry'][i],                PostCode=RoutePlaceName['TargetPostCode'][i],                Latitude=round(RoutePlaceName['SourceLatitude'][i],4),                Longitude=round(RoutePlaceName['SourceLongitude'][i],4))     sNode1='L-' + RoutePlaceName['TargetPlaceName'][i]  + '-'     sNode1=sNode1 + RoutePlaceName['TargetPostCode'][i]  + '-'     sNode1=sNode1 + RoutePlaceName['TargetCountry'][i]     G.add_node(sNode1,                Nodetype='PlaceName',                Country=RoutePlaceName['TargetCountry'][i],                PostCode=RoutePlaceName['TargetPostCode'][i],                PlaceName=RoutePlaceName['TargetPlaceName'][i],                Latitude=round(RoutePlaceName['TargetLatitude'][i],4),                Longitude=round(RoutePlaceName['TargetLongitude'][i],4))     G.add_edge(sNode0,sNode1,distance=round(RoutePlaceName['Distance'][i],3))     #print(sNode0,sNode1) ################################################################ You now have a full graph with every possible route. Let’s store your hard work. sFileName=sFileDir + '/' + OutputFileName print('################################') print('Storing:', sFileName) print('################################') nx.write_gml(G,sFileName) sFileName=sFileName +'.gz' nx.write_gml(G,sFileName) You will now start to extract extra features that are stored in the graph structure. The required feature you are seeking is the shortest path between two locations. ################################################################ print('################################') print('Path:', nx.shortest_path(G,source='P-SW1-GB',target='P-01001-US',weight='distance')) print('Path length:', nx.shortest_path_length(G,source='P-SW1-GB',target='P-01001-US',weight='distance')) print('Path length (1):', nx.shortest_path_length(G,source='P-SW1-GB',target='C-GB',weight='distance')) print('Path length (2):', nx.shortest_path_length(G,source='C-GB',target='C-US',weight='distance')) print('Path length (3):', nx.shortest_path_length(G,source='C-US',target='P-01001-US',weight='distance')) print('################################') The required feature you are seeking is the shortest path between a given location and any other within one hop of that source. print('Routes from P-SW1-GB < 2:', nx.single_source_shortest_path(G,source='P-SW1-GB', cutoff=1)) print('Routes from P-01001-US < 2:', nx.single_source_shortest_path(G,source='P-01001-US', cutoff=1)) print('################################') There are many features hidden in this graph you’ve built. If you want to test your new skills, I suggest you look at the online information for networkx, available at [https://networkx.github.io/documentation/networkx-1.10/reference/algorithms.html](https://networkx.github.io/documentation/networkx-1.10/reference/algorithms.html) . There are hundreds of possible options and combinations. Enjoy! But before you start testing your skills, you must perform some database maintenance. ################################################################ print('################') print('Vacuum Database') sSQL="VACUUM;" sql.execute(sSQL,conn) print('################') ################################################################ print('### Done!! ############################################') ################################################################ ################################################################ Save the Assess-Best-Fit-Logistics.py file, then compile and execute with your Python compiler. This will produce a set of demonstrated values onscreen, plus a graph data file named Assess_Best_Logistics.gml. There is a point to the maintenance , as, over the years, I have seen many data scientists complain about their data science running slowly but who neglected simple housekeeping of their systems, which can restore expensive processing power. Tip Always clean up after any major data science session. Compress your databases, remove unused files, and clean up the environment. Take care of your tools, and they will take care of you.

##### 成就摘要

*   现在，您可以从图表中查询要素，例如位置之间的短缺路径和来自给定位置的路径。
*   您可以从数据科学中执行内务处理，以最大限度地提高处理能力。

#### 决定集装箱运输的最佳包装方案

Hillman wants to introduce new shipping containers into its logistics strategy. You need to model a data set that will support this project. I will guide you through a process of assessing the possible container sizes. Tip This example introduces features with ranges or tolerances. These not-so-perfect data values can cause major issues in the data science ecosystem. I will guide you through how to take precise measurements and add the given tolerances and create the ranges of values you need for the data science in Chapters [9](09.html) and [10](10.html). Open your Python editor and create a file named Assess-Shipping-Containers.py in directory C:\VKHCG\03-Hillman\02-Assess. Now copy the following code into the file: ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import pandas as pd import sqlite3 as sq from pandas.io import sql ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + 'VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base:',Base,'using',sys.platform) print('################################') ################################################################ Company='03-Hillman' InputDir='01-Retrieve/01-EDS/02-Python' InputFileName1='Retrieve_Product.csv' InputFileName2='Retrieve_Box.csv' InputFileName3='Retrieve_Container.csv' EDSDir='02-Assess/01-EDS' OutputDir=EDSDir + '/02-Python' OutputFileName='Assess_Shipping_Containers.csv' ################################################################ sFileDir=Base + '/' + Company + '/' + EDSDir if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ sFileDir=Base + '/' + Company + '/' + OutputDir if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ sDataBaseDir=Base + '/' + Company + '/02-Assess/SQLite' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataBaseDir + '/hillman.db' conn = sq.connect(sDatabaseName) ################################################################ ################################################################ ### Import Product Data ################################################################ sFileName=Base + '/' + Company + '/' + InputDir + '/' + InputFileName1 print('###########') print('Loading:',sFileName) ProductRawData=pd.read_csv(sFileName,                     header=0,                     low_memory=False,                     encoding="latin-1"                     ) ProductRawData.drop_duplicates(subset=None, keep="first", inplace=True) ProductRawData.index.name = 'IDNumber' I have limited the data set to speed up the processing. This code refers only to the current top-ten products having a length value of at least a half-meter. Note Feel free to experiment with different parameters, if you want to. Think what if you only use products between 25 centimeters and 50 centimeters, or an entire product range with a width greater than 1 meter? This is data science. Have fun with it. ProductData=ProductRawData[ProductRawData.Length <= 0.5].head(10) print('Loaded Product:',ProductData.columns.values) print('################################') ################################################################ print('################')   sTable='Assess_Product' print('Storing:',sDatabaseName,'Table:',sTable) ProductData.to_sql(sTable, conn, if_exists="replace") print('################')   ################################################################ print(ProductData.head()) print('################################') print('Rows:',ProductData.shape[0]) print('################################') ################################################################ ################################################################ ### Import Box Data ################################################################ sFileName=Base + '/' + Company + '/' + InputDir + '/' + InputFileName2 print('###########') print('Loading:',sFileName) BoxRawData=pd.read_csv(sFileName,                     header=0,                     low_memory=False,                     encoding="latin-1"                     ) BoxRawData.drop_duplicates(subset=None, keep="first", inplace=True) BoxRawData.index.name = 'IDNumber' I have limited the data set to speed up the processing. This code refers only to the current top-1000 packing boxes , with a length value of less than one meter. You may alter these limitations, if you want to. BoxData=BoxRawData[BoxRawData.Length <= 1].head(1000) print('Loaded Product:',BoxData.columns.values) print('################################') ################################################################ print('################')   sTable='Assess_Box' print('Storing:',sDatabaseName,'Table:',sTable) BoxData.to_sql(sTable, conn, if_exists="replace") print('################')   ################################################################ print(BoxData.head()) print('################################') print('Rows:',BoxData.shape[0]) print('################################') ################################################################ ################################################################ ### Import Container Data ################################################################ sFileName=Base + '/' + Company + '/' + InputDir + '/' + InputFileName3 print('###########') print('Loading:',sFileName) ContainerRawData=pd.read_csv(sFileName,                     header=0,                     low_memory=False,                     encoding="latin-1"                     ) ContainerRawData.drop_duplicates(subset=None, keep="first", inplace=True) ContainerRawData.index.name = 'IDNumber' I have limited the data set to speed up the processing. This code is only for current top-ten shipping containers, with a length value of less than two meters. You may alter these limitations, as you wish. ContainerData=ContainerRawData[ContainerRawData.Length <= 2].head(10) print('Loaded Product:',ContainerData.columns.values) print('################################') ################################################################ print('################')   sTable='Assess_Container' print('Storing:',sDatabaseName,'Table:',sTable) BoxData.to_sql(sTable, conn, if_exists="replace") print('################')   ################################################################ print(ContainerData.head()) print('################################') print('Rows:',ContainerData.shape[0]) print('################################') ################################################################ You are tasked with finding the correct packing box for each of your selected products. The product must fit in the box , with an amount of packing foam to protect the product. ################################################################ ### Fit Product in Box ################################################################ print('################')   sView='Assess_Product_in_Box' print('Creating:',sDatabaseName,'View:',sView) sSQL="DROP VIEW IF EXISTS " + sView + ";" sql.execute(sSQL,conn) sSQL="CREATE VIEW" + sView + "AS" sSQL=sSQL+ "SELECT" sSQL=sSQL+ "P.UnitNumber AS ProductNumber," sSQL=sSQL+ "B.UnitNumber AS BoxNumber," sSQL=sSQL+ "(B.Thickness * 1000) AS PackSafeCode," sSQL=sSQL+ "(B.BoxVolume - P.ProductVolume) AS PackFoamVolume," I have given you a formula to calculate the air freight volumetric (chargeable weight) , because when you ship by air, you pay for the shipment per kilogram, against the higher value of either the air freight volumetric (chargeable weight) or your item’s true weight. The general value is 167 kilograms per cubic centimeter. sSQL=sSQL+ "((B.Length*10) * (B.Width*10) * (B.Height*10)) * 167 AS Air_Dimensional_Weight," I have given you a formula here to calculate the road freight volumetric (chargeable weight), because when you ship by road, you pay for the shipment per kilogram, against the higher value of either the road freight volumetric (chargeable weight) or your item’s true weight. The general value is 333 kilograms per cubic centimeter. sSQL=sSQL+ "((B.Length*10) * (B.Width*10) * (B.Height*10)) * 333 AS Road_Dimensional_Weight," I have given you a formula here to calculate the sea freight volumetric (chargeable weight), because when you ship by sea, you pay for the shipment per kilogram, against the higher value of either the sea freight volumetric (chargeable weight) or your item’s true weight. The general value is 1000 kilograms per cubic centimeter. sSQL=sSQL+ "((B.Length*10) * (B.Width*10) * (B.Height*10)) * 1000 AS Sea_Dimensional_Weight," sSQL=sSQL+ "P.Length AS Product_Length," sSQL=sSQL+ "P.Width AS Product_Width," sSQL=sSQL+ "P.Height AS Product_Height," sSQL=sSQL+ "P.ProductVolume AS Product_cm_Volume," sSQL=sSQL+ "((P.Length*10) * (P.Width*10) * (P.Height*10)) AS Product_ccm_Volume," Remember I said earlier that you will require ranges or options. Here we have a tolerance set up for packing the product in the prescribed box for a range of 5% below and 5% above the true size. Hence, a product will fit in more than one box, with variances in packing material thickness. Note I saved one of my customers nearly £2000 per day in shipping by using smaller boxes for packing. Remember to consider the volumetric (chargeable weight) you pay for to take up extra shipping space. sSQL=sSQL+ "(B.Thickness * 0.95) AS Minimum_Pack_Foam," sSQL=sSQL+ "(B.Thickness * 1.05) AS Maximum_Pack_Foam," sSQL=sSQL+ "B.Length - (B.Thickness * 1.10) AS Minimum_Product_Box_Length," sSQL=sSQL+ "B.Length - (B.Thickness * 0.95) AS Maximum_Product_Box_Length," sSQL=sSQL+ "B.Width - (B.Thickness * 1.10) AS Minimum_Product_Box_Width," sSQL=sSQL+ "B.Width - (B.Thickness * 0.95) AS Maximum_Product_Box_Width," sSQL=sSQL+ "B.Height - (B.Thickness * 1.10) AS Minimum_Product_Box_Height," sSQL=sSQL+ "B.Height - (B.Thickness * 0.95) AS Maximum_Product_Box_Height," sSQL=sSQL+ "B.Length AS Box_Length," sSQL=sSQL+ "B.Width AS Box_Width," sSQL=sSQL+ "B.Height AS Box_Height," sSQL=sSQL+ "B.BoxVolume AS Box_cm_Volume," sSQL=sSQL+ "((B.Length*10) * (B.Width*10) * (B.Height*10)) AS Box_ccm_Volume," sSQL=sSQL+ "(2 * B.Length * B.Width) + (2 * B.Length * B.Height) + (2 * B.Width * B.Height) AS Box_sqm_Area," The next limitation is the boxes’ packing-strength rating. Any box has a maximum approved weight rating. Note In logistics, if you overload a box, and it gets damaged, your insurance does not cover the cost. That small miscalculation is the most common cause of damage in logistics. Do not overload the boxes! Our example uses three different strength of boxes ratings : 3.5 kilograms, 7.7 kilograms, and 10 kilograms per cubic centimeter. sSQL=sSQL+ "((B.Length*10) * (B.Width*10) * (B.Height*10)) *  3.5 AS Box_A_Max_Kg_Weight," sSQL=sSQL+ "((B.Length*10) * (B.Width*10) * (B.Height*10)) *  7.7 AS Box_B_Max_Kg_Weight," sSQL=sSQL+ "((B.Length*10) * (B.Width*10) * (B.Height*10)) * 10.0 AS Box_C_Max_Kg_Weight" sSQL=sSQL+ "FROM" sSQL=sSQL+ "Assess_Product as P" sSQL=sSQL+ "," sSQL=sSQL+ "Assess_Box as B" sSQL=sSQL+ "WHERE" Here is the main range mapping process for the volume of the product against the box: sSQL=sSQL+ "P.Length >= (B.Length - (B.Thickness * 1.10))" sSQL=sSQL+ "AND" sSQL=sSQL+ "P.Width >= (B.Width - (B.Thickness * 1.10))" sSQL=sSQL+ "AND" sSQL=sSQL+ "P.Height >= (B.Height - (B.Thickness * 1.10))" sSQL=sSQL+ "AND" sSQL=sSQL+ "P.Length <= (B.Length - (B.Thickness * 0.95))" sSQL=sSQL+ "AND" sSQL=sSQL+ "P.Width <= (B.Width - (B.Thickness * 0.95))" sSQL=sSQL+ "AND" sSQL=sSQL+ "P.Height <= (B.Height - (B.Thickness * 0.95))" It is also advisable that the box be bigger than the product. But you probably know that. One of my customers did not, however, and ordered 20,000 boxes per year that were too small. sSQL=sSQL+ "AND" sSQL=sSQL+ "(B.Height - B.Thickness) >= 0" sSQL=sSQL+ "AND" sSQL=sSQL+ "(B.Width - B.Thickness) >= 0" sSQL=sSQL+ "AND" sSQL=sSQL+ "(B.Height - B.Thickness) >= 0" sSQL=sSQL+ "AND" sSQL=sSQL+ "B.BoxVolume >= P.ProductVolume;" sql.execute(sSQL,conn) I will now show you how to fit the boxes on a shipping pallet. ################################################################ ### Fit Box in Pallet ################################################################ t=0 for l in range(2,8):     for w in range(2,8):         for h in range(4):             t += 1                         PalletLine=[('IDNumber',[t]),                        ('ShipType',['Pallet']),                        ('UnitNumber',('L-'+format(t,"06d"))),                        ('Box_per_Length',(format(2**l,"4d"))),                        ('Box_per_Width',(format(2**w,"4d"))),                        ('Box_per_Height',(format(2**h,"4d")))]             if t==1:                PalletFrame = pd.DataFrame.from_items(PalletLine)             else:                 PalletRow = pd.DataFrame.from_items(PalletLine)                 PalletFrame = PalletFrame.append(PalletRow) PalletFrame.set_index(['IDNumber'],inplace=True) ################################################################ PalletFrame.head() print('################################') print('Rows:',PalletFrame.shape[0]) print('################################') ################################################################ ### Fit Box on Pallet ################################################################ print('################')   sView='Assess_Box_on_Pallet' print('Creating:',sDatabaseName,'View:',sView) sSQL="DROP VIEW IF EXISTS" + sView + ";" sql.execute(sSQL,conn) sSQL="CREATE VIEW" + sView + "AS" sSQL=sSQL+ "SELECT DISTINCT" sSQL=sSQL+ "P.UnitNumber AS PalletNumber," sSQL=sSQL+ "B.UnitNumber AS BoxNumber," The size of the pallet is determined by your stacking plan. Do you stack three boxes wide by four boxes long by three boxes high? Or do you stack them five by five by five? That is the question. Experiment once you have the data. sSQL=sSQL+ "round(B.Length*P.Box_per_Length,3) AS Pallet_Length," sSQL=sSQL+ "round(B.Width*P.Box_per_Width,3) AS Pallet_Width," sSQL=sSQL+ "round(B.Height*P.Box_per_Height,3) AS Pallet_Height," sSQL=sSQL+ "P.Box_per_Length * P.Box_per_Width * P.Box_per_Height AS Pallet_Boxes" sSQL=sSQL+ "FROM" sSQL=sSQL+ "Assess_Box as B" sSQL=sSQL+ "," sSQL=sSQL+ "Assess_Pallet as P" The only limitation I suggest is that the pallet not be bigger than 20 meters × 9 meters × 5 meters, the biggest item that can be picked up with the crane available. But, once again, feel free to change this restriction, if you want to experiment. sSQL=sSQL+ "WHERE" sSQL=sSQL+ "round(B.Length*P.Box_per_Length,3) <= 20" sSQL=sSQL+ "AND" sSQL=sSQL+ "round(B.Width*P.Box_per_Width,3) <= 9" sSQL=sSQL+ "AND" sSQL=sSQL+ "round(B.Height*P.Box_per_Height,3) <= 5;" sql.execute(sSQL,conn) Let’s now calculate the product in box and box on pallet options. ################################################################ sTables=['Assess_Product_in_Box','Assess_Box_on_Pallet'] for sTable in sTables:     print('################')       print('Loading:',sDatabaseName,'Table:',sTable)     sSQL="SELECT "     sSQL=sSQL+ "*"     sSQL=sSQL+ "FROM"     sSQL=sSQL+ " " + sTable + ";"     SnapShotData=pd.read_sql_query(sSQL, conn)     print('################')       sTableOut=sTable + '_SnapShot'     print('Storing:',sDatabaseName,'Table:',sTable)     SnapShotData.to_sql(sTableOut, conn, if_exists="replace")     print('################')   I will now show you how to calculate the options for loading pallets into a correct shipping container. The rules are simple: if the pallet fits the container opening, it is OK to ship via that container. ################################################################ ### Fit Pallet in Container ################################################################ sTables=['Length','Width','Height'] for sTable in sTables:     sView='Assess_Pallet_in_Container_' + sTable     print('Creating:',sDatabaseName,'View:',sView)     sSQL="DROP VIEW IF EXISTS" + sView + ";"     sql.execute(sSQL,conn)     sSQL="CREATE VIEW" + sView + "AS"     sSQL=sSQL+ "SELECT DISTINCT"     sSQL=sSQL+ "C.UnitNumber AS ContainerNumber,"     sSQL=sSQL+ "P.PalletNumber,"     sSQL=sSQL+ "P.BoxNumber,"     sSQL=sSQL+ "round(C." + sTable + "/P.Pallet_" + sTable + ",0)"     sSQL=sSQL+ "AS Pallet_per_" + sTable + ","     sSQL=sSQL+ "round(C." + sTable + "/P.Pallet_" + sTable + ",0)"     sSQL=sSQL+ "* P.Pallet_Boxes AS Pallet_" + sTable + "_Boxes,"     sSQL=sSQL+ "P.Pallet_Boxes"     sSQL=sSQL+ "FROM"     sSQL=sSQL+ "Assess_Container as C"     sSQL=sSQL+ ","     sSQL=sSQL+ "Assess_Box_on_Pallet_SnapShot as P"     sSQL=sSQL+ "WHERE" Here is the check: if it fits, it goes. You can load many pallets on a container, but it has to fit.     sSQL=sSQL+ "round(C.Length/P.Pallet_Length,0) > 0"     sSQL=sSQL+ "AND"     sSQL=sSQL+ "round(C.Width/P.Pallet_Width,0) > 0"     sSQL=sSQL+ "AND"     sSQL=sSQL+ "round(C.Height/P.Pallet_Height,0) > 0;"     sql.execute(sSQL,conn)     print('################')       print('Loading:',sDatabaseName,'Table:',sView)     sSQL="SELECT"     sSQL=sSQL+ "*"     sSQL=sSQL+ "FROM"     sSQL=sSQL+ " " + sView + ";"     SnapShotData=pd.read_sql_query(sSQL, conn)     print('################')       sTableOut= sView + '_SnapShot'     print('Storing:',sDatabaseName,'Table:',sTableOut)     SnapShotData.to_sql(sTableOut, conn, if_exists="replace")     print('################')   Now we simply extract the pallet-in-container option for later use. ################################################################ print('################')   sView='Assess_Pallet_in_Container' print('Creating:',sDatabaseName,'View:',sView) sSQL="DROP VIEW IF EXISTS" + sView + ";" sql.execute(sSQL,conn) sSQL="CREATE VIEW" + sView + "AS" sSQL=sSQL+ "SELECT" sSQL=sSQL+ "CL.ContainerNumber," sSQL=sSQL+ "CL.PalletNumber," sSQL=sSQL+ "CL.BoxNumber," sSQL=sSQL+ "CL.Pallet_Boxes AS Boxes_per_Pallet," sSQL=sSQL+ "CL.Pallet_per_Length," sSQL=sSQL+ "CW.Pallet_per_Width," sSQL=sSQL+ "CH.Pallet_per_Height," sSQL=sSQL+ "CL.Pallet_Length_Boxes * CW.Pallet_Width_Boxes * CH.Pallet_Height_Boxes AS Container_Boxes" sSQL=sSQL+ "FROM" sSQL=sSQL+ "Assess_Pallet_in_Container_Length_SnapShot as CL" sSQL=sSQL+ "JOIN" sSQL=sSQL+ "Assess_Pallet_in_Container_Width_SnapShot as CW" sSQL=sSQL+ "ON" sSQL=sSQL+ "CL.ContainerNumber = CW.ContainerNumber" sSQL=sSQL+ "AND" sSQL=sSQL+ "CL.PalletNumber = CW.PalletNumber" sSQL=sSQL+ "AND" sSQL=sSQL+ "CL.BoxNumber = CW.BoxNumber" sSQL=sSQL+ "JOIN" sSQL=sSQL+ "Assess_Pallet_in_Container_Height_SnapShot as CH" sSQL=sSQL+ "ON" sSQL=sSQL+ "CL.ContainerNumber = CH.ContainerNumber" sSQL=sSQL+ "AND" sSQL=sSQL+ "CL.PalletNumber = CH.PalletNumber" sSQL=sSQL+ "AND" sSQL=sSQL+ "CL.BoxNumber = CH.BoxNumber;" sql.execute(sSQL,conn) ################################################################ sTables=['Assess_Product_in_Box','Assess_Pallet_in_Container'] for sTable in sTables:     print('################')       print('Loading:',sDatabaseName,'Table:',sTable)     sSQL="SELECT "     sSQL=sSQL+ "*"     sSQL=sSQL+ "FROM"     sSQL=sSQL+ " " + sTable + ";"     PackData=pd.read_sql_query(sSQL, conn)     print('################')       print(PackData)     print('################')       print('################################')     print('Rows:',PackData.shape[0])     print('################################')     sFileName=sFileDir + '/' + sTable + '.csv'     print(sFileName)     PackData.to_csv(sFileName, index = False) ################################################################ print('### Done!! ############################################') ################################################################ Save the file Assess-Shipping-Containers.py file, then compile and execute with your Python compiler. This will produce a set of demonstrated values onscreen, plus a data file named Assess_Shipping_Containers.csv.

#### 创建交货路线

Hillman requires the complete grid plan of the delivery routes for the company, to ensure the suppliers, warehouses, shops, and customers can be reached by its new strategy. This new plan will enable the optimum routes between suppliers, warehouses, shops, and customers. Open your Python editor and create a file named Assess-Shipping-Routes.py in directory C:\VKHCG\03-Hillman\02-Assess. Now copy the following code into the file: ################################################################ # -*- coding: utf-8 -*- ################################################################ import sys import os import pandas as pd import sqlite3 as sq from pandas.io import sql import networkx as nx from geopy.distance import vincenty ################################################################ nMax=100 nMaxPath=100 nSet=True nVSet=False ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + 'VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base:',Base,'using',sys.platform) print('################################') ################################################################ Company='03-Hillman' InputDir1='01-Retrieve/01-EDS/01-R' InputDir2='01-Retrieve/01-EDS/02-Python' InputFileName1='Retrieve_GB_Postcode_Warehouse.csv' InputFileName2='Retrieve_GB_Postcodes_Shops.csv' EDSDir='02-Assess/01-EDS' OutputDir=EDSDir + '/02-Python' OutputFileName1='Assess_Shipping_Routes.gml' OutputFileName2='Assess_Shipping_Routes.txt' ################################################################ sFileDir=Base + '/' + Company + '/' + EDSDir if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ sFileDir=Base + '/' + Company + '/' + OutputDir if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ sDataBaseDir=Base + '/' + Company + '/02-Assess/SQLite' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataBaseDir + '/hillman.db' conn = sq.connect(sDatabaseName) ################################################################ ################################################################ ### Import Warehouse Data ################################################################ sFileName=Base + '/' + Company + '/' + InputDir1 + '/' + InputFileName1 print('###########') print('Loading:',sFileName) WarehouseRawData=pd.read_csv(sFileName,                     header=0,                     low_memory=False,                     encoding="latin-1"                     ) WarehouseRawData.drop_duplicates(subset=None, keep="first", inplace=True) WarehouseRawData.index.name = 'IDNumber' Note I have taken a subset of warehouse locations here, to enable quicker processing. Feel free to change the parameters if you want more or fewer locations. WarehouseData=WarehouseRawData.head(nMax) WarehouseData=WarehouseData.append(WarehouseRawData.tail(nMax)) WarehouseData=WarehouseData.append(WarehouseRawData[WarehouseRawData.postcode=='KA13']) WarehouseData=WarehouseData.append(WarehouseRawData[WarehouseRawData.postcode=='SW1W']) WarehouseData.drop_duplicates(subset=None, keep="first", inplace=True) print('Loaded Warehouses:',WarehouseData.columns.values) print('################################')   ################################################################ print('################')   sTable='Assess_Warehouse_UK' print('Storing:',sDatabaseName,'Table:',sTable) WarehouseData.to_sql(sTable, conn, if_exists="replace") print('################')    ################################################################ print(WarehouseData.head()) print('################################') print('Rows:',WarehouseData.shape[0]) print('################################') ################################################################ ### Import Shop Data ################################################################ sFileName=Base + '/' + Company + '/' + InputDir1 + '/' + InputFileName2 print('###########') print('Loading:',sFileName) ShopRawData=pd.read_csv(sFileName,                     header=0,                     low_memory=False,                     encoding="latin-1"                     ) ShopRawData.drop_duplicates(subset=None, keep="first", inplace=True) ShopRawData.index.name = 'IDNumber' ShopData=ShopRawData print('Loaded Shops:',ShopData.columns.values) print('################################')   Note I have taken a subset of shop locations here, to enable quicker processing. Feel free to change the parameters, if you want more or fewer locations. ################################################################ print('################')   sTable='Assess_Shop_UK' print('Storing:',sDatabaseName,'Table:',sTable) ShopData.to_sql(sTable, conn, if_exists="replace") print('################') ################################################################ print(ShopData.head()) print('################################') print('Rows:',ShopData.shape[0]) print('################################') ################################################################ ### Connect HQ ################################################################ print('################')   sView='Assess_HQ' print('Creating:',sDatabaseName,'View:',sView) sSQL="DROP VIEW IF EXISTS" + sView + ";" sql.execute(sSQL,conn) sSQL="CREATE VIEW" + sView + "AS" sSQL=sSQL+ "SELECT" sSQL=sSQL+ "W.postcode AS HQ_PostCode," sSQL=sSQL+ "'HQ-' || W.postcode AS HQ_Name," sSQL=sSQL+ "round(W.latitude,6) AS HQ_Latitude," sSQL=sSQL+ "round(W.longitude,6) AS HQ_Longitude" sSQL=sSQL+ "FROM" sSQL=sSQL+ "Assess_Warehouse_UK as W" sSQL=sSQL+ "WHERE" sSQL=sSQL+ "TRIM(W.postcode) in ('KA13','SW1W');" sql.execute(sSQL,conn) To calculate the warehouse-to-warehouse routes , I suggest a simple process for all warehouses to ship to all warehouses. ################################################################ ### Connect Warehouses ################################################################ print('################')   sView='Assess_Warehouse' print('Creating:',sDatabaseName,'View:',sView) sSQL="DROP VIEW IF EXISTS" + sView + ";" sql.execute(sSQL,conn) sSQL="CREATE VIEW" + sView + "AS" sSQL=sSQL+ "SELECT" sSQL=sSQL+ "W.postcode AS Warehouse_PostCode," sSQL=sSQL+ "'WH-' || W.postcode AS Warehouse_Name," sSQL=sSQL+ "round(W.latitude,6) AS Warehouse_Latitude," sSQL=sSQL+ "round(W.longitude,6) AS Warehouse_Longitude" sSQL=sSQL+ "FROM" sSQL=sSQL+ "Assess_Warehouse_UK as W;" sql.execute(sSQL,conn) To calculate the warehouse-to-shop routes , I suggest a simple process for all warehouses to ship only to shops in the same post code. ################################################################ ### Connect Warehouse to Shops by PostCode ################################################################ print('################')   sView='Assess_Shop' print('Creating:',sDatabaseName,'View:',sView) sSQL="DROP VIEW IF EXISTS" + sView + ";" sql.execute(sSQL,conn) sSQL="CREATE VIEW" + sView + "AS" sSQL=sSQL+ "SELECT" sSQL=sSQL+ "TRIM(S.postcode) AS Shop_PostCode," sSQL=sSQL+ "'SP-' || TRIM(S.FirstCode) || '-' || TRIM(S.SecondCode) AS Shop_Name," sSQL=sSQL+ "TRIM(S.FirstCode) AS Warehouse_PostCode," sSQL=sSQL+ "round(S.latitude,6) AS Shop_Latitude," sSQL=sSQL+ "round(S.longitude,6) AS Shop_Longitude" sSQL=sSQL+ "FROM" sSQL=sSQL+ "Assess_Warehouse_UK as W" sSQL=sSQL+ "JOIN" sSQL=sSQL+ "Assess_Shop_UK as S" sSQL=sSQL+ "ON" sSQL=sSQL+ "TRIM(W.postcode) = TRIM(S.FirstCode);" sql.execute(sSQL,conn) ################################################################ I suggest you now use your knowledge in graph theory to calculate the routes. Make a graph, as follows: ################################################################ G=nx.Graph() ################################################################ print('################')   sTable = 'Assess_HQ' print('Loading:',sDatabaseName,'Table:',sTable) sSQL="SELECT DISTINCT" sSQL=sSQL+ "*" sSQL=sSQL+ "FROM" sSQL=sSQL+ " " + sTable + ";" RouteData=pd.read_sql_query(sSQL, conn) print('################')   Add the HQ nodes to the graph. ################################################################   print(RouteData.head()) print('################################') print('HQ Rows:',RouteData.shape[0]) print('################################') ################################################################ for i in range(RouteData.shape[0]):     sNode0=RouteData['HQ_Name'][i]     G.add_node(sNode0,                Nodetype='HQ',                PostCode=RouteData['HQ_PostCode'][i],                Latitude=round(RouteData['HQ_Latitude'][i],6),                Longitude=round(RouteData['HQ_Longitude'][i],6)) Add the warehouse nodes. ################################################################ print('################')   sTable = 'Assess_Warehouse' print('Loading:',sDatabaseName,'Table:',sTable) sSQL="SELECT DISTINCT" sSQL=sSQL+ "*" sSQL=sSQL+ "FROM" sSQL=sSQL+ " " + sTable + ";" RouteData=pd.read_sql_query(sSQL, conn) print('################')   ################################################################   print(RouteData.head()) print('################################') print('Warehouse Rows:',RouteData.shape[0]) print('################################') ################################################################ for i in range(RouteData.shape[0]):     sNode0=RouteData['Warehouse_Name'][i]         G.add_node(sNode0,                Nodetype='Warehouse',                PostCode=RouteData['Warehouse_PostCode'][i],                Latitude=round(RouteData['Warehouse_Latitude'][i],6),                Longitude=round(RouteData['Warehouse_Longitude'][i],6)) Add the shops’ nodes. ################################################################ print('################')   sTable = 'Assess_Shop' print('Loading:',sDatabaseName,'Table:',sTable) sSQL=" SELECT DISTINCT" sSQL=sSQL+ "*" sSQL=sSQL+ "FROM" sSQL=sSQL+ " " + sTable + ";" RouteData=pd.read_sql_query(sSQL, conn) print('################')   ################################################################   print(RouteData.head()) print('################################') print('Shop Rows:',RouteData.shape[0]) print('################################') ################################################################ for i in range(RouteData.shape[0]):     sNode0=RouteData['Shop_Name'][i]         G.add_node(sNode0,                Nodetype='Shop',                PostCode=RouteData['Shop_PostCode'][i],                WarehousePostCode=RouteData['Warehouse_PostCode'][i],                Latitude=round(RouteData['Shop_Latitude'][i],6),                Longitude=round(RouteData['Shop_Longitude'][i],6)) Now we create the shipping routes. We will calculate a Vincenty’s distance for each route, plus a transport method, in the form of a correct-size truck or a forklift. ################################################################ ## Create Edges ################################################################ print('################################') print('Loading Edges') print('################################') Let’s loop through all the nodes and set up the routes that are valid. for sNode0 in nx.nodes_iter(G):     for sNode1 in nx.nodes_iter(G): Let’s create the HQ routes with Vincenty’s distance between the HQs:     if G.node[sNode0]['Nodetype']=='HQ' and \         G.node[sNode1]['Nodetype']=='HQ' and \         sNode0 != sNode1:             distancemeters=round(\                                  vincenty(\                                           (\                                            G.node[sNode0]['Latitude'],\                                            G.node[sNode0]['Longitude']\                                            ),\                                            (\                                             G.node[sNode1]['Latitude']\                                             ,\                                             G.node[sNode1]['Longitude']\                                             )\                                             ).meters\                                 ,0)             distancemiles=round(\                                  vincenty(\                                           (\                                            G.node[sNode0]['Latitude'],\                                            G.node[sNode0]['Longitude']\                                            ),\                                            (\                                             G.node[sNode1]['Latitude']\                                             ,\                                             G.node[sNode1]['Longitude']\                                             )\                                             ).miles\                                 ,3)             if distancemiles >= 0.05:                 cost = round(150+(distancemiles * 2.5),6)                 vehicle='V001'             else:                 cost = round(2+(distancemiles * 0.10),6)                 vehicle='ForkLift'             G.add_edge(sNode0,sNode1,DistanceMeters=distancemeters,\                        DistanceMiles=distancemiles,\                        Cost=cost,Vehicle=vehicle)              if nVSet==True:                print('Edge-H-H:',sNode0,' to ', sNode1,\                    ' Distance:',distancemeters,'meters',\                  distancemiles,'miles','Cost', cost,'Vehicle',vehicle) Let’s create the headquarters (HQ)-to-warehouse routes with the Vincenty’s distance between the HQ’s and warehouses’.     if G.node[sNode0]['Nodetype']=='HQ' and\         G.node[sNode1]['Nodetype']=='Warehouse' and\         sNode0 != sNode1:             distancemeters=round(\                                  vincenty(\                                           (\                                            G.node[sNode0]['Latitude'],\                                            G.node[sNode0]['Longitude']\                                            ),\                                            (\                                             G.node[sNode1]['Latitude']\                                             ,\                                             G.node[sNode1]['Longitude']\                                             )\                                             ).meters\                                 ,0)             distancemiles=round(\                                  vincenty(\                                           (\                                            G.node[sNode0]['Latitude'],\                                            G.node[sNode0]['Longitude']\                                            ),\                                            (\                                             G.node[sNode1]['Latitude']\                                             ,\                                             G.node[sNode1]['Longitude']\                                             )\                                             ).miles\                                  ,3)            if distancemiles >= 10:                 cost = round(50+(distancemiles * 2),6)                 vehicle='V002'             else:                 cost = round(5+(distancemiles * 1.5),6)                 vehicle='V003'             if distancemiles <= 50:                    G.add_edge(sNode0,sNode1,DistanceMeters=distancemeters,\                            DistanceMiles=distancemiles,\                            Cost=cost,Vehicle=vehicle)                 if nVSet==True:                     print('Edge-H-W:',sNode0,'to',sNode1,\                       'Distance:',distancemeters,'meters',\                       distancemiles,'miles','Cost',cost,'Vehicle',vehicle) Let’s create the intra-warehouse routes with the Vincenty’s distance between the warehouses.      if nSet==True and \          G.node[sNode0]['Nodetype']=='Warehouse' and\          G.node[sNode1]['Nodetype']=='Warehouse' and\          sNode0 != sNode1:              distancemeters=round(\                                   vincenty(\                                            (\                                             G.node[sNode0]['Latitude'],\                                             G.node[sNode0]['Longitude']\                                             ),\                                             (\                                              G.node[sNode1]['Latitude']\                                              ,\                                              G.node[sNode1]['Longitude']\                                              )\                                              ).meters\                                  ,0)              distancemiles=round(\                                   vincenty(\                                            (\                                             G.node[sNode0]['Latitude'],\                                             G.node[sNode0]['Longitude']\                                             ),\                                             (\                                              G.node[sNode1]['Latitude']\                                              ,\                                              G.node[sNode1]['Longitude']\                                              )\                                              ).miles\                                  ,3)              if distancemiles >= 10:                  cost = round(50+(distancemiles * 1.10),6)                  vehicle='V004'              else:                  cost = round(5+(distancemiles * 1.05),6)                  vehicle='V005'              if distancemiles <= 20:                  G.add_edge(sNode0,sNode1,DistanceMeters=distancemeters,\                             DistanceMiles=distancemiles,\                             Cost=cost,Vehicle=vehicle)                  if nVSet==True:                      print('Edge-W-W:',sNode0,'to',sNode1,\                        'Distance:',distancemeters,'meters',\                        distancemiles,'miles','Cost',cost,'Vehicle',vehicle) Let’s create the warehouse-to-shop routes with the Vincenty’s distance.      if G.node[sNode0]['Nodetype']=='Warehouse' and \          G.node[sNode1]['Nodetype']=='Shop' and \          G.node[sNode0]['PostCode']==G.node[sNode1]['WarehousePostCode'] and\          sNode0 != sNode1:              distancemeters=round(\                                  vincenty(\                                            (\                                             G.node[sNode0]['Latitude'],\                                             G.node[sNode0]['Longitude']\                                             ),\                                             (\                                              G.node[sNode1]['Latitude']\                                              ,\                                              G.node[sNode1]['Longitude']\                                              )\                                              ).meters\                                  ,0)              distancemiles=round(\                                   vincenty(\                                            (\                                             G.node[sNode0]['Latitude'],\                                             G.node[sNode0]['Longitude']\                                             ),\                                             (\                                              G.node[sNode1]['Latitude']\                                              ,\                                              G.node[sNode1]['Longitude']\                                              )\                                              ).miles\                                  ,3)              if distancemiles >= 10:                  cost = round(50+(distancemiles * 1.50),6)                  vehicle='V006'              else:                  cost = round(5+(distancemiles * 0.75),6)                  vehicle='V007'              if distancemiles <= 10:                      G.add_edge(sNode0,sNode1,DistanceMeters=distancemeters,\                             DistanceMiles=distancemiles,\                             Cost=cost,Vehicle=vehicle)                 if nVSet==True:                      print('Edge-W-S:',sNode0,'to',sNode1,\                       'Distance:',distancemeters,'meters',\                        distancemiles,'miles','Cost', cost,'Vehicle',vehicle) Let’s create the intra-shop routes with the Vincenty's distance.      if nSet==True and\          G.node[sNode0]['Nodetype']=='Shop' and\          G.node[sNode1]['Nodetype']=='Shop' and\          G.node[sNode0]['WarehousePostCode']==G.node[sNode1]['WarehousePostCode'] and\          sNode0 != sNode1:              distancemeters=round(\                                  vincenty(\                                            (\                                             G.node[sNode0]['Latitude'],\                                             G.node[sNode0]['Longitude']\                                             ),\                                             (\                                             G.node[sNode1]['Latitude']\                                             ,\                                              G.node[sNode1]['Longitude']\                                              )\                                              ).meters\                                  ,0)              distancemiles=round(\                                   vincenty(\                                            (\                                             G.node[sNode0]['Latitude'],\                                             G.node[sNode0]['Longitude']\                                             ),\                                             (\                                              G.node[sNode1]['Latitude']\                                              ,\                                              G.node[sNode1]['Longitude']\                                              )\                                              ).miles\                                  ,3)              if distancemiles >= 0.05:                  cost = round(5+(distancemiles * 0.5),6)                  vehicle='V008'              else:                  cost = round(1+(distancemiles * 0.1),6)                  vehicle='V009'              if distancemiles <= 0.075:                  G.add_edge(sNode0,sNode1,DistanceMeters=distancemeters,\                             DistanceMiles=distancemiles, \                             Cost=cost,Vehicle=vehicle)                  if nVSet==True:                      print('Edge-S-S:',sNode0,'to',sNode1,\                        'Distance:',distancemeters,'meters',\      if nSet==True and\          G.node[sNode0]['Nodetype']=='Shop' and\          G.node[sNode1]['Nodetype']=='Shop' and\          G.node[sNode0]['WarehousePostCode']!=G.node[sNode1]['WarehousePostCode'] and \          sNode0 != sNode1:              distancemeters=round(\                                   vincenty(\                                            (\                                             G.node[sNode0]['Latitude'],\                                             G.node[sNode0]['Longitude']\                                             ),\                                             (\                                             G.node[sNode1]['Latitude']\                                              ,\                                              G.node[sNode1]['Longitude']\                                              )\                                              ).meters\                                  ,0)              distancemiles=round(\                                      vincenty(\                                            (\                                             G.node[sNode0]['Latitude'],\                                             G.node[sNode0]['Longitude']\                                             ),\                                             (\                                              G.node[sNode1]['Latitude']\                                              ,\                                              G.node[sNode1]['Longitude']\                                              )\                                              ).miles\                                  ,3)             cost = round(1+(distancemiles * 0.1),6)             vehicle='V010'              if distancemiles <= 0.025:                  G.add_edge(sNode0,sNode1,DistanceMeters=distancemeters,\                            DistanceMiles=distancemiles,\                            Cost=cost,Vehicle=vehicle)                  if nVSet==True:                      print('Edge-S-S:',sNode0,'to',sNode1,\                        'Distance:',distancemeters,'meters',\                        distancemiles,'miles','Cost', cost,'Vehicle',vehicle) ################################################################ sFileName=sFileDir + '/' + OutputFileName1 print('################################') print('Storing:',sFileName) print('################################') nx.write_gml(G,sFileName) sFileName=sFileName +'.gz' nx.write_gml(G,sFileName) ################################################################ print('Nodes:',nx.number_of_nodes(G)) print('Edges:',nx.number_of_edges(G)) ################################################################ sFileName=sFileDir + '/' + OutputFileName2 print('################################') print('Storing:',sFileName) print('################################') I will now guide you through the process of extracting the routes or path from the shipping plan in the graph. We will use nx.shortest_path(G,x,y) to extract the shortest path between nodes x and y on graph G. ################################################################ ## Create Paths ################################################################ print('################################') print('Loading Paths') print('################################') f = open(sFileName,'w') l=0 sline = 'ID|Cost|StartAt|EndAt|Path|Measure' if nVSet==True: print ('0', sline) f.write(sline+ '\n') for sNode0 in nx.nodes_iter(G):     for sNode1 in nx.nodes_iter(G):         if sNode0 != sNode1 and\             nx.has_path(G, sNode0, sNode1)==True and\             nx.shortest_path_length(G, \               source=sNode0, \               target=sNode1, \               weight='DistanceMiles') < nMaxPath:                 l+=1                 sID='{:.0f}'.format(l)                 spath = ','.join(nx.shortest_path(G,\                   source=sNode0,\                   target=sNode1,\                   weight='DistanceMiles'))                 slength= '{:.6f}'.format(\                   nx.shortest_path_length(G,\                   source=sNode0,\                   target=sNode1,\                   weight='DistanceMiles'))                 sline = sID + '|"DistanceMiles"|"' + sNode0 + '"|"'\                 + sNode1 + '"|"' + spath + '"|' + slength                 if nVSet==True: print (sline)                 f.write(sline + '\n')                 l+=1                 sID='{:.0f}'.format(l)                 spath = ','.join(nx.shortest_path(G,\                   source=sNode0,\                   target=sNode1,\                   weight='DistanceMeters'))                 slength= '{:.6f}'.format(\                   nx.shortest_path_length(G,\                   source=sNode0,\                   target=sNode1,\                   weight='DistanceMeters'))                 sline = sID + '|"DistanceMeters"|"' + sNode0 + '"|"'\                 + sNode1 + '"|"' + spath + '"|' + slength                 if nVSet==True: print(sline)                 f.write(sline + '\n')                 l+=1                 sID='{:.0f}'.format(l)                 spath = ','.join(nx.shortest_path(G,\                   source=sNode0,\                   target=sNode1,\                   weight='Cost'))                 slength= '{:.6f}'.format(\                   nx.shortest_path_length(G,\                   source=sNode0,\                   target=sNode1,\                   weight='Cost'))                 sline = sID + '|"Cost"|"' + sNode0 + '"|"'\                 + sNode1 + '"|"' + spath + '"|' + slength                 if nVSet==True: print (sline)                 f.write(sline + '\n') f.close() ################################################################ print('Nodes:',nx.number_of_nodes(G)) print('Edges:',nx.number_of_edges(G)) print('Paths:',sID) ################################################################ ################################################################ print('################') print('Vacuum Database') sSQL="VACUUM;" sql.execute(sSQL,conn) print('################') ################################################################ print('### Done!! ############################################') ################################################################ Save the Assess-Shipping-Routes.py file, then compile and execute with your Python compiler. This will produce a set of demonstrated values onscreen, plus a graph data file named Assess_Shipping_Routes.gml . Note You can view the GML file with a text editor. The path finder generates a file with shortest paths, named Assess_Shipping_Routes.txt. You now have the basis of a working shipping route graph that you can use to generate any queries you may have against the solution. This final Python script completes the access step for Hillman Ltd.

### 克拉克有限公司

Clark Ltd is the accountancy company that handles everything related to the VKHCG’s finances and personnel. I will let you investigate Clark with your new knowledge. You can do this!

#### 简单的外汇交易规划

Clark requires the assessment of the group’s forex data, for processing and data quality issues. I will guide you through an example of a forex solution. Open your Python editor and create a file named Assess-Forex.py in directory C:\VKHCG\04-Clark\02-Assess. Now copy the following code into the file: ################################################################ import sys import os import sqlite3 as sq import pandas as pd ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + 'VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base:',Base,'using',sys.platform) print('################################') ################################################################ Company='04-Clark' sInputFileName1='01-Vermeulen/01-Retrieve/01-EDS/02-Python/Retrieve-Country-Currency.csv' sInputFileName2='04-Clark/01-Retrieve/01-EDS/01-R/Retrieve_Euro_ExchangeRates.csv' ################################################################ sDataBaseDir=Base + '/' + Company + '/02-Assess/SQLite' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataBaseDir + '/clark.db' conn = sq.connect(sDatabaseName) ################################################################ ### Import Country Data ################################################################ sFileName1=Base + '/' + sInputFileName1 print('################################') print('Loading:',sFileName1) print('################################') CountryRawData=pd.read_csv(sFileName1,header=0,low_memory=False, encoding="latin-1") CountryRawData.drop_duplicates(subset=None, keep="first", inplace=True) CountryData=CountryRawData print('Loaded Company:',CountryData.columns.values) print('################################') ################################################################ print('################')   sTable='Assess_Country' print('Storing:',sDatabaseName,' Table:',sTable) CountryData.to_sql(sTable, conn, if_exists="replace") print('################')   ################################################################ print(CountryData.head()) print('################################') print('Rows: ',CountryData.shape[0]) print('################################') ################################################################ ### Import Forex Data ################################################################ sFileName2=Base + '/' + sInputFileName2 print('################################') print('Loading:',sFileName2) print('################################') ForexRawData=pd.read_csv(sFileName2,header=0,low_memory=False, encoding="latin-1") ForexRawData.drop_duplicates(subset=None, keep="first", inplace=True) ForexData=ForexRawData.head(5) print('Loaded Company:',ForexData.columns.values) print('################################') ################################################################ print('################')   sTable='Assess_Forex' print('Storing:',sDatabaseName,'Table:',sTable) ForexData.to_sql(sTable, conn, if_exists="replace") print('################')   ################################################################ print(ForexData.head()) print('################################') print('Rows:',ForexData.shape[0]) print('################################') ################################################################ print('################')   sTable='Assess_Forex' print('Loading:',sDatabaseName,'Table:',sTable) sSQL="select distinct" sSQL=sSQL+ "A.CodeIn" sSQL=sSQL+ "from" sSQL=sSQL+ "Assess_Forex as A;" CodeData=pd.read_sql_query(sSQL, conn) print('################')   ################################################################ for c in range(CodeData.shape[0]):     print('################')       sTable='Assess_Forex & 2x Country > ' + CodeData['CodeIn'][c]     print('Loading:',sDatabaseName,'Table:',sTable)     sSQL="select distinct"     sSQL=sSQL+ "A.Date,"     sSQL=sSQL+ "A.CodeIn,"     sSQL=sSQL+ "B.Country as CountryIn,"     sSQL=sSQL+ "B.Currency as CurrencyNameIn,"     sSQL=sSQL+ "A.CodeOut,"     sSQL=sSQL+ "C.Country as CountryOut,"     sSQL=sSQL+ "C.Currency as CurrencyNameOut,"     sSQL=sSQL+ "A.Rate"     sSQL=sSQL+ "from"     sSQL=sSQL+ "Assess_Forex as A"     sSQL=sSQL+ "JOIN"     sSQL=sSQL+ "Assess_Country as B"     sSQL=sSQL+ "ON A.CodeIn = B.CurrencyCode"     sSQL=sSQL+ "JOIN"     sSQL=sSQL+ "Assess_Country as C"     sSQL=sSQL+ "ON A.CodeOut = C.CurrencyCode"     sSQL=sSQL+ "WHERE"     sSQL=sSQL+ "A.CodeIn ='" + CodeData['CodeIn'][c] + "';"     ForexData=pd.read_sql_query(sSQL, conn).head(1000)     print('################')       print(ForexData)     print('################')       sTable='Assess_Forex_' + CodeData['CodeIn'][c]     print('Storing:',sDatabaseName,'Table:',sTable)     ForexData.to_sql(sTable, conn, if_exists="replace")     print('################')       print('################################')     print('Rows:',ForexData.shape[0])     print('################################') ################################################################ print('### Done!! ############################################') ################################################################ Save the Assess-Forex.py file, then compile and execute with your Python compiler. This will produce a set of demonstrated values onscreen

#### 金融的

Clark requires you to process the balance sheet for the VKHCG group companies. I will guide you through a sample balance sheet data assessment, to ensure that only the good data is processed. Open your Python editor and create a file named Assess-Financials.py in directory C:\VKHCG\04-Clark\02-Assess. Now copy the following code into the file: ################################################################ import sys import os import sqlite3 as sq import pandas as pd ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + 'VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base:',Base,'using',sys.platform) print('################################') ################################################################ Company='04-Clark' sInputFileName='01-Retrieve/01-EDS/01-R/Retrieve_Profit_And_Loss.csv' ################################################################ sDataBaseDir=Base + '/' + Company + '/02-Assess/SQLite' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataBaseDir + '/clark.db' conn = sq.connect(sDatabaseName) ################################################################ ### Import Financial Data ################################################################ sFileName=Base + '/' + Company + '/' + sInputFileName print('################################') print('Loading:',sFileName) print('################################') FinancialRawData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") FinancialData=FinancialRawData print('Loaded Company:',FinancialData.columns.values) print('################################') ################################################################ print('################')   sTable='Assess-Financials' print('Storing:',sDatabaseName,'Table:',sTable) FinancialData.to_sql(sTable, conn, if_exists="replace") print('################')   ################################################################ print(FinancialData.head()) print('################################') print('Rows:',FinancialData.shape[0]) print('################################') ################################################################ ################################################################ print('### Done!! ############################################') ################################################################ Save the Assess-Financials.py file, then compile and execute with your Python compiler. This will produce a set of demonstrated values onscreen

#### 财务日历

Clark stores all the master records for the financial calendar. So, I suggest we import the calendar from the retrieve step’s data storage. Open your Python editor and create a file named Assess-Calendar.py in directory C:\VKHCG\04-Clark\02-Assess. Now copy the following code into the file: ################################################################ import sys import os import sqlite3 as sq import pandas as pd ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + 'VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base:',Base,'using',sys.platform) print('################################') ################################################################ Company='04-Clark' ################################################################ sDataBaseDirIn=Base + '/' + Company + '/01-Retrieve/SQLite' if not os.path.exists(sDataBaseDirIn):     os.makedirs(sDataBaseDirIn) sDatabaseNameIn=sDataBaseDirIn + '/clark.db' connIn = sq.connect(sDatabaseNameIn) ################################################################ sDataBaseDirOut=Base + '/' + Company + '/01-Retrieve/SQLite' if not os.path.exists(sDataBaseDirOut):     os.makedirs(sDataBaseDirOut) sDatabaseNameOut=sDataBaseDirOut + '/clark.db' connOut = sq.connect(sDatabaseNameOut) ################################################################ sTableIn='Retrieve_Date' sSQL='select * FROM' + sTableIn + ';' print('################')   sTableOut='Assess_Time' print('Loading:',sDatabaseNameIn,'Table:',sTableIn) dateRawData=pd.read_sql_query(sSQL, connIn) dateData=dateRawData ################################################################ print('################################') print('Load Rows:',dateRawData.shape[0],'records') print('################################') dateData.drop_duplicates(subset='FinDate', keep="first", inplace=True) ################################################################ print('################')   sTableOut='Assess_Date' print('Storing:',sDatabaseNameOut,'Table:',sTableOut) dateData.to_sql(sTableOut, connOut, if_exists="replace") print('################')   ################################################################ print('################################') print('Store Rows:',dateData.shape[0],' records') print('################################') ################################################################ ################################################################ sTableIn='Retrieve_Time' sSQL='select * FROM' + sTableIn + ';' print('################')   sTableOut='Assess_Time' print('Loading:',sDatabaseNameIn,'Table:',sTableIn) timeRawData=pd.read_sql_query(sSQL, connIn) timeData=timeRawData ################################################################ print('################################') print('Load Rows: ',timeData.shape[0],'records') print('################################') timeData.drop_duplicates(subset=None, keep="first", inplace=True) ################################################################ print('################')   sTableOut='Assess_Time' print('Storing:',sDatabaseNameOut,'Table:',sTableOut) timeData.to_sql(sTableOut, connOut, if_exists="replace") print('################')   ################################################################ print('################################') print('Store Rows:',timeData.shape[0],'records') print('################################') ################################################################ print('### Done!! ############################################') ################################################################ Save the Assess-Calendar.py file, then compile and execute with your Python compiler. This will produce database tables named Assess_Date and Assess_Time.

#### 人

Clark Ltd generates the payroll, so it holds all the staff records. Clark also handles all payments to suppliers and receives payments from customers’ details on all companies. Open your Python editor and create a file named Assess-People.py in directory C:\VKHCG\04-Clark\02-Assess. Now copy the following code into the file: ################################################################ import sys import os import sqlite3 as sq import pandas as pd ################################################################ if sys.platform == 'linux':     Base=os.path.expanduser('~') + 'VKHCG' else:     Base='C:/VKHCG' print('################################') print('Working Base:',Base,'using',sys.platform) print('################################') ################################################################ Company='04-Clark' sInputFileName1='01-Retrieve/01-EDS/02-Python/Retrieve-Data_female-names.csv' sInputFileName2='01-Retrieve/01-EDS/02-Python/Retrieve-Data_male-names.csv' sInputFileName3='01-Retrieve/01-EDS/02-Python/Retrieve-Data_last-names.csv' sOutputFileName1='Assess-Staff.csv' sOutputFileName2='Assess-Customers.csv' ################################################################ sDataBaseDir=Base + '/' + Company + '/02-Assess/SQLite' if not os.path.exists(sDataBaseDir):     os.makedirs(sDataBaseDir) ################################################################ sDatabaseName=sDataBaseDir + '/clark.db' conn = sq.connect(sDatabaseName) ################################################################ ### Import Female Data ################################################################ sFileName=Base + '/' + Company + '/' + sInputFileName1 print('################################') print('Loading :',sFileName) print('################################') print(sFileName) FemaleRawData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") FemaleRawData.rename(columns={'NameValues':'FirstName'},inplace=True) FemaleRawData.drop_duplicates(subset=None, keep="first", inplace=True) FemaleData=FemaleRawData.sample(100) print('################################') ################################################################ print('################')   sTable='Assess_FemaleName' print('Storing:',sDatabaseName,'Table:',sTable) FemaleData.to_sql(sTable, conn, if_exists="replace") print('################')   ################################################################ print('################################') print('Rows:',FemaleData.shape[0],'records') print('################################') ################################################################ ### Import Male Data ################################################################ sFileName=Base + '/' + Company + '/' + sInputFileName2 print('################################') print('Loading:',sFileName) print('################################') MaleRawData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") MaleRawData.rename(columns={'NameValues':'FirstName'},inplace=True) MaleRawData.drop_duplicates(subset=None, keep="first", inplace=True) MaleData=MaleRawData.sample(100) print('################################') ################################################################ print('################')   sTable='Assess_MaleName' print('Storing:',sDatabaseName,'Table:',sTable) MaleData.to_sql(sTable, conn, if_exists="replace") print('################')   ################################################################ print('################################') print('Rows:',MaleData.shape[0],'records') print('################################') ################################################################ ### Import Surname Data ################################################################ sFileName=Base + '/' + Company + '/' + sInputFileName3 print('################################') print('Loading:',sFileName) print('################################') SurnameRawData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1") SurnameRawData.rename(columns={'NameValues':'LastName'},inplace=True) SurnameRawData.drop_duplicates(subset=None, keep="first", inplace=True) SurnameData=SurnameRawData.sample(200) print('################################') ################################################################ print('################')   sTable='Assess_Surname' print('Storing:',sDatabaseName,'Table:',sTable) SurnameData.to_sql(sTable, conn, if_exists="replace") print('################')   ################################################################ print('################################') print('Rows:',SurnameData.shape[0],'records') print('################################') ################################################################ ################################################################ print('################')   sTable='Assess_FemaleName & Assess_MaleName' print('Loading:',sDatabaseName,'Table:',sTable) sSQL="select distinct" sSQL=sSQL+ "A.FirstName," sSQL=sSQL+ "'Female' as Gender" sSQL=sSQL+ "from" sSQL=sSQL+ "Assess_FemaleName as A" sSQL=sSQL+ "UNION" sSQL=sSQL+ "select distinct" sSQL=sSQL+ "A.FirstName," sSQL=sSQL+ "'Male' as Gender" sSQL=sSQL+ "from" sSQL=sSQL+ "Assess_MaleName as A;" FirstNameData=pd.read_sql_query(sSQL, conn) print('################')   ################################################################# #print('################')   sTable='Assess_FirstName' print('Storing:',sDatabaseName,'Table:',sTable) FirstNameData.to_sql(sTable, conn, if_exists="replace") print('################')   ################################################################ ################################################################ print('################')   sTable='Assess_FirstName x2 & Assess_Surname' print('Loading:',sDatabaseName,'Table:',sTable) sSQL="select distinct" sSQL=sSQL+ "A.FirstName," sSQL=sSQL+ "B.FirstName AS SecondName," sSQL=sSQL+ "C.LastName," sSQL=sSQL+ "A.Gender" sSQL=sSQL+ "from" sSQL=sSQL+ "Assess_FirstName as A" sSQL=sSQL+ "," sSQL=sSQL+ "Assess_FirstName as B" sSQL=sSQL+ "," sSQL=sSQL+ "Assess_Surname as C" sSQL=sSQL+ "WHERE" sSQL=sSQL+ "A.Gender = B.Gender" sSQL=sSQL+ "AND" sSQL=sSQL+ "A.FirstName <> B.FirstName;" PeopleRawData=pd.read_sql_query(sSQL, conn) People1Data=PeopleRawData.sample(10000) sTable='Assess_FirstName & Assess_Surname' print('Loading:',sDatabaseName,'Table:',sTable) sSQL="select distinct" sSQL=sSQL+ "A.FirstName," sSQL=sSQL+ "'' AS SecondName," sSQL=sSQL+ "B.LastName," sSQL=sSQL+ "A.Gender" sSQL=sSQL+ "from" sSQL=sSQL+ "Assess_FirstName as A" sSQL=sSQL+ "," sSQL=sSQL+ "Assess_Surname as B;" PeopleRawData=pd.read_sql_query(sSQL, conn) People2Data=PeopleRawData.sample(10000) PeopleData=People1Data.append(People2Data) print(PeopleData) print('################')   ################################################################# #print('################')   sTable='Assess_People' print('Storing:',sDatabaseName,'Table:',sTable) PeopleData.to_sql(sTable, conn, if_exists="replace") print('################')   ################################################################ sFileDir=Base + '/' + Company + '/02-Assess/01-EDS/02-Python' if not os.path.exists(sFileDir):     os.makedirs(sFileDir) ################################################################ sOutputFileName = sTable+'.csv' sFileName=sFileDir + '/' + sOutputFileName print('################################') print('Storing:', sFileName) print('################################') PeopleData.to_csv(sFileName, index = False) print('################################') ################################################################ print('### Done!! ############################################') ################################################################ Save the Assess-People.py file, then compile and execute with your Python compiler. This will produce a set of demonstrated values onscreen

## 摘要

Congratulations! You have completed the Assess superstep. You now have a stable base for blending data sources into a data vault. I have guided you through your first data-quality processing steps. The results you achieved will serve as the base data for Chapter [9](09.html). So, what should you know at this stage in the book? You should know how to

*   有效解决不同的数据质量问题。
*   执行评估处理时，转换并更新数据映射矩阵。您评估的每个数据源都是数据科学信息的干净来源。
*   成功执行特征工程。
*   通过评估引入针对其他数据源的标准，以及针对货币代码和国家代码等标准的查找。
*   将列格式数据转换为图形关系。这是需要掌握的一项重要的数据科学技能。

Note In real-life data science projects, you would by now have consumed as high as 50% of the project resources to reach the stable data set you accomplished today. Well done! Data preparation is the biggest drain on your labor, budget, and computational resources. Now, you have mastered the Assess superstep and become skilled at handling data quality. The next steps are more structured in nature, as you will now create a common data structure to form the basis of the full-scale data science investigation that begins in Chapter [9](09.html). I recommend that you get a good assortment of snacks, and then we can advance to the next chapter, in which we will construct a data vault for your data science.