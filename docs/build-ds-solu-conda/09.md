

# 七、选择最佳 AI 算法

如果说**人工智能与机器学习**(俗称 **AI/ML** )的领域是汽车，那么车型就是发动机。虽然有其他部分对其运作至关重要，但没有其他方面得到如此多的关注和重视。这是有充分理由的。最终，模型是决定你的结果是否准确的核心对象，并且是整个数据科学工作流程中最重要的工件。

哪种建模方法是最好的？那很简单，*看情况*。出于同样的原因，所有的汽车都没有相同的引擎，有许多不同的方面可以成为最佳的使用方法。

问问你自己，我在试图解决什么问题？在这一章中，我们将从这个问题开始，并由此引导你找到最适合你情况的建模方法。我们将通过每个算法的一个例子来看看问题类型，并看看属于每个家族的一些最广泛使用的算法，如随机森林和 KNN。到本章结束时，你将清楚地了解哪些算法适合你的情况，以及何时不使用它们。

在本章中，我们将讨论以下主题:

*   定义您的问题
*   用例子理解回归问题
*   通过示例了解分类
*   通过示例了解异常检测
*   通过示例了解聚类问题

现在，我们已经对将要学习的内容有了一个大致的了解，让我们在下一节看看本章的先决条件。

# 技术要求

所有需要的库都可以用 conda 轻松安装，conda 是 Anaconda 发行版附带的。本章内容需要以下工具:

*   Anaconda 发行版(包括 conda 和 Navigator)
*   Python 3.8+(这包含在 Anaconda 发行版中)
*   熊猫 1.3 以上
*   Matplotlib 3.4+
*   Jupyter 笔记本 6.4 以上

现在设置已经准备好了，让我们开始学习本章吧！

# 定义你的问题

很多时候，你会看到人工智能书籍和博客谈论不同类型的人工智能问题，这些问题分为以下几类:

*   监督
*   无人监督的
*   半监督的
*   加强

我们在 [*第一章*](B16589_01_ePub.xhtml#_idTextAnchor015)*中做了同样的事情，理解人工智能/人工智能的前景*，你可以在*图 7.1* 中找到如何决定你的情况属于哪一类的流程图:

![Figure 7.1 – Dataset heuristics for choosing your AI family
](img/Figure_7.1.jpg)

图 7.1-选择人工智能家族的数据集试探法

这是一个好主意，但是当你开始处理一个问题时，你并不总是从问题的类型来考虑，而是更多地从你试图找出的解决方案来考虑。

在接下来的几节中，我们将研究一些不同的、非常常见的问题类型。它们并不包含你会遇到的每一个问题家庭，但是它们会为其中的许多家庭服务。

## 模型问题类型

以下是我们将重点关注的四种核心问题类型:

*   回归
*   分类
*   使聚集
*   异常检测

它们中的每一个都将用于计算不同的结果，如下表所示:

![Figure 7.2 – AI/ML algorithm types with examples
](img/Figure_7.2.jpg)

图 7.2–AI/ML 算法类型及示例

一旦你身处问题之中，知道你想弄清楚什么，问题类型就很容易挑选出来。您还应该注意，这不是一个详尽的列表。现实世界中有许多类型的问题(例如自动驾驶)是非常复杂的问题，不适合这里列出的简单类别。这些只是我们将关注的最常见的领域。我们将在本章后面更详细地讨论每一个问题，但首先，让我们停下来谈谈作为数据科学家的一个重要观点。

### 建立一个模型不是目标

随着我们继续我们的旅程，重要的是记住我们的最终目标不是建立一个模型。在这一点上你可能会感到困惑，但是让我来解释一下。在任何时候，当你的任务是找到一个问题的解决方案时，后退一步，意识到我们正在试图回答问题，提出更好的问题，并解决问题，这总是好的。模型是让你达到回答问题(或者能够提出更好的问题)的最终目标的东西。当你可能不需要的时候，你永远不会想那么执着于建立一个特定的模型，或者使用*完美的*算法。

有时候，简单的统计可以让你获得你需要的洞察力，比如在前面的章节中，我们试图弄清楚未被充分代表的群体是否在数据集中有适当的数据参与。当你只需要挖一个小洞来种一朵花的时候，不要用推土机。

一定要确保你知道你想要完成的目标。有许多不同的算法会给你一个答案，每种算法都有其利弊。了解你想要实现的目标会对你有所帮助。

在开始考虑使用什么模型之前，您应该先问自己以下几个问题:

*   我们试图解决什么问题？
*   我们把这件事做好有多重要？
*   我们看重速度还是准确度？
*   我们现在能得到一个像样的答案，以后能得到一个更好的答案吗？

每个答案都将让你立足于为企业创造价值。

在我们深入研究每种模型类型之前，最好确保我们在一些通用术语上保持一致。如果你对这个领域感到满意，可以随意跳到问题类型的算法部分。

### 人工智能/人工智能术语

以下是您将在本章中看到的一些术语，它们将在数据科学领域中经常出现:

*   **特征**:数据集的单个方面。如果我们正在处理结构化的数据，比如一个 CSV 文件，这被表示为一个单独的列。汽车的颜色、房子的平方英尺和电影的类型都是特征的例子。
*   **目标**:您试图使用模型推断的因变量。一些例子是汽车的销售价格，电子邮件是否是垃圾邮件，或者推文中的情绪是积极的还是消极的。这是关键变量，也是你试图首先创建一个模型的全部原因。注意，对于无监督的问题，比如聚类，你可能没有标签。
*   **推论**:一个模型被训练后，这是通过它运行真实世界数据并做出预测的行为。模型*推断*它所看到的目标变量。你会经常听到这被称为*运行*模型。

我们刚刚学习的问题和术语将帮助我们为下一节做好准备，下一节将根据我们需要解决的问题类型来研究我们可以使用什么算法。

## 按问题类型划分的算法

有许多方法可以处理特定的问题类型，甚至有重叠的算法可以用于每一种不同的类型。为了让您更直观地了解这种情况，这里有一个图表，显示了我们将在本章中讨论的不同问题类型的流程，以及一些您可以在每个问题中使用的算法。

在前面的图 7.3-算法流程图中，左边是问题类型，右边是各自的算法。虚线除了区分不同的问题类型以稍微简化对行的解析之外，没有任何意义。

![Figure 7.3 – Algorithm flow diagram
](img/Figure_7.3.jpg)

图 7.3–算法流程图

对于前面所有的算法，都有优点和缺点。我们将对其中大多数进行评级，为了理解这些评级的含义，我们将看看我们是如何得出这些评级的。

### 分解算法等级

以下每个类别将被给予一个从 0 到 5 的等级。理解每个类别很重要，这样你就不会对其含义有错误的理解:

*   **可解释性**:数据科学家能够解释模型，并给出模型为什么会对其他人产生影响的感觉的能力。这是一个广泛的领域，在 [*第 10 章*](B16589_10_ePub.xhtml#_idTextAnchor249) *《可解释的人工智能——使用 LIME 和 SHAP* 》中有更详细的解释。
*   **精确度**:一个模型的输出一般有多精确。这是一个通用术语，它涉及到一个模型有多接近正确的东西，标准化分类，回归，等等。
*   **所需数据**:需要多少数据才能得到好的结果。虽然总的来说数据越多越好(并不总是如此)，但有些方法在真正有用之前需要比其他方法多得多的数据。(我们在看你，神经网络。)
*   **易用性**:启动并运行有多简单，但这种方法的概念有多复杂。一些技术更容易理解和学习，而另一些技术可能很复杂，学习曲线要陡峭得多。
*   **费用**:训练模型的 CPU 或 GPU 密集程度。这是对时间、功率和硬件资源的度量。

诸如此类的评级总会有警告，我们的评级也不例外。在我们讨论的广泛群体中，可能存在某些方法或场景，其中一些评级可能变得更好或更差。例如，如果有少量数据，神经网络不会对系统造成很大负担，因此**费用**会下降。此外，在许多结构化数据的情况下，这些神经网络可能比其他方法具有更低的**精度**。

说了这么多，我们觉得在书中有这些评级比没有更有用，但把它们作为指南而不是绝对的。

现在，让我们浏览一下您将看到的四种核心问题类型。你会发现很多现实世界的案例都会落入这四个桶里。当你需要预测一个数字结果时，我们将从使用什么开始。

# 用例子理解回归问题

计算一只股票的价格，你的房子应该值多少钱，以及地球未来的温度都有一个共同点:它们都可以被认为是回归问题。它的简单目标是在给定一组独立变量的情况下，计算出一个数字是多少。

属于这种问题类型的几个例子如下:

*   汽车的价格
*   明年的销售预测
*   将报名参加晋升的人数

当你看到这样的问题时，你可以尝试几种不同的模型。您可以使用许多特定的算法，每种算法都有自己的优缺点。让我们在下一节看看其中的一些算法。

以下是一些你想尝试的最常见的回归算法。对于这些算法中的每一个，我们都要举一个例子并创建一个回归模型:

*   **线性回归**
*   **随机森林**
*   **支持向量机** ( **SVMs** )
*   **人工神经网络**
*   **K-最近邻居** ( **KNN** )

当你试图预测一个单一项目时，它被称为**单变量**回归问题。如果我们试图预测不止一个项目，这将被称为一个**多变量**问题。多变量问题的一个例子是房价和出售房子的天数。

现在让我们更详细地介绍一下前面的每一个算法。

## 线性回归

线性回归可能是统计学和 AI/ML 中最基本的算法。假设输入特征和目标变量之间存在线性关系。它用一条线(或多个要素的超平面)来压缩和近似关系，这条线可以跨该数据绘制。这可能是一个天真的假设，但它可能是你在许多情况下应该采取的第一种方法。

它通常很快，并且在您需要时，为您提供了一个更复杂方法的良好起点。很多时候，它可以给你一个足够好的答案。

在我们的例子中，我们会说它对我们来说是正确的，为了使用它，我们需要一些数据。

### 使用线性回归导入糖尿病数据

你要做的第一件事是导入数据并做一些常规分析。你可以在第五章 *“清理和可视化数据*”的 [*中的“分析和清理数据”一节中找到更详细的分析。你的大部分时间可以用来清理数据，但是如果你从一个很好的例子开始，那么它可以大大加快你找到解决方案的时间。*](B16589_05_ePub.xhtml#_idTextAnchor101)

导入所需的库，然后将数据集加载到 pandas 数据框架中:

```
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets
data = datasets.load_diabetes(as_frame=True)
diabetes_X = pd.DataFrame(data['data'])
diabetes_y = data['target']
```

现在我们有了数据，我们需要拿出一些来测试，一些来训练。

### 将数据分为训练和测试

为了确保我们有机会了解我们对模型的训练有多好,我们需要确保从训练数据中截取一部分数据。这将允许模型使用一些数据进行训练，并使用一些数据来验证经过训练的模型在以前没有见过的数据上的表现。数据的测试切片不应该用于训练。

这就像让你的孩子记住书中的每一个单词。他们可以读给你听，但实际上，他们可能只是在那种背景下记住了单词。他们在现实世界中可能表现不佳。如果你让他们试着自己读最后几页，你可能会发现他们没有完全记下某些音。

Scikit-learn 可以很容易地将东西分成独立的块。

运行下面的代码来导入函数，并告诉它为测试集保留 20%的数据:

```
from sklearn.model_selection import train_test_split
train_features, test_features, train_target, test_target = train_test_split(diabetes_X, diabetes_y, test_size=.2, random_state=33)
```

`random_state`允许我们在需要时每次都重新创建完全相同的训练/测试分割。

命名您的变量

标签被称为`X_train`、`x_test`、`Y_train`、`y_test`而不是`train_features`、`test_features`、`train_target`、 `test_target`是很常见的。在其他代码和在线示例中，这些名称可能会有所不同。你给它们起什么名字并不重要，只要它们对你和任何阅读代码的人来说都是清楚的。如果你看到其他的惯例，不要惊讶。

#### 何时不使用线性回归

线性回归在许多情况下都是一个极好的首选，但它不是灵丹妙药。很多时候会有更好的选择，以下是其中的一些:

*   当关系不遵循线性形状时。如果事物是聚类的，你可能想利用聚类算法。
*   如果数据中有很多异常值。
*   如果输入变量或特征变量与目标变量之间没有统计上的显著关系。用月球黑暗面的温度来预测德克萨斯州奥斯汀有多少人会穿外套，可能不会神奇地创造出一个有益的模型。

线性回归是一个很好的选择，但是也有其他人使用不同的技术来获得相同的结果。现在让我们来看看一种叫做**随机森林**的集合技术。

## 随机森林

随机森林就是所谓的**集合**技术。这仅仅意味着不同的模型输出被组合成一个单一的输出。在回归的情况下，它使用输出的平均值。这在某种程度上是一种**群体智慧**的方法，也就是说如果你从一群人那里收集答案，你会得到一个更好的结果。

在随机森林的情况下，这将采用一组决策树的组合。

### 决策树

一个**决策树**是一个简单的真/假问题和答案的流程，引导你根据特定特性的价值得出最终结论。例如，如果某人年龄超过 35 岁(正确)，并且住在农村地区(正确)，那么他们的保险可能是每月 100 美元。如果他们年龄超过 35 岁并且不住在农村地区，这个模型会得出不同的结果。采用许多不同决策树的输出往往会给你一个比最准确的决策树更好的结果，这正是随机森林算法所做的。

接下来，让我们看一个简单的例子。

### 使用 scikit-learn 实现随机森林

回到`sklearn`，我们先从进口`RandomForestRegressor`。然后，我们将使用与之前算法相同的方法；我们将训练我们的模型:

```
from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(n_estimators = 10, random_state = 33)
rf.fit(train_features, train_target);
```

最后，我们将使用该模型进行一些预测，然后绘制结果:

```
predictions = rf.predict(test_features)
plt.scatter(test_features.bmi, test_target, color="black")
plt.scatter(test_features.bmi, predictions, color="red")
```

绘制测试目标和模型预测的结果将如下所示:

![Figure 7.4 – Plotting a random forest test versus predictions
](img/Figure_7.4.jpg)

图 7.4–绘制随机森林测试与预测的对比图

从图表中可以看出，预测值遵循相关性，即较高的身体质量指数可能会导致较高的糖尿病风险。

让我们看看 random forest 在评级中的表现，从而结束这一部分。

### 评估和评级随机森林

随机森林可以给你体面的解释，并在大多数领域有很好的整体评级:

*   **可解释性**:2/5——一棵单独的树很容易解析，但是几百棵就不容易了。一个随机森林可能包含大量的决策树，并且不容易理解。
*   准确性:3/5——它通常会在许多不同的问题类型中给你一个好的结果。
*   **所需数据**:2/5-需要小型到中型数据集作为输入，不需要非常大的数据量也可以。
*   **易用性**:3/5——基于决策树构建，因此这是一个易于掌握的概念，并且有许多库使其易于实现。
*   **费用**:3/5——如果你允许无限制的深度，并想在训练中创建许多单独的树，你可以进入长期运行的训练工作。可以选择并行运行任务。

虽然 random forest 除了能够了解事情背后的*为什么*之外，并没有做什么特别好的事情，但它是一个很好的全面选择，尤其是如果你需要能够解释它为什么会得出这样的答案，但前提是你使用像 SHAP 这样的方法。接下来，让我们深入研究一下**支持向量机**。

## 支持向量机

最常见的和不必要的复杂方法之一是支持向量机。支持向量机试图创建两个最接近类别之间距离最大的直线/平面/超平面，同时仍然将它们分成单独的组。

不同类别中最接近分割线的两个点是该分割线的*支持点*，该点是绘制作为主分割线一部分的向量的位置，因此称为*支持向量*。这两个支持向量之间的距离被称为**余量**。

本质上，SVM 只不过是在两个组之间创建一条线或一个平面的最佳猜测。使用了一些技术来确保这个最佳猜测是一个非常有教养的猜测，其中最主要的是内核技巧。内核技巧是一种获得使用高次多项式的好处的方法，而没有将每个数据点实际转换到那些更高维的训练成本。

### 使用 scikit-learn 实现 SVM 有一些简单明了的方法:

```
from sklearn import svm
clf = svm.SVR(kernel='linear')
clf.fit(X_train, Y_train)
pred = clf.predict(x_test)
```

通过查看这里的官方文档可以找到更多的选项:[https://sci kit-learn . org/stable/modules/SVM . html # scores-probabilities](https://scikit-learn.org/stable/modules/svm.html#scores-probabilities)。

### 评估和评定 SVM

当类在实体之间有清晰的分离时，支持向量机是一个很好的选择。另一方面，如果您有大型数据集或者空间中有许多分散的实体，那么支持向量机可能不是最好的主意。在寻找和适应非线性关系方面，他们比其他人做得更好。

这也不是一个很好的工具，当你有复杂的交织点，或者有很多维度在起作用的时候。您将需要利用不同的内核类型，并在超参数调优上花费时间。这在第 11 章 *【调整超参数和模型版本化】*中有更深入的介绍。

虽然有一些方法可以让模型更容易理解，但如果你真的需要理解和解释答案是如何得出的，这并不是最好的选择。由于它处理问题的方式，你可能会听到它被称为黑盒方法。如果理解答案是如何得出的对你来说至关重要，那就去别处看看。

SVM 的总体评分如下:

*   **可解释性**:3/5——这种算法的黑盒性质使其具有挑战性，但概念和技术使其在需要解释的领域变得可行。
*   **准确性**:3/5——好的结果，取决于数据是否适合您使用的内核类型，线性还是非线性。
*   **所需数据**:3/5–支持向量机非常适合中小型数据集。
*   **易用性**:3/5——可能需要对 ML 有深入的了解才能使内核正确。您还需要注意特征缩放，因为 SVM 对此很敏感。
*   **费用**:2/5–在较大的数据集上可能需要更长的时间。

正如你所看到的，支持向量机是另一个很好的选择，具有全面的良好评级。它相当灵活，但随着数据集变得越来越大，关系变得越来越复杂，它开始遇到麻烦。有一种算法可以更好地处理这类问题，我们现在来看看。

## 人工神经网络

很多时候被简单地称为**神经网络**，这项技术从神经元如何在人脑中形成连接中获得灵感。取得更好结果的联系会得到加强，而那些没有取得更好结果的联系会逐渐减轻。

这可能是数据科学和 ML 中最热门的领域，但是要小心。很多时候，当你需要一辆丰田 Carrola 时，这可以是一辆兰博基尼。兰博基尼可能是去杂货店的一种非常昂贵的方式，最终你可能会意识到它们没有空间做你真正想做的事情，那就是提杂货。神经网络通常应该是你考虑的最后一种方法，因为硬件和实现它们的困难。

神经网络将具有两层或更多层，其中每层都可以关注并调整特定特征的权重，以获得比仅基于激活函数的结果更好的结果。**激活函数**是一个简单的数学公式，用于确定一个神经元应该是活跃的还是休眠的。

### 神经网络的类型

你可以遇到并使用大量的神经网络，每一个都可以成为书籍本身的主题。让我们来看几个例子:

*   **感知器**——最简单的 ANN 架构之一，感知器由一层输入和一层**阈值逻辑单元** ( **TLU** )组成。TLU 是根据接收到的输入计算加权和的神经元。在这种情况下，激活器功能被称为阶跃功能。在训练中，如果有不正确的预测，其他输入的权重被调整以移动到正确的答案。感知器不太适合学习非线性、复杂的关系
*   **多层感知器**(**MLP**)——有什么比一层更好的？不止一层。MLP 只是在感知机的基础上增加了更多的层，以发现更复杂的关系。
*   **卷积神经网络**(**CNN**)——常见于视觉问题，因为它能够查看视频/图片的区域，并获得关于某个区域的洞察力，而不是逐个像素地查看事物。
*   **深度神经网络**(**DNN**)——这是一个简单的术语，用来表示网络有多层。这意味着除了输入和输出层之外，还包括隐藏层。理论上，你可以拥有多少层是没有限制的。这也是术语**深度学习** ( **DL** )的由来。MLP 是 DNN 的一个例子。

由于神经网络的复杂性，我们将避免详细讨论如何使用它们。如果你不知道你是否需要使用它们，你可能不需要。我知道他们可以解决一些非常具有挑战性的问题，但可能非常昂贵，并且很难解释为什么会得到这样的结果。

你可以找到一些关于神经网络的惊人资源。我要推荐的一个是使用 Scikit-Learn、Keras 和 TensorFlow 的*动手机器学习:*【https://amzn.to/36A0Akk】T2。

还有一类与回归密切相关的问题，就是分类。在下一节中，我们将再次探索这个问题设置中使用的常见模型，以及如何才能选择正确的模型。

# 分类

能够将事物归入特定的类别可能是你在这个世界上看到的最常见的 ML 应用类型，并且长期以来一直是这个行业的主要部分。

分类主要有两种:二元分类和多类分类。顾名思义，二元分类就是当结果只有两种可能的选择时。在这种设置中，出现*真*或*假*的结果是很常见的。

**多类分类**是指有两个以上可能的类。这可以用于各种场景，例如电影类型。对它们采取的方法非常类似于二元分类问题。

让我们来看看一些例子，它们可能会帮助你更好地理解属于分类范畴的问题:

电子邮件是否是垃圾邮件(二进制)

*   你是否能在泰坦尼克号沉没中幸存(二进制)
*   识别花的类型(多类)
*   标记手写信件(多类)
*   一个软件是否是恶意的(二进制)
*   情感分析(二元或多类)

提到的一些例子可能对你来说很熟悉，或者你可能甚至没有意识到它们会被认为是 AI/ML。虽然这不是一个详尽的列表，但是随着你看到越来越多的这些问题，你将很快能够识别它们所采取的模式，并且知道你试图理解的是否是一个分类。

既然我们对什么是分类问题有了一个概念，让我们来看看可以用来解决这个问题的一些算法。

## 分类算法

以下是一些你可以用来对现实世界中的物品进行分类的技巧:

*   逻辑回归
*   支持向量机
*   安妮丝
*   决策树/随机森林
*   贝叶斯朴素分类器
*   KNN

您会注意到，这些算法中有许多与前面的回归算法重叠。这是因为很多时候你其实可以把一个回归问题重塑为分类，反之亦然。让我们看一个例子。

让我们假设你经营着一家非常成功的比萨饼店，但最近它一直在遭受损失。你决定进行一次民意调查，试图弄清真相。您的顾客对他们在您的餐厅的体验进行评分，从非常差到非常好，共 5 分。从收据上你可以知道他们点了什么，他们坐在哪里，以及其他有用的信息。

这是分类问题还是回归问题？嗯，那要看你怎么设置了。您可以将所有输入表示为从 1 到 5 的数字，或者表示为它们的对应字符串。无论哪种方式，你都会得出同样的结论。请注意，这两者之间存在一些差异，但现在理解这些差异并不重要。

由于许多算法是相同的，我们将跳过一些已经讨论过的。

## 分类举例

我们将使用著名的 Iris 数据集来比较和检验不同的分类方法。该数据集在许多分类实例中使用，以便很好地介绍每种方法。

该数据集对不同的花朵进行测量，并可用于根据花瓣宽度等因素判断它们属于哪种类型。它是 scikit-learn 数据集的标准配置，因此非常容易访问。

有 150 个训练实例，具有 4 个独立的特征，包括以下内容:

*   萼片长度
*   萼片宽度
*   花瓣长度
*   花瓣宽度

有了这些关于植物的信息，我们可以找出哪些物种属于这三个不同的类别。

关于这个数据集的更多信息可以在这里找到:[https://bit.ly/3INU8UC](https://bit.ly/3INU8UC)。

我们现在将导入该数据集，以便在以下分类示例中使用它。在你的 Jupyter 笔记本中，导入这个玩具数据集。让我们只抓住两个特征，这样我们就可以很容易地将其绘制在图表上:

```
from sklearn import datasets
iris = datasets.load_iris()
pd_features = iris.data[:, :2]
target = iris.target
```

然后，我们可以快速绘制图表来查看这些数据。我们可以将训练数据中的类分开，以便看得更清楚一些。让我们将目标列添加到主数据集中，然后使用`query` 方法将实体分成各自的类:

```
import matplotlib.pyplot as plt
features['target'] = target
class_0 = pd_features.query('target==0')
class_1 = pd_features.query('target==1')
class_2 = pd_features.query('target==2')
```

现在我们可以用不同的颜色和形状把它们都画出来，向我们展示我们正在看的东西。为了更加清晰，还将添加一个图例:

```
import matplotlib.pyplot as plt
plt.scatter(class_0.iloc[:,0], class_0.iloc[:,1], marker="^", color="black", label="setosa")
plt.scatter(class_1.iloc[:,0], class_1.iloc[:,1], marker="X",  color="blue", label="versicolor")
plt.scatter(class_2.iloc[:,0], class_2.iloc[:,1], marker="d",color="red", label="virginica")
plt.legend(loc="upper left")
plt.show()
```

图表的结果将类似于以下内容:

![Figure 7.5 – Iris classification
](img/Figure_7.5.jpg)

图 7.5–虹膜分类

那好一点了！这只是训练数据，但是像这样的可视化将帮助我们快速理解我们的数据。有时我们的问题和数据点足够小，我们可以利用人类非常擅长在视觉数据中寻找模式的事实。我们利用这一优势对这样的图表进行健全性检查，以确定我们构建的内容中是否有大的错误。

jupyter 笔记本与 ide

注意，如果你在 Jupyter 笔记本上，一些命令是不需要的。要显示一个变量或一个图，只要它在笔记本的最后一个单元格它就会显示出来。在普通的 IDE 中，或者在命令行中的 Python REPL 中，情况并非如此。例如，要显示绘图，您需要使用`plt.show()`。

有了这个数据集，我们现在可以看看我们的第一个算法，它将帮助我们解决分类问题，逻辑回归。

## 逻辑回归

线性回归的姊妹，逻辑回归，更适合处理你需要分类的离散输出。不过，这种方法有点不同，因为目标是预测一个实例属于某个类的概率。然后，这被转换成简单的 1 或 0，分别表示该实例是否属于该类。

例如，如果您卡上的信用卡交易是欺诈性的概率大于 50%，那么您的逻辑回归模型将。如果少于这个数字，那么系统就认为一切正常。您可以通过更改决策边界，根据需要调整这个确切的阈值以适应您的用例。一个**决策边界**仅仅是决策向一个方向或另一个方向发展的点。

在下图中，您可以看到一个欺诈性购买的示例:

![Figure 7.6 – Logistic regression sigmoid function 
](img/Figure_7.6.jpg)

图 7.6–逻辑回归 sigmoid 函数

x 轴上是交易的规模，随着金额的增加，很有可能不是你做的交易。虚线表示决策边界，在该边界处，模型确定这很可能是欺诈性收费。很多时候默认大于 50%。你会注意到 sigmoid 函数永远不会达到. 0 或 1.0。这是因为你可能接近 100%，但你永远不能完全确定某人没有一笔有效的大额交易。也许他们决定花钱买些东西。另一方面，你可以让人偷你的卡，然后买一包口香糖，看看是否能通过。这就是为什么您通常希望在现实世界模型中组合多个特征。

我们现在可以回到 sci kit——学习尝试构建一个逻辑回归模型。

### 使用 scikit-learn 实现逻辑回归

我们将通过将数据分成训练和测试部分来重新开始。使用下面的代码来实现:

```
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(pd_features.iloc[:,:2], pd_target, test_size = 0.25, random_state = 33)
```

接下来我们将导入所需的模型，并训练一个名为`classifier`的回归模型:

```
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression().fit(X_train,y_train)
```

最后但并非最不重要的是，我们将创建我们的预测，然后将这些预测添加回我们的数据，供以后绘制。

```
predictions = classifier.predict(X_test)
X_test['target'] = predictions
```

为了查看它与训练数据的关系，我们可以在同一个图表中绘制它。请注意，为了使事情更容易理解，我们只绘制三个类中的两个:

```
import matplotlib.pyplot as plt
pred_class_0 = X_test.query('target==0')
pred_class_2 = X_test.query('target==2')
plt.scatter(class_0.iloc[:,0], class_0.iloc[:,1], marker="^", color="black", label="setosa")
plt.scatter(class_2.iloc[:,0], class_2.iloc[:,1], marker="d",color="black", label="virginica")
plt.scatter(pred_class_0.iloc[:,0], pred_class_0.iloc[:,1], marker="^", color="red", label="setosa_pred")
plt.scatter(pred_class_2.iloc[:,0], pred_class_2.iloc[:,1], marker="d",color="red", label="virginica_pred")
plt.legend()
```

现在，您应该能够看到预测的结果，如下图所示:

![Figure 7.7 - KNN Animal Example
](img/Figure_7.7.jpg)

图 7.7 - KNN 动物示例

这种回归技术多次有效，但是现在让我们看看解决这类问题的其他方法。

## 决策树/随机森林

这些与我们之前讨论过的非常相似，所以我们将简单介绍一下。与上一节的主要区别是随机森林现在将简单地取决策树输出的平均值。你将需要在 scikit 中切换到使用`RandomForestClassifier`而不是`RandomForestRegressor`——尽管了解这些问题。关于它们如何工作的更多信息，请参见本章前面的*随机森林*一节。

我们还没有介绍的新技术是 KNN，所以现在让我们来介绍一下。

## K-最近邻居

还有另一个非常简单的算法，只要设置正确，就能产生奇妙的结果。 **KNN** 是一种算法，它使用懒惰学习技术来记忆训练集，然后确定新输入最匹配的训练实例。

这是一种归纳技术，假设如果你有一个被其他物体包围的物体，那么它可能与帮助你想象这是一群动物的物体是同一类型。假设在大草原上有三种动物:斑马、狮子和鳄鱼。所有的都有追踪标签，但是有两个发送的信号破坏了它们的分类信息。你认为下图中的标签 **1** 和 **2** 最有可能是什么？

![Figure 7.8 – KNN animal example
](img/Figure_7.8.jpg)

图 7.8-KNN 动物示例

你可能已经猜到 **1** 是鳄鱼， **2** 是斑马。但你为什么这么想？你可能使用了和 KNN 一样的技术。人类非常擅长观察数据中的模式，在这里你将数字 **1** 与一群鳄鱼联系起来。偶数号 **2** ，有一头狮子靠得很近，你可以很大概率假设它是斑马。

### 与 KNN 的问题

继续我们在图 7.8 的*中展示的野生动物园的例子，你可以说，也许靠近编号 **2** 的那只狮子实际上是在狩猎中支援它的姐妹，而编号 **2** 实际上也是一只狮子。*

我们提到 KNN 很懒。这并不意味着它不工作，这只是意味着它是一个基于**实例的学习者**，这意味着它使用训练集的实例作为它试图学习数据中模式的唯一方式。它使用这个静态映射来假设任何新的项目都是最接近它的，这在某种程度上是一种设计上的邻近偏差。

有了这些信息，让我们看看 KNN 在几个不同领域的得分情况。

### 评估和评级 KNNs

KNN 往往很容易解释，因为你需要做的就是检查哪些训练实例最接近新的预测实例。这个关系可以很容易地被可视化(在它没有太多维度的情况下)，并且可以相当精确。

它不能很好地处理噪声，因为它必须假设所有的东西都属于一个类别，如果没有令人满意的高 k 值，即使几个异常值也会使模型失控。

这也是一个非常快速的计算，因为欧几里德距离是一个非常快速的计算公式，所以没有巨大的计算成本。随着维数的增加，你将需要使用一些其他的距离函数，这超出了本书的范围。

KNN 的总体评分如下:

*   **可解释性**:5/5——概念和最终结果往往非常容易掌握。因为懒惰，你总是知道事情的立场。
*   **精确度**:3/5——一般来说精确度较高，但是当你开始处理更高维度的数据时，精确度就会降低。
*   **所需数据**:3/5–不需要大量数据，即使是很小的样本也能产生不错的预测。
*   **易用性**:4/5–KNNs 往往不需要很高的专业知识就能获得好的结果，尽管它确实需要处理规模数据。
*   **开销**:3/5–对于低维数据，这往往很快，但随着数据量的增加，需要的时间也越来越多。

你可以看到，像随机森林一样，KNNs 可以提供一些很好的优势，能够解释你在看什么。

分类是用于识别现实世界中实体的 AI/ML 领域的核心支柱。在本节中，我们已经讨论了一些不同的算法，如逻辑回归和 KNN，以及它们如何从不同的角度处理这个问题。

分类是一个有监督的问题，在下一节中，我们将研究一个问题，它属于带有异常检测的无监督家族。

# 异常检测

如果你曾经收到一条短信说你的银行已经注意到一些可疑的活动，很可能他们已经使用了异常检测。**异常检测**是尝试确定某个事件、项目或对象是否与其他事件、项目或对象不一致。*这些事情中有一件不像另一件*是思考这个问题的好方法。你可能看到的另一个名字是**异常检测**。

你会发现无监督、有监督和半监督的方法都可以在这些场景中工作。对这种情况的描述可以在第一章 、*理解 AI/ML 场景*的*图 1.4*[*中找到。*](B16589_01_ePub.xhtml#_idTextAnchor015)

这个领域中的许多例子都处理更严重的安全问题。您可以在以下列表中找到一些示例:

*   信用卡诈骗
*   如果有人试图通过随机登录来侵入你的账户
*   发电厂的不安全操作
*   客户购买模式
*   股票的非法交易活动

有几种不同的方法来解决这个问题。我们将只涉及一些更常见的，如一级 SVM 和隔离森林。

我们将创建一个玩具数据集。我们将假设这些数据代表有效的信用卡交易，并添加一些异常值，我们稍后将添加这些异常值，以指示出现的代表潜在欺诈性费用的新交易。

我们的示例将在很大程度上基于官方 scikit-learn 网站提供的示例，网址为 https://scikit-learn . org/stable/auto _ examples/SVM/plot _ one class . html。

下面的代码将利用`numpy`函数来创建一些模拟数据。最终输出将是简单的`x`和`y`坐标:

```
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.font_manager
from sklearn import svm
X = 0.2 * np.random.randn(50, 2)
X_train = np.r_[X + 3]
X = 0.2 * np.random.randn(20, 2)
X_test = np.r_[X + 3]
X_outliers = np.random.uniform(low=-5, high=5, size=(20, 2))
```

我们将使用这个玩具数据集，从一种方法开始，这种方法与我们之前讨论过的算法之一，SVM，属于同一家族。

## 一等 SVM

我们之前讨论过支持向量机，有一种方法可以让利用它们来检测异常。单类 SVM 采用无监督的方法，仅识别给定数据中的一个类，而不是试图在两个类别之间创建平面(或超平面)，例如，在二进制分类器中创建。接下来的任务是简单地决定一个新点是否包含在这个边界中。

### 用 scikit-learn 训练单级 SVM

SVM 的训练设置与之前的分类和回归问题略有不同。为此，训练数据将由有效实体的项目组成，目的是让您的模型知道允许的东西是什么样子，以便它可以确定其他任何进来的东西是否无效。

对于异常，您的模型的输出将是一个简单的`–1`，如果它认为值是有效的，则是一个`1`。我们将从从`sklearn`引入 SVM 库开始，然后使用`OneClassSVM`函数来设置我们的算法。我们正在使用一些默认值，这是一个很好的起点:

```
from sklearn import svm
one_svm = svm.OneClassSVM(nu=0.1, kernel="rbf", gamma=0.1)
one_svm.fit(X_train)
y_pred_train = one_svm.predict(X_train)
y_pred_test = one_svm.predict(X_test)
y_pred_outliers = one_svm.predict(X_outliers)
```

现在，我们将使用这些预测，并将它们与测试输入进行叠加，以便能够在以后绘制它们。我们将切换到 pandas 数据框架，使用一些简单的工具在另一个步骤中进行过滤:

```
X_outliers = np.column_stack((X_outliers, y_pred_outliers))
df = pd.DataFrame(X_outliers, columns= ['x','y','anomaly_value'])
```

现在我们有了模型、预测和数据框架，我们可以绘制它们来理解模型选择标记为异常的内容，以及哪些内容是有效的。对于测试数据，我们将把它分为有效点和看似异常的点:

```
valid = df.query('anomaly_value == 1')
anomaly = df.query('anomaly_value == -1')
```

最后，我们将使用以下代码绘制这些点:

```
plt.title("One SVM")

plt.scatter(X_train[:, 0], X_train[:, 1], c="white",  edgecolors="k")
plt.scatter(X_test[:, 0], X_test[:, 1], c="blueviolet",  edgecolors="k")
plt.scatter(valid.iloc[:, 0], valid.iloc[:, 1], c="gold", edgecolors="k", label="normal_test")
plt.scatter(anomaly.iloc[:, 0], anomaly.iloc[:, 1], marker="x",color="red", label="anomaly")

plt.legend()
plt.show()
```

您将看到显示我们结果的以下输出:

![Figure 7.9 – One SVM plot
](img/Figure_7.9.jpg)

图 7.9-一个 SVM 图

正如你所看到的，我们的模型已经做了很好的工作来检测什么时候有东西超出了正常的界限。所有这些 **X** 标志都是重要的领域，检查并确保它们不是欺诈性的！

### 评估和评级一类支持向量机

这里有一些关于这种技术的缺点和一些好处的注释。与之前“一般 SVM”部分的评级有很多重叠,因此我们在此不再赘述。

训练数据中的异常值可能会扭曲被视为有效的点，从而导致出现假阳性。这将导致应该被约束的空间的不适合。如果我们在训练集中有一些项目变得更高，我们可能会错过一个可能是欺诈性的实体。

使用二次函数意味着，随着数据集越来越大，事情会变得非常慢，因为存储和计算需求不会线性增长。从 CPU 和资源的角度来看，它也很昂贵。使用五重交叉验证来计算概率，这是进行五次的 k 重运算。对于较大的数据集，这可能需要一段时间。你可以通过查看这里找到的 scikit-learn 文档找到更多提示:[https://sci kit-learn . org/stable/modules/SVM . html # tips-on-practical-use](https://scikit-learn.org/stable/modules/svm.html#tips-on-practical-use)。

现在让我们转到另一种方法，该方法采用另一种集成技术，并将其应用于异常检测。

## 隔离森林

类似于前面讨论的随机森林，隔离森林创建一条向下到树节点的路径，以将项目分为异常或不异常。它假设异常的东西会突出出来，更容易解析，所以如果树更短，到达终止节点更快，它就假设它是异常。

这就像试图弄清楚一种动物是否是一群哺乳动物中的异类(因此可能不属于那个群体)。识别什么时候不在一个群体中比识别什么时候在要快得多。如果你从*特征腿数> 4* 开始，那么*真*路径会立即向你显示这是一个异常。然而，如果答案是*假*，那么你将需要走一条更远的路来确定这种动物在这个哺乳动物数据集中是一个正常的实体。

隔离森林是另一种集合方法，在这种方法中，平均值的力量发挥了作用。创建的每棵树被称为**隔离树** ( **iTree** )。

任何给定的异常可能不会产生更短的 iTree，但是当许多异常产生时，它们的平均长度会更短。

### 使用 scikit-learn 实现隔离林

到本章的这一点，我们将再次使用的流程应该已经很熟悉了。我们将首先从 scikit-learn 导入所需的项目，并根据训练数据创建一个模型:

```
from sklearn.ensemble import IsolationForest
isol_forest = IsolationForest(n_estimators=300, random_state=33)
isol_forest.fit(X_train)
```

接下来，我们尝试从训练数据中预测，这样我们就可以将与测试数据进行比较。我们还特别获得了异常值，这样我们就可以看到我们的模型如何处理这种类型的异常数据点。以下三行代码完成了所有这些工作:

```
y_pred_train = isol_forest.predict(X_train)
y_pred_test = isol_forest.predict(X_test)
y_pred_outliers = isol_forest.predict(X_outliers)
```

现在，让我们再次对训练集进行预测，并将它们堆叠在训练数据集旁边，以发现训练数据集的样子。然后，我们将把它分成正常实体和异常实体:

```
full_test_outliers = np.column_stack((X_outliers, y_pred_outliers))
normal = full_test_outliers[(full_test_outliers[:,2] > 0)]
anomaly = full_test_outliers[(full_test_outliers[:,2] < 0)]
plt.scatter(X_train[:, 0], X_train[:, 1], c="white",  edgecolors="k")
plt.scatter(X_test[:, 0], X_test[:, 1], c="blueviolet",  edgecolors="k")
plt.scatter(normal[:, 0], normal[:, 1], c="gold", edgecolors="k", label="normal_test")
plt.scatter(anomaly[:, 0], anomaly[:,  1], marker="x",color="red", label="anomaly")
plt.legend()
plt.show()
```

结果图显示，有一个来自离群组的实体被模型识别为正常组的成员，这似乎是一个有效的假设。

![Figure 7.10 – Isolation forest results
](img/Figure_7.10.jpg)

图 7.10–隔离林结果

注意

图 7.10 中的精确点与我们之前运行的 SVM 不同，这是由于再次运行随机数据生成器，这会给我们不同的数字。

这让我们很好地看到了我们可以从这种类型的算法中期待什么，可视化真的有助于将它结合在一起。这可能对你有帮助，也有助于向他人解释结果是什么以及为什么应该相信它。

### 对隔离林进行评估和评分

与其他树方法类似，隔离森林在可解释性度量中得分更高，尽管它们可能很难可视化，因为可能有大量的树基于随机分裂而分支成许多不同的路径。不过，它确实以一种快速的方式做到了这一点，而且比许多其他方法都要快:

*   **可解释性**:3–这个概念很清楚，但是树的数量会使它变得很有挑战性。
*   **准确性**:3-如果有足够的训练样本可以学习，并且允许训练多棵树来发现更短的路径，这往往会做得很好。即使当许多训练特征与我们关心的目标特征相关性较低时，也能很好地解决高维问题。
*   **所需数据**:3–这在所需数据量上处于中间位置，即使没有完全干净的数据集也能很好地工作。
*   易用性:你需要的第三个超参数需要调整，但是许多库都有现成的中间地带。算法的思想和复杂性也不太难处理。
*   **Expense**:2–这(像许多算法一样)取决于你的超参数和数据的大小。然而，它并不需要大量的资源来开始实现良好的结果。这也是并行化工作的一个很好的选择，因为每个树都是独立的。****

正如我们所见，异常检测是安全领域的重要工具。使用诸如支持向量机和隔离森林之类的工具，我们可以创建能够确定新条目何时超出正常聚类的模型。

说到集群，这本身是一个相关但不同的问题类型，我们将在下一节中讨论。

# 聚类问题

除了异常检测之外，还有另一类问题需要一种无监督的方法来尝试将实体分组在一起，以便了解更多关于数据集的信息。聚类是查找数据集元素的过程，这些元素包含足够多的相似属性，您可以从各个点中确定明显的区别。

这种技术有许多应用，现在我们来看下面几个例子:

*   对客户群的细分进行分组
*   知道哪些邮件是促销，哪些更重要

为此，我们可以使用如下几种不同的算法:

*   DBScan
*   k 均值聚类

虽然还有很多，但可以肯定的是，这些已经在各种数据集上显示出有希望的结果，并且是一个很好的起点。

我们先来看看 DBscan。

## 数据库扫描

**基于密度的带噪声应用空间聚类**(或简称为 **DBScan** )是一种无监督的建模技术，它试图找到高密度元素的组，这些元素通过较低密度的区域与其他组分开。

它通过创建所谓的**核心点**来做到这一点，核心点是元素密集点(本质上是迷你星团)的最佳猜测的质心。然后，它递归地将其与其他相邻的核心点联系起来，以构建每个集群的图像。最后，它拉进了接近核心点簇但没有完全进入的个体离散元素。

这是有用的，因为它不会像 K-means 那样偏向于假设聚类的形状更凸。因此，从某些方面来说，这是一种更通用、更灵活的方法。

## K-均值聚类

我们最后要讨论的算法是 K 均值聚类。不要与 KNN 混淆， **K 均值聚类**是一种无监督算法，它试图将数据集的所有实体分组为 K 个组，所有点和质心之间的平均距离最小。

这意味着，当我们在一个更密集的点区域中选择一个中心点，然后将这些项目组合在一起时，我们试图将一些数学公式应用到我们可以视觉化做的过程中。根据你的 k 值，有一些变化，k 值是你认为可以作为数据的聚类数。在开始时，没有什么模型标签或类别会被分配给你的任何点，然后 K-means 的过程实际上是一组非常简单的步骤，如下所示:

1.  为您的观测值或点指定 k 个质心
2.  根据最近的质心标记每个观察值。您现在将拥有与以前不同的标签。
3.  现在，在组中添加或删除了其他点后，更新新组的质心
4.  重复*步骤 2-3* 直到质心停留在局部最小值上，在这里它根据从每个点到质心的平均欧几里德距离寻找最小值。

由于初始质心的随机性，如果您多次运行该算法，将会得到不同的结果，因此不采用您得到的第一个结果是有益的。

像这里的许多算法家族一样，还有许多其他的算法可以用来处理这些问题。这两种方法是推荐的第一种方法，适用于最广泛的问题集。一定要评估你需要做什么，你的最终目标是什么。

# 总结

在这一章中，我们已经讨论了如何从问题本身开始比从一种使用的技术开始更有价值。根据我们需要达到的目标，我们可以寻找不同的模型方法来帮助我们解决我们需要解决的问题。

我们了解到，当我们想要将元素分类时，分类问题是有用的，一些方法(如线性回归和随机森林)允许您创建模型来实现这一点。我们还看到了 scikit-learn 如何让您用很少的几行代码就得到一个解决方案。

我们还研究了用于预测值的回归、用于将实体分组到相似桶中的聚类，以及用于查找不属于其他元素的异常检测。与分类类似，我们看到了如何通过 scikit-learn 快速入门。Matplotlib 还可以方便地绘制出问题，以便为您提供预测结果的可视化表示。

本章中建立的所有模型都有非常干净的数据，但在现实世界中几乎不会这样。在下一章中，我们将更深入地探讨你将面临的最常见的数据问题，以及如何克服这些问题来尽可能地构建最佳模型。