# 四、加载和规范化数据

原始数据有多种形式:CSV、JSON、SQL、HTML 等等。pandas 提供数据输入和输出功能，用于将数据加载到 pandas 数据帧中，并将 pandas 数据帧中的数据输出为各种常见格式。在这一章中，我们将深入研究其中的一些输入函数，并探索它们提供的各种加载和规范化选项。

将数据加载到 pandas 中的函数提供了广泛的规范化和优化功能，可以提高程序的性能，甚至可以改善能够将数据加载到 pandas 和耗尽内存之间的差异。然而，每个输入函数都是不同的，因此它实际上取决于您正在使用的输入/输出格式，并且总是值得查看您正在使用的特定函数的文档。表 [4-1](#Tab1) 列出了Pandas支持的各种输入输出功能。

表 4-1

IO Pandas数据功能

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

投入

 | 

输出

 |
| --- | --- |
| 阅读 _csv | to_csv |
| 读取 _excel | 到 excel |
| read_hdf | to_hdf |
| 读取 _sql | 目标 sql |
| read_json | to_JSON |
| read_html | 收件人 _html |
| readstate | 去 stata |
| 阅读 _ 剪贴板 | 收件人 _ 剪贴板 |
| 阅读 _ 泡菜 | 腌制泡菜 |

第 [3](3.html) 章提到了几个非常好的数据归一化的理由；它可以节省内存，优化数据分析。通过选择与 C 兼容的数据类型，规范化数据可以让您受益于利用 Python 的字符串缓存或用 C 而不是 Python 来运行计算。作为加载过程的一部分，前面的许多输入函数为规范化数据的各种方式提供了选项。许多输入函数允许您在加载过程中删除并指定列的类型，而不是使用大内存空间加载数据，然后删除不必要的列或将列转换为较小的数据类型以减少加载后的内存空间。这意味着您可以在不耗尽内存的情况下加载更多的数据，并且数据加载和规范化过程会更快，因为您是同时做两件事，而不是连续加载然后再进行规范化。

导致创建和删除数据的操作可能代价很高，因为它们需要分配和释放大量内存。将数据从一种类型转换为另一种类型(通常称为强制转换)也很昂贵，因为它需要分配和释放大量内存。使用大块内存通常意味着缓存未命中，并且 IO 会花费大量时间将数据从距离 CPU 较远的内存移动到距离 CPU 较近的内存(例如，从主内存移动到一级缓存)。因此，虽然您可能认为内存与处理速度无关，但它实际上会对运行时产生巨大影响。

在这些输入函数中，pandas 通常在加载数据时推断数据类型。虽然这一开始可能很好，看起来是一个您应该始终利用的奇妙特性，但它对性能也有很大的负面影响。通常，正在加载的数据尚未被规范化，例如，数值列可能包含非数值，从而强制推断的数据类型为对象，这是它可以是最大的数据类型。许多数据加载函数允许您指定列的类型，并将占位符值转换为 nan，这可以防止 pandas 推断出错误的数据类型。

## pd.read_csv

pandas CSV 加载器 pd.read_csv 是应用最广泛的加载器，也是迄今为止数据规范化选项最完整的加载器。因为 Python 标准库有一个内置的 CSV 加载器，而 pandas 加载器有一些相当奇特的 Python 选项，所以它有两个不同的解析引擎:C 引擎和 Python 引擎。现在您可能已经猜到，C 引擎比 Python 引擎性能更好，但是根据您指定的选项，您可能别无选择，只能使用 Python 引擎进行解析。因此，建议您小心使用哪些选项以及为这些选项提供的值，以保证您使用的是 C 解析引擎，并获得可能的最佳加载性能。CSV 加载器有一个显式引擎参数，允许您强制解析引擎为 Python 或 C。在加载时始终显式指定此参数是保证 CSV 加载器使用 C 引擎进行解析的一种简单方法。如果在 engine 被显式设置为“C”时指定另一个 Python 解析器特定的选项，CSV 加载器将抛出一个异常，通知您特定设置与 C 解析引擎不兼容，如清单 [4-1](#PC1) 所示。

```py
>> data = io.StringIO(
    """
    id,age,height,weight
    129237,32,5.4,126
    123083,20,6.1,145
    """
)
>> df = pd.read_csv(data, sep=None, engine="c")
     ValueError: the 'c' engine does not support sep=None
     with delim_whitespace=False

Listing 4-1read_csv will raise a ValueError when engine is set to ‘c’ and other settings are not compatible

```

使用 C 解析引擎的另一个原因是，它通过 float_precision 参数支持更高精度的浮点。通常，Python 引擎使用双浮点精度，C 引擎使用自己的低级字符串到十进制解析器，与 Python 引擎相当。两者都会导致浮点舍入误差，例如–15.361 和–15.3610 不相等的情况。但是，C 解析引擎支持附加选项高精度和往返精度。如果您希望浮动尽可能精确，请使用往返精度选项。这是浮点的一个常见问题，因此，在处理金融数据时经常使用的另一种方法是将一个浮点数分成两个整数:一个整数表示小数点以上的数字，另一个表示小数点以下的数字。

read_csv 函数的第一个参数是 filepath_or_buffer。通常，CSV 文件的路径在这里传递，但是注意，出于单元测试的目的以及本章剩余部分的示例目的，StringIO 对象可以代替它传递。例如，如果 CSV 文件由第三方应用程序托管，它也接受 URL 路径。文档上正式写着 <sup>[1](#Fn1)</sup>

*所谓类似文件的对象，我们指的是具有* *read()方法* *的对象，比如文件处理程序(例如通过内置的 open 函数)或 StringIO。*

这种类似文件的对象是另一种 Python-ism，通常被称为 duck typing。这个术语来源于成语“如果它走路像鸭子，叫起来像鸭子，那它就是一只鸭子。”在这种情况下，如果它有一个 read 方法，它就是一个类似文件的对象。这就是为什么 StringIO 也可以代替文件处理程序，因为它也有一个 read 方法。StringIO 是单元测试中的一个很好的替代品，因为它允许您假装它是一个文件，而实际上不必包含 test.csv 来验证您的加载器是否按预期工作。

read_csv 提供了一个 sep 参数，该参数指定了用于描述数据的字符。默认值是逗号。注意 sep 将除了\s+以外的任何长于一个字符的值都视为正则表达式。在这里使用复杂分隔符可以强制使用 Python 解析引擎而不是 C，因此，建议尽可能使用单字符分隔符，并且不要指定复杂的正则表达式。参数 delim_whitespace 也可以设置为 True，作为设置 sep= "\s+"的一种替代方法，专门用来表示空白文件描述。sep 参数也可以设置为 None，在这种情况下将使用 Python 解析引擎，它将自动检测分隔符。skipinitialspace 参数可用于忽略分隔符周围的空格。默认情况下，这是禁用的，所以如果文件中的分隔符之间有空格，您需要将它设置为 True。清单 [4-2](#PC2) 展示了如何结合使用 sep 和 skipinitialspace 来配置非逗号分隔数据的加载。

```py
>> data = io.StringIO(
     """
     id| age| height| weight
     129237| 32| 5.4| 126
     123083| 20| 6.1| 145
     """
)
>> pd.read_csv(data, sep="|", skipinitialspace=True)
     idageheightweight
     0129237325.4126
     1123083206.1145

Listing 4-2Loading non-comma-delimited data

```

参数 usecols 缩小了要加载的列的列表。CSV 文件中可能有您不关心的列，因此这是在加载时消除它们的有效方法，而不是加载所有数据后再删除它们。请注意，usecols 也可以是一个函数，其中列名是一个输入，输出是一个布尔值，指示在加载时是包含该列还是丢弃它。然而，函数并不理想，因为它需要在 C 解析引擎和自定义函数之间进行调用，这会降低加载程序的速度。清单 [4-3](#PC3) 展示了一个在加载期间使用 use_cols 来消除列 id 和 age 的例子。

```py
>> data = io.StringIO(
     """
     id,age,height,weight
     129237,32,5.4,126
     123083,20,6.1,145
     """
)
>> pd.read_csv(data, usecols=["height", "age"])
     heightweight
     05.4126
     16.1145

Listing 4-3Eliminating columns during load

```

skiprows 参数允许您跳过文件中的某些行。最简单的形式是，它可以用来跳过文件中的前 n 行；但是，通过指定要跳过的索引列表，它也可以用于跳过特定的行。它也可以是一个接受行索引的函数，如果应该跳过该行，则返回 True。注意，如果在这里传递一个函数，它将不幸地在 C 解析引擎和 skiprows Python 函数之间跳转，这可能导致解析大型数据集时速度大大降低。因此，建议将 skiprows 保持为一个简单的整数或列表值。

skipfooter 参数允许您指定要跳过的文件末尾的行数。文档指出，C 解析引擎不支持这一点。由于 Python 引擎使用 Python CSV 解析器，CSV 解析器运行，然后文件的最后几行被删除。如果您更深入地思考这个问题，这是有意义的:在不知道文件中有多少行的情况下，解析器如何知道跳过哪些行(这需要首先解析文件)？对于一些用户来说，这种行为可能有些令人惊讶，例如，当他们试图避免文件中的行时，因为他们破坏了解析器，并发现解析器仍在试图解析他们配置解析器跳过的那些行。如果您在自己的程序中遇到这种情况，nrows 是一个不错的选择。清单 [4-4](#PC4) 展示了一个遇到解析错误的例子，尽管加载器被配置为跳过那一行。

```py
import pandas as pd

data = io.StringIO(
  """
  student_id, grade
  1045,"a"
  2391,"b"
  8723,"c"
  1092,"a"
  """
)
try:
  grades = pd.read_csv(
    data,
    skipfooter=1,
  )
except pd.errors.ParserError as e:
pass

Listing 4-4Encountering a parsing error when using skipfooter

```

参数 comment 允许您指定一个表示注释的字符，该行的其余部分在该字符之后被忽略。对于在解析之前手动过滤掉某些行来说，这是一个很好的策略。如果您注释掉该行，那么它将不会包含在数据集中。您还可以考虑将 error_bad_lines 设置为 False。注意默认情况下，pandas 仍然会在每个错误行上发出警告，所以如果您也想禁用警告，您可以将 warn_bad_lines 设置为 False。

默认情况下，标题或列名由 read_csv 推断，数据的第一行被视为标题。如果数据包含多级列，您可以使用参数标题指定将哪些行号视为列。类似地，使用 index_col，您可以通过列索引指定哪些列将被视为多索引的一部分。在清单 [4-5](#PC5) 中，使用 pandas to_csv 函数将多索引多级别列数据帧转储到 CSV 文件中。这些数据包含了夜行动物的种类和家族信息。索引包含科和物种的 id，而列包含实际名称。这里，前两行包含多级列数据，因此 header 设置为[0，1]，前两列是多索引，因此 index_col 设置为[0，1]。

```py
>> data = io.StringIO(
     """
     family,,nightshade,nightshade,nightshade
     species,,tomatoe,deadly-nightshade,potato
     family_id,species_id,,,
     61248,129237,1,0,0
     61248,123083,0,1,0
     61248,123729,0,0,1
     """
)
>> df = pd.read_csv(data, header=[0,1], index_col=[0,1])
     family                nightshade
     species               tomatoes    deadly-nightshade potato
     family_id  species_id
     61248      12937      1           0                 0
     61248      123083     0           1                 0
     61248      123729     0           0                 1

Listing 4-5Example of loading a multi-index multi-level column DataFrame

```

如果启用了 squeeze 参数，并且 csv 文件中只有一列，read_csv 将返回一个系列，而不是 DataFrame。当您需要从多个源加载数据并将其合并到单个数据帧中时，这可能会很有用。如果数据被加载到一个序列中，您可以简单地将它作为一个新列添加到一个现有的 DataFrame 中，如清单 [4-6](#PC6) 所示。

```py
import pandas as pd
site_data = pd.read_csv('site1.csv')
site_data['site2'] = pd.read_csv('site2.csv', squeeze=True)

Listing 4-6Example of using squeeze

```

dtype 参数允许您为数据中的每一列指定一种类型。如果未指定，read_csv 将尝试推断数据类型，这通常会导致推断的类型是一个对象，该对象是数据类型所能达到的最大大小。在加载期间指定 dtype 可以极大地提高性能，但这也意味着您必须在加载时了解数据集中的列。如果在查看数据之前不知道具体会发生什么，可以考虑使用 nrows 首先加载数据的标题或前几行，确定列类型，然后使用指定的适当类型加载整个数据文件。

```py
>> data = io.StringIO(
     """
     id,age,height,weight
     129237,32,5.4,126
     123083,20,6.1,145
     """
)
>> df = pd.read_csv(data, index_col=[0])
                    age    height    weight
     id
     129237         32     5.398438  126
     123083         20     6.101562  145
>> df.memory_usage(deep=True)
     Index     16
     age       16
     height    16
     weight    16
>> df.dtypes
     age          int64
     height       float64
     weight       int64
>> df.index.dtype
     dtype('int64')

Listing 4-7Example of not specifying the types of the columns when loading

```

清单 [4-7](#PC7) 展示了一个在不指定每一列类型的情况下加载数据的例子。注意，所有类型占用 8 个字节，默认为最大的 int 和 float，而在清单 [4-8](#PC8) 中，它们占用的内存要少得多。该数据只有两行数据，但是就在这两行中，我们通过指定列的类型，将数据帧的内存占用减少了一半以上。

```py
>> data = io.StringIO(
     """
     id,age,height,weight
     129237,32,5.4,126
     123083,20,6.1,145
     """
)
>> df = pd.read_csv(
     data,
     dtype={
         'id': np.int32,
         'age': np.int8,
         'height': np.float16,
         'weight': np.int16},
     index_col=[0],
)
                    age    height    weight
     id
     129237         32     5.398438  126
     123083         20     6.101562  145
>> df.memory_usage(deep=True)
     Index     16
     age       2
     height    4
     weight    4
>> df.dtypes
     age       int8
     height    float16
     weight    int16
>> df.index.dtype
     dtype('int64')

Listing 4-8Example of specifying the types of the columns when loading

```

converters 参数允许您指定一个函数来转换特定列中的值，如清单 [4-9](#PC9) 所示。例如，如果一列中有多个值表示同一个值，而您希望将其规范化为单个值，那么这是一个很好的规范化特性。然而，这是有代价的。因为这些函数是用 Python 编写的，所以 C 引擎必须在 C 和 Python 之间进行调用来转换每个值，这在处理大型数据集时非常耗时。因此，虽然数据在加载时被规范化，但它的加载速度也会变慢，因为对于每列中要转换的每个值，它会在 C 和 Python 之间跳转。在这种情况下，在使用 Apply-Cython 实现后转换列值会更高效，这样转换可以在 C 中快速进行，并避免来回跳转。关于如何在 Cython 中实现应用，请参见第 [6](6.html) 章。

```py
import pandas as pd

MEDICATIONS_MAPPER = {"atg": "atg", "aftg": "atg", "bta": "bta"}
def medication_converter(value):
     return MEDICATIONS_MAPPER[value.lower()]

data = io.StringIO(
     """
     id,age,height,weight,med
     129237,32,5.4,126,bta
     123083,20,6.1,145,aftg
     """
)
>> treatments = pd.read_csv(
     data,
     converters={'med': medication_converter},
)
     id        age  height   weight   med
     129237    32   5.4      126      bta
     123083    20   6.1      145      atg

Listing 4-9Standardizing values at load time with converters

```

nrows 参数允许您指定从文件中读取的行数。这里可能不直观的是，在使用 Python 解析引擎时，nrows 实际上并不跳过读取行。这是因为 Python 解析引擎首先读取整个文件。这意味着，如果在您打算从文件中读取的行数之后有导致解析错误的行，那么在使用 Python 解析引擎运行时，您将无法通过使用 nrows 来避免它们。因为 Python 解析引擎首先读取整个文件，所以它仍然会在这些行上抛出解析错误，即使您告诉 CSV 加载器不要读取这些行。因此，这是避免使用 Python 解析引擎的另一个原因，尤其是在使用这种设置时。注意，另一方面，skipfooter 甚至在 C 解析引擎中实际上也读取页脚行。这仅仅是因为为了将它标识为文件的页脚，它必须读取它并到达文件的结尾以将其标识为页脚。清单 [4-10](#PC10) 展示了如何使用 nrows 和 C 解析引擎来避免可能导致解析错误的行。

```py
import pandas as pd
data = io.StringIO(
     """
     student_id, grade
     1045,"a"
     2391,"b"
     8723,"c"
     1092,"a"
     """
)
grades = pd.read_csv(
     data,
     nrows=3,
)

Listing 4-10Avoiding a parsing error by using nrows

```

nrows 参数与 skiprows 和 header 结合使用，对于将文件分段读入内存、处理它，然后读取下一个块也很有用。这在处理大量数据时特别有用，否则由于内存限制，您可能无法一次读取所有数据。清单 [4-9](#PC9) 展示了一个这样的例子。注意 process 是一个包装 read_csv 函数的函数。它从 read_csv 中获取加载的数据，并对其进行一些处理，以减少内存占用和/或使其正常化，使其超出 read_csv 的能力，并将其返回以与其余数据连接。在清单 [4-11](#PC11) 中，我们加载前 1000 行，处理它们，并使用前 1000 行初始化数据。然后我们继续按行读取，一次处理 1000 行，直到我们读取的行数少于 1000 行。一旦我们读取了不到 1000 行，我们就知道我们已经读取了整个文件并退出了循环。

```py
import pandas as pd

ROWS_PER_CHUNK = 1000
data = process(pd.read_csv(
     'data.csv',
     nrows=ROWS_PER_CHUNK,
))
read_rows = len(data)
chunk = 1
while chunk * ROWS_PER_CHUNK == read_rows:
     chunk_data = process(pd.read_csv(
          'data.csv',
          skiprows=chunk * ROWS_PER_CHUNK,
          nrows=ROWS_PER_CHUNK,
          header=None,
          names=data.columns,
     ))
     read_rows += len(chunk_data)
     data = data.append(process(chunk_data), ignore_index=True)

Listing 4-11Reading a file and processing it nrows at a time to reduce memory overhead

```

与 chunksize 结合使用的参数迭代器也允许您以类似于清单 [4-11](#PC11) 的块的形式读取数据。同样，出于性能原因，这可能是必要的。可能您正在读取的数据无法使用 read_csv 以较小的内存占用量读入，并且在加载后必须进行一些规范化，从而减少内存占用量，这意味着虽然您无法使用 read_csv 将所有数据一次性读入内存，因为生成的数据帧会太大，但您可以一次读取一个数据块，并减少每个数据块的内存占用量，以便生成的数据帧适合内存。注意如果您一次读取整个文件块，使用迭代器和 chunksize 比使用 nrows 和 skiprows 更好，因为它使文件在正确的位置打开，而不是不断地重新打开它并滚动到下一个位置。清单 [4-12](#PC12) 显示了一个这样的例子。

```py
import pandas as pd

ROWS_PER_CHUNK = 1000
data = pd.DataFrame({})
reader = pd.read_csv(
     'data.csv',
     chunksize=ROWS_PER_CHUNK,
     iterator=True
)
for data_chunk in reader:
     processed_data_chunk = process(data_chunk)
     data = data.append(processed_data_chunk)

Listing 4-12Reading a file in chunks to reduce memory overhead

```

当使用 C 解析引擎时，缺省为 True 的参数 low_memory 实际上已经在块中处理文件，以便节省内存。但是，read_csv 选项限制了它对每个块的处理，因此在某些情况下，定制处理和手动迭代块可能更好。

Pandas read_csv 函数还提供了一个名为 memory_map 的选项。当设置为 True 并且提供了文件路径时，它会将文件直接映射到虚拟内存中，并直接从那里访问数据。使用此选项可以提高性能，因为不再有任何 IO 开销等待下一个文件块加载到内存中。一般来说，访问内存映射文件更快，因为内存对于程序来说是本地的，并且内存映射已经在页面缓存中，所以不需要动态加载。实际上，在从头到尾连续加载文件的典型用例中，内存映射文件通常不会提供太多的性能优势。如果您遇到大量缓存未命中，这意味着通常加载到缓存(靠近 CPU 的内存)中的文件数据不存在，必须从主内存加载，这可能会提高性能。如果其他程序同时运行，将内存添加到高速缓存中，从而将文件数据从高速缓存中取出，则可能会发生高速缓存未命中。参见第 [8](8.html) 章，了解存储器层级和高速缓存未命中的详细说明。如果您在程序运行过程中多次读取这个文件，或者程序定期运行，并且您不希望每次运行时都必须将同一个文件加载到内存中，这也可能会带来性能优势。因此，虽然这个功能听起来可以为您提供实质性的加速，但实际情况是，除非您在标准的从头到尾读取文件工作流程之外工作，否则不太可能做到这一点。

na_values 参数允许您指定要解释为非数字的值，也称为 NaNs。这种类型来自 NumPy，如果你还记得第二章的话，它是Pandas的附属品。它通常在 NumPy 中用作无效计算结果的占位符，例如除以 0。注默认情况下，pandas 会自动将任何字符串 nan 或 NaN 解释为 Nan 类型。例如，如果您正在处理 nan 或 Nan 实际上可能是有效名称的数据，这种自动转换可能会有问题。这就是 keep_default_na 派上用场的地方。将参数 keep_default_na 设置为 False 会关闭 pandas 将某些值自动解释为 nan。有关 pandas 自动转换为 NaN 类型的值的完整列表，请参见附录。

当参数 na_filter 设置为 False 时，将完全禁用对 nan 的检查，文档指出，当您确定数据中没有 nan 时，这可以提高性能。另一方面，参数 na_values 允许您指定除缺省值之外的其他值，您也希望将这些值转换为 nan。

当使用 Python 解析引擎时，参数 verbose 输出每个包含 NaN 的列中 NaN 值的数量，当使用 C 解析引擎时，输出解析性能指标。pandas 文档声明它为非数字列显式输出 NaN 值。然而，这可能有些欺骗性，因为非数字确定是在解析引擎运行时做出的，而不是基于结果数据帧中列的最终类型。任何在解析时包含 NaN 的列都被视为非数字列，即使该列的类型最终是数字类型(如下面示例中的 float64)。Python 解析器必须解析列中的所有值，并在分配最终类型之前将它们适当地转换为 nan。这意味着在分配列的最终类型之前，NaN 值在解析期间被计算。清单 [4-13](#PC13) 展示了这种行为。

```py
>> import pandas as pd
>> data = io.StringIO(
     """
     student_id,grade
     1045,"a"
     2391,"b"
     ,"c"
     1092,"a"
     """
)
>> grades = pd.read_csv(
     data,
     verbose=True,
     index_col="student_id",
     engine='python',
)
     Filled 1 NA values in column student_id
>> grades
                grade
     student_id
     1045        a
     2391        b
     NaN         c
     1092        a
>> grades.index.dtype
     dtype('float64')

Listing 4-13Unexpectedly counting NaNs in numeric columns

```

pandas read_csv 处理占位符类型的一个限制是，例如，您不能指定一个转换器将 NaN 转换为 0，也不能将列转换为特定的 dtype。清单 [4-14](#PC14) 展示了您可能希望这样做的情况。数据集中的权重列并不总是有值，输入非值的方式也不一致。有时它是空的；其他时候是“未知”。如果我们不为这个列指定类型，让 pandas 推断类型，pandas 将列值存储为对象，这意味着其中一些是整数，一些是 nan，一些是字符串。注意，这个例子中的对象每个元素占用 32 个字节，还有一些额外的开销。这比每个元素占用 2 个字节的 int16 的理想类型要多得多。得到的数据帧不仅占用更多的内存，而且在运行计算时也是不可用的。例如，由于某些值是对象，所以对列中的所有权重求和可能会导致字符串相加，而不是整数相加。因此，让 pandas 来推断这种场景中的数据类型并不理想。

```py
>> data = io.StringIO(
     """
     id,age,height,weight
     129237,32,5.4,126
     123083,20,6.1,
     123087,25,4.5,unknown
     """
)
>> df = pd.read_csv(
     data,
     dtype={
         'id': np.int32,
         'age': np.int8,
         'height': np.float16},
     index_col=[0],
)
                 age    height     weight
     id
     129237      32     5.398438   126
     123083      20     6.101562   NaN
     123083      20     6.101562   unknown
>> df.memory_usage(deep=True)
     Index      24
     age        3
     height     6
     weight     155
>> df.dtypes
     age          int8
     height       float16
     weight       object
>> df.index.dtype
     dtype('int64')

Listing 4-14Example of how pandas handles NaNs in the data by default

```

与其让 pandas 推断数据类型，不如让我们使用 na_values 将所有占位符值转换为 nan。尽管理想情况下我们希望它们是 int16s，float16 占用相同数量的内存，并且 pandas 支持 nan 作为 float 存储，而不支持它们在加载期间作为整数存储，所以我们将 weight 列的 dtype 设置为 float 16。注意如果我们不指定 dtype，它将是一个 float64。如果我们真的需要它们是整数，我们可以使用 fillna 用零替换 nan，并在加载后使用 astype 转换它们，如清单 [4-15](#PC15) 所示。

```py
>> data = io.StringIO(
     """
     id,age,height,weight
     129237,32,5.4,126
     123083,20,6.1,
     123087,25,4.5,unknown
     """
)
>> df = pd.read_csv(
     data,
     dtype={
         'id': np.int32,
         'age': np.int8,
         'height': np.float16,
         'weight': np.float16},
     na_values={"unknown"},
     index_col=[0],
)
                   age    height     weight
     id
     129237        32     5.398438   126
     123083        20     6.101562   NaN
     123083        20     6.101562   NaN
>> df.memory_usage(deep=True)
     Index      16
     age        3
     height     6
     weight     6
>> df.dtypes
     age       int8
     height    float16
     weight    float16
>> df.index.dtype
     dtype('int64')
>> df["weight"].fillna(0, inplace=True)
>> df["weight"] = df["weight"].astype(np.int16)
>> df
                 age    height    weight
     id
     129237      32     5.398438  126
     123083      20     6.101562  0
     123083      20     6.101562  0
>> df.memory_usage(deep=True)
     Index      16
     age        3
     height     6
     weight     6
>> df.dtypes
     age       int8
     height    float16
     weight    int16

Listing 4-15Example of using na_values and dtype to convert placeholder values to float16 NaNs during load

```

当使用 C 解析引擎时，以详细模式输出的解析性能度量对于确定解析引擎在哪里花费时间是有用的。清单 [4-16](#PC16) 显示了一个示例输出。标记化是解析器将数据分解成单独的值，类型转换是将每一列转换成特定的类型，无论是由 pandas 推断的还是显式指定的，解析器内存清理是在读取数据后释放所有不再需要的内存所花费的时间。根据这些值，它们可能会指出需要改进的地方，例如，如果标记化需要很长时间，您可以通过指定附加选项(如如何解释引号、空格、坏行等)来提高性能。如果在类型转换上花费了大量的时间，这可能表明您有一些定制的转换器减慢了这个过程，或者您可能需要指定 dtype 而不是让 pandas 推断它们。如果您发现在内存清理上花费了大量时间，您可能不需要一次解析整个文件，或者您可能正在进行太多的数据转换，导致大量的内存重复，从而导致大量的内存清理。

```py
>> grades = pd.read_csv(verbose=True, engine="c")
     Tokenization took: 0.01 ms
     Type conversion took: 0.45 ms
     Parser memory cleanup took: 0.01 ms

Listing 4-16Example output when running in verbose mode with 

C parsing engine

```

如果 parse_dates 参数设置为 True，将尝试自动检测带有日期格式字符串的列，并将其转换为 datetime 对象。然而，与其将它设置为 True，不如明确列出哪些列应该转换为 datetime 对象。此参数允许您显式指定要转换为列表形式的列，甚至在将多个列指定为列列表的列表时，将多个列组合成一个 datetime 对象。清单 [4-17](#PC17) 显示了一个显式指定要转换的列的例子。注意，每个 datetime 对象占用 8 个字节，但这仍然比我们根本不转换它要少得多。

```py
>> data = io.StringIO(
     """
     id,birth,height,weight
     129237,04/10/1999,5.4,126
     123083,07/03/2000,6.1,150
     123087,11/23/1989,4.5,111
     """
)
>> df = pd.read_csv(
     data,
     dtype={
         'id': np.int32,
         'height': np.float16,
         'weight': np.int16},
     parse_dates = ["birth"],
     index_col=[0],
)
                    birth        height     weight
     id
     129237         1999-04-10   5.398438   26
     123083         2000-07-03   6.101562   150
     123083         1989-11-23   6.101562   111
>> df.memory_usage(deep=True)
     Index      24
     birth      24
     height     6
     weight     6
>> df.dtypes
     age       int8
     height    float16
     weight    datetime64[ns]
>> df.index.dtype
     dtype('int64')

Listing 4-17Explicitly converting certain columns to datetime objects

```

注意清单 [4-17](#PC17) 假设列中没有 NaNs 或占位符值。如果有，就像清单 [4-18](#PC18) 中一样，必须指定 na_values 来将所有占位符值转换成 NaNs 否则，该列将是对象而不是日期时间，因为占位符值将保留为字符串。

```py
>> data = io.StringIO(
     """
     id,birth,height,weight
     129237,04/10/1999,5.4,126
     123083,unknown,6.1,150
     123087,11/23/1989,4.5,111
     """
)
>> df = pd.read_csv(
     data,
     dtype={
         'id': np.int32,
         'height': np.float16,
         'weight': np.int16},
     parse_dates=["birth"],
     na_values=["unknown"],
     index_col=[0],
)
                  birth          height      weight
     id
     129237       1999-04-10     5.398438    26
     123083       NaT            6.101562    150
     123083       1989-11-23     6.101562    111
>> df.memory_usage(deep=True)
     Index      24
     birth      24
     height     6
     weight     6
>> df.dtypes
     age             int8
     height          float16
     weight          datetime64[ns]
>> df.index.dtype
     dtype('int64')

Listing 4-18Explicitly converting certain columns to datetime objects and handling NaNs

```

一般来说，parse_dates 虽然很方便，但从性能的角度来看却适得其反。转换日期需要时间，一旦转换，数据类型就不能转换为 C 语言，因此，建议将日期时间转换为自纪元以来的时间，或者一些简单的 C 语言可转换的数值。如果您需要处理特定的日期、月份和年份，将它们存储在单独的列中可能更有意义。

read_csv 中还包含许多其他特定于日期的参数。默认情况下，参数 infer_datetime_format 是启用的，因此只要有可能，pandas 就会尝试自动推断任何 datetime 值的格式。文档说，在某些情况下，当它能够检测和使用特定的日期解析格式时，这可以将解析速度提高五到十倍。 <sup>[2](#Fn2)</sup> 当设置为 True 时，如果 parse_dates 指定多个日期列应该组合在一起，则参数 keep_date_col 同时保留组合列和原始的单独日期列。date_parser 参数允许您指定日期解析函数。文档指出，可以用几种不同的方法调用这个函数，从在每一行调用一次，到一次传递所有行和列。因为这个函数在 C 和 Python 之间跳跃，所以尽可能少地调用它是有利的。这意味着最好实现这个函数来操作所有 datetime 行和列，并输出 datetime 实例的数组。这个函数可以是一个现有的解析器(默认为 dateutil.parser.parser)，也可以是一个自定义函数。如果您需要进行一些特殊的时区处理，或者数据以特殊的日期时间格式存储，这可能会很有用。并非所有国家都指定月份的前一天，因此 pandas 提供了 dayfirst 参数，以便您可以指定该天是否在您解析的日期中排在第一位。默认情况下启用的参数 cache_dates 会对转换后的日期进行缓存查找，这样，如果同一日期在数据集中出现多次，它就不必再次运行转换，而只需使用缓存的值即可。

参数 escapechar 允许您对某些字符进行转义。例如，在大多数编程语言中，一个常用的转义字符是反斜杠(\)，因此可能需要对引号内的某些引号字符进行转义，或者对元素分隔符字符进行转义。清单 [4-19](#PC19) 展示了这个用例。如果温度记录是由使用逗号作为小数点分隔符并且还使用逗号作为 CSV 元素分隔符的国家/地区记录的，read_csv 将无法使用其默认配置解析该文件，并将引发解析错误“pandas.errors.ParserError:错误标记数据。c 错误:第 5 行中应有 2 个字段，但看到了 3”。相反，如果使用反斜杠字符来转义所有分隔小数位的逗号(\，)，那么 read_csv 可以配置为正确解析数据。

```py
>> import pandas as pd
>> data = io.StringIO(
     """
     temp, location
     35,234unf923
     32,2340inf012
     33,2340inf351
     33\,1,2340abe045
     """
)
>> grades = pd.read_csv(
     data,
     decimal=",",
     escapechar="\\",
     index_col="location",
)
                 temp
     location
     234unf923   35.000000
     2340inf012  32.000000
     2340inf351  33.000000
     2340abe045  33.100000

Listing 4-19Using commas as a delimiter and a decimal point

```

## pd.read_json

read_json 加载器完全用 C 解析，不像 read_csv 在某些情况下可能使用 Python 解析器。

参数 orient 定义了如何将 JSON 格式转换成 pandas 数据帧。有六个不同的选项:拆分、记录、索引、列、值和表。如果 JSON 的格式已经将列、数据和索引定义为键，如清单 [4-20](#PC20) 中的情况，那么应该使用 split 选项。同样值得注意的是，JSON 解析器对包括空格在内的间距特别挑剔。

```py
>> data = io.StringIO(
     """
     {
         "columns": ["temp"],
         "index": ["234unf923", "340inf351", "234abe045"],
         "data": [[35.2],[32.5],[33.1]],
     }
     """
)
>> temperatures = pd.read_json(
     data,
     orient="split",
)
                 temp
     234unf923   35.200000
     340inf351   32.500000
     234abe045   33.100000

Listing 4-20Using orient split

```

如果 JSON 的格式使得每个值都是数据中的一行，列名作为键，如清单 [4-21](#PC21) 中的情况，那么应该使用 records 选项。

```py
>> data = io.StringIO(
     """
     [
         {"location": "234unf923", "temp": 35.2},
         {"location": "340inf351", "temp": 32.5},
         {"location": "234abe045", "temp": 33.1},
     ]
     """
)
>> temperatures = pd.read_json(
     data,
     orient="records",
)
     location     temp
     234unf923    35.200000
     340inf351    32.500000
     234abe045    33.100000

Listing 4-21Using orient records

```

如果 JSON 的格式使得每个键都是索引值，每个键的值都是列和值的字典，如清单 [4-22](#PC22) 中的情况，那么应该使用 index 选项。

```py
>> data = io.StringIO(
     """
     {
         "234unf923": {"temp": 35.2},
         "340inf351": {"temp": 32.5},
         "234abe045": {"temp": 33.1},
     }
     """
)
>> temperatures = pd.read_json(
     data,
     orient="index",
)
                 temp
     234unf923   35.200000
     340inf351   32.500000
     234abe045   33.100000

Listing 4-22Using orient index

```

如果 JSON 的格式是每个键是一列，每个值是一个字典，其中键是索引，值是列值，如清单 [4-23](#PC23) 中的情况，应该使用 columns 选项。

```py
>> data = io.StringIO(
     """
     {
         "temp": {
             "234unf923": 35.2,
             "340inf351": 32.5,
             "234abe045": 33.1,
         },
     }
     """
)
>> temperatures = pd.read_json(
     data,
     orient="columns",
)
                 temp
     234unf923   35.200000
     340inf351   32.500000
     234abe045   33.100000

Listing 4-23Using orient columns

```

如果 JSON 的格式使得每一行都简单地表示为一个值列表，如清单 [4-24](#PC24) 中的情况，那么应该使用 values 选项。

```py
>> data = io.StringIO(
     """
     [
         ["234unf923", 35.2],
         ["340inf351", 32.5],
         ["234abe045", 33.1],
     ]
     """
)
>> temperatures = pd.read_json(
     data,
     orient="values",
)
             0           1
     0       234unf923   35.200000
     1       340inf351   32.500000
     2       234abe045   33.100000

Listing 4-24Using orient values

```

如果 JSON 的格式提供了详细的数据模式，如清单 [4-25](#PC25) 中的情况，那么应该使用 table 选项。

```py
>> data = io.StringIO(
     """
     {
         "schema": {
             "fields": [
                 {"name": "location", "type": "string"},
                 {"name": "temp", "type": "string"},
             ],
             "primaryKey": "location",
         },
         "data": [
                 {"location": "234unf923", "temp": 35.2},
                 {"location": "340inf351", "temp": 32.5},
                 {"location": "234abe045", "temp": 33.1},
         ]
     }
     """
)
>> temperatures = pd.read_json(
     data,
     orient="table",
)
                  temp
     location
     234unf923    35.200000
     340inf351    32.500000
     234abe045    33.100000

Listing 4-25Using orient table

```

与 read_csv 类似，read_json 也有一个 chunksize，允许您一次成块地读取文件。然而，只有当 lines 选项也被设置为 True 时，这才是允许的，这意味着 JSON 格式被定向为没有列表括号的记录。清单 [4-26](#PC26) 演示了这一点。

```py
>> data = io.StringIO(
     """
     {"location": "234unf923", "temp": 35.2}
     {"location": "340inf351", "temp": 32.5}
     {"location": "234abe045", "temp": 33.1}
     """
)
>> temperatures = pd.DataFrame({})
>> reader = pd.read_json(
     data,
     lines=True,
     chunksize=2,
)
>> for chunk in reader:
     temperatures = temperatures.append(process(chunk))
>> temperatures
     location       temp
     234unf923      35.200000
     340inf351      32.500000
     234abe045      33.100000

Listing 4-26Loading the file in chunks

```

默认情况下，JSON loader 根据列名确定某些列是否类似于日期，这与查看值的其他读取器不同。它接受 convert_dates 参数，该参数可以是列名列表或布尔值。如果设置为 True，它会将以 _at 或 _time 结尾、以 timestamp 开头或命名为 modified 或 date 的列转换为 datetimes。若要禁用日期列的自动检测，可以将 keep_default_dates 设置为 False。

默认情况下，JSON 加载器将尝试推断每个列和轴的类型，除非 orient 设置为 table，在这种情况下，类型作为 JSON 模式的一部分提供。就像读取一个 CSV 文件一样，如果 JSON 文件中没有指定类型，那么提供它们会节省大量内存。清单 [4-27](#PC27) 显示了在没有指定类型的情况下加载的 JSON 文件的内存占用，而清单 [4-28](#PC28) 显示了指定类型的相同 JSON 文件的内存占用。注意在清单 [4-28](#PC28) 中，类型被明确指定，结果数据帧的内存减少了大约 40%。

```py
>> data = io.StringIO(
     """
    {
          "birth": {
              "129237": "04/10/1999",
              "123083": "05/18/1989",
          },
          "height": {
              "129237": 5.4,
              "123083": 6.1,
          },
          "weight": {
              "129237": 126,
              "123083": 130,
          },
      }
      """
)
>> patient_info = pd.read_json(
      data,
      orient="columns",
      convert_dates=["birth"],
      dtype={"height": np.float16, "weight": np.int16},
)
                 birth          height  weight
      129237     1999-04-10     5.4     126
      123083     1989-05-18     6.1     130
>> df.dtypes
      birth          datetime64[ns]
      height         float16
      weight         int16
>> df.index.dtype
      dtype('int64')
>> df.memory_usage()
      Index          16
      birth          16
      height         4
      weight         4

Listing 4-28Explicitly specifying the type when loading a JSON

```

```py
>> data = io.StringIO(
     """
     {
         "birth": {
             "129237": "04/10/1999",
             "123083": "05/18/1989",
         },
         "height": {
             "129237": 5.4,
             "123083": 6.1,
         },
         "weight": {
             "129237": 126,
             "123083": 130,
         },
     }
     """
)
>> patient_info = pd.read_json(
     data,
     orient="columns",
)
                birth          height  weight
     129237     04/10/1999     5.4     126
     123083     05/18/1989     6.1     130
>> df.dtypes
     birth          object
     height         float64
     weight         int64
>> df.index.dtype
     dtype('int64')
>> df.memory_usage()
     Index          16
     birth          16
     height         16
     weight         16

Listing 4-27Loading a JSON with pandas type inference

```

## pd.read_sql、pd.read_sql_table 和 pd.read_sql_query

pandas read_sql 加载器是 read_sql_table 和 read_sql_query 的包装器。根据传递给它的参数，它调用下面这两个函数中的一个。它还内置了对 SQLAlchemy 的支持。

SQLAlchemy 是一个非常流行的对象关系映射库，也称为 ORM。pandas SQL reader 还支持直接与 DBAPI 对话，DBAPI 是 SQLAlchemy 所依赖的一个低级数据库库。虽然 DBAPI 在 pandas 中仅限于 SQLite3，但 SQLAlchemy 可以与所有类型的关系数据库对话，因此在切换数据库时无需重写所有的 SQL 查询。ORM 在应用程序开发领域非常流行，因为它们允许您将数据库表映射到 Python 中的对象或类。这很好，因为您可以在代码库中跟踪数据库表定义。您可以将数据库表定义为类，然后用一个简单的命令将它们添加到数据库中。例如，您还可以使用 Alembic 之类的迁移库，通过迁移脚本修改现有的数据库表，这使您可以前滚和回滚数据库更改，几乎没有不可恢复的生产数据库灾难的风险。在构建表定义时，还可以指定 Python 和数据库之间的列类型转换。SQLAlchemy 也很好，因为它用容易参数化的表达式将 SQL 查询抽象成更容易阅读的查询语言，如清单 [4-29](#PC29) 所示。

```py
cur.execute(
     """
     SELECT * FROM temperature_readings
     WHERE temperature_readings.temp > 45
     """
)

session.query(TemperatureReadings).filter(
     temp > 45
)

Listing 4-29Using a raw SQL string query vs. the SQLAlchemy ORM to generate a query

```

清单 [4-30](#PC30) 展示了如何构建一个数据库并将数据插入其中的例子。在本例中，我们创建了一个包含 id 和 name 两列的用户表，并将一个 id 为零、名为 Eric 的新用户插入到该表中。注意，代码中定义了两个不同的 URL，一个连接到 sqlite，另一个连接到本地 Postgres 数据库实例。

```py
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy import Column, Integer, String
from sqlalchemy.ext.declarative import declarative_base
Base = declarative_base()

SQLITE_URL = "sqlite://"
POSTGRES_URL = "postgresql://postgres@localhost:5432"

class User(Base):
     __tablename__ = 'user'
     id = Column(Integer, primary_key=True)
     name =  Column(String(50))

engine = create_engine(SQLITE_URL)
Session = sessionmaker(bind=engine)

def create_tables():
     Base.metadata.create_all(engine)

def add_user():
     session = Session()
     user = User(id=0, name="Eric")
     session.add(user)
     session.commit()
     session.close()

>> create_tables()
>> add_user()

Listing 4-30Creating tables in a postgres database and adding data to it using SQLAlchemy

```

清单 [4-31](#PC31) 提供了一个简单的 docker-compose.yml 文件，它将使用清单 [4-32](#PC32) 中的命令在你的机器上运行一个本地 Postgres 数据库。注意，该数据库被配置为将数据写入磁盘，因此您可以在任何时候终止 docker 容器，数据将保存在 postgres-data 目录中，并在下次启动 docker 容器时保存在那里。

```py
>> docker-compose up -d

Listing 4-32Starting the Postgres database

```

```py
version: '3'
services:
     postgres:
          image: postgres:9.4-alpine
          ports:
               - '127.0.0.1:5432:5432'
          volumes:
              - ./postgres-data:/var/lib/postgresql/data

Listing 4-31The contents of the docker-compose.yml that will create a local Postgres database

```

SQL 装载器通常可以装载整个表，也可以根据查询装载部分表。尽管 SQL 加载器接受 SQLAlchemy 引擎，但它们只接受 select 语句，而不接受查询对象。这意味着虽然您可以使用 SQLAlchemy 的查询 API，但是您必须在将它传递到加载器之前将其转换为可选的，如清单 [4-34](#PC34) 所示。可选的本质上是原始 SQL 查询字符串。清单 [4-33](#PC33) 展示了如何将数据库中的所有用户数据加载到数据帧中的示例，而清单 [4-34](#PC34) 展示了如何将 id=0 的用户加载到数据帧中。

```py
>> select_user0 = session.query(Patient).filter_by(id=0).selectable
>> pd.read_sql(
     sql=select_user0,
     con=engine,
     columns=["id", "name"],
)
     id   name
0    0    Eric

Listing 4-34Loading the user with id=0 into a DataFrame using read_sql

```

```py
>> pd.read_sql(
     sql=User.__tablename__,
     con=engine,
     columns=["id", "name"],
)
     id   name
0    0    Eric

Listing 4-33Loading all the users into a DataFrame using read_sql

```

SQL 装载器与我们看到的其他装载器有类似的选项，例如，在时间或日期时间转换时装载数据块。然而，也有不同之处。与我们已经讨论过的一些其他加载器不同，SQL 加载器没有数据类型规范选项。这经常给使用数据库的 pandas 用户带来一个问题，因为他们可能将数据的规范化版本存储在数据库中，然后希望将其加载回来，却发现数据类型并不相同。如果遇到这种情况，SQLAlchemy 和一些定制加载代码会有所帮助。SQLAlchemy 提供了一个自定义类型选项，允许您在数据库类型和 Python 类型之间进行转换。正如我们在其他没有显式指定类型的加载器中看到的，pandas 将 id 列存储为 int64。清单 [4-35](#PC35) 展示了一个例子，展示了我们如何将整数 id 列的 Python 类型指定为 int32，而不是更通用、更大的整数类型。使用这个表定义，现在当我们添加一个用户时，id 将作为一个整数存储在数据库中，但是当我们读出它时，它将是一个 NumPy int32 类型。

```py
from sqlalchemy import Column, String
from sqlalchemy.ext.declarative import declarative_base
import sqlalchemy.types as types
import numpy as np
Base = declarative_base()

class Int32(types.TypeDecorator):
     impl = types.Integer

     def process_bind_param(self, value, dialect):
         return value

     def process_result_value(self, value, dialect):
         return np.int32(value)

class User(Base):
     __tablename__ = 'user'
     id = Column(Int32, primary_key=True)
     name =  Column(String(50))

Listing 4-35Using SQLAlchemy TypeDecorator to specify a data type for pandas

```

清单 [4-36](#PC36) 显示了 read_sql 内部实现的代码片段，其中 self.pd_sql 是 sqlAlchemy 引擎对象，sql_select 是作为 SQL 参数传入的可选对象，self.frame 是返回的结果 DataFrame。在这里，您可以看到 pandas 是如何从数据库加载数据并将其转换成数据帧的。fetchall 函数以元组列表的形式返回数据，例如[(0，' Eric')]。这个实现与下一步我们如何让Pandas使用我们在清单 [4-35](#PC35) 中定义的正确数据类型相关。

```py
result = self.pd_sql.execute(sql_select)
column_names = result.keys()

data = result.fetchall()
self.frame = DataFrame.from_records(
     data, columns=column_names, coerce_float=coerce_float
)

Listing 4-36Part of the pandas implementation of read_sql

```

我们不依赖 pandas read_sql 实现，而是编写自己的定制 sql 加载代码，该代码将维护我们在创建 DataFrame 时在 SQLAlchemy 用户表中定义的数据类型。清单 [4-37](#PC37) 中显示了定制的 SQL 加载代码，与加载后使用 astype 转换类型相比，它速度更快，占用的内存更少。

```py
>> sql = session.query(User).selectable
>> results = engine.execute(sql).fetchall()
>> data = {
     columns[col]: np.array(
           [row[col] for row in results],
           dtype=type(results[0][col]))
     for col, v in enumerate(results[0])}
>> df = pd.DataFrame(data)
>> df.dtypes
     0    int32
     1    object

Listing 4-37Custom SQL loader code that maintains the data types defined on the SQLAlchemy table in Listing 4-35

```

我们已经在本章中介绍了几种最流行的加载器及其选项，但是还有更多。请务必阅读您正在使用的特定加载器的文档，并查看您可以使用哪些类型的加载期间规范化功能，如果没有，您可能需要自己编写一些自定义代码。请记住，降低内存开销和减少加载和规范化过程中的步骤可以节省性能。pandas 提供了许多提高规范化和负载性能的方法，这取决于您在特定情况下遇到的瓶颈。在第 [5](5.html) 章中，我们将探讨一旦数据被加载和规范化，如何将数据重新整形为所需的数据帧格式。

<aside aria-label="Footnotes" class="FootnoteSection" epub:type="footnotes">Footnotes [1](#Fn1_source)

[https:// pandas。pydata。org/pandas-docs/stable/reference/API/pandas。read_ hdf。html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_hdf.html)

  [2](#Fn2_source)

[https:// pandas。pydata。org/pandas-docs/stable/reference/API/pandas。read_ csv。html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)

 </aside>