# 6.该应用方法

应用是 pandas 中使用最不正确的功能之一。如果你正在使用它，你就不应该使用它。这是因为 apply 将函数“应用”到数据集中的每一行或每一列，这实际上违反了使用 pandas 的一条基本规则:不要迭代数据集。在这一章中，我们将探讨什么时候应用是正确的选择，并为不正确的选择提供替代解决方案。

## 何时不使用应用程序

对于那些熟悉基本编程特性的人来说，迭代是一种熟悉的操纵数据的方式。我们对自己说:我想在每一行或每一列上运行这个操作，因此 apply 看起来非常友好。然而，这种组织问题的方式在熊猫身上是完全错误的。在处理关系数据库时使用的许多相同的原则也可以在处理 pandas 时使用。当你对数据库中的数据执行操作时，你不是一次做一行，而是定义一个范围；熊猫也是如此。当操作一个数据集时，你定义所有你想要操作的元素，然后提供操作。最简单的形式可能是 df["col 1"] + df["col 2"]，在更复杂的情况下，可能是 df.where(100 > df >= 90，" A ")。

pandas 有许多用于执行数据计算操作的内置函数。附录中提供了一份完整的清单。这些计算通常直接转化为一个 NumPy 函数，在 C 中运行，这使得它们比它们的 apply 等价函数性能更好。它们可以直接从 pandas 数据帧和 pandas 系列对象(pandas 数据帧的一列或一行)中访问。

清单 [6-1](#PC1) 展示了一个简单的应用示例。我们将 sum 函数传递给它，指定要应用该函数的轴，然后我们得到每一行的总和。

```py
>> df = pd.DataFrame([[4, 9],[6, 7]], columns=['A', 'B'])
>> df
        A  B
     0  4  9
     1  6  7
>> df.apply(np.sum, axis=1)
     0    13
     1    13

Listing 6-1Example of using apply

```

虽然清单 [6-1](#PC1) 中的例子很简单，并且说明了*如何使用*，但是使用的用例*是非常错误的。这是一个教科书式的例子，说明什么时候使用*而不是*apply，因为 np.sum 函数是数据帧本身的内置函数，因此应该使用内置函数，因为它的性能更好。但是为什么它的性能更好呢？让我们更详细地探讨一下。*

为什么内置的 pandas sum 比将 NumPy sum 应用于每一行要高效得多，这个问题的答案在于行上的迭代发生在哪里。清单 [6-2](#PC2) 中的以下循环是 pandas apply 方法的底层实现。

```py
for i, v in enumerate(series_gen):
     results[i] = self.f(v)
     keys.append(v.name)

Listing 6-2Main loop in the pandas apply implementation

```

正如您在清单 [6-2](#PC2) 中看到的，行的循环发生在 Python 中。这里您可以看到 series_gen，它是将要应用的函数(保存在 self.f 中)将要应用到的列或行。这与内置的 pandas sum 函数相反，后者只是将一个要操作的 ndarray 传递给 NumPy sum 函数，后者然后在 C 中对数据进行迭代和求和，并将结果 ndarray 返回给 Python。用 C 语言而不是 Python 对数据运行操作的过程称为矢量化。从本质上来说，与用 Python 运行操作相比，矢量化能够实现巨大的加速。出于第三章[中提到的所有原因，在 C 中循环和执行操作比 Python 更高效。然而，加速并不总是来自于 c 语言中的循环。](3.html)

矢量化运算允许您对一系列数字应用数学运算。例如，如果要将 4 添加到 ndarray 中的每个元素，可以使用语法 arr + 4 来指定。在 NumPy ufuncs 的情况下(参见附录中的完整列表)，它们实际上利用了 CPU 本身的专用向量寄存器。向量寄存器是可以包含一系列值的寄存器，当对它们执行操作时，会同时对寄存器中的每个值执行操作。因此，CPU 中八个值和八个连续加法指令的数组上的循环变成了 CPU 中对八个值进行操作的一个加法指令。正如你所想象的，这导致了巨大的加速。矢量化还会填充不匹配维度的数组，以使维度匹配，从而可以运行操作。这个过程被称为广播。当您通过 df["new_col"] = 4 在 pandas 中添加一个新列时，4 将被广播，使其行数与 DataFrame 中所有其他列的行数相同。类似地，sum 等聚合函数使用矢量化对一系列数字进行运算。所有这些归结起来就是 apply 不是一个矢量化的操作——它在 Python 中循环，应该尽可能避免。这实际上与遍历行并自己应用函数是一回事，如清单 [6-3](#PC3) 所示。

```py
results = [0]*len(df)
for i, v in df.itterrows():
     results[i] = v.sum()
df["sum"] = results

Listing 6-3Equivalent of apply implemented manually

```

事实上，清单 [6-3](#PC3) 中展示的 apply 定制实现的性能比清单 [6-1](#PC1) 中直接使用 apply 的性能稍好，因为在实现这个简单的定制替代方案时，获得操作核心的开销更少。

与使用内置的 pandas 操作相比，apply 慢了多少？我们来看一些具体的例子，比较一下性能。将清单 [6-1](#PC1) 中的 apply 示例与清单 [6-4](#PC4) 中的替代方法的性能进行比较，当对 100，000 行执行时，apply 函数平均需要大约 8.5 秒，而直接从 pandas 数据帧运行 sum 函数平均需要大约 0.4 毫秒。

```py
df.sum(axis=1)

Listing 6-4Alternative implementation of Listing 6-1

```

让我们看另一个例子。假设您有一个数据集，其中有一个名为 A 的列，但该列的数据不完整，您希望用 B 列和 c 列中的最大值来替换缺失的值。这可以使用清单 [6-5](#PC5) 中演示的 apply 来实现，或者可以使用清单 [6-6](#PC6) 中演示的 where 方法以更高效的方式来实现。

```py
df["A"].where(
     ~df["A"].isna(),
     df[["B", "C"]].max(axis=1),
     inplace=True,
)

Listing 6-6Replacing missing data using the where method

```

```py
def replace_missing(series):
     if np.isnan(series["A"]):
         series["A"] = max(series["B"], series["C"])
     return series

df = df.apply(replace_missing, axis=1)

Listing 6-5Replacing missing data using apply

```

where 方法用第二个参数中提供的值替换 falsey 值。这意味着，在清单 [6-6](#PC6) 中，所有的 NaN 值都被替换为 B 列和 c 列中的最大值。注意，我们还指定了 inplace=True，以便这种替换发生在当前数据帧上，而不是创建一个会导致内存重复的新数据帧。

让我们看看清单 [6-7](#PC7) 和 [6-8](#PC8) 中一个更复杂的例子。假设您有一个包含两列(水果和订单)的数据框架，并且您希望删除每行订单中没有水果的所有数据。pandas 确实有字符串操作，包括 Series.str.find，如果一个字符串中的每个值都有一个子字符串，它将返回 True。然而，它只允许你传入一个常量。换句话说，您不能指定一系列子字符串，而只能指定一个字符串值，因此 find 在这种情况下不起作用。pandas 中也没有内置对两个系列对象进行操作的“in”检查，所以尽管这正是我们想要的，但 pandas 不支持它。这意味着我们必须自己实现某种定制的解决方案，所以让我们来探索各种选项的性能。

```py
mask = [fruit.lower() in order.lower()
     for (fruit, order) in data[["fruit", "order"]].values]
data = data[mask]

Listing 6-8Solving Listing 6-7 using a list comprehension

```

```py
def test_fruit_in_order(series):
     if (series["fruit"].lower() in
          series["order"].lower()
     ):
          return series
     return np.nan

>> data = pd.DataFrame({
     "fruit": ["orange", "lemon", "mango"],
     "order": [
         "I'd like an orange",
         "Mango please.",
         "May I have a mango?",
     ],
})
          fruit     order
     0    orange    I'd like an orange
     1    lemon     Mango please.
     2    mango     May I have a mango?

>> data.apply(
     test_fruit_in_order,
     axis=1,
     result_type="reduce",
).dropna()
          fruit     order
     0    orange    I'd like an orange
     2    mango     May I have a mango?

Listing 6-7Dropping rows whose order column does not contain the substring in the fruit column using apply

```

使用清单 [6-7](#PC7) 中的 apply 来解决这个问题在 100，000 行上需要大约 14 秒，而使用清单 [6-8](#PC8) 中的 list comprehension 需要大约 100 毫秒。但是为什么列表理解比应用快得多呢？不都是用 Python 循环的吗？列表理解是 Python 解释器中特别优化的循环。它们翻译成的字节码更像是用 C 编写的循环，因为它们不加载一堆专门的 Python 列表属性。接下来是 for 循环的字节码(清单 [6-9](#PC9) )与列表理解(清单 [6-10](#PC10) )。注意，与 for 循环相比，list 理解的字节码要简单得多，也小得多，尽管它们做的是同样的事情。

```py
def list_comprehension():
     l = [x % 2 for x in range(5)]

     0          0 LOAD_CONST                1
                2 LOAD_CONST                2
                4 MAKE_FUNCTION             0
                6 LOAD_GLOBAL               0 (range)
                8 LOAD_CONST                3 (5)
                10 CALL_FUNCTION            1
                12 GET_ITER
                14 CALL_FUNCTION            1
                16 STORE_FAST               0 (l)
                18 LOAD_CONST               0 (None)
                20 RETURN_VALUE             None

Listing 6-10Bytecode translation of a list comprehension

```

```py
def for_loop():
     l = []
     for x in range(5):
          l.append( x % 2 )

     0       0 BUILD_LIST               0
             2 STORE_FAST               0 (l)
     1       4 SETUP_LOOP               30 (to 36)
             6 LOAD_GLOBAL              0 (range)
             8 LOAD_CONST               1 (5)
            10 CALL_FUNCTION            1
            12 GET_ITER
       >>   14 FOR_ITER                 18 (to 34)
            16 STORE_FAST               1 (x)
     2      18 LOAD_FAST                0 (l)
            20 LOAD_METHOD              1 (append)
            22 LOAD_FAST                1 (x)
            24 LOAD_CONST               2 (2)
            26 BINARY_MODULO
            28 CALL_METHOD              1
            30 POP_TOP
            32 JUMP_ABSOLUTE            14
       >>   34 POP_BLOCK
       >>   36 LOAD_CONST               0 (None)
            38 RETURN_VALUE             None

Listing 6-9Bytecode translation of a simple for loop

```

## 何时使用应用

到目前为止，我们已经看了一些不保证使用 apply 的例子。让我们来看一个这样的例子。通常，在野外处理数据的现实会导致一些更加复杂的场景。假设您想要计算数据帧中每个元素的分数百分比，清单 [6-11](#PC11) 中提供了它的实现。

```py
def percentileofscore(a, score):
     """
     Three-quarters of the given values lie below a given score:
     >>> stats.percentileofscore([1, 2, 3, 4], 3)
     75.0

     With multiple matches, note how the scores of the two
     matches, 0.6 and 0.8 respectively, are averaged:
     >>> stats.percentileofscore([1, 2, 3, 3, 4], 3)
     70.0
     """
     n = len(a)
     left = np.count_nonzero(a < score)
     right = np.count_nonzero(a <= score)
     pct = (right + left + (1 if right > left else 0)) * 50.0/n
     return pct

Listing 6-11Implementation of scipy.stats.percentileofscore

```

这意味着如果我们有下面的输入数据帧，在使用 pandas apply 函数应用 scipy.stats.percentileofscore 后，我们将看到下面的输出数据帧(清单 [6-12](#PC12) )。

```py
>> from scipy import stats
>> data = pd.DataFrame(np.arange(20).reshape(4,5))
               0   1   2   3   4
           0   0   1   2   3   4
           1   5   6   7   8   9
           2   10  11  12  13  14
           3   15  16  17  18  19
>> def apply_percentileofscore(series):
     return series.apply(
         lambda x:stats.percentileofscore(series,x)
     )
>> data.apply(apply_percentileofscore, axis=1)
               0     1     2     3      4
           0   20.0  40.0  60.0  80.0  100.0
           1   20.0  40.0  60.0  80.0  100.0
           2   20.0  40.0  60.0  80.0  100.0
           3   20.0  40.0  60.0  80.0  100.0

Listing 6-12Applying scipy.percentileofscore to each element in a DataFrame

```

这是一个相当复杂的用例，并且是一个非常低效的实现，因为它为每一行调用 apply 方法两次。不幸的是，没有内置的 pandas 函数来匹配 SciPy 的 percentileofscore 函数应用于每个元素的行为，就像我们在本例中需要做的那样。虽然我们可以在数据帧上一次一列地单独进行这种计算，并将结果拼凑在一起，但这将是一个非常麻烦的实现。清单 [6-13](#PC13) 展示了这种方法。

```py
def percentileofscore(df):
     res_df = pd.DataFrame({})
     for col in df.columns:
         score = pd.DataFrame([df[col]]*5, index=df.columns).T
         left = df[df < score].count(axis=1)
         right = df[df <= score].count(axis=1)
         right_is_greater = (
             df[df <= score].count(axis=1)
             > df[df < score].count(axis=1)
         ).astype(int)
         res_df[f'res{col}'] = (
             left + right + right_is_greater
             ) * 50.0 / len(df.columns)
     return res_df

percentileofscore(data)

Listing 6-13A more performant implementation of Listing 6-12

```

清单 [6-13](#PC13) 的实现带来了更好的性能，因为我们通过一次对所有行执行 pandas 操作，消除了 Python 中的一个循环(行上的循环)。然而，我们还被迫创建一个重复的数据帧，其中所有列都填充了分数，以便实现这些按行的操作，这会导致不希望的内存开销。注意，我们也没有重用 SciPy 中的实现，而是使用 pandas 操作重新实现了它，这对于可读性来说不够理想，并且增加了实现的复杂性和脆弱性。幸运的是，还有另一种方法可以实现这一点，我们将在下一节讨论。

## 使用 Cython 提高应用程序的性能

从上一个简单求和的例子中吸取教训，如果 pandas 的开发人员在数据框架之外为我们提供了这个函数，它可能已经用 C 实现了。你可能会对自己说，“我不知道 C——听起来很难。”但是，事实上，Cython 库使它变得非常容易，并且您不需要知道 C 语法就可以做到这一点！Cython 让你编写 Python 并编译成 C，作为 C 扩展使用。首先，我们需要编写 percentileofscore 函数，它将对整个数据帧进行操作，如清单 [6-14](#PC14) 所示。然后如清单 [6-15](#PC15) 所示编译，最后我们可以如清单 [6-16](#PC16) 所示使用。

```py
from percentileofscore import percentileofscore
percentileofscore(data.values)

Listing 6-16Using the compiled Cython function implemented in Listing 6-14

```

```py
import pyximport; pyximport.install(language_level=3)
from distutils.core import setup
from Cython.Build import cythonize

setup(
     ext_modules = cythonize("percentileofscore.pyx")
)

>> python setup.py build_ext --inplace

Listing 6-15setup.py for compiling Cython in Listing 6-14

```

```py
from scipy.stats import percentileofscore as pctofscore
from copy import deepcopy

def percentileofscore(values):
     percentiles = [0]*len(values[0])
     num_rows = len(values)
     for row_index in range(num_rows):
         row_vals = values[row_index]
         for col_index, col_val in enumerate(row_vals):
             percentiles[col_index] = \
                 pctofscore(row_vals, col_val)
         values[row_index] = percentiles

Listing 6-14A more performant implementation of Listing 6-12 using Cython

```

注意，Cython 函数接受值，而不是完整的 pandas 数据帧；这是因为 values 是一个二维数组，可以很容易地转换成 C，而 pandas DataFrame 是一个 Python 对象，而不是。还要注意，该函数就地修改数据，而不是返回一个全新的二维数组。这是一个性能优势，因为我们不必为新阵列分配新内存，并且一旦数据被转换，我们就不再需要原始数据集(至少在这种特殊情况下)。

那么，当运行超过 100，000 行时，这些方法的性能有多大差别呢？在清单 [6-12](#PC12) 中使用 apply 平均需要 58 秒。使用 pandas 操作有效地重新实现清单 [6-13](#PC13) 中的 SciPy 等价物平均需要 24 秒左右。构建定制 Cython 函数的第三种方法平均需要 4 秒钟。除了性能，使用 Cython 方法还有其他优势。SciPy 函数可以按原样使用，不需要重新实现，所以从实现和可读性的角度来看，它看起来也很有吸引力。

总之，只有当所有其他选择都已用尽时，才应使用。它在性能上等同于 iterrows 和 itercolumns，应该以同样的谨慎程度对待。如果需要在大型数据集上使用 apply，并导致第二次或更多次的速度下降，则应实施定制的 Cython apply 等效工具，以免降低数据分析性能。