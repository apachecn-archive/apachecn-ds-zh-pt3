# 5.熊猫:读取和写入数据

在前一章中，您已经熟悉了 pandas 库以及它为数据分析提供的基本功能。您已经看到数据帧和系列是这个库的核心。这些是执行所有数据操作、计算和分析的材料。

在这一章中，您将看到 pandas 提供的所有工具，用于读取存储在许多类型的介质(如文件和数据库)中的数据。同时，您还将看到如何直接在这些格式上编写数据结构，而不必太担心所使用的技术。

本章重点介绍 pandas 提供的一系列 I/O API 函数，这些函数可以直接读写作为 datafram e 对象的数据。我们从文本文件开始，然后逐渐转向更复杂的二进制格式。

在本章的最后，您还将学习如何与所有常见的数据库交互，包括 SQL 和 NoSQL，包括展示如何在数据帧中存储数据的示例。同时，您将学习如何读取数据库中包含的数据，并将它们作为数据帧进行检索。

## I/O API 工具

pandas 是一个专门用于数据分析的库，因此您会认为它主要侧重于计算和数据处理。从/向外部文件写入和读取数据的过程可以被认为是数据处理的一部分。事实上，您将看到，即使在这个阶段，您也可以执行一些操作，以便为操作准备传入的数据。

因此，这一步对于数据分析非常重要，因此在 pandas 库中必须有一个用于此目的的特定工具——一组称为 I/O API 的函数。这些功能分为两大类:*读者*和*作者*。

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

读者

 | 

作家

 |
| --- | --- |
| `read_csv` | `to_csv` |
| `read_excel` | `to_excel` |
| `read_hdf` | `to_hdf` |
| `read_sql` | `to_sql` |
| `read_json` | `to_json` |
| `read_html` | `to_html` |
| `read_stata` | `to_stata` |
| `read_clipboard` | `to_clipboard` |
| `read_pickle` | `to_pickle` |
| `read_msgpack` | `to_msgpack`(实验) |
| `read_gbq` | `to_gbq`(实验) |

## CSV 和文本文件

多年来，每个人都已经习惯于以文本形式读写文件。特别是，数据通常以表格形式报告。如果一行中的值用逗号分隔，就有了 CSV(逗号分隔值)格式，这可能是最著名和最流行的格式。

其他形式的表格数据可以用空格或制表符分隔，通常包含在各种类型的文本文件中(通常带有`.txt`扩展名)。

这种类型的文件是最常见的数据来源，更容易转录和解释。在这方面，pandas 提供了一组专门针对这类文件的功能。

*   `read_csv`

*   `read_table`

*   `to_csv`

## 读取 CSV 或文本文件中的数据

根据经验，处理数据分析的人最常见的操作是读取 CSV 文件中包含的数据，或者至少是文本文件中包含的数据。

但是在开始处理文件之前，您需要导入以下库。

```
>>> import numpy as np
>>> import pandas as pd

```

为了查看 pandas 如何处理这种数据，我们将从在工作目录中创建一个小的 CSV 文件开始，如清单 [5-1](#PC2) 所示，并将其保存为`ch05_01.csv`。

```
white,red,blue,green,animal
1,5,2,3,cat
2,7,8,5,dog
3,3,6,7,horse
2,2,8,3,duck
4,4,2,1,mouse

Listing 5-1ch05_01.csv

```

因为这个文件是用逗号分隔的，所以你可以使用`read_csv()`函数来读取它的内容并把它转换成一个 dataframe 对象。

```
>>> csvframe = pd.read_csv('ch05_01.csv')
>>> csvframe
   white  red  blue  green animal
0      1    5     2      3    cat
1      2    7     8      5    dog
2      3    3     6      7  horse
3      2    2     8      3   duck
4      4    4     2      1  mouse

```

如您所见，读取 CSV 文件中的数据相当简单。CSV 文件是表格数据，其中同一列中的值用逗号分隔。由于 CSV 文件被认为是文本文件，您也可以使用`read_table()`功能，但是要指定分隔符。

```
>>> pd.read_table('ch05_01.csv',sep=',')
   white  red  blue  green animal
0      1    5     2      3    cat
1      2    7     8      5    dog
2      3    3     6      7  horse
3      2    2     8      3   duck
4      4    4     2      1  mouse

```

在本例中，您可以看到在 CSV 文件中，标识所有列的标题位于第一行。但这不是一般情况；通常情况下，列表数据直接从第一行开始(见清单 [5-2](#PC5) )。

```
1,5,2,3,cat
2,7,8,5,dog
3,3,6,7,horse
2,2,8,3,duck
4,4,2,1,mouse

>>> pd.read_csv('ch05_02.csv')
   1  5  2  3    cat
0  2  7  8  5    dog
1  3  3  6  7  horse
2  2  2  8  3   duck
3  4  4  2  1  mouse

Listing 5-2ch05_02.csv

```

在这种情况下，您可以通过将`header`选项设置为`None`来确保 pandas 将默认名称分配给列。

```
>>> pd.read_csv('ch05_02.csv', header=None)
   0  1  2  3      4
0  1  5  2  3    cat
1  2  7  8  5    dog
2  3  3  6  7  horse
3  2  2  8  3   duck
4  4  4  2  1  mouse

```

此外，您可以通过将标签列表分配给`names`选项来直接指定名称。

```
>>> pd.read_csv('ch05_02.csv', names=['white','red','blue','green','animal'])
   white  red  blue  green animal
0      1    5     2      3    cat
1      2    7     8      5    dog
2      3    3     6      7  horse
3      2    2     8      3   duck
4      4    4     2      1  mouse

```

在更复杂的情况下，您希望通过读取 CSV 文件来创建具有分层结构的数据帧，您可以通过添加`index_col`选项来扩展`read_csv()`函数的功能，指定所有要转换为索引的列。

为了更好地理解这种可能性，创建一个新的 CSV 文件，其中有两列用作层次结构的索引。然后，在工作目录中将其保存为`ch05_03.csv`(参见清单 [5-3](#PC8) )。

```
color,status,item1,item2,item3
black,up,3,4,6
black,down,2,6,7
white,up,5,5,5
white,down,3,3,2
white,left,1,2,1
red,up,2,2,2
red,down,1,1,4

>>> pd.read_csv('ch05_03.csv', index_col=['color','status'])
              item1  item2  item3
color status
black up          3      4      6
      down        2      6      7
white up          5      5      5
      down        3      3      2
      left        1      2      1
red   up          2      2      2
      down        

1      1      4

Listing 5-3ch05_03.csv

```

### 使用 RegExp 解析 TXT 文件

在其他情况下，解析数据的文件可能没有显示定义为逗号或分号的分隔符。在这些情况下，正则表达式可以帮助我们。事实上，您可以使用`sep`选项在`read_table()` f 函数中指定一个正则表达式。

为了更好地理解 regexp 并理解如何将它作为值分离的标准，让我们从一个简单的案例开始。例如，假设您的 TXT 文件包含由空格或制表符以不可预知的顺序分隔的值。在这种情况下，您必须使用 regexp，因为这是考虑两种分隔符类型的唯一方法。您可以使用通配符 **/s*** 来实现。`/s`代表空格或制表符(如果要表示制表符，用`/t`)，星号表示可能有多个字符(其他常用通配符见表 [5-1](#Tab1) )。也就是说，这些值可以由更多的空格或制表符分隔。

表 5-1

元字符

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

`.`

 | 

单个字符，换行符除外

 |
| --- | --- |
| `\d` | 手指 |
| `\D` | 非数字字符 |
| `\s` | 空白字符 |
| `\S` | 非空白字符 |
| `\n` | 新行字符 |
| `\t` | 制表符 |
| `\uxxxx` | 十六进制数字指定的 Unicode 字符`xxxx` |

举一个极端的例子，我们用制表符或空格以随机顺序分隔这些值(见清单 [5-4](#PC9) )。

```
white red blue green
    1   5    2     3
    2   7    8     5
    3   3    6     7

>>> pd.read_table('ch05_04.txt',sep='\s+', engine="python")
   white  red  blue  green
0      1    5     2      3
1      2    7     8      5
2      3    3     6      7

Listing 5-4ch05_04.txt

```

如您所见，结果是一个完美的数据帧，其中的值完全有序。

现在，您将看到一个看似奇怪或不寻常的例子，但它并不像看起来那样罕见。这个例子非常有助于理解正则表达式的巨大潜力。事实上，您可能通常认为分隔符是特殊字符，如逗号、空格、制表符等。，但实际上您可以考虑像字母数字字符这样的分隔符，或者像 0 这样的整数。

在这个例子中，您需要从一个 TXT 文件中提取数字部分，其中有一个带有数字值的字符序列，并且文字字符完全融合。

每当 TXT 文件中不存在列标题时，记得将`header`选项设置为`None`(参见清单 [5-5](#PC10) )。

```
000END123AAA122
001END124BBB321
002END125CCC333

>>> pd.read_table('ch05_05.txt', sep='\D+', header=None, engine="python")
   0    1    2
0  0  123  122
1  1  124  321
2  2  125  333

Listing 5-5ch05_05.txt

```

另一个非常常见的事件是从解析中排除行。事实上，你并不总是想在文件中包含标题或不必要的注释(见清单 [5-6](#PC11) )。使用`skiprows`选项，您可以排除所有想要的行，只需分配一个包含行号的数组，在解析中不考虑这些行号。

使用此选项时请注意。要排除前五行就得写`skiprows = 5`，要排除五行就得写`skiprows = [5]`。

```
########### LOG FILE ############
This file has been generated by automatic system
white,red,blue,green,animal
12-Feb-2015: Counting of animals inside the house
1,5,2,3,cat
2,7,8,5,dog
13-Feb-2015: Counting of animals outside the house
3,3,6,7,horse
2,2,8,3,duck
4,4,2,1,mouse

>>> pd.read_table('ch05_06.txt',sep=',',skiprows=[0,1,3,6])
   white  red  blue  green animal
0      1    5     2      3    cat
1      2    7     8      5    dog
2      3    3     6      7  horse
3      2    2     8      3   duck
4      4    4     2      1  mouse

Listing 5-6ch05_06.txt

```

### 将 TXT 文件读入部分

当处理大文件时，或者当您只对这些文件的一部分感兴趣时，您通常需要将文件读入多个部分(块)。这既是为了应用任何迭代，也是因为我们对解析整个文件不感兴趣。

例如，如果您只想读取文件的一部分，您可以显式指定要解析的行数。借助`nrows`和`skiprows`选项，您可以选择起始行`n` ( `n = SkipRows`)和随后要阅读的行(`nrows = i`)。

```
>>> pd.read_csv('ch05_02.csv',skiprows=[2],nrows=3,header=None)
   0  1  2  3     4
0  1  5  2  3   cat
1  2  7  8  5   dog
2  2  2  8  3  duck

```

另一个有趣且相当常见的操作是将需要解析的文本分成几部分。然后，对于每个部分，可以执行特定的操作，以便逐部分地获得迭代。

例如，您希望将一列中每三行的值相加，然后将这些总和插入到一个序列中。这个例子是琐碎的和不切实际的，但是非常容易理解，所以一旦你学会了底层的机制，你将能够在更复杂的情况下应用它。

```
>>> out = pd.Series()
>>> i = 0
>>> pieces = pd.read_csv('ch05_01.csv',chunksize=3)
>>> for piece in pieces:
...    out.set_value(i,piece['white'].sum())
...    i = i + 1
...
0    6
dtype: int64
0    6
1    6
dtype: int64
>>> out
0    6
1    6
dtype: int64

```

### 以 CSV 格式写入数据

除了读取文件中包含的数据之外，通常还会编写由计算产生的数据文件，或者通常是包含在数据结构中的数据。

例如，您可能希望将数据帧中包含的数据写入 CSV 文件。为了完成这个编写过程，您将使用`to_csv()`函数，它接受您生成的文件名作为参数(参见清单 [5-7](#PC15) )。

```
>>> frame = pd.DataFrame(np.arange(16).reshape((4,4)),
               index = ['red', 'blue', 'yellow', 'white'],
               columns = ['ball', 'pen', 'pencil', 'paper'])

>>> frame.to_csv('ch05_07.csv')

```

如果您打开由 pandas 库生成的名为`ch05_07.csv`的新文件，您将看到清单 [5-7](#PC15) 中的数据。

```
,ball,pen,pencil,paper
0,1,2,3
4,5,6,7
8,9,10,11
12,13,14,15

Listing 5-7ch05_07.csv

```

正如您在前面的示例中看到的，当您将数据帧写入文件时，默认情况下会在文件上标记索引和列。这个默认行为可以通过将两个选项`index`和`header`设置为`False`来改变(参见清单 [5-8](#PC17) )。

```
1,2,3
5,6,7
9,10,11
13,14,15

Listing 5-8ch05_07b.csv

```

```
>>> frame.to_csv('ch05_07b.csv', index=False, header=False)

```

写文件时要记住的一点是数据结构中的`NaN`值在文件中显示为空字段(见清单 [5-9](#PC19) )。

```
,ball,mug,paper,pen,pencil
blue,6.0,,,6.0,
green,,,,,
red,,,,,
white,20.0,,,20.0,
yellow,19.

0,,,19.0,

Listing 5-9ch05_08.csv

```

```
>>> frame3 = pd.DataFrame([[6,np.nan,np.nan,6,np.nan],
...           [np.nan,np.nan,np.nan,np.nan,np.nan],
...           [np.nan,np.nan,np.nan,np.nan,np.nan],
...           [20,np.nan,np.nan,20.0,np.nan],
...           [19,np.nan,np.nan,19.0,np.nan]
...           ],
...                  index=['blue','green','red','white','yellow'],
                     columns=['ball','mug','paper','pen','pencil'])
>>> frame3
        ball  mug  paper  pen  pencil
blue     6.0  NaN    NaN  6.0     NaN
green    NaN  NaN    NaN  NaN     NaN
red      NaN  NaN    NaN  NaN     NaN
white   20.0  NaN    NaN 20.0     NaN
yellow  19.0  NaN    NaN 19.0     NaN
>>> frame3.to_csv('ch05_08.csv')

```

但是，您可以使用`to_csv()`函数中的`na_rep`选项将这个空字段替换为您喜欢的值。常用值可能是`NULL`、`0`或同一个`NaN`(见清单 [5-10](#PC21) )。

```
,ball,mug,paper,pen,pencil
blue,6.0,NaN,NaN,6.0,NaN
green,NaN,NaN,NaN,NaN,NaN
red,NaN,NaN,NaN,NaN,NaN
white,20.0,NaN,NaN,20.0,NaN
yellow,19.0,NaN,NaN,19.0,NaN

Listing 5-10ch05_09.csv

```

```
>>> frame3.to_csv('ch05_09.csv', na_rep ='NaN')

```

### 注意

在指定的情况下，dataframe 一直是讨论的主题，因为这些是写入文件的数据结构。但是所有这些功能和选项对该系列也有效。

## 读取和写入 HTML 文件

pandas 为 HTML 格式提供了相应的 I/O API 函数对。

*   `read_html()`

*   `to_html()`

这两个功能非常有用。您将欣赏将复杂的数据结构(如数据帧)直接转换成 HTML 表格的能力，而不必在 HTML 中编写一长串列表，尤其是在处理 Web 时。

相反的操作会非常有用，因为现在数据的主要来源只是网络世界。事实上，互联网上的许多数据并不总是以“随时可用”的形式打包在一些 TXT 或 CSV 文件中。然而，很多时候，数据是作为网页文本的一部分被报告的。因此，拥有一个阅读的功能会被证明是非常有用的。

这种活动非常普遍，目前被称为*网络抓取*。这个过程正在成为数据分析的第一部分(数据挖掘和数据准备)中集成的一系列过程的基本部分。

### 注意

许多网站现在已经采用 HTML5 格式，以避免任何模块丢失和错误信息的问题。我强烈建议你安装模块`html5lib`。Anaconda 指定:

`conda install html5lib`

### 用 HTML 编写数据

现在，您学习了如何将数据帧转换成 HTML 表格。dataframe 的内部结构被自动转换成嵌套标签、和, retaining any internal hierarchy. You don't need to know HTML to use this function.

因为作为数据帧的数据结构可能非常复杂和庞大，所以当您需要开发网页时，拥有这样的功能是非常好的。

为了更好地理解这种潜力，这里有一个例子。您可以从定义一个简单的数据帧开始。

由于有了`to_html()`功能，您可以直接将数据帧转换成 HTML 表格。

```
>>> frame = pd.DataFrame(np.arange(4).reshape(2,2))

```

因为 I/O API 函数是在 pandas 数据结构中定义的，所以您可以直接在 dataframe 的实例上调用`to_html()`函数。

```
>>> print(frame.to_html())
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td> 0</td>
      <td> 1</td>
    </tr>
    <tr>
      <th>1</th>
      <td> 2</td>
      <td> 3</td>
    </tr>
  </tbody>
</table>

```

正如您所看到的，创建 HTML 表所需的 HTML 标记形成的整个结构都是正确生成的，以符合 dataframe 的内部结构。

在下一个例子中，您将看到表格是如何在 HTML 文件中自动生成的。在这方面，我们创建了一个比前一个稍微复杂一点的数据帧，其中有索引标签和列名。

```
>>> frame = pd.DataFrame( np.random.random((4,4)),
...                    index = ['white','black','red','blue'],
...                    columns = ['up','down','right','left'])
>>> frame
             up      down     right      left
white  0.292434  0.457176  0.905139  0.737622
black  0.794233  0.949371  0.540191  0.367835
red    0.204529  0.981573  0.118329  0.761552
blue   0.628790  0.585922  0.039153  0.461598

```

现在，您将重点关注如何通过生成字符串来编写 HTML 页面。这是一个简单而琐碎的例子，但是对于理解和直接在 web 浏览器上测试 pandas 的功能非常有用。

首先，我们创建一个包含 HTML 页面代码的字符串。

```
>>> s = ['<HTML>']
>>> s.append('<HEAD><TITLE>My DataFrame</TITLE></HEAD>')
>>> s.append('<BODY>')
>>> s.append(frame.to_html())
>>> s.append('</BODY></HTML>')
>>> html = ".join(s)

```

既然 HTML 页面的所有列表都包含在`html`变量中，您可以直接在将被称为`myFrame.html`的文件上写:

```
>>> html_file = open('myFrame.html','w')
>>> html_file.write(html)
>>> html_file.close()

```

现在在你的工作目录中会出现一个新的 HTML 文件，`my` `Frame.html`。双击它可以直接从浏览器中打开它。左上角会出现一个 HTML 表格，如图 [5-1](#Fig1) 所示。

![../images/336498_2_En_5_Chapter/336498_2_En_5_Fig1_HTML.jpg](../images/336498_2_En_5_Chapter/336498_2_En_5_Fig1_HTML.jpg)

图 5-1

数据帧在网页中显示为 HTML 表格

### 从 HTML 文件中读取数据

正如你刚才看到的，pandas 可以很容易地从数据帧开始生成 HTML 表格。相反的过程也是可能的；函数`read_html ()`将解析一个 HTML 页面，寻找一个 HTML 表格。如果找到了，它会将该表转换为对象数据帧，以便在我们的数据分析中使用。

更准确地说，`read_html()`函数返回一个数据帧列表，即使只有一个表。将被解析的源可以是不同的类型。例如，您可能必须读取任何目录中的 HTML 文件。例如，您可以解析您在上一个示例中创建的 HTML 文件:

```
>>> web_frames = pd.read_html('myFrame.html')
>>> web_frames[0]
  Unnamed: 0        up      down     right      left
0      white  0.292434  0.457176  0.905139  0.737622
1      black  0.794233  0.949371  0.540191  0.367835
2        red  0.204529  0.981573  0.118329  0.761552
3       blue  0.628790  0.585922  0.039153  0.461598

```

如你所见，所有与 HTML 表格无关的标签都不是绝对的。此外,`web_frames`是一个数据帧列表，尽管在您的例子中，您提取的数据帧只有一个。但是，您可以在列表中选择想要使用的项目，以传统方式调用它。在这种情况下，该项是唯一的，因此索引将为 0。

然而，关于`read_html()`函数最常用的模式是直接解析 Web 上的 URL。这样，网络中的网页通过提取其中的表格而被直接解析。

例如，现在您将调用一个网页，其中有一个 HTML 表，显示一个带有一些名称和分数的排名列表。

```
>>> ranking = pd.read_html('https://www.meccanismocomplesso.org/en/meccanismo-complesso-sito-2/classifica-punteggio/')
>>> ranking[0]
    Member        points  levels  Unnamed: 3
0        1   BrunoOrsini    1075         NaN
1        2     Berserker     700         NaN
2        3  albertosallu     275         NaN
3        4          Mr.Y     180         NaN
4        5           Jon     170         NaN
5        6  michele sisi     120         NaN
6        7  STEFANO GUST     120         NaN
7        8  Davide Alois     105         NaN
8        9  Cecilia Lala     105         NaN
...

```

相同的操作可以在具有一个或多个表格的任何网页上运行。

## 从 XML 读取数据

在 I/O API 函数列表中，没有关于 XML(可扩展标记语言)格式的特定工具。其实虽然没有列出来，但是这种格式非常重要，因为很多结构化数据都是 XML 格式的。这不成问题，因为 Python 有许多其他库(除了 pandas)来管理 XML 格式数据的读写。

其中一个库是`lxml`库，它在解析非常大的文件时表现出色。在本节中，您将学习如何使用这个模块解析 XML 文件，以及如何将它与 pandas 集成，以最终获得包含所请求数据的 dataframe。关于这个库的更多信息，我强烈推荐访问 lxml 的官方网站 [`http://lxml.de/index.html`](http://lxml.de/index.html) 。

以清单 [5-11](#PC29) 中显示的 XML 文件为例。记下并以名称`books.xml`直接保存在您的工作目录中。

```
<?xml version="1.0"?>
<Catalog>
   <Book id="ISBN9872122367564">
      <Author>Ross, Mark</Author>
      <Title>XML Cookbook</Title>
      <Genre>Computer</Genre>
      <Price>23.56</Price>
      <PublishDate>2014-22-01</PublishDate>
   </Book>
   <Book id="ISBN9872122367564">
      <Author>Bracket, Barbara</Author>
      <Title>XML for Dummies</Title>
      <Genre>Computer</Genre>
      <Price>35.95</Price>
      <PublishDate>2014-12-16</PublishDate>
   </Book>
</Catalog>

Listing 5-11books.xml

```

在这个例子中，您将把 XML 文件中描述的数据结构直接转换成数据帧。首先要做的是使用`lxml`库的子模块`objectify`，以下面的方式导入它。

```
>>> from lxml import objectify

```

现在您可以只用`parse()`函数来解析 XML 文件。

```
>>> xml = objectify.parse('books.xml')
>>> xml
<lxml.etree._ElementTree object at 0x0000000009734E08>

```

你得到了一个对象树，这是一个`lxml`模块的内部数据结构。

更详细地看看这种类型的对象。要在这个树结构中导航，以便逐个元素地选择元素，您必须首先定义根。您可以使用`getroot()`功能来完成此操作。

```
>>> root = xml.getroot()

```

现在已经定义了结构的根，您可以访问树的各个节点，每个节点对应于原始 XML 文件中包含的标记。这些项目将与相应的标签具有相同的名称。因此，要选择它们，只需用点编写各种单独的标签，以某种方式反映树中节点的层次结构。

```
>>> root.Book.Author
'Ross, Mark'
>>> root.Book.PublishDate
'2014-22-01'

```

通过这种方式，你可以单独访问节点，但是你可以使用`getchildren()`同时访问不同的元素。使用这个函数，您将获得引用元素的所有子节点。

```
>>> root.getchildren()
[<Element Book at 0x9c66688>, <Element Book at 0x9c66e08>]

```

使用`tag` attr 属性，您可以获得与子节点对应的标签的名称。

```
>>> [child.tag for child in root.Book.getchildren()]
['Author', 'Title', 'Genre', 'Price', 'PublishDate']

```

而使用`text`属性，您可以获得包含在相应标签之间的值。

```
>>> [child.text for child in root.Book.getchildren()]
['Ross, Mark', 'XML Cookbook', 'Computer', '23.56', '2014-22-01']

```

然而，不管在`lxml.etree`树结构中移动的能力如何，你需要的是将它转换成数据帧。定义以下函数，其任务是分析 eTree 的内容，以逐行填充数据帧。

```
>>> def etree2df(root):
...    column_names = []
...    for i in range(0,len(root.getchildren()[0].getchildren())):
...       column_names.append(root.getchildren()[0].getchildren()[i].tag)
...    xml:frame = pd.DataFrame(columns=column_names)
...    for j in range(0, len(root.getchildren())):
...       obj = root.getchildren()[j].getchildren()
...       texts = []
...       for k in range(0, len(column_names)):
...          texts.append(obj[k].text)
...       row = dict(zip(column_names, texts))
...       row_s = pd.Series(row)
...       row_s.name = j
...       xml:frame = xml:frame.append(row_s)
...    return xml:frame
...
>>> etree2df(root)
             Author            Title     Genre  Price PublishDate
0        Ross, Mark     XML Cookbook  Computer  23.56  2014-22-01
1  Bracket, Barbara  XML for Dummies  Computer  35.95  2014-12-16

```

## 读取和写入 Microsoft Excel 文件中的数据

在上一节中，您看到了如何轻松地从 CSV 文件中读取数据。然而，在 Excel 电子表格中以表格形式收集数据并不罕见。

pandas 为这种格式提供了特定的功能。您已经看到 I/O API 为此提供了两个函数:

*   `to_excel()`

*   `read_excel()`

`read_excel()`函数可以读取 Excel 2003(。xls)文件和 Excel 2007(。xlsx)文件。这要归功于内部模块`xlrd`的集成。

首先打开一个 Excel 文件，输入数据，如图 [5-2](#Fig2) 所示。复制工作表 1 和工作表 2 中的数据。然后另存为`ch05_data.xlsx`。

![../images/336498_2_En_5_Chapter/336498_2_En_5_Fig2_HTML.jpg](../images/336498_2_En_5_Chapter/336498_2_En_5_Fig2_HTML.jpg)

图 5-2

Excel 文件的 sheet1 和 sheet2 中的两个数据集

要读取 XLS 文件中包含的数据并将其转换成数据帧，只需使用`read_excel()`功能。

```
>>> pd.read_excel('ch05_data.xlsx')
   white  red  green  black
a     12   23     17     18
b     22   16     19     18
c     14   23     22     21

```

如您所见，默认情况下，返回的数据帧由第一个电子表格中的数据组成。但是，如果您需要在第二个电子表格中加载数据，那么您必须指定工作表的名称或工作表的编号(索引),就像第二个参数一样。

```
>>> pd.read_excel('ch05_data.xlsx','Sheet2')
   yellow  purple  blue  orange
A      11      16    44      22
B      20      22    23      44
C      30      31    37      32
>>> pd.read_excel('ch05_data.xlsx',1)
   yellow  purple  blue  orange
A      11      16    44      22
B      20      22    23      44
C      30      31    37      32

```

这同样适用于写作。要在 Excel 上将数据帧转换成电子表格，您必须编写以下代码。

```
>>> frame = pd.DataFrame(np.random.random((4,4)),
...                      index = ['exp1','exp2','exp3','exp4'],
...                      columns = ['Jan2015','Fab2015','Mar2015','Apr2005'])
>>> frame
       Jan2015   Fab2015   Mar2015   Apr2005
exp1  0.030083  0.065339  0.960494  0.510847
exp2  0.531885  0.706945  0.964943  0.085642
exp3  0.981325  0.868894  0.947871  0.387600
exp4  0.832527  0.357885  0.538138  0.357990
>>> frame.to_excel('data2.xlsx')

```

在工作目录中，你会发现一个新的包含数据的 Excel 文件，如图 [5-3](#Fig3) 所示。

![../images/336498_2_En_5_Chapter/336498_2_En_5_Fig3_HTML.jpg](../images/336498_2_En_5_Chapter/336498_2_En_5_Fig3_HTML.jpg)

图 5-3

Excel 文件中的数据框架

## JSON 数据

JSON (JavaScript Object Notation)已经成为最常见的标准格式之一，尤其是对于网络上的数据传输。因此，如果您想在 Web 上使用数据，使用这种数据格式是正常的。

这种格式的特别之处在于它非常灵活，尽管它的结构远不是您所熟悉的那种，即表格。

在本节中，您将看到如何使用`read_json()`和`to_json()`函数来保持在本章讨论的 I/O API 函数中。但是在第二部分中，您将看到另一个例子，在这个例子中，您将不得不处理 JSON 格式的结构化数据，这与实际情况更加相关。

在我看来，检查 JSON 格式的一个有用的在线应用是 JSONViewer，可以在 [`http://jsonviewer.stack.hu/`](http://jsonviewer.stack.hu/) 找到。一旦您以 JSON 格式输入或复制数据，这个 web 应用程序就允许您查看您输入的格式是否有效。此外，它显示树形结构，以便您可以更好地理解其结构(参见图 [5-4](#Fig4) )。

![../images/336498_2_En_5_Chapter/336498_2_En_5_Fig4_HTML.jpg](../images/336498_2_En_5_Chapter/336498_2_En_5_Fig4_HTML.jpg)

图 5-4

json 查看器

让我们从更有用的情况开始，也就是说，当你有一个数据帧，你需要把它转换成一个 JSON 文件。因此，定义一个 dataframe，然后对其调用`to_json()`函数，将想要创建的文件的名称作为参数传递。

```
>>> frame = pd.DataFrame(np.arange(16).reshape(4,4),
...                      index=['white','black','red','blue'],
...                      columns=['up','down','right','left'])
>>> frame.to_json('frame.json')

```

在工作目录中，您会发现一个新的 JSON 文件(见清单 [5-12](#PC42) )，其中包含了转换成 JSON 格式的 dataframe 数据。

```
{"up":{"white":0,"black":4,"red":8,"blue":12},"down":{"white":1,"black":5,"red":9,"blue":13},"right":{"white":2,"black":6,"red":10,"blue":14},"left":{"white":3,"black":7,"red":11,"blue":15}}

Listing 5-12
frame.json

```

反过来也是可能的，使用带有文件名的`read_json()`作为参数。

```
>>> pd.read_json('frame.json')
       down  left  right  up
black     5     7      6   4
blue     13    15     14  12
red       9    11     10   8
white     1     3      2   0

```

您看到的例子是一个相当简单的例子，其中 JSON 数据是表格形式的(因为文件`frame.json`来自数据帧)。然而，一般来说，JSON 文件没有表格结构。因此，您需要以某种方式将结构文件`dict`转换成表格形式。这个过程叫做*正常化* *n* 。

pandas 库提供了一个名为`json_normalize()`的函数，它能够转换表格中的一个`dict`或一个列表。首先，您必须导入函数:

```
>>> from pandas.io.json import json_normalize

```

然后用任何文本编辑器编写一个 JSON 文件，如清单 [5-13](#PC45) 所示。在工作目录中保存为`books.json`。

```
[{"writer": "Mark Ross",
 "nationality": "USA",
 "books": [
         {"title": "XML Cookbook", "price": 23.56},
         {"title": "Python Fundamentals", "price": 50.70},
         {"title": "The NumPy library", "price": 12.30}
             ]
},
{"writer": "Barbara Bracket",
 "nationality": "UK",
 "books": [
         {"title": "Java Enterprise", "price": 28.60},
         {"title": "HTML5", "price": 31.35},
         {"title": "Python for Dummies", "price": 28.00}
             ]
}]

Listing 5-13books.json

```

如您所见，文件结构不再是表格形式，而是更加复杂。那么使用`read_json()`功能的方法不再有效。从这个例子中可以看出，您仍然可以从这个结构中获得表格形式的数据。首先，您必须加载 JSON 文件的内容，并将其转换成一个字符串。

```
>>> import json
>>> file = open('books.json','r')
>>> text = file.read()
>>> text = json.loads(text)

```

现在你已经准备好应用`json_normalize()`功能了。例如，通过快速查看 JSON 文件中的数据内容，您可能希望提取一个包含所有书籍的表。然后把`books`键写成第二个参数。

```
>>> json_normalize(text,'books')
   price                title
0  23.56         XML Cookbook
1  50.70  Python Fundamentals
2  12.30    The NumPy library
3  28.60      Java Enterprise
4  31.35                HTML5
5  28.00   Python for Dummies

```

该函数将读取所有以`books`为关键字的元素的内容。所有属性都将被转换为嵌套的列名，而相应的值将填充数据框架。对于索引，该函数分配一个递增的数字序列。

但是，您得到的数据帧只包含一些内部信息。将同一级别上的其他键的值相加会很有用。在这种情况下，您可以通过插入一个键列表作为函数的第三个参数来添加其他列。

```
>>> json_normalize(text,'books',['nationality','writer'])
   price                title nationality           writer
0  23.56         XML Cookbook         USA        Mark Ross
1  50.70  Python Fundamentals         USA        Mark Ross
2  12.30    The NumPy library         USA        Mark Ross
3  28.60      Java Enterprise          UK  Barbara Bracket
4  31.35                HTML5          UK  Barbara Bracket
5  28.00   Python for Dummies          UK  Barbara Bracket

```

现在结果是你从一个开始的树形结构中得到一个数据帧。

## HDF5 格式

到目前为止，您已经了解了如何以文本格式读写数据。当您的数据分析涉及大量数据时，最好使用二进制格式。Python 中有几个工具可以处理二进制数据。在这一领域取得一些成功的图书馆是`HDF5`图书馆。

HDF 术语代表*分层数据格式*，事实上这个库关注的是读取和写入包含节点结构的 HDF5 文件，以及存储多个数据集的可能性。

然而，这个完全用 C 开发的库也可以与其他类型的语言接口，如 Python、MATLAB 和 Java。它非常高效，尤其是在使用这种格式保存大量数据时。与其他更简单的二进制格式相比，HDF5 支持实时压缩，从而利用数据结构中的重复模式来压缩文件大小。

目前 Python 中可能的选择有`PyTables`和`h5py`。这两种形式在几个方面有所不同，因此它们的选择在很大程度上取决于使用者的需求。

`h5py`提供了与高级 APIs HDF5 的直接接口，而`PyTables`抽象了 HDF5 的许多细节，以提供更灵活的数据容器、索引表、查询功能和其他计算媒体。

pandas 有一个名为`HDFStore`的类类`dict`，使用`PyTables`来存储 pandas 对象。因此，在使用 HDF5 格式之前，您必须导入`HDFStore`类:

```
>>> from pandas.io.pytables import HDFStore

```

现在，您已经准备好将数据帧的数据存储在一个`.h5`文件中。首先，创建一个数据框架。

```
>>> frame = pd.DataFrame(np.arange(16).reshape(4,4),
...                      index=['white','black','red','blue'],
...                      columns=['up','down','right','left'])

```

现在创建一个调用 i t `mydata.h5`的文件 HDF5，然后在 dataframe 中输入数据。

```
>>> store = HDFStore('mydata.h5')
>>> store['obj1'] = frame
From here, you can guess how you can store multiple data structures within the same HDF5 file, specifying for each of them a label.
>>> frame
       up  down  right  left
white   0   0.5      1   1.5
black   2   2.5      3   3.5
red     4   4.5      5   5.5
blue    6   6.5      7   7.5
>>> store['obj2'] = frame

```

因此，使用这种格式，您可以在一个文件中存储多个数据结构，由`store`变量表示。

```
>>> store
<class 'pandas.io.pytables.HDFStore'>
File path: mydata.h5
/obj1            frame        (shape->[4,4])

```

甚至反过来的过程也很简单。考虑到有一个包含各种数据结构的 HDF5 文件，可以按以下方式调用其中的对象:

```
>>> store['obj2']
       up  down  right  left
white   0   0.5      1   1.5
black   2   2.5      3   3.5
red     4   4.5      5   5.5
blue    6   6.5      7   7.5

```

## pickle——Python 对象序列化

`pickle`模块实现了一个强大的算法，用于 Python 中实现的数据结构的序列化和反序列化。酸洗是将对象的层次结构转换成字节流的过程。

这允许对象被传输和存储，然后由接收器本身重建，保留所有原始特征。

在 Python 中，挑选操作由`pickle`模块执行，但目前有一个名为`cP` `ickle`的模块，它是优化`pickle`模块(用 C 语言编写)的大量工作的结果。事实上，这个模块在很多情况下甚至比`pickle`模块快 1000 倍。然而，无论您使用哪个模块，这两个模块的接口几乎是相同的。

在开始明确提及 pandas 基于这种格式的 I/O 函数之前，让我们更详细地看看`cPickle`模块，看看如何使用它。

### 用 cPickle 序列化 Python 对象

`pickle`(或`cPickle`)模块使用的数据格式是 Python 特有的。默认情况下，使用 ASCII 表示来表示它，以便从人的角度来看是可读的。然后，通过用文本编辑器打开一个文件，您也许能够理解它的内容。要使用此模块，您必须首先导入它:

```
>>> import pickle

```

然后创建一个足够复杂的对象来拥有一个内部数据结构，例如一个`dict`对象。

```
>>> data = { 'color': ['white','red'], 'value': [5, 7]}

```

现在，您将通过`cPickle`模块的`dumps()`函数来执行`data`对象的序列化。

```
>>> pickled_data = pickle.dumps(data)

```

现在，要查看它如何序列化`dict`对象，您需要查看`pickled_data`变量的内容。

```
>>> print(pickled_data)
(dp1
S'color'
p2
(lp3
S'white'
p4
aS'red'
p5
asS'value'
p6
(lp7
I5
aI7
as.

```

一旦有了序列化的数据，就可以很容易地将它们写入文件或通过套接字、管道等发送。

被传输后，可以用`cPickle`模块的`loads()`函数重构序列化的对象(反序列化)。

```
>>> nframe = pickle.loads(pickled_data)
>>> nframe
{'color': ['white', 'red'], 'value': [5, 7]}

```

### 用熊猫腌制

当谈到与熊猫图书馆的交流时，一切都变得简单多了。不需要在 Python 会话中导入`cPickle`模块，整个操作都是隐式执行的。

还有，熊猫使用的序列化格式不完全是 ASCII。

```
>>> frame = pd.DataFrame(np.arange(16).reshape(4,4), index = ['up','down','left','right'])
>>> frame.to_pickle('frame.pkl')

```

在您的工作目录中有一个名为`frame.pkl`的新文件，它包含了关于`frame`数据帧的所有信息。

要打开一个 PKL 文件并阅读其内容，只需使用以下命令:

```
>>> pd.read_pickle('frame.pkl')
        0   1   2   3
up      0   1   2   3
down    4   5   6   7
left    8   9  10  11
right  12  13  14  15

```

正如您所看到的，对 pandas 用户来说，酸洗和拆洗操作的所有含义都是完全隐藏的，对于那些必须专门处理数据分析的人来说，这使得工作尽可能简单和容易理解。

### 注意

使用这种格式时，请确保打开的文件是安全的。事实上，pickle 格式并不是为防止错误和恶意构造的数据而设计的。

## 与数据库交互

在许多应用程序中，数据很少来自文本文件，因为这肯定不是存储数据的最有效方式。

数据通常存储在基于 SQL 的关系数据库中，也存储在最近变得非常流行的许多替代 NoSQL 数据库中。

将 SQL 中的数据加载到 dataframe 中非常简单，pandas 提供了一些函数来简化这个过程。

`pandas.io.sql`模块提供了一个独立于数据库的统一接口，称为`sqlalchemy`。该接口简化了连接模式，因为不管 DB 如何，命令总是相同的。使用`create_engine()`功能进行连接。通过该特性，您可以配置使用驱动程序所需的所有属性，如用户、密码、端口和数据库实例。

以下是各种类型数据库的示例列表:

```
>>> from sqlalchemy import create_engine

```

对于 PostgreSQL:

```
>>> engine = create_engine('postgresql://scott:tiger@localhost:5432/mydatabase')
For MySQL
>>> engine = create_engine('mysql+mysqldb://scott:tiger@localhost/foo')
For Oracle
>>> engine = create_engine('oracle://scott:tiger@127.0.0.1:1521/sidname')
For MSSQL
>>> engine = create_engine('mssql+pyodbc://mydsn')
For SQLite
>>> engine = create_engine('sqlite:///foo.db')

```

### 使用 SQLite3 加载和写入数据

作为第一个例子，您将使用驱动程序内置的 Python `sqlite3`来使用 SQLite 数据库。SQL ite3 是一个以非常简单和轻量级的方式实现 DBMS SQL 的工具，因此它可以集成到任何用 Python 语言实现的应用程序中。事实上，这个实用的软件允许你在一个文件中创建一个嵌入式数据库。

这使得它成为任何想拥有数据库功能而不必安装真正数据库的人的完美工具。对于那些想在使用真正的数据库之前进行实践的人，或者对于那些需要使用数据库的功能来收集数据，但仍留在单个程序中，而不必与数据库交互的人来说，SQLite3 可能是正确的选择。

创建一个 dataframe，用于在 SQLite3 数据库上创建新表。

```
>>> frame = pd.DataFrame( np.arange(20).reshape(4,5),
...                       columns=['white','red','blue','black','green'])
>>> frame
   white  red  blue  black  green
0      0    1     2      3      4
1      5    6     7      8      9
2     10   11    12     13     14
3     15   16    17     18     19

```

现在是时候实现到 SQLite3 数据库的连接了。

```
>>> engine = create_engine('sqlite:///foo.db')

```

转换数据库表中的数据帧。

```
>>> frame.to_sql('colors',engine)

```

相反，要读取数据库，您必须使用带有表名的`read_sql()`函数和`engine`。

```
>>> pd.read_sql('colors',engine)
   index  white  red  blue  black  green
0      0      0    1     2      3      4
1      1      5    6     7      8      9
2      2     10   11    12     13     14
3      3     15   16    17     18     19

```

正如您所看到的，即使在这种情况下，由于 pandas 库中可用的 I/O API，对数据库的写操作也变得非常简单。

现在，您将看到相同的操作，但不是使用 I/O API。这对于了解 pandas 是如何被证明是一个向数据库读写数据的有效工具非常有用。

首先，您必须建立到数据库的连接，并通过定义正确的数据类型来创建一个表，以便容纳要加载的数据。

```
>>> import sqlite3
>>> query = """
... CREATE TABLE test
... (a VARCHAR(20), b VARCHAR(20),
...  c REAL,        d INTEGER
... );"""
>>> con = sqlite3.connect(':memory:')
>>> con.execute(query)
<sqlite3.Cursor object at 0x0000000009E7D730>
>>> con.commit()

```

现在您可以使用 SQL `INSERT`语句输入数据。

```
>>> data = [('white','up',1,3),
...         ('black','down',2,8),
...         ('green','up',4,4),
...         ('red','down',5,5)]
>>> stmt = "INSERT INTO test VALUES(?,?,?,?)"
>>> con.executemany(stmt, data)
<sqlite3.Cursor object at 0x0000000009E7D8F0>
>>> con.commit()

```

既然您已经看到了如何在表上加载数据，那么是时候看看如何查询数据库以获得您刚刚记录的数据了。这可以使用 SQL `SELECT`语句来实现。

```
>>> cursor = con.execute('select * from test')
>>> cursor
<sqlite3.Cursor object at 0x0000000009E7D730>
>>> rows = cursor.fetchall()
>>> rows
[(u'white', u'up', 1.0, 3), (u'black', u'down', 2.0, 8), (u'green', u'up', 4.0, 4), (u'red', 5.0, 5)]

```

可以将元组列表传递给 dataframe 的构造函数，如果需要列名，可以在游标的`description`属性中找到它们。

```
>>> cursor.description
(('a', None, None, None, None, None, None), ('b', None, None, None, None, None, None), ('c
one, None, None, None, None), ('d', None, None, None, None, None, None))
>>> pd.DataFrame(rows, columns=zip(*cursor.description)[0])
       a     b  c  d
0  white    up  1  3
1  black  down  2  8
2  green    up  4  4
3    red  down  5  5

```

如您所见，这种方法相当费力。

### 使用 PostgreSQL 加载和写入数据

从 pandas 0.14 开始，也支持 PostgreSQL 数据库。因此，请仔细检查您电脑上的版本是否与此版本或更高版本一致。

```
>>> pd.__version__
>>> '0.22.0'

```

若要运行此示例，您必须在系统上安装一个 Pos tgreSQL 数据库。在我的例子中，我创建了一个名为`postgres`的数据库，用`postgres`作为用户，用`password`作为密码。将这些值替换为与您的系统相对应的值。

要做的第一件事是安装`psycopg2`库，它被设计用来管理和处理与数据库的连接。

有了蟒蛇:

```
conda install psycopg2

```

或者，如果您使用的是 PyPi:

```
pip install psycopg2

```

现在，您可以建立与数据库的连接:

```
>>> import psycopg2
>>> engine = create_engine('postgresql://postgres:password@localhost:5432/postgres')

```

### 注意

在此示例中，根据您在 Windows 上安装软件包的方式，您通常会收到以下错误消息:

`from psycopg2._psycopg import BINARY, NUMBER, STRING, DATETIME, ROWID`

`ImportError: DLL load failed: The specified module could not be found.`

这可能意味着您的`PATH`中没有 PostgreSQL DLLs(特别是`libpq.dll`)。将一个`postgres\x.x\bin`目录添加到您的`PATH`中，您应该能够从 Python 连接到您的 PostgreSQL 安装。

创建 dataframe 对象:

```
>>> frame = pd.DataFrame(np.random.random((4,4)),
              index=['exp1','exp2','exp3','exp4'],
              columns=['feb','mar','apr','may']);

```

现在我们看到了将这些数据转移到表格中是多么容易。用`to_sql()`你将在一个叫做`dataframe`的表格中记录数据。

```
>>> frame.to_sql('dataframe',engine)

```

pgAdmin III 是一个用于管理 PostgreSQL 数据库的图形应用程序。这是一个非常有用的工具，在 Linux 和 Windows 上都有。使用该应用程序，很容易看到您刚刚创建的表格数据框(参见图 [5-5](#Fig5) )。

![../images/336498_2_En_5_Chapter/336498_2_En_5_Fig5_HTML.jpg](../images/336498_2_En_5_Chapter/336498_2_En_5_Fig5_HTML.jpg)

图 5-5

pgAdmin II I 应用程序是 PostgreSQL 的一个完美的图形化数据库管理器

如果您非常了解 SQL 语言，查看新创建的表及其内容的一种更经典的方式是使用`psql`会话。

```
>>> psql -U postgres

```

在我的例子中，我连接到了`postgres`用户；你的情况可能不一样。连接到数据库后，对新创建的表执行 SQL 查询。

```
postgres=# SELECT * FROM DATAFRAME;
index|       feb       |       mar       |       apr       |       may
-----+-----------------+-----------------+-----------------+-----------------
exp1 |0.757871296789076|0.422582915331819|0.979085739226726|0.332288515791064
exp2 |0.124353978978927|0.273461421503087|0.049433776453223|0.0271413946693556
exp3 |0.538089036334938|0.097041417119426|0.905979807772598|0.123448718583967
exp4 |0.736585422687497|0.982331931474687|0.958014824504186|0.448063967996436
(4 righe)

```

即使转换数据帧中的表也是一个微不足道的操作。甚至这里还有一个直接读取数据库并返回数据帧的`read_sql_table()`函数。

```
>>> pd.read_sql_table('dataframe',engine)
  index       feb       mar       apr       may
0  exp1  0.757871  0.422583  0.979086  0.332289
1  exp2  0.124354  0.273461  0.049434  0.027141
2  exp3  0.538089  0.097041  0.905980  0.123449
3  exp4  0.736585  0.982332  0.958015  0.448064

```

但是当您想要读取数据库中的数据时，将整个单个表转换成 dataframe 并不是最有用的操作。事实上，那些使用关系数据库的人更喜欢使用 SQL 语言来选择通过插入 SQL 查询来导出什么数据以及以什么形式导出数据。

SQL 查询的文本可以集成到`read_sql_query()`函数中。

```
>>> pd.read_sql_query('SELECT index,apr,may FROM DATAFRAME WHERE apr > 0.5',engine)
  index       apr       may
0  exp1  0.979086  0.332289
1  exp3  0.905980  0.123449
2  exp4  0.958015  0.448064

```

## 使用 NoSQL 数据库读写数据:MongoDB

在所有的 NoSQL 数据库(BerkeleyDB、东京内阁和 MongoDB)中，MongoDB 正在成为最广泛的数据库。考虑到它在许多系统中的扩散，考虑在数据分析期间读写由 pandas 库产生的数据的可能性似乎是合适的。

首先，如果您的 PC 上安装了 MongoDB，那么您可以启动服务来指向给定的目录。

```
mongod --dbpath C:\MongoDB_data

```

现在服务正在监听端口 27017，您可以使用 MongoDB 的官方驱动程序连接到这个数据库:`pymongo`。

```
>>> import pymongo
>>> client = MongoClient('localhost',27017)

```

MongoDB 的单个实例能够同时支持多个数据库。所以现在你需要指向一个特定的数据库。

```
>>> db = client.mydatabase
>>> db
Database(MongoClient('localhost', 27017), u'mycollection')
In order to refer to this object, you can also use
>>> client['mydatabase']
Database(MongoClient('localhost', 27017), u'mydatabase')

```

现在您已经定义了数据库，您必须定义集合。集合是存储在 MongoDB 中的一组文档，可以被视为 SQL 数据库中的表的等价物。

```
>>> collection = db.mycollection
>>> db['mycollection']
Collection(Database(MongoClient('localhost', 27017), u'mydatabase'), u'mycollection')
>>> collection
Collection(Database(MongoClient('localhost', 27017), u'mydatabase'), u'mycollection')
Now it is the time to load the data in the collection. Create a DataFrame.
>>> frame = pd.DataFrame( np.arange(20).reshape(4,5),
...                       columns=['white','red','blue','black','green'])
>>> frame
   white  red  blue  black  green
0      0    1     2      3      4
1      5    6     7      8      9
2     10   11    12     13     14
3     15   16    17     18     19

```

在添加到集合之前，必须将其转换为 JSON 格式。转换过程没有你想象的那么直接；这是因为您需要设置要记录在 DB 上的数据，以便尽可能公平和简单地将其重新提取为数据帧。

```
>>> import json
>>> record = json.loads(frame.T.to_json()).values()
>>> record
[{u'blue': 7, u'green': 9, u'white': 5, u'black': 8, u'red': 6}, {u'blue': 2, u'green': 4, u'white':
 0, u'black': 3, u'red': 1}, {u'blue': 17, u'green': 19, u'white': 15, u'black': 18, u'red': 16}, {u
'blue': 12, u'green': 14, u'white': 10, u'black': 13, u'red': 11}]
Now you are finally ready to insert a document in the collection, and you can do this with the insert() fu

nction.
>>> collection.mydocument.insert(record)
[ObjectId('54fc3afb9bfbee47f4260357'), ObjectId('54fc3afb9bfbee47f4260358'), ObjectId('54fc3afb9bfbee47f4260359'), Obj

ectId('54fc3afb9bfbee47f426035a')]

```

如您所见，记录的每一行都有一个对象。现在数据已经加载到 MongoDB 数据库的文档中，您可以执行相反的过程，即读取文档中的数据，然后将它们转换成 dataframe。

```
>>> cursor = collection['mydocument'].find()
>>> dataframe = (list(cursor))
>>> del dataframe['_id']
>>> dataframe
   black  blue  green  red  white
0      8     7      9    6      5
1      3     2      4    1      0
2     18    17     19   16     15
3     13    12     14   11     10

```

您已经删除了包含 Mon goDB 内部参考 ID 号的列。

## 结论

在本章中，您了解了如何使用 pandas 库的 I/O API 特性，以便在保存数据帧结构的同时向文件和数据库读写数据。特别说明了根据格式类型读写数据的几种模式。

在本章的最后一部分，您看到了如何连接到最流行的数据库模型，以数据帧的形式直接记录和/或读取数据，以便用 pandas 工具进行处理。

在下一章，你将看到图书馆熊猫的最先进的功能。像 GroupBy 这样的复杂仪器和其他形式的数据处理被详细讨论。