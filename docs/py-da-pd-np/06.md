# 6.深度熊猫:数据处理

在前一章中，您看到了如何从数据库和文件等数据源获取数据。一旦有了数据帧格式的数据，就可以对它们进行操作了。准备数据很重要，这样它们可以更容易地进行分析和处理。特别是在为下一阶段做准备时，数据必须为可视化做好准备。

在这一章中，你将深入了解 pandas 库为这一阶段的数据分析提供的功能。数据操作的三个阶段将被单独处理，用一系列的例子说明各种操作，以及如何最好地使用这个库的功能来执行这样的操作。数据操作的三个阶段是:

*   数据准备

*   数据转换

*   数据聚合

## 数据准备

在开始操作数据之前，有必要准备数据并以数据结构的形式组装它们，以便以后可以使用 pandas 库提供的工具来操作它们。这里列出了数据准备的不同程序。

*   装货

*   装配
    *   合并

    *   连结

    *   结合

*   重塑(旋转)

*   消除

上一章介绍了装载。在加载阶段，还有一部分准备工作是将许多不同的格式转换成数据结构，如数据帧。但是，即使你有了可能来自不同来源和格式的数据，并将其统一到一个数据框架中，你还需要进行进一步的准备工作。在本章中，尤其是在这一节中，您将看到如何执行所有必要的操作来将数据放入一个统一的数据结构中。

熊猫对象中包含的数据可以用不同的方式组合:

*   *合并*—`pandas.merge()`函数根据一个或多个键连接数据帧中的行。熟悉 SQL 语言的人对这种模式非常熟悉，因为它也实现了连接操作。

*   *串联*—`pandas.concat`()函数沿轴串联对象。

*   *组合*—`pandas.DataFrame.combine_first()`函数是一种方法，允许您连接重叠的数据，以便通过从另一个结构中获取数据来填充数据结构中缺失的值。

此外，准备过程的一部分也是旋转，包括行和列之间的交换。

### 合并

对于熟悉 SQL 的人来说，合并操作相当于联接操作，它通过使用一个或多个键连接行来组合数据。

事实上，任何使用关系数据库的人通常使用 SQL 的`JOIN`查询来从不同的表中获取数据，这些表使用它们之间共享的一些引用值(键)。在这些关键字的基础上，可以获得表格形式的新数据，作为其他表格的组合结果。对库熊猫的这个操作叫做*合并*，而`merge()`就是执行这种操作的函数。

首先，您必须导入 pandas 库，并定义两个数据帧作为本节的示例。

```py
>>> import numpy as np
>>> import pandas as pd
>>> frame1 = pd.DataFrame( {'id':['ball','pencil','pen','mug','ashtray'],
...                      'price': [12.33,11.44,33.21,13.23,33.62]})
>>> frame1
        id  price
0     ball  12.33
1   pencil  11.44
2      pen  33.21
3      mug  13.23
4  ashtray  33.62
>>> frame2 = pd.DataFrame( {'id':['pencil','pencil','ball','pen'],
...                      'color': ['white','red','red','black']})
>>> frame2
   color      id
0  white  pencil
1    red  pencil
2    red    ball
3  black     pen

```

通过对两个 dataframe 对象应用`merge()`函数进行合并。

```py
>>> pd.merge(frame1,frame2)
       id  price  color
0    ball  12.33    red
1  pencil  11.44  white
2  pencil  11.44    red
3     pen  33.21  black

```

正如您从结果中看到的，返回的数据帧包含所有具有共同的`ID`的行。除了公共列之外，还添加了来自第一个和第二个数据帧的列。

在这种情况下，您使用了`merge()`函数，而没有显式指定任何列。事实上，在大多数情况下，您需要决定合并所基于的列。

为此，添加`on`选项，将列名作为合并的键。

```py
>>> frame1 = pd.DataFrame( {'id':['ball','pencil','pen','mug','ashtray'],
...                      'color': ['white','red','red','black','green'],
...                      'brand': ['OMG','ABC','ABC','POD','POD']})
>>> frame1
  brand  color       id
0   OMG  white     ball
1   ABC    red   pencil
2   ABC    red      pen
3   POD  black      mug
4   POD  green  ashtray
>>> frame2 = pd.DataFrame( {'id':['pencil','pencil','ball','pen'],
...                      'brand': ['OMG','POD','ABC','POD']})
>>> frame2
  brand      id
0   OMG  pencil
1   POD  pencil
2   ABC    ball
3   POD     pen

```

现在，在这种情况下，您有两个数据帧，它们的列具有相同的名称。因此，如果您启动合并，您不会得到任何结果。

```py
>>> pd.merge(frame1,frame2)
Empty DataFrame
Columns: [brand, color, id]
In

dex: []

```

有必要明确定义 pandas 必须遵循的合并标准，在`on`选项中指定键列的名称。

```py
>>> pd.merge(frame1,frame2,on='id')
  brand_x  color      id brand_y
0     OMG  white    ball     ABC
1     ABC    red  pencil     OMG
2     ABC    red  pencil     POD
3     ABC    red     pen     POD
>>> pd.merge(frame1,frame2,on='brand')
  brand  color     id_x    id_y
0   OMG  white     ball  pencil
1   ABC    red   pencil    ball
2   ABC    red      pen    ball
3   POD  black      mug  pencil
4   POD  black      mug     pen
5   POD  green  ashtray  pencil
6   POD  green  ashtray     pen

```

不出所料，根据合并的标准，结果会有很大的不同。

然而，经常会出现相反的问题，即有两个数据帧，其中的键列没有相同的名称。为了补救这种情况，您必须使用`left_on`和`right_on`选项，它们为第一个和第二个数据帧指定键列。现在你可以看到一个例子。

```py
>>> frame2.columns = ['brand','sid']
>>> frame2
  brand     sid
0   OMG  pencil
1   POD  pencil
2   ABC    ball
3   POD     pen
>>> pd.merge(frame1, frame2, left_on="id", right_on="sid")
  brand_x  color      id brand_y     sid
0     OMG  white    ball     ABC    ball
1     ABC    red  pencil     OMG  pencil
2     ABC    red  pencil     POD  pencil
3     ABC    red     pen     POD     pen

```

默认情况下，`merge()`函数执行一个*内部连接；*结果中的键是交集的结果。

其他可能的选项有*左连接*、*右连接、*和*外连接*。外部连接产生所有键的联合，结合了左连接和右连接的效果。要选择连接类型，必须使用`how`选项。

```py
>>> frame2.columns = ['brand','id']
>>> pd.merge(frame1,frame2,on='id')
  brand_x  color      id brand_y
0     OMG  white    ball     ABC
1     ABC    red  pencil     OMG
2     ABC    red  pencil     POD
3     ABC    red     pen     POD
>>> pd.merge(frame1,frame2,on='id',how='outer')
  brand_x  color       id brand_y
0     OMG  white     ball     ABC
1     ABC    red   pencil     OMG
2     ABC    red   pencil     POD
3     ABC    red      pen     POD
4     POD  black      mug     NaN
5     POD  green  ashtray     NaN
>>> pd.merge(frame1,frame2,on='id',how='left')
  brand_x  color       id brand_y
0     OMG  white     ball     ABC
1     ABC    red   pencil     OMG
2     ABC    red   pencil     POD
3     ABC    red      pen     POD
4     POD  black      mug     NaN
5     POD  green  ashtray     NaN
>>> pd.merge(frame1,frame2,on='id',how='right')
  brand_x  color      id brand_y
0     OMG  white    ball     ABC
1     ABC    red  pencil     OMG
2     ABC    red  pencil     POD
3     ABC    red     pen     POD

```

要合并多个键，只需在`on`选项中添加一个列表。

```py
>>> pd.merge(frame1,frame2,on=['id','brand'],how='outer')
  brand  color   

    id
0   OMG  white     ball
1   ABC    red   pencil
2   ABC    red      pen
3   POD  black      mug
4   POD  green  ashtray
5   OMG    NaN   pencil
6   POD    NaN   pencil
7   ABC    NaN     ball
8   POD    NaN      pen

```

#### 在索引上合并

在某些情况下，索引可以用作合并的键，而不是将数据帧的列视为键。然后，为了决定考虑哪些索引，您将`left_index`或`right_index`选项设置为`True`来激活它们，并且能够同时激活它们。

```py
>>> pd.merge(frame1,frame2,right_index=True, left_index=True)
  brand_x  color    id_x brand_y    id_y
0     OMG  white    ball     OMG  pencil
1     ABC    red  pencil     POD  pencil
2     ABC    red     pen     ABC    ball
3     POD  black     mug     POD     pen

```

但是 dataframe 对象有一个`join()`函数，当您想通过索引进行合并时，这个函数要方便得多。它还可以用于组合许多具有相同或相同索引但没有列重叠的 dataframe 对象。

```py
In fact, if you launch

>>> frame1.join(frame2)

```

您将得到一个错误代码，因为 frame1 中的某些列与 frame2 同名。然后在启动 **join** **()** 功能之前，重命名 frame2 中的列。

```py
>>> frame2.columns = ['brand2','id2']
>>> frame1.join(frame2)
  brand  color       id brand2     id2
0   OMG  white     ball    OMG  pencil
1   ABC    red   pencil    POD  pencil
2   ABC    red      pen    ABC    ball
3   POD  black      mug    POD     pen
4   POD  green  ashtray    NaN     NaN

```

这里您已经执行了合并，但是基于索引的值而不是列。这次还有只出现在帧 1 中的索引 4，但是对应于帧 2 的列的值将`NaN`报告为一个值。

## 连结

另一种类型的数据组合被称为*串联*。NumPy 提供了一个`concatenate()`函数来对数组进行这种操作。

```py
>>> array1 = np.arange(9).reshape((3,3))
>>> array1
array([[0, 1, 2],
       [3, 4, 5],
       [6, 7, 8]])
>>> array2 = np.arange(9).reshape((3,3))+6
>>> array2
array([[ 6,  7,  8],
       [ 9, 10, 11],
       [12, 13, 14]])
>>> np.concatenate([array1,array2],axis=1)
array([[ 0,  1,  2,  6,  7,  8],
       [ 3,  4,  5,  9, 10, 11],
       [ 6,  7,  8, 12, 13, 14]])
>>> np.concatenate([array1,array2],axis=0)
array([[ 0,  1,  2],
       [ 3,  4,  5],
       [ 6,  7,  8],
       [ 6,  7,  8],
       [ 9, 10, 11],
       [12, 13, 14]])

```

有了 pandas 库及其数据结构(如 series 和 dataframe ),有了带标签的轴就可以进一步概括数组的连接。`concat()`功能是熊猫为这种操作提供的。

```py
>>> ser1 = pd.Series(np.random.rand(4), index=[1,2,3,4])
>>> ser1
1    0.636584
2    0.345030
3    0.157537
4    0.070351
dtype: float64
>>> ser2 = pd.Series(np.random.rand(4), index=[5,6,7,8])
>>> ser2
5    0.411319
6    0.359946
7    0.987651
8    0.329173
dtype: float64
>>> pd.concat([ser1,ser2])
1    0.636584
2    0.345030
3    0.157537
4    0.070351
5    0.411319
6    0.359946
7    0.987651
8    0.329173
dtype: float64

```

默认情况下，`concat(` **)** 函数作用于`axis = 0`，返回的对象是一系列。如果你设置了`axis = 1`，那么结果将是一个数据帧。

```py
>>> pd.concat([ser1,ser2],axis=1)
          0         1
1  0.636584       NaN
2  0.345030       NaN
3  0.157537       NaN
4  0.070351       NaN
5       NaN  0.411319
6       NaN  0.359946
7       NaN  0.987651
8       NaN  0.329173

```

这种操作的问题是连接的部分在结果中是不可识别的。例如，您希望在串联轴上创建一个分层索引。为此，您必须使用`keys`选项。

```py
>>> pd.concat([ser1,ser2], keys=[1,2])
1  1    0.636584
   2    0.345030
   3    0.157537
   4    0.070351
2  5    0.411319
   6    0.359946
   7    0.987651
   8    0.329173
dtype: float64

```

对于沿`axis = 1`的系列之间的组合，关键字成为数据帧的列标题。

```py
>>> pd.concat([ser1,ser2], axis=1, keys=[1,2])
          1         2
1  0.636584       NaN
2  0.345030       NaN
3  0.157537       NaN
4  0.070351       NaN
5       NaN  0.411319
6       NaN  0.359946
7       NaN  0.987651
8       NaN  0.329173

```

到目前为止，您已经看到了应用于系列的串联，但是同样的逻辑也可以应用于数据帧。

```py
>>> frame1 = pd.DataFrame(np.random.rand(9).reshape(3,3), index=[1,2,3], columns=['A','B','C'])
>>> frame2 = pd.DataFrame(np.random.rand(9).reshape(3,3), index=[4,5,6], columns=['A','B','C'])
>>> pd.concat([frame1, frame2])
          A         B         C
1  0.400663  0.937932  0.938035
2  0.202442  0.001500  0.231215
3  0.940898  0.045196  0.723390
4  0.568636  0.477043  0.913326
5  0.598378  0.315435  0.311443
6  0.619859  0.198060  0.647902

>>> pd.concat([frame1, frame2], axis=1)
          A         B         C         A         B         C
1  0.400663  0.937932  0.938035       NaN       NaN       NaN
2  0.202442  0.001500  0.231215       NaN       NaN       NaN
3  0.940898  0.045196  0.723390       NaN       NaN       NaN
4       NaN       NaN       NaN  0.568636  0.477043  0.913326
5       NaN       NaN       NaN  0.598378  0.315435  0.311443
6       NaN       NaN       NaN  0.619859  0.198060  0.647902

```

### 结合

存在另一种情况，其中存在不能通过合并或拼接获得的数据组合。假设您希望两个数据集的索引完全重叠或至少部分重叠。

一个适用于 series 的函数是`combine_first()`，它执行这种操作以及数据对齐。

```py
>>> ser1 = pd.Series(np.random.rand(5),index=[1,2,3,4,5])
>>> ser1
1    0.942631
2    0.033523
3    0.886323
4    0.809757
5    0.800295
dtype: float64
>>> ser2 = pd.Series(np.random.rand(4),index=[2,4,5,6])
>>> ser2
2    0.739982
4    0.225647
5    0.709576
6    0.214882
dtype: float64
>>> ser1.combine_first(ser2)
1    0.942631
2    0.033523
3    0.886323
4    0.809757
5    0.800295
6    0.214882
dtype: float64
>>> ser2.combine_first(ser1)
1    0.942631
2    0.739982
3    0.886323
4    0.225647
5    0.709576
6    0.214882
dtype: float64

```

相反，如果需要部分重叠，可以只指定系列中要重叠的部分。

```py
>>> ser1[:3].combine_first(ser2[:3])
1    0.942631
2    0.033523
3    0.886323
4    0.225647
5    0.709576
dtype: float64

```

### 绕轴旋转

除了组合数据以便统一从不同来源收集的值之外，另一个相当常见的操作是*旋转*。事实上，按行或按列排列值并不总是适合您的目标。有时，您希望按行上的列值重新排列数据，反之亦然。

#### 使用分层索引进行旋转

您已经看到数据帧可以支持分层索引。可以利用这一特性来重新排列数据帧中的数据。在旋转的上下文中，有两个基本操作:

*   *堆叠*—旋转或旋转数据结构，将列转换为行

*   *拆分*—将行转换成列

```py
>>> frame1 = pd.DataFrame(np.arange(9).reshape(3,3),
...                    index=['white','black','red'],
...                    columns=['ball','pen','pencil'])
>>> frame1
       ball  pen  pencil
white     0    1       2
black     3    4       5
red       6    7       8

```

使用数据框上的`stack()`功能，您将获得行中列的旋转，从而产生一系列:

```py
>>> ser5 = frame1.stack()
white  ball      0
       pen       1
       pencil    2
black  ball      3
       pen       4
       pencil    5
red    ball      6
       pen       7
       pencil    8
dtype: int32

```

通过使用`unstack()`函数，您可以从这个分层索引的序列中将数据帧重组为一个透视表。

```py
>>> ser5.unstack()
       ball  pen  pencil
white     0    1       2
black     3    4       5
red       6    7       8

```

您也可以在不同的级别上进行拆分，将级别数或其名称指定为函数的参数。

```py
>>> ser5.unstack(0)
        white  black  red
ball        0      3    6
pen         1      4    7
pencil      2      5    8

```

#### 从“长”格式转换到“宽”格式

存储数据集最常见的方式是通过准时注册数据来产生，这些数据将填充文本文件的一行，例如 CSV 或数据库的一个表。当您有仪器读数、随时间迭代的计算结果或一系列值的简单手动输入时，尤其会发生这种情况。这些文件的一个类似例子是日志文件，通过在其中累积数据来逐行填充。

这种类型的数据集的独特特征是在不同的列上有条目，这些条目通常在后续的行中重复。总是保持表格格式的数据，当你在这种情况下你可以把它们称为*长*或*叠*格式。

为了更清楚地了解这一点，请考虑下面的数据框架。

```py
>>> longframe = pd.DataFrame({ 'color':['white','white','white',
...                                  'red','red','red',
...                                  'black','black','black'],
...                         'item':['ball','pen','mug',
...                                 'ball','pen','mug',
...                                 'ball','pen','mug'],
...                         'value': np.random.rand(9)})
>>> longframe
   color  item     value
0  white  ball  0.091438
1  white   pen  0.495049
2  white   mug  0.956225
3    red  ball  0.394441
4    red   pen  0.501164
5    red   mug  0.561832
6  black  ball  0.879022
7  black   pen  0.610975
8  black   mug  0.093324

```

这种数据记录模式有一些缺点。例如，一个是某些领域的多重性和重复。将列视为键，这种格式的数据将难以阅读，特别是在完全理解键值和其余列之间的关系时。

除了长格式之外，还有另一种方法来在一个叫做*宽*的表格中排列数据。这种模式更容易阅读，允许与其他表轻松连接，并且它占用的空间少得多。因此，总的来说，这是一种更有效的存储数据的方式，尽管不太实用，尤其是在填充数据时。

作为一个标准，选择一列或一组列作为主键；然后，其中包含的值必须是唯一的。

在这方面，pandas 提供了一个函数，允许您将数据帧从长类型转换为宽类型。这个函数是`pivot()`，它接受一列或多列作为参数，这些列将承担键的角色。

从前面的例子开始，您选择创建一个宽格式的数据帧，方法是选择`color`列作为键，选择`item`作为第二个键，它们的值将构成数据帧的新列。

```py
>>> wideframe = longframe.pivot('color','item')
>>> wideframe
          value
item       ball       mug       pen
color
black  0.879022  0.093324  0.610975
red    0.394441  0.561832  0.501164
white  0.091438  0.956225  0.495049

```

正如您现在看到的，在这种格式下，数据帧更加紧凑，其中包含的数据可读性更好。

### 消除

数据准备的最后一个阶段是删除列和行。这部分你已经在第 [4](04.html) 章看到了。然而，为了完整起见，这里重复了描述。通过举例的方式定义数据帧。

```py
>>> frame1 = pd.DataFrame(np.arange(9).reshape(3,3),
...                    index=['white','black','red'],
...                    columns=['ball','pen','pencil'])
>>> frame1
       ball  pen  pencil
white     0    1       2
black     3    4       5
red       6    7       8

```

要删除一列，只需使用指定列名的数据帧上的`del`命令。

```py
>>> del frame1['ball']
>>> frame1
       pen  pencil
white    1       2
black    4       5
red      7       8

```

相反，要删除不需要的行，必须使用`drop()`函数，将相应索引的标签作为参数。

```py
>>> frame1.drop('white')
       pen  pencil
black    4       5
red      7       8

```

## 数据转换

到目前为止，您已经看到了如何为分析准备数据。这一过程实际上代表了一个数据帧中所含数据的重新组合，可能添加其他数据帧并删除不需要的部分。

现在我们开始数据操作的第二阶段:数据**转换* *。*在您安排好数据的形式及其在数据结构中的处理方式后，转换它们的值是很重要的。事实上，在本节中，您将看到一些常见问题以及使用 pandas 库的功能来克服这些问题所需的步骤。*

 *其中一些操作涉及重复或无效值的存在，并可能被删除或替换。其他操作则通过修改索引来实现。其他步骤包括处理数据和字符串的数值。

### 删除重复项

由于各种原因，数据帧中可能会出现重复行。在巨大的数据帧中，这些行的检测可能是非常困难的。在这种情况下，pandas 提供了一系列工具来分析大型数据结构中的重复数据。

首先，创建一个包含一些重复行的简单 dataframe。

```py
>>> dframe = pd.DataFrame({ 'color': ['white','white','red','red','white'],
...                      'value': [2,1,3,3,2]})
>>> dframe
   color  value
0  white      2
1  white      1
2    red      3
3    red      3
4  white      2

```

应用于数据帧的`duplicated()`函数可以检测看起来重复的行。它返回一系列布尔值，其中每个元素对应一行，如果该行是重复的(即，只有其他的出现，而不是第一个)，则返回`True`，如果前面的元素中没有重复的，则返回`False`。

```py
>>> dframe.duplicated()
0    False
1    False
2    False
3     True
4     True
dtype: bool

```

在许多情况下，将布尔序列作为返回值是很有用的，尤其是对于过滤。事实上，如果您想知道哪些是重复的行，只需键入以下内容:

```py
>>> dframe[dframe.duplicated()]
   color  value
3    red      3
4  white      2

```

通常，所有重复的行都将从数据帧中删除；为此，pandas 提供了`drop_duplicates()`函数，该函数返回没有重复行的数据帧。

```py
>>> dframe[dframe.duplicated()]
   color  value
3    red      3
4  white      2

```

### 绘图

pandas 库提供了一组函数，正如您将在本节中看到的，这些函数利用映射来执行一些操作。映射只不过是创建两个不同值之间的匹配列表，能够将一个值绑定到特定的标签或字符串。

要定义映射，没有比`dict`对象更好的对象了。

```py
map = {
   'label1' : 'value1,
   'label2' : 'value2,
   ...
}

```

您将在本节中看到的函数执行特定的操作，但是它们都接受一个`dict`对象。

*   `replace()`—替换值

*   `map()`—创建新列

*   `rename()`—替换索引值

#### 通过映射替换值

在你组装的数据结构中，经常有不符合你需要的值。例如，文本可能是外语，或者可能是另一个值的同义词，或者可能不是以期望的形状表达的。在这种情况下，各种值的替换操作通常是一个必要的过程。

例如，定义一个包含各种对象和颜色的数据帧，包括两种非英语的颜色。通常，在组装操作过程中，可能会以不期望的形式维护带有值的数据。

```py
>>> frame = pd.DataFrame({ 'item':['ball','mug','pen','pencil','ashtray'],
...                     'color':['white','rosso','verde','black','yellow'],
                        'price':[5.56,4.20,1.30,0.56,2.75]})
>>> frame
    color     item     price
0   white     ball      5.56
1   rosso      mug      4.20
2   verde      pen      1.30
3   black   pencil      0.56
4  yellow  ashtray      2.75

```

为了能够用新值替换不正确的值，有必要定义对应关系的映射，包含新值作为关键字。

```py
>>> newcolors = {
...    'rosso': 'red',
...    'verde': 'green'
... }

```

现在您唯一能做的就是使用`replace()`函数，将映射作为参数。

```py
>>> frame.replace(newcolors)
     color     item  price
0    white     ball   5.56
1      red      mug   4.20
2    green      pen   1.30
3    black   pencil   0.56
4   yellow  ashtray   2.75

```

从结果中可以看出，这两种颜色已经被数据帧中的正确值所取代。例如，一种常见的情况是用另一个值替换`NaN`值，例如`0`。你可以使用`replace()`，它的表现非常好。

```py
>>> ser = pd.Series([1,3,np.nan,4,6,np.nan,3])
>>> ser
0   1.0
1   3.0
2   NaN
3   4.0
4   6.0
5   NaN
6   3.0
dtype: float64
>>> ser.replace(np.nan,0)
0    1.0
1    3.0
2    0.0
3    4.0
4    6.0
5    0.0
6    3.0
dtype: float64

```

#### 通过映射添加值

在前一个例子中，您看到了如何通过映射对应关系来替换值。在这种情况下，您将继续使用另一个示例来利用值的映射。在这种情况下，您利用映射根据一列中包含的值在另一列中添加值。映射总是单独定义的。

```py
>>> frame = pd.DataFrame({ 'item':['ball','mug','pen','pencil','ashtray'],
...                     'color':['white','red','green','black','yellow']})
>>> frame
    color     item
0   white     ball
1     red      mug
2   green      pen
3   black   pencil
4  yellow  ashtray

```

假设您想要添加一个列来表示数据框中显示的商品的价格。在此之前，假设您在某处有一份价目表，其中描述了每种商品的价格。然后定义一个包含每种商品价格列表的`dict`对象。

```py
>>> prices = {
...    'ball' : 5.56,
...    'mug' : 4.20,
...    'bottle' : 1.30,
...    'scissors' : 3.41,
...    'pen' : 1.30,
...    'pencil' : 0.56,
...    'ashtray' : 2.75
... }

```

应用于数据帧的系列或列的`map()`函数接受包含带有映射的`dict`的函数或对象。因此，在您的案例中，您可以将价格映射应用到列项目上，确保将一个列添加到价格数据帧中。

```py
>>> frame['price'] = frame['item'].map(prices)
>>> frame
    color     item  price
0   white     ball   5.56
1     red      mug   4.20
2   green      pen   1.30
3   black   pencil   0.56
4  yellow  ashtray   2.75

```

#### 重命名轴的索引

与您看到的序列和数据框中包含的值非常相似，甚至轴标签也可以使用映射以非常相似的方式进行转换。所以为了替换标签索引，pandas 提供了`rename()`函数，它将映射作为一个参数，即一个`dict`对象。

```py
>>> frame
    color     item  price
0   white     ball   5.56
1     red      mug   4.20
2   green      pen   1.30
3   black   pencil   0.56
4  yellow  ashtray   2.75
>>> reindex = {
...   0: 'first',
...   1: 'second',
...   2: 'third',
...   3: 'fourth',
...   4: 'fifth'}
>>> frame.rename(reindex)
         color     item  price
first    white     ball   5.56
second     red      mug   4.20
third    green      pen   1.30
fourth   black   pencil   0.56
fifth   yellow  ashtray   2.75

```

如您所见，默认情况下，索引被重命名。如果要重命名列，必须使用`columns`选项。这一次，您将各种映射明确地分配给两个`index`和`columns`选项。

```py
>>> recolumn = {
...    'item':'object',
...    'price': 'value'}
>>> frame.rename(index=reindex, columns=recolumn)
         color   object  value
first    white     ball   5.56
second     red      mug   4.20
third    green      pen   1.30
fourth   black   pencil   0.56
fifth   yellow  ashtray   2.75

```

同样在这里，对于需要替换单个值的最简单的情况，您可以避免编写和分配许多变量。

```py
>>> frame.rename(index={1:'first'}, columns={'item':'object'})
        color   object  price
0       white     ball   5.56
first     red      mug   4.20
2       green      pen   1.30
3       black   pencil   0.56
4      yellow  ashtray   2.75

```

到目前为止，您已经看到了`rename()`函数返回了一个有变化的数据帧，而没有改变原始数据帧。如果您希望更改在您调用函数的对象上生效，您将把`inplace`选项设置为`True`。

```py
>>> frame.rename(columns={'item':'object'}, inplace=True)
>>> frame
    color   object  price
0   white     ball   5.56
1     red      mug   4.20
2   green      pen   1.30
3   black   pencil   0.56
4  yellow  ashtray   2.75

```

## 离散化和宁滨

您将在本节看到的一个更复杂的转换过程是*离散化*。有时，特别是在一些实验情况下，它可以用来处理顺序生成的大量数据。然而，为了对数据进行分析，有必要将这些数据转换成离散的类别，例如，通过将这些读数的值的范围分成更小的区间，并对这些区间中的出现或统计进行计数。另一种情况可能是，由于对人口的精确读数，你有大量的样本。即使在这里，为了便于数据的分析，也有必要将值的范围分成不同的类别，然后分析与每个类别相关的事件和统计数据。

例如，在你的情况下，你可能有一个介于 0 和 100 之间的实验值读数。这些数据收集在一个列表中。

```py
>>> results = [12,34,67,55,28,90,99,12,3,56,74,44,87,23,49,89,87]

```

你知道实验值的范围是从 0 到 100；因此，例如，您可以将该间隔均匀地分成四个相等的部分，即二进制数。第一个包含 0 到 25 之间的值，第二个包含 26 到 50 之间的值，第三个包含 51 到 75 之间的值，最后一个包含 76 到 100 之间的值。

要对 pandas 进行宁滨，首先必须定义一个包含 bin 的分隔值的数组:

```py
>>> bins = [0,25,50,75,100]

```

然后，使用一个名为`cut()`的特殊函数，并将其应用于同样通过 bin 的结果数组。

```py
>>> cat = pd.cut(results, bins)
>>> cat
   (0, 25]
  (25, 50]
  (50, 75]
  (50, 75]
  (25, 50]
 (75, 100]
 (75, 100]
   (0, 25]
   (0, 25]
  (50, 75]
  (50, 75]
  (25, 50]
 (75, 100]
   (0, 25]
  (25, 50]
 (75, 100]
 (75, 100]
Levels (4): Index(['(0, 25]', '(25, 50]', '(50, 75]', '(75, 100]'], dtype=object)

```

函数`cut()`返回的对象是一个*分类*类型的特殊对象。你可以把它看作是一个字符串数组，表示 bin 的名称。在内部，它包含一个指示不同内部类别名称的`categories`数组和一个包含与`results`元素相等的数字列表的`codes`数组(即，服从宁滨的数组)。该数字对应于`results`的相应元素被分配到的容器。

```py
>>> cat.categories
IntervalIndex([0, 25], (25, 50], (50, 75], (75, 100]]
               closed='right'
             dtype='interval[int64]')
>>> cat.codes
array([0, 1, 2, 2, 1, 3, 3, 0, 0, 2, 2, 1, 3, 0, 1, 3, 3], dtype=int8)

```

最后，要知道每个 bin 的出现次数，也就是有多少结果属于每个类别，您必须使用`value_counts()`函数。

```py
>>> pd.value_counts(cat)
(75, 100]    5
(0, 25]      4
(25, 50]     4
(50, 75]     4
dtype: int64

```

如您所见，每个类都有一个带括号的下限和一个带括号的上限。这个符号与用来表示间隔的数学符号一致。如果括号是方的，数字属于区间(限闭)，如果是圆的，数字不属于区间(限开)。

您可以通过首先在一个字符串数组中调用它们，然后在您用来创建分类对象的`cut()`函数中为标签选项赋值，来为不同的容器命名。

```py
>>> bin_names = ['unlikely','less likely','likely','highly likely']
>>> pd.cut(results, bins, labels=bin_names)
      unlikely
   less likely
        likely
        likely
   less likely
 highly likely
 highly likely
      unlikely
      unlikely
        likely
        likely
   less likely
 highly likely
      unlikely
   less likely
 highly likely
 highly likely
Levels (4): Index(['unlikely', 'less likely', 'likely', 'highly likely'], dtype=object)

```

如果将`cut(` **)** 函数作为参数传递给一个整数，而不是解释 bin 的边缘，这将把数组的值的范围分成由数字指定的许多区间。

间隔的极限将由样本数据的最小值和最大值，即经受宁滨的阵列来取值。

```py
>>> pd.cut(results, 5)
 (2.904, 22.2]
  (22.2, 41.4]
  (60.6, 79.8]
  (41.4, 60.6]
  (22.2, 41.4]
    (79.8, 99]
    (79.8, 99]
 (2.904, 22.2]
 (2.904, 22.2]
  (41.4, 60.6]
  (60.6, 79.8]
  (41.4, 60.6]
    (79.8, 99]
  (22.2, 41.4]
  (41.4, 60.6]
    (79.8, 99]
    (79.8, 99]
Levels (5): Index(['(2.904, 22.2]', '(22.2, 41.4]', '(41.4, 60.6]',
                   '(60.6, 79.8]', '(79.8, 99]'], dtype=object)

```

除了`c` `ut()`，熊猫为宁滨提供了另一种方法:`qcut()`。该函数将样本直接分成五等分。事实上，根据数据样本的分布情况，通过使用`cut()`，您将获得每个 bin 的不同出现次数。相反，`qcut()`将确保每个面元的出现次数相等，但是每个面元的边缘不同。

```py
>>> quintiles = pd.qcut(results, 5)
>>> quintiles
    [3, 24]
   (24, 46]
 (62.6, 87]
 (46, 62.6]
   (24, 46]
   (87, 99]
   (87, 99]
    [3, 24]
    [3, 24]
 (46, 62.6]
 (62.6, 87]
   (24, 46]
 (62.6, 87]
    [3, 24]
 (46, 62.6]
   (87, 99]
 (62.6, 87]
Levels (5): Index(['[3, 24]', '(24, 46]', '(46, 62.6]', '(62.6, 87]',
                   '(87, 99]'], dtype=object)

>>> pd.value_counts(quintiles)
[3, 24]       4
(62.6, 87]    4
(87, 99]      3
(46, 62.6]    3
(24, 46]      3
dtype: int64

```

正如您所看到的，在五分位数的情况下，界定 bin 的区间不同于由`cu` `t()`函数生成的区间。此外，如果你查看每个库的出现次数，你会发现`qcut()`试图标准化每个库的出现次数，但是在五分位数的情况下，前两个库出现的次数更多，因为结果的数目不能被 5 整除。

### 检测和过滤异常值

在数据分析过程中，经常需要检测数据结构中是否存在异常值。举例来说，从 1，000 个完全随机的值中创建具有三列的数据帧:

```py
>>> randframe = pd.DataFrame(np.random.randn(1000,3))

```

使用`describe()`功能，您可以看到每一列的统计数据。

```py
>>> randframe.describe()
                 0            1            2
count  1000.000000  1000.000000  1000.000000
mean      0.021609    -0.022926    -0.019577
std       1.045777     0.998493     1.056961
min      -2.981600    -2.828229    -3.735046
25%      -0.675005    -0.729834    -0.737677
50%       0.003857    -0.016940    -0.031886
75%       0.738968     0.619175     0.718702
max       3.104202     2.942778     3.458472

```

例如，您可能会考虑异常值，即值大于标准偏差三倍的值。要获得数据帧每一列的标准偏差，使用`std()`功能。

```py
>>> randframe.std()
0    1.045777
1    0.998493
2    1.056961
dtype: float64

```

现在，对数据帧的所有值进行过滤，对每一列应用相应的标准偏差。由于有了`any()`功能，您可以对每一列应用过滤器。

```py
>>> randframe[(np.abs(randframe) > (3*randframe.std())).any(1)]
            0         1         2
69  -0.442411 -1.099404  3.206832
576 -0.154413 -1.108671  3.458472
907  2.296649  1.129156 -3.735046

```

## 排列

使用`numpy.random.permutation()`功能可以很容易地对数据帧的序列或行进行排列(随机重新排序)操作。

对于本例，创建一个包含升序整数的数据帧。

```py
>>> nframe = pd.DataFrame(np.arange(25).reshape(5,5))
>>> nframe
    0   1   2   3   4
0   0   1   2   3   4
1   5   6   7   8   9
2  10  11  12  13  14
3  15  16  17  18  19
4  20  21  22  23  24

```

现在用`permutation()`函数创建一个由 0 到 4 的五个整数组成的数组，这些整数随机排列。这将是设置数据帧的一行的值的新顺序。

```py
>>> new_order = np.random.permutation(5)
>>> new_order
array([2, 3, 0, 1, 4])

```

现在使用`take()`功能将其应用于所有行的数据帧。

```py
>>> nframe.take(new_order)
    0   1   2   3   4
2  10  11  12  13  14
3  15  16  17  18  19
0   0   1   2   3   4
1   5   6   7   8   9
4  20  21  22  23  24

```

如您所见，行的顺序已经改变；现在，索引遵循与`new_order`数组中指示的顺序相同的顺序。

您甚至可以将整个数据帧的一部分提交给一个排列。它生成一个数组，该数组的序列限制在某个范围内，例如，在我们的例子中是从 2 到 4。

```py
>>> new_order = [3,4,2]
>>> nframe.take(new_order)
    0   1   2   3   4
3  15  16  17  18  19
4  20  21  22  23  24
2  10  11  12  13  14

```

### 随意采样

您已经看到了如何提取数据帧的一部分，这是通过对其进行排列来确定的。有时，当您有一个巨大的数据帧时，您可能需要随机地对它进行采样，最快的方法是使用`np.random.randint()`函数。

```py
>>> sample = np.random.randint(0, len(nframe), size=3)
>>> sample
array([1, 4, 4])
>>> nframe.take(sample)
    0   1   2   3   4
1   5   6   7   8   9
4  20  21  22  23  24
4  20  21  22  23  24

```

从这种随机抽样中可以看出，您可以更频繁地获得相同的样本。

## 字处理

Python 是一种流行的语言，这要归功于它在处理字符串和文本时的易用性。大多数操作可以通过使用 Python 提供的内置函数轻松完成。对于更复杂的匹配和操作，有必要使用正则表达式。

### 用于字符串操作的内置方法

在许多情况下，你有复合字符串，你想分开不同的部分，然后将它们分配给正确的变量。`split()`功能允许您将分隔符(例如逗号)作为参考点来分隔文本的各个部分。

```py
>>> text = '16 Bolton Avenue , Boston'
>>> text.split(',')
['16 Bolton Avenue ', 'Boston']

```

正如您在第一个元素中看到的，您有一个末尾带有空格字符的字符串。为了克服这个常见的问题，您必须使用`sp` `lit()`函数和 **strip()** 函数，该函数修剪空白(包括换行符)。

```py
>>> tokens = [s.strip() for s in text.split(',')]
>>> tokens
['16 Bolton Avenue', 'Boston']

```

结果是一个字符串数组。如果元素的数量很少并且总是相同，那么一种非常有趣的赋值方式可能是这样的:

```py
>>> address, city = [s.strip() for s in text.split(',')]
>>> address
'16 Bolton Avenue'
>>> city
'Boston'

```

到目前为止，您已经看到了如何将文本拆分成多个部分，但是通常您还需要相反的操作，即将它们之间的各种字符串连接起来，形成一个更加扩展的文本。

最直观和简单的方法是用`+`操作符连接文本的各个部分。

```py
>>> address + ',' + city
'16 Bolton Avenue, Boston'

```

当您只有两三个字符串要连接时，这很有用。如果您有许多部分要连接，在这种情况下，更实用的方法是使用分配给分隔符的`join()`函数，用它来连接各种字符串。

```py
>>> strings = ['A+','A','A-','B','BB','BBB','C+']
>>> ';'.join(strings)
'A+;A;A-;B;BB;BBB;C+'

```

可以对字符串执行的另一类操作是在字符串中搜索文本片段，即子字符串。Python 提供了代表检测子字符串的最佳方式的关键字。

```py
>>> 'Boston' in text
True

```

然而，有两个函数可以达到这个目的:`index()`和`find()`。

```py
>>> text.index('Boston')
19
>>> text.find('Boston')
19

```

在这两种情况下，它都返回文本中有子字符串的对应字符的数量。但是，当找不到子字符串时，可以看出这两个函数的行为有所不同:

```py
>>> text.index('New York')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ValueError: substring not found
>>> text.find('New York')
-1

```

事实上，`index()`函数会返回一个错误消息，如果没有找到子串，`find()`会返回`-1`。在同一区域中，您可以知道一个字符或字符组合(子串)在文本中出现的次数。`count()`功能为您提供这个号码。

```py
>>> text.count('e')
2
>>> text.count('Avenue')
1

```

可以对字符串执行的另一个操作是替换或消除子字符串(或单个字符)。在这两种情况下，您都将使用`replace()`函数，如果提示您用空白字符替换子字符串，该操作将等同于从文本中删除子字符串。

```py
>>> text.replace('Avenue','Street')
'16 Bolton Street , Boston'
>>> text.replace('1',")
'16 Bolton Avenue, Boston'

```

### 正则表达式

正则表达式提供了一种非常灵活的方法来搜索和匹配文本中的字符串模式。单个表达式，一般称为 *regex* ，是根据正则表达式语言形成的字符串。有一个内置的 Python 模块叫做`re`，负责 regex 的操作。

首先，当你想使用正则表达式时，你需要导入模块:

```py
>>> import re

```

`re`模块提供了一组可分为三类的功能:

*   模式匹配

*   代替

*   剧烈的

现在你从几个例子开始。例如，表示一个或多个空白字符序列的正则表达式是`\s+`。正如您在上一节中看到的，要通过分隔符将文本分成几部分，您使用了`split()`。甚至对于执行相同操作的`re`模块也有一个`split()`函数，只是它可以接受一个正则表达式模式作为分隔标准，这使得它更加灵活。

```py
>>> text = "This is      an\t odd  \n text!"
>>> re.split('\s+', text)
['This', 'is', 'an', 'odd', 'text!']

```

下面我们更深入的分析一下`re`模块的机制。当您调用`re.split()`函数时，正则表达式首先被编译，然后在文本参数上调用`split()`函数。您可以用`re.compile()`函数编译 regex 函数，从而获得一个可重用的对象 regex，并因此获得 CPU 周期。

在字符串集合或数组中迭代搜索子字符串的操作中尤其如此。

```py
>>> regex = re.compile('\s+')

```

所以如果你用`compile()`函数创建一个`regex`对象，你可以用下面的方法直接对它应用`split()`。

```py
>>> regex.split(text)
['This', 'is', 'an', 'odd', 'text!']

```

要将 regex 模式与文本中的任何其他业务子字符串进行匹配，可以使用`findall()`函数。它返回文本中满足正则表达式要求的所有子字符串的列表。

例如，如果您想在一个字符串中查找所有以大写字母“A”开头的单词，或者例如，无论大写字母还是小写字母都以“A”开头，您需要输入以下内容:

```py
>>> text = 'This is my address: 16 Bolton Avenue, Boston'
>>> re.findall('A\w+',text)
['Avenue']
>>> re.findall('[A,a]\w+',text)
['address', 'Avenue']

```

还有另外两个与`findall()`功能相关的功能— `match()`和`search()`。当`findall()`返回列表中的所有匹配时，`search()`函数只返回第一个匹配。此外，此函数返回的对象是一个特定的对象:

```py
>>> re.search('[A,a]\w+',text)
<_sre.SRE_Match object; span=(11, 18), match="address">

```

该对象不包含响应 regex 模式的子字符串的值，但返回它在字符串中的开始和结束位置。

```py
>>> search = re.search('[A,a]\w+',text)
>>> search.start()
11
>>> search.end()
18
>>> text[search.start():search.end()]
'address'

```

`match()`函数只在字符串的开头执行匹配；如果与第一个字符不匹配，就不会在字符串中继续搜索。如果您没有找到任何匹配，那么它不会返回任何对象。

```py
>>> re.match('[A,a]\w+',text)
>>>

```

如果`match()`有响应，它将返回一个与您看到的`search()`函数相同的对象。

```py
>>> re.match('T\w+',text)
<_sre.SRE_Match object; span=(0, 4), match="This">
>>> match = re.match('T\w+',text)
>>> text[match.start():match.end()]
'This'

```

## 数据聚合

数据操作的最后一个阶段是数据聚合。数据聚合涉及从数组生成单个整数的转换。其实你已经做了很多数据聚合的操作，比如你计算`sum()`、`mean()`、`count()`的时候。事实上，这些函数对一组数据进行操作，并执行计算，得到由单个值组成的一致结果。然而，一种更正式的方式，也是在数据聚集中更具控制力的方式，是包括集合的分类。

为分组而对一组数据进行的分类通常是数据分析过程中的一个关键阶段。这是一个转换过程，因为在划分成不同的组之后，您会应用一个函数，根据数据所属的组以某种方式转换或变换数据。通常，功能的分组和应用这两个阶段是在一个步骤中完成的。

同样对于这部分数据分析，pandas 提供了一个非常灵活和高性能的工具。

同样，与 join 的情况一样，熟悉关系数据库和 SQL 语言的人可以找到相似之处。然而，当应用于对组的操作时，像 SQL 这样的语言是相当有限的。事实上，考虑到 Python 这样的编程语言的灵活性，有了所有可用的库，尤其是 pandas，您可以对组执行非常复杂的操作。

### 群组依据

现在你就来详细分析一下 GroupBy 的流程，以及它是如何运作的。一般将其内部机制称为*拆分-应用-合并*的过程。在它的操作模式中，你可以把这个过程分成三个阶段，用三个操作来表示:

*   *分割*—分割成数据集组

*   *应用*—对每组应用一个函数

*   *组合*——不同组得到的所有结果的组合

分析三个不同的阶段(见图 [6-1](#Fig1) )。在第一阶段，即拆分阶段，包含在数据结构(如序列或数据帧)中的数据根据给定的标准被分成几组，这些标准通常与索引或列中的某些值相关联。在 SQL 的行话中，包含在该列中的值被报告为键。此外，如果您正在处理二维对象，如 dataframe，则分组标准可以应用于该列(`axis = 1`)的行(`axis = 0`)。

![img/336498_2_En_6_Chapter/336498_2_En_6_Fig1_HTML.jpg](img/336498_2_En_6_Chapter/336498_2_En_6_Fig1_HTML.jpg)

图 6-1

分离-应用-组合机制

第二阶段，即应用阶段，包括应用一个函数，或者更确切地说是一个由函数精确表达的计算，这将产生一个特定于该组的新的单一值。

最后一个阶段，即组合阶段，将收集从每个组获得的所有结果，并将它们组合成一个新对象。

### 实际例子

您刚才已经看到，pandas 中的数据聚合过程分为多个阶段，称为拆分-应用-合并。有了这些，熊猫并不像你所期望的那样用函数来明确表达，而是由一个生成一个对象的`groupby()`函数来表达，这是整个过程的核心。

为了更好地理解这个机制，让我们切换到一个实际的例子。首先定义一个包含数值和字符串值的 dataframe。

```py
>>> frame = pd.DataFrame({ 'color': ['white','red','green','red','green'],
...                     'object': ['pen','pencil','pencil','ashtray','pen'],
...                     'price1' : [5.56,4.20,1.30,0.56,2.75],
...                     'price2' : [4.75,4.12,1.60,0.75,3.15]})
>>> frame
   color   object  price1  price2
0  white      pen    5.56    4.75
1    red   pencil    4.20    4.12
2  green   pencil    1.30    1.60
3    red  ashtray    0.56    0.75
4  green      pen    2.75    3.15

```

假设您想使用“颜色”列中列出的分组标签来计算“价格 1”列的平均值。有几种方法可以做到这一点。例如，您可以访问 price1 列，并使用 color 列调用`groupby()`函数。

```py
>>> group = frame['price1'].groupby(frame['color'])
>>> group
<pandas.core.groupby.SeriesGroupBy object at 0x00000000098A2A20>

```

我们得到的对象是一个`GroupBy`对象。在你刚刚做的运算中，没有任何计算。只有计算平均值所需的所有信息的集合。您所做的是分组，其中具有相同颜色值的所有行被分组到单个项目中。

为了详细分析数据帧是如何被分成行组的，您调用属性组的`GroupBy`对象。

```py
>>> group.groups
{'green': Int64Index([2, 4], dtype="int64"),
 'red': Int64Index([1, 3], dtype="int64"),
 'white': Int64Index([0], dtype="int64")}

```

如您所见，列出了每个组，并明确指定了分配给每个组的数据帧的行。现在，对组应用该操作就足以获得每个单独组的结果了。

```py
>>> group.mean()
color
green    2.025
red      2.380
white    5.560
Name: price1, dtype: float64
>>> group.sum()
color
green    4.05
red      4.76
white    5.56
Name: price1, dtype: float64

```

### 分层分组

您已经看到了如何根据作为关键选择的列的值对数据进行分组。同样的事情可以扩展到多个列，即，使多个键的分组具有层次结构。

```py
>>> ggroup = frame['price1'].groupby([frame['color'],frame['object']])
>>> ggroup.groups
{('green', 'pen'): Int64Index([4], dtype="int64"),
 ('green', 'pencil'): Int64Index([2], dtype="int64"),
 ('red', 'ashtray'): Int64Index([3], dtype="int64"),
 ('red', 'pencil'): Int64Index([1], dtype="int64"),
 ('white', 'pen'): Int64Index([0], dtype="int64")}

>>> ggroup.sum()
color  object
green  pen        2.75
       pencil     1.30
red    ashtray    0.56
       pencil     4.20
white  pen        5.56
Name: price1, dtype: float64

```

到目前为止，您已经对单个数据列应用了分组，但实际上它可以扩展到多个列或整个数据帧。此外，如果你不需要多次重用对象`GroupBy`,在一次传递中合并所有要完成的分组和计算是很方便的，而不需要定义任何中间变量。

```py
>>> frame[['price1','price2']].groupby(frame['color']).mean()
       price1  price2
color
green   2.025   2.375
red     2.380   2.435
white   5.560   4.750
>>> frame.groupby(frame['color']).mean()
       price1  price2
color
green   2.025   2.375
red     2.380   2.435
white   5.560   

4.750

```

## 群迭代法

`GroupBy`对象支持迭代操作，以生成包含组名和数据部分的二元组序列。

```py
>>> for name, group in frame.groupby('color'):
...     print(name)
...     print(group)
...
green
   color  object  price1  price2
2  green  pencil    1.30    1.60
4  green     pen    2.75    3.15
red
  color   object  price1  price2
1   red   pencil    4.20    4.12
3   red  ashtray    0.56    0.75
white
   color object  price1  price2
0  white    pen    5.56    4.75

```

在您刚刚看到的示例中，您只应用了 print 变量进行说明。事实上，你用要应用的函数来代替变量的打印操作。

### 变换链

从这些示例中，您已经看到，对于每个分组，当进行一些函数计算或其他一般操作时，不管它是如何获得的以及选择标准如何，结果都将是数据结构系列(如果我们选择了单个列数据)或数据框架，然后保留索引系统和列的名称。

```py
>>> result1 = frame['price1'].groupby(frame['color']).mean()
>>> type(result1)
<class 'pandas.core.series.Series'>
>>> result2 = frame.groupby(frame['color']).mean()
>>> type(result2)
<class 'pandas.core.frame.DataFrame'>

```

因此，可以在此过程的各个阶段的任何时候选择单个色谱柱。这里有三种情况，其中在过程的三个不同阶段选择单个列适用。这个例子说明了熊猫提供的这种分组系统的巨大灵活性。

```py
>>> frame['price1'].groupby(frame['color']).mean()
color
green    2.025
red      2.380
white    5.560
Name: price1, dtype: float64
>>> frame.groupby(frame['color'])['price1'].mean()
color
green    2.025
red      2.380
white    5.560
Name: price1, dtype: float64
>>> (frame.groupby(frame['color']).mean())['price1']
color
green    2.025
red      2.380
white    5.560
Name: price1, dtype: float64

```

此外，在聚合操作之后，一些列的名称可能没有太大意义。事实上，在描述业务组合类型的列名前添加一个前缀通常是很有用的。添加一个前缀，而不是完全替换名称，对于跟踪从中获得聚合值的源数据非常有用。如果您应用转换链(一个系列或数据帧是相互生成的)的过程，这是很重要的，在这一过程中，保留一些与源数据的引用是很重要的。

```py
>>> means = frame.groupby('color').mean().add_prefix('mean_')
>>> means
       mean_price1  mean_price2
color
green        2.025        2.375
red          2.380        2.435
white        5.560        4.750

```

### 组上的函数

尽管许多方法没有被实现为专门与`GroupBy`一起使用，但是它们实际上可以正确地与数据结构一起工作。在上一节中，您看到了通过一个`GroupBy`对象，通过指定列名，然后应用方法进行计算，获得序列是多么容易。例如，您可以使用`quantiles()`函数计算分位数。

```py
>>> group = frame.groupby('color')
>>> group['price1'].quantile(0.6)
color
green    2.170
red      2.744
white    5.560
Name: price1, dtype: float64

```

您还可以定义自己的聚合函数。单独定义函数，然后作为参数传递给`mark()`函数。例如，您可以计算每组值的范围。

```py
>>> def range(series):
...      return series.max() - series.min()
...
>>> group['price1'].agg(range)
color
green    1.45
red      3.64
white    0.00
Name: price1, dtype: float64

```

`agg()`函数允许您在整个数据帧上使用聚合函数。

```py
>>> group.agg(range)
       price1  price2
color
green    1.45    1.55
red      3.64    3.37
white    0.00    0.00

```

还可以同时使用更多的聚合函数，用`mark()`函数传递一个包含要完成的操作列表的数组，它将成为新的列。

```py
>>> group['price1'].agg(['mean','std',range])
        mean       std  range
color
green  2.025  1.025305   1.45
red    2.380  2.573869   3.64
white  5.560       NaN   0.00

```

## 高级数据聚合

在本节中，将向您介绍`transform()`和`a` `pply()`功能，它们允许您执行多种分组操作，有些非常复杂。

现在，假设我们希望将以下数据放在同一个数据帧中:原始数据帧(包含数据的数据帧)和通过组聚合计算获得的数据帧，例如 sum。

```py
>>> frame = pd.DataFrame({ 'color':['white','red','green','red','green'],
...                     'price1':[5.56,4.20,1.30,0.56,2.75],
...                     'price2':[4.75,4.12,1.60,0.75,3.15]})
>>> frame
   color  price1  price2
0  white    5.56    4.75
1    red    4.20    4.12
2  green    1.30    1.60
3    red    0.56    0.75
4  green    2.75    3.15
>>> sums = frame.groupby('color').sum().add_prefix('tot_')
>>> sums
       tot_price1  tot_price2
color
green        4.05        4.75
red          4.76        4.87
white        5.56        4.75
>>> merge(frame,sums,left_on='color',right_index=True)
   color  price1  price2  tot_price1  tot_price2
0  white    5.56    4.75        5.56        4.75
1    red    4.20    4.12        4.76        4.87
3    red    0.56    0.75        4.76        4.87
2  green    1.30    1.60        4.05        4.75
4  green    2.75    3.15        4.05        4.75

```

由于有了`merge()`，可以将聚合的结果添加到开始的每一行数据帧中。但是有另一种方法来做这种类型的操作。那就是通过使用`transform(` `)`。这个函数像您之前看到的那样执行聚合，但同时显示基于要开始的数据帧的每一行上的键值计算的值。

```py
>>> frame.groupby('color').transform(np.sum).add_prefix('tot_')
   tot_price1  tot_price2
0        5.56        4.75
1        4.76        4.87
2        4.05        4.75
3        4.76        4.87
4        4.05        4.75

```

如您所见，`transform()`方法是一个更加专门化的函数，它有非常具体的要求:作为参数传递的函数必须产生一个要广播的标量值(聚合)。

覆盖更一般`GroupBy`的方法也适用于`apply()`。这种方法完全适用于分割-应用-组合方案。事实上，这个函数将对象分成几个部分以便操作，调用每个部分上的函数通道，然后尝试将各个部分链接在一起。

```py
>>> frame = pd.DataFrame( { 'color':['white','black','white','white','black','black'],
...                      'status':['up','up','down','down','down','up'],
...                      'value1':[12.33,14.55,22.34,27.84,23.40,18.33],
...                      'value2':[11.23,31.80,29.99,31.18,18.25,22.44]})
>>> frame
   color status  value1  value2
0  white     up   12.33   11.23
1  black     up   14.55   31.80
2  white   down   22.34   29.99
3  white   down   27.84   31.18
4  black   down   23.40   18.25

>>> frame.groupby(['color','status']).apply( lambda x: x.max())
              color status  value1  value2
color status
black down    black   down   23.40   18.25
      up      black     up   18.33   31.80
white down    white   down   27.84   31.18
      up      white     up   12.33   11.23
5  black     up   18.33   22.44

>>> frame.rename(index=reindex, columns=recolumn)
         color   object  value
first    white     ball   5.56
second     red      mug   4.20
third    green      pen   1.30
fourth   black   pencil   0.56
fifth   yellow  ashtray   2.75
>>> temp = pd.date_range('1/1/2015', periods=10, freq= 'H')
>>> temp
DatetimeIndex(['2015-01-01 00:00:00', '2015-01-01 01:00:00',
               '2015-01-01 02:00:00', '2015-01-01 03:00:00',
               '2015-01-01 04:00:00', '2015-01-01 05:00:00',
               '2015-01-01 06:00:00', '2015-01-01 07:00:00',
               '2015-01-01 08:00:00', '2015-01-01 09:00:00'],
              dtype='datetime64[ns]', freq="H")
Length: 10, Freq: H, Timezone: None
>>> timeseries = pd.Series(np.random.rand(10), index=temp)
>>> timeseries
2015-01-01 00:00:00    0.368960
2015-01-01 01:00:00    0.486875
2015-01-01 02:00:00    0.074269
2015-01-01 03:00:00    0.694613
2015-01-01 04:00:00    0.936190
2015-01-01 05:00:00    0.903345
2015-01-01 06:00:00    0.790933
2015-01-01 07:00:00    0.128697
2015-01-01 08:00:00    0.515943
2015-01-01 09:00:00    0.227647
Freq: H, dtype: float64

>>> timetable = pd.DataFrame( {'date': temp, 'value1' : np.random.rand(10),
...                                       'value2' : np.random.rand(10)})
>>> timetable
                 date    value1    value2
0 2015-01-01 00:00:00  0.545737  0.772712
1 2015-01-01 01:00:00  0.236035  0.082847
2 2015-01-01 02:00:

00  0.248293  0.938431
3 2015-01-01 03:00:00  0.888109  0.605302
4 2015-01-01 04:00:00  0.632222  0.080418
5 2015-01-01 05:00:00  0.249867  0.235366
6 2015-01-01 06:00:00  0.993940  0.125965
7 2015-01-01 07:00:00  0.154491  0.641867
8 2015-01-01 08:00:00  0.856238  0.521911
9 2015-01-01 09:00:00  0.307773  0.332822

```

然后，在 dataframe 前面添加一列，表示一组将用作键值的文本值。

```py
>>> timetable['cat'] = ['up','down','left','left','up','up','down','right','right','up']
>>> timetable
                 date    value1    value2    cat
0 2015-01-01 00:00:00  0.545737  0.772712     up
1 2015-01-01 01:00:00  0.236035  0.082847   down
2 2015-01-01 02:00:00  0.248293  0.938431   left
3 2015-01-01 03:00:00  0.888109  0.605302   left
4 2015-01-01 04:00:00  0.632222  0.080418     up
5 2015-01-01 05:00:00  0.249867  0.235366     up
6 2015-01-01 06:00:00  0.993940  0.125965   down
7 2015-01-01 07:00:00  0.154491  0.641867  right
8 2015-01-01 08:00:00  0.856238  0.521911  right
9 2015-01-01 09:00:00  0.307773  0.332822     up

```

但是，此处显示的示例具有重复的键值。

## 结论

在本章中，您看到了划分数据操作的三个基本部分:准备、处理和数据聚合。通过一系列例子，您了解了一组库函数，这些库函数允许 pandas 执行这些操作。

您看到了如何在简单的数据结构上应用这些函数，这样您就可以熟悉它们是如何工作的，并理解它们对更复杂情况的适用性。

最终，您现在拥有了为下一阶段的数据分析(数据可视化)准备数据集所需的知识。

在下一章中，将向您介绍 Python 库 matplotlib，它可以转换任何图表中的数据结构。*