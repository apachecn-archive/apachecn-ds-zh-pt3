# 13.用 NLTK 进行文本数据分析

在本书中，您已经看到了各种分析技术和大量以数字或表格形式处理数据的示例，这些数据很容易通过数学表达式和统计技术进行处理。但是大部分数据是由文本组成的，这些文本响应不同语言的语法规则(有时甚至不是语法规则:)。在文本中，单词和单词的含义(以及它们传递的情感)是非常有用的信息来源。

在这一章中，你将学习一些使用 NLTK(自然语言工具包)库的文本分析技术，它将允许你执行复杂的操作。此外，所涵盖的主题将帮助您理解数据分析的这一重要部分。

## 文本分析技术

近年来，随着大数据的出现和来自互联网的海量文本数据，许多文本分析技术应运而生。事实上，这种形式的数据可能很难分析，但同时也代表了大量有用信息的来源，因为数据的可用性非常大。想一想所有产生的文献，例如在互联网上发表的大量帖子。社交网络和聊天上的评论也可以是一个很好的数据来源，特别是为了了解对某个特定话题的赞同或不赞同程度。

因此，分析这些文本已经成为人们极大兴趣的来源，为此目的已经引入了许多技术，这本身就创造了一门真正的学科。一些比较重要的技术如下:

*   词的频率分布分析

*   模式识别

*   磨尖

*   链接和关联分析

*   情感分析

### 自然语言工具包(NLTK)

如果您使用 Python 编程，并且想要分析文本形式的数据，目前最常用的工具之一是 Python 自然语言工具包(NLTK)。

NLTK 无非就是一个 Python 库( [`https://www.nltk.org/`](https://www.nltk.org/) )，里面有很多专门处理和文本数据分析的工具。NLTK 创建于 2001 年，用于教育目的，随后随着时间的推移，它发展到如此程度，以至于成为一个真正的分析工具。

在 NLTK 库中，也有大量的样本文本，称为*语料库*。这个文本集合很大程度上取自文献，作为应用 NLTK 库开发的技术的基础非常有用。特别是，它被用来执行测试(类似于 TensorFlow 中的 MNIST 数据集，这将在第 [9](09.html) 章中讨论)。

在您的计算机上安装 NLTK 是一个非常简单的操作。作为一个非常流行的 Python 库，你只需要使用 pip 或 conda 安装它。

在 Linux 系统上，使用以下命令:

```py
pip install nltk

```

在 Windows 系统上(通过 Anaconda)，使用以下命令:

```py
conda install nltk

```

### 导入 NLTK 库和 NLTK 下载工具

为了对 NLTK 更有信心，没有比直接使用 Python 代码更好的方法了。这样你就能看到并逐渐理解这个库的运作。

因此，您需要做的第一件事是在 IPython 或 Jupyter 笔记本上打开一个会话。第一个命令导入 NLTK 库。

```py
import nltk

```

然后，您需要从语料库集合中导入文本。要做到这一点，有一个名为`nltk.download_shell()`的功能，它打开一个名为 NLTK Downloader 的工具，允许您通过引导选择选项来进行选择。

如果您在终端上输入以下命令:

```py
nltk.download_shell()

```

您将看到文本格式的各种选项。

```py
NLTK Downloader
---------------------------------------------------------------------------
    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit
---------------------------------------------------------------------------
Downloader>

```

现在，该工具正在等待一个选项。如果您想查看可能的 NLTK 扩展的列表，输入 **L** 作为 list 并按 enter 键。您将立即看到属于 NLTK 的所有可能的包的列表，您可以下载这些包来扩展 NLTK 的功能，包括语料库集合的文本。

```py
Packages:
  [ ] abc................. Australian Broadcasting Commission 2006
  [ ] alpino.............. Alpino Dutch Treebank
  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger
  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)
  [ ] basque_grammars..... Grammars for Basque
  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information
                           Extraction Systems in Biology)
  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model
  [ ] book_grammars....... Grammars from NLTK Book
  [ ] brown............... Brown Corpus
  [ ] brown_tei........... Brown Corpus (TEI XML Version)
  [ ] cess_cat............ CESS-CAT Treebank
  [ ] cess_esp............ CESS-ESP Treebank
  [ ] chat80.............. Chat-80 Data Files
  [ ] city_database....... City Database
  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)
  [ ] comparative_sentences Comparative Sentence Dataset
  [ ] comtrans............ ComTrans Corpus Sample
  [ ] conll2000........... CONLL 2000 Chunking Corpus
  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus
Hit Enter to continue:

```

再次按 Enter 键将继续显示列表，按字母顺序显示其他软件包。按 Enter 键，直到列表结束，以查看所有可能的包。在列表的最后，NLTK 下载器的不同初始选项会重新出现。

为了能够创建一系列的例子来了解这个库，你需要一系列的文本来处理。一个很好的文本来源是古腾堡语料库，存在于语料库中。古腾堡语料库是从称为古腾堡计划( [`http://www.gutenberg.org/`](http://www.gutenberg.org/) )的电子档案中提取的一小部分文本。这个档案馆里有超过 25，000 本电子书。

要下载这个包，首先进入`d`选项进行下载。该工具将要求您输入包名，因此您输入名称`gutenberg`。

```py
---------------------------------------------------------------------------
    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit
---------------------------------------------------------------------------
Downloader> d

Download which package (l=list; x=cancel)?
  Identifier> gutenberg

```

此时，软件包将开始下载。

对于下面的时间，如果你已经知道你要下载的包的名字，只需输入命令`nltk.download()`并把包名作为参数。这不会打开 NLTK Downloader 工具，而是直接下载所需的包。所以前面的操作相当于写:

```py
nltk.download ('gutenberg')

```

一旦完成，你就可以看到包的内容，这要感谢`fileids()`函数，它显示了包中包含的文件名。

```py
gb = nltk.corpus.gutenberg
print ("Gutenberg files:", gb.fileids ())

```

一个数组会出现在终端上，包含了`gutenberg`包中的所有文本文件。

```py
Gutenberg files :  ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']

```

要访问其中一个文件的内部内容，首先选择一个文件，例如莎士比亚的《麦克白》(`shakespeare-macbeth.txt`)，然后将它赋给一个方便的变量。提取模式是针对单词的，也就是说，您希望创建一个包含单词作为元素的数组。在这方面，你需要使用`words()`函数。

```py
macbeth = nltk.corpus.gutenberg.words ('shakespeare-macbeth.txt')

```

如果你想查看这段文字的长度(以单词为单位)，可以使用`len()`函数。

```py
len (macbeth)
23140

```

因此，用于这些示例的文本由 23140 个单词组成。

我们创建的`macbeth`变量是一个包含文本单词的长数组。例如，如果您想查看文本的前 10 个单词，您可以编写以下命令。

```py
macbeth [:10]
['[',
 'The',
 'Tragedie',
 'of',
 'Macbeth',
 'by',
 'William',
 'Shakespeare',
 '1603',
 ']']

```

正如你所看到的，前 10 个单词包含了作品的标题，还包含了方括号，表示一个句子的开始和结束。如果您使用带有`sents()`函数的句子提取模式，您将获得一个更结构化的数组，每个句子作为一个元素。反过来，这些元素将是包含元素单词的数组。

```py
macbeth_sents = nltk.corpus.gutenberg.sents ('shakespeare-macbeth.txt')
macbeth_sents [: 5]
[['[',
  'The',
  'Tragedie',
  'of',
  'Macbeth',
  'by',
  'William',
  'Shakespeare',
  '1603',
  ']'],
 ['Actus', 'Primus', '.'],
 ['Scoena', 'Prima', '.'],
 ['Thunder', 'and', 'Lightning', '.'],
 ['Enter', 'three', 'Witches', '.']]

```

### 用 NLTK 搜索一个单词

当您拥有一个 NLTK 语料库(即从文本中提取的一组单词)时，您需要做的最基本的事情之一就是在其中进行研究。研究的概念和你习惯的略有不同。

`concordance()`函数在语料库中查找作为参数传递的所有单词。

第一次运行该命令时，系统将花费几秒钟时间返回结果。后面的时间会更快。事实上，第一次在一个语料库上执行这个命令时，它会创建一个内容索引来执行搜索，这个索引一旦创建，就会在后续调用中使用。这解释了为什么系统第一次需要更多的时间。

首先确定语料库是一个对象`nltk.Text`，然后在内部搜索单词`'Stage'`。

```py
text = nltk.Text(macbeth)
text.concordance('Stage')
Displaying 3 of 3 matches:
nts with Dishes and Seruice ouer the Stage . Then enter Macbeth Macb . If it we
with mans Act , Threatens his bloody Stage : byth ' Clock ' tis Day , And yet d
 struts and frets his houre vpon the Stage , And then is heard no more . It is

```

您获得了文本的三个不同的出现。

在 NLTK 中搜索单词的另一种形式是上下文。也就是你要找的单词的前一个单词和后一个单词。为此，必须使用`common_contexts()`功能。

```py
text.common_contexts(['Stage'])
the_ bloody_: the_,

```

如果你看看之前的研究结果，你会发现这三个结果与之前所说的一致。

一旦理解了 NLTK 在搜索过程中是如何构思单词的概念及其上下文的，就很容易理解同义词的概念了。也就是说，假设具有相同上下文的所有单词都可能是同义词。要搜索与被搜索单词具有相同上下文的所有单词，必须使用`similar()`函数。

```py
text.similar('Stage')
fogge ayre bleeding reuolt good shew heeles skie other sea feare
consequence heart braine seruice herbenger lady round deed doore

```

对于那些不习惯于处理和分析文本的人来说，这些研究方法可能看起来很奇怪，但你很快就会明白，这些研究方法非常适合于单词及其在文本中的含义。

### 分析单词的频率

分析文本的一个最简单和最基本的例子是计算文本中包含的单词的频率。这个操作如此常见，以至于它被合并到一个单独的`nltk.FreqDist()`函数中，包含单词数组的变量作为参数传递给这个函数。

因此，要获得文本中所有单词的统计分布，您必须输入一个简单的命令。

```py
fd = nltk.FreqDist(macbeth)

```

如果想查看文本中最常见的前 10 个单词，可以使用`most_common()`功能。

```py
fd.most_common(10)
[(',', 1962),
 ('.', 1235),
 ("'", 637),
 ('the', 531),
 (':', 477),
 ('and', 376),
 ('I', 333),
 ('of', 315),
 ('to', 311),
 ('?', 241)]

```

从得到的结果中，您可以看到最常见的元素是标点符号、介词和冠词，这适用于许多语言，包括英语。因为这些在文本分析中没有什么意义，所以经常需要消除它们。这些被称为*停用词*。

停用词是在分析中没有什么意义的词，必须过滤掉。没有通用的规则来确定一个单词是否是停用词(要删除)。然而，NLTK 库通过为您提供一组预先选择的停用词来帮助您。要下载停用词，您可以使用`nltk.download()`命令。

```py
nltk.download('stopwords')

```

一旦你下载了所有的停用词，你可以只选择那些与英语相关的，将它们保存在一个变量`sw`中。

```py
sw = set(nltk.corpus.stopwords.words ('english'))
print(len(sw))
list(sw) [:10]
179

['through',
 'are',
 'than',
 'nor',
 'ain',
 "didn't",
 'didn',
 "shan't",
 'down',
 'our']

```

根据 NLTK，英语词汇中有 179 个停用词。现在您可以使用这些停用词来过滤`macbeth`变量。

```py
macbeth_filtered = [w for w in macbeth if w.lower() not in sw]
fd = nltk.FreqDist (macbeth_filtered)
fd.most_common(10)
[(',', 1962),
 ('.', 1235),
 ("'", 637),
 (':', 477),
 ('?', 241),
 ('Macb', 137),
 ('haue', 117),
 ('-', 100),
 ('Enter', 80),
 ('thou', 63)]

```

现在返回了前 10 个最常见的单词，您可以看到停用词已经被消除，但结果仍然不尽如人意。事实上，标点符号仍然存在于单词中。要消除所有标点，您可以通过在过滤器中插入包含标点符号的标点数组来更改以前的代码。这个标点数组可以通过导入`string`函数得到。

```py
import string
punctuation = set (string.punctuation)
macbeth_filtered2 = [w.lower () for w in macbeth if w.lower () not in sw and w.lower () not in punctuation]

```

现在你可以重新计算单词的频率分布。

```py
fd = nltk.FreqDist (macbeth_filtered2)
fd.most_common(10)
[('macb', 137),
 ('haue', 122),
 ('thou', 90),
 ('enter', 81),
 ('shall', 68),
 ('macbeth', 62),
 ('vpon', 62),
 ('thee', 61),
 ('macd', 58),
 ('vs', 57)]

```

最后，结果就是你想要的。

### 从文本中选择单词

另一种形式的处理和数据分析是基于特定特征选择包含在文本主体中的单词的过程。例如，您可能对根据单词的长度提取单词感兴趣。

要获取所有最长的单词，例如超过 12 个字符的单词，您可以输入以下命令。

```py
long_words = [w for w in macbeth if len(w)> 12]

```

所有超过 12 个字符的单词已输入到`long_words`变量中。您可以使用`sort()`功能按字母顺序列出它们。

```py
sorted(long_words)
['Assassination',
 'Chamberlaines',
 'Distinguishes',
 'Gallowgrosses',
 'Metaphysicall',
 'Northumberland',
 'Voluptuousnesse',
 'commendations',
 'multitudinous',
 'supernaturall',
 'vnaccompanied']

```

如你所见，有 11 个单词符合这个标准。

再比如寻找包含某个字符序列的所有单词，比如`'ious'`。您只需改变`for in`循环中的条件，即可获得所需的选择。

```py
ious_words = [w for w in macbeth if 'ious' in w]
ious_words = set(ious_words)
sorted(ious_words)
['Auaricious',
 'Gracious',
 'Industrious',
 'Iudicious',
 'Luxurious',
 'Malicious',
 'Obliuious',
 'Pious',
 'Rebellious',
 'compunctious',
 'furious',
 'gracious',
 'pernicious',
 'pernitious',
 'pious',
 'precious',
 'rebellious',
 'sacrilegious',
 'serious',
 'spacious',
 'tedious']

```

在本例中，您使用了`sort()`进行列表转换，这样它就不会包含重复的单词。

这两个例子只是一个起点，向您展示这个工具的潜力和过滤单词的容易程度。

### 二元结构和搭配

文本分析的另一个基本要素是考虑成对的单词(*二元组*)而不是单个单词。例如，单词“是”和“黄色”是一个二元组，因为它们的组合是可能的和有意义的。所以“是黄色的”可以在文本数据中找到。我们都知道，在我们的文学作品中，有些二元结构是如此常见，以至于它们几乎总是被一起使用。例子包括“快餐”，“注意”，“早上好”，等等。这些二元模型被称为*搭配*。

文本分析也可以包括在文本中搜索任何二元模型。要找到它们，只需使用`bigrams()`功能。为了从二元模型中排除停用词和标点符号，您必须使用之前已经过滤过的词集，比如`macbeth_filtered2`。

```py
bgrms = nltk.FreqDist(nltk.bigrams(macbeth_filtered2))
bgrms.most_common(15)
[(('enter', 'macbeth'), 16),
 (('exeunt', 'scena'), 15),
 (('thane', 'cawdor'), 13),
 (('knock', 'knock'), 10),
 (('st', 'thou'), 9),
 (('thou', 'art'), 9),
 (('lord', 'macb'), 9),
 (('haue', 'done'), 8),
 (('macb', 'haue'), 8),
 (('good', 'lord'), 8),
 (('let', 'vs'), 7),
 (('enter', 'lady'), 7),
 (('wee', 'l'), 7),
 (('would', 'st'), 6),
 (('macbeth', 'macb'), 6)]

```

通过显示文本中最常见的二元模型，可以找到语言位置。

除了二元模型，还可以有基于三元模型的布局，三元模型是三个词的组合。在这种情况下，使用`trigrams()`功能。

```py
tgrms = nltk.FreqDist(nltk.trigrams (macbeth_filtered2))
tgrms.most_common(10)
[(('knock', 'knock', 'knock'), 6),
 (('enter', 'macbeth', 'macb'), 5),
 (('enter', 'three', 'witches'), 4),
 (('exeunt', 'scena', 'secunda'), 4),
 (('good', 'lord', 'macb'), 4),
 (('three', 'witches', '1'), 3),
 (('exeunt', 'scena', 'tertia'), 3),
 (('thunder', 'enter', 'three'), 3),
 (('exeunt', 'scena', 'quarta'), 3),
 (('scena', 'prima', 'enter'), 3)]

```

### 在网络上使用文本

到目前为止，您已经看到了一系列使用 NLTK 库中已经排序和包含的文本(称为语料库)作为`gutenberg`的例子。但是在现实中，您将需要访问互联网来提取文本并收集它作为一个语料库来使用 NLTK 进行分析。

在本节中，您将看到这种操作是多么简单。首先，您需要导入一个允许您连接到网页内容的库。库是实现这一目的的绝佳选择，因为它允许您从互联网上下载文本内容，包括 HTML 页面。

所以首先从`urllib`库中导入专门处理这种操作的`request()`函数。

```py
from urllib import request

```

然后，您必须写出包含要提取的文本的页面的 URL。还是参考`gutenberg`项目，可以选择比如陀思妥耶夫斯基写的一本书( [`http://www.gutenberg.org`](http://www.gutenberg.org) )。网站上有不同格式的文本；此时，我们将选择 raw 格式的文件(。txt)。

```py
url = "http://www.gutenberg.org/files/2554/2554-0.txt"
response = request.urlopen(url)
raw = response.read().decode('utf8')

```

原始文本是从互联网上下载的该书的所有文本内容。总是检查你下载的内容。要做到这一点，前 75 个字符就足够了。

```py
raw[:75]
'\ufeffThe Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\r'

```

如你所见，这些字符与文本的标题相对应。我们看到课文的第一个单词也有错误。事实上还有 Unicode 字符`BOM \ufeff`。发生这种情况是因为我们使用了 utf8 解码系统，这在大多数情况下是有效的，但在这种情况下是无效的。这种情况下最合适的系统是`utf-8-sig`。用正确的值替换不正确的值。

```py
raw = response.read().decode('utf8-sig')
raw[:75]
'The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\r\n'

```

现在，为了能够处理它，您必须将它转换成与 NLTK 兼容的语料库。为此，请输入以下转换命令。

```py
tokens = nltk.word_tokenize (raw)
webtext = nltk.Text (tokens)

```

这些命令无非是用函数`nltk.word_tokenize()`将字符文本分割成记号(即单词),然后用`nltk.Text()`将记号转换成适合 NLTK 的文本体。

您可以通过输入以下命令来查看标题

```py
webtext[:12]
['The',
 'Project',
 'Gutenberg',
 'EBook',
 'of',
 'Crime',
 'and',
 'Punishment',
 ',',
 'by',
 'Fyodor',
 'Dostoevsky']

```

现在你有了一个正确的语料库来开始进行你的分析。

### 从 HTML 页面中提取文本

在前面的例子中，您从互联网下载的文本中创建了一个 NLTK 语料库。但是互联网上的大多数文档都是 HTML 页面的形式。在本节中，您将看到如何从 HTML 页面中提取文本。

你总是使用`urllib`库的`request()`函数来下载网页的 HTML 内容。

```py
url = "http://news.bbc.co.uk/2/hi/health/2284783.stm"
html = request.urlopen(url).read().decode('utf8')
html[:120]
'<!doctype html public "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">\r\n<html>\r\n<hea'

```

然而，现在转换成 NLTK 语料库需要一个额外的库`bs4 (BeautifulSoup)`，它为您提供了合适的解析器，可以识别 HTML 标记并提取其中包含的文本。

```py
from bs4 import BeautifulSoup
raw = BeautifulSoup(html, "lxml").get_text()
tokens = nltk.word_tokenize(raw)
text = nltk.Text(tokens)

```

现在你也有了这种情况下的语料库，即使你经常要进行比前一种情况更复杂的清洗操作，以消除你不感兴趣的单词。

### 情感分析

情感分析是最近发展起来的一个新的研究领域，旨在评估人们对某个特定话题的看法。这一学科基于不同的技术，这些技术使用文本分析及其在社交媒体和论坛领域的工作(*意见挖掘*)。

由于用户的评论和评论，情感分析算法可以根据某些关键词来评估欣赏或评价的程度。这种欣赏程度被称为*意见*，有三种可能的值:积极、中立或消极。因此，对这一意见的评估成为一种分类形式。

这么多情感分析技术实际上是分类算法，类似于你在前面章节看到的涵盖机器学习和深度学习的算法(见第 [8](08.html) 和 [9](09.html) 章)。

作为更好地理解这种方法的例子，我们参考官方网站上使用朴素贝叶斯算法的分类教程( [`https://www.nltk.org/book/ch06.html`](https://www.nltk.org/book/ch06.html) )，在那里可以找到许多其他非常有用的例子来更好地理解这个库。

作为一个训练集，这个例子使用了 NLTK 中的另一个语料库，这对于这些类型的分类问题非常有用:`movie_reviews`。这个语料库包含许多电影评论，其中有离散长度的文本以及另一个指定评论是正面还是负面的字段。因此，它是很好的学习材料。

本教程的目的是找到在负面文档中出现频率最高的词，或者在正面文档中出现频率更高的词，从而将重点放在与某个观点相关的关键词上。这种评估将通过集成到 NLTK 中的朴素贝叶斯分类进行。

首先，名为`movie_reviews`的语料库很重要。

```py
nltk.download('movie_reviews')

```

然后从获得的语料库中构建训练集，创建一个名为`documents`的元素对数组。该数组在第一个字段中包含单个评论的文本，在第二个字段中包含负面或正面评价。最后，您将随机混合数组中的所有元素。

```py
import random
reviews = nltk.corpus.movie_reviews
documents = [(list(reviews.words(fileid)), category)
                for category in reviews.categories()
          for fileid in reviews.fileids(category)]
random.shuffle(documents)

```

要更好地理解，请详细查看文档内容。第一个元素包含两个字段；第一个是包含所有使用过的单词的评论。

```py
first_review = ' '.join(documents[0][0])
print(first_review)
topless women talk about their lives falls into that category that i mentioned in the devil ' s advocate : movies that have a brilliant beginning but don ' t know how to end . it begins by introducing us to a selection of characters who all know each other . there is liz , who oversleeps and so is running late for her appointment , prue who is getting married ,...

```

第二个字段包含对审查的评估:

```py
documents[0][1]
'neg'

```

但是训练集还没有准备好；事实上，你必须创建语料库中所有单词的频率分布。这个分布会用`list()`函数转换成演员表。

```py
all_words = nltk.FreqDist(w.lower() for w in reviews.words())
word_features = list(all_words)

```

然后，下一步是定义用于计算特征的函数，即，足够重要以建立评论意见的词。

```py
def document_features(document, word_features):
     document_words = set(document)
     features = {}
     for word in word_features:
         features ['{}'.format(word)] = (word in document_words)
     return features

```

一旦定义了`document_features()`函数，就可以从文档中创建特性集。

```py
featuresets = [(document_features (d, c)) for (d, c) in documents]

```

目的是创建一个包含在整个电影语料库中的所有单词的集合，分析它们是否出现在每个单个评论中(真或假)，并查看它们对它的正面或负面判断的贡献大小。一个词出现在负面评论中的次数越多，出现在正面评论中的次数越少，这个词就越被认为是“坏”词。对于“好”字评价来说，情况正好相反。

要确定如何将这个特征集细分为训练集和测试集，首先必须了解它包含多少元素。

```py
len (featuresets)
2000

```

然后，为了评估模型的准确性，将集合的前 1，500 个元素用于训练集，将最后 500 个项目用于测试集。

```py
train_set, test_set = featuresets[1500:], featuresets[: 500]

```

最后，应用 NLTK 库提供的朴素贝叶斯分类器对这个问题进行分类。然后计算它的准确性，将测试集提交给模型。

```py
classifier = nltk.NaiveBayesClassifier.train(train_set)
print (nltk.classify.accuracy(classifier, test_set))
0.85

```

精确度不如前几章中的例子高，但是我们处理的是文本中包含的单词，因此很难创建与数值问题相关的精确模型。

现在你已经完成了分析，你可以看到哪些词在评价一篇评论的负面或正面意见中最有分量。

```py
classifier.show_most_informative_features(10)
Most Informative Features
                   badly = True              neg : pos    =     11.1 : 1.0
                   julie = True              neg : pos    =      9.5 : 1.0
                  finest = True              pos : neg    =      9.0 : 1.0
                  forgot = True              neg : pos    =      8.8 : 1.0
                   naked = True              neg : pos    =      8.8 : 1.0
              refreshing = True              pos : neg    =      7.9 : 1.0
                  stolen = True              pos : neg    =      7.3 : 1.0
                 luckily = True              pos : neg    =      7.3 : 1.0
                 directs = True              pos : neg    =      7.3 : 1.0
                    rain = True              neg : pos    =      7.3 : 1.0

```

看看结果，你不会惊讶地发现“很差”是一个不好的意见词，而“最好”是一个好的意见词。这里有趣的是，“朱莉”是一个不好的意见词。

## 结论

在这一章中，你对文本分析世界有了一点了解。事实上，还有许多其他的技术和例子可以讨论。然而，在本章结束时，你应该熟悉这个分析分支，尤其是已经开始了解 NLTK(自然语言工具包)库，一个强大的文本分析工具。