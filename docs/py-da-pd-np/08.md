# 8.使用 scikit-learn 进行机器学习

在构成数据分析的流程链中，预测模型的构建阶段及其验证是由一个名为`scikit-learn`的强大库完成的。在本章中，您将看到一些用不同方法说明预测模型的基本构造的示例。

## scikit-learn 库

`scikit-learn`是一个 Python 模块，集成了很多机器学习算法。这个库最初是由 Cournapeu 在 2007 年开发的，但是第一次真正的发布是在 2010 年。

这个库是 SciPy (Scientific Python)组的一部分，SciPy 组是为科学计算特别是数据分析而创建的一组库，本书中讨论了其中的许多库。通常这些库被定义为 *SciKits* ，因此是这个库的名字的第一部分。库名的第二部分来源于*机器学习*，与该库相关的学科。

## 机器学习

机器学习是一门研究在进行数据分析的数据集中进行模式识别的方法的学科。特别是，它涉及从数据中学习并做出预测的算法的开发。每种方法都基于构建一个特定的模型。

有非常多的方法属于学习机，每种方法都有其独特的特征，这些特征特定于数据的性质和您想要建立的预测模型。选择应用哪种方法被称为*学习问题*。

在学习阶段要接受模式的数据可以是由每个元素的单个值组成的数组，也可以是由多元值组成的数组。这些值通常被称为*特征*或*属性*。

### 监督和非监督学习

根据要构建的数据和模型的类型，您可以将学习问题分为两大类:

#### 监督学习

它们是训练集包含您想要预测的附加属性的方法(*目标*)。由于这些值，当您必须提交新值时，您可以指示模型提供类似的值(T2 测试集 T3)。

*   *分类*—训练集中的数据属于两个或两个以上的类或类别；然后，已经被标记的数据允许你教系统识别区分每个类的特征。当您需要考虑一个系统未知的新值时，系统将根据其特征评估其类别。

*   *回归*—待预测值为连续变量时。最容易理解的情况是，当您想要从散点图中表示的一系列点中找到描述趋势的直线时。

#### 无监督学习

在这些方法中，训练集由一系列输入值 x 组成，而没有任何相应的目标值。

*   *聚类*—这些方法的目标是发现数据集中相似例子的组。

*   *降维*—将高维数据集降维为只有两个或三个维度的数据集不仅有利于数据可视化，而且有利于将非常高维度的数据转换为低得多的维度的数据，使得每个较低的维度传达更多的信息。

除了这两个主要类别，还有一组方法的目的是验证和评估模型。

### 训练集和测试集

机器学习能够通过模型从数据集中学习一些属性，并将它们应用于新数据。这是因为机器学习中的一个常见做法是评估一个算法。这种评估包括将数据分成两部分，一部分称为*训练集*，我们将通过它学习数据的属性，另一部分称为*测试集*，在其上测试这些属性。

## 使用 scikit-learn 进行监督学习

在这一章中，你会看到一些监督学习的例子。

*   分类，使用虹膜数据集
    *   k 近邻分类器

    *   支持向量机

*   回归，使用糖尿病数据集
    *   线性回归

    *   支持向量机

监督学习包括学习从训练集中读取值的两个或多个特征之间的可能模式；学习是可能的，因为训练集包含已知的结果(目标或标签)。scikit-learn 中的所有模型都被称为*监督估计器，*使用`fit(x, y)`函数进行训练。`x`包括观察到的特征，而`y`表示目标。一旦估计器完成了训练，它将能够为任何未标记的新观察值`x`预测`y`的值。该操作将通过`predict(x)`功能。

## 鸢尾花数据集

鸢尾花数据集是罗纳德·费雪爵士在 1936 年首次使用的特殊数据集。它通常也被称为安德森鸢尾数据集，因为收集数据的人直接测量了鸢尾花各部分的大小。在该数据集中，收集了三种不同鸢尾(丝毛鸢尾、海滨鸢尾和杂色鸢尾)的数据，这些数据对应于萼片的长度和宽度以及花瓣的长度和宽度(见图 [8-1](#Fig1) )。

![../images/336498_2_En_8_Chapter/336498_2_En_8_Fig1_HTML.jpg](../images/336498_2_En_8_Chapter/336498_2_En_8_Fig1_HTML.jpg)

图 8-1

杂色鸢尾的花瓣和萼片的宽度和长度

该数据集目前正被用作多种类型分析的良好示例，尤其是可通过机器学习方法处理的*分类、*问题。这个数据集作为一个 150x4 的 NumPy 数组与`scikit-learn`库一起提供，这并不是巧合。

现在，您将详细研究这个数据集，在 IPython QtConsole 或普通 Python 会话中导入它。

```py
In [ ]: from sklearn import datasets
   ...: iris = datasets.load_iris()

```

这样，您就在`iris`变量中加载了与虹膜数据集相关的所有数据和元数据。为了查看其中收集的数据的值，调用变量`iris`的属性`data`就足够了。

```py
In [ ]: iris.data
Out[ ]:
array([[ 5.1,  3.5,  1.4,  0.2],
       [ 4.9,  3\. ,  1.4,  0.2],
       [ 4.7,  3.2,  1.3,  0.2],
       [ 4.6,  3.1,  1.5,  0.2],
       ...

```

如您所见，您将获得一个包含 150 个元素的数组，每个元素包含四个数值:分别是萼片和花瓣的大小。

相反，要知道每一个项目属于哪一种花，您将参考`target`属性。

```py
In [ ]: iris.target
Out[ ]:
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])

```

您获得了 150 个具有三个可能的整数值(0、1 和 2)的项目，它们对应于三种鸢尾。要知道物种和数量的对应关系，就得调用`target_names`属性。

```py
In [ ]: iris.target_names
Out[ ]:
array(['setosa', 'versicolor', 'virginica'],
      dtype='|S10')

```

为了更好地理解这个数据集，你可以使用 matplotlib 库，使用你在第 [7](07.html) 章中学到的技术。因此，创建一个散点图，用三种不同的颜色显示三种不同的物种。x 轴代表萼片的长度，而 y 轴代表萼片的宽度。

```py
In [ ]: import matplotlib.pyplot as plt
   ...: import matplotlib.patches as mpatches
   ...: from sklearn import datasets
   ...:
   ...: iris = datasets.load_iris()
   ...: x = iris.data[:,0]  #X-Axis - sepal length
   ...: y = iris.data[:,1]  #Y-Axis - sepal length
   ...: species = iris.target     #Species
   ...:
   ...: x_min, x_max = x.min() - .5,x.max() + .5
   ...: y_min, y_max = y.min() - .5,y.max() + .5
   ...:
   ...: #SCATTERPLOT
   ...: plt.figure()
   ...: plt.title('Iris Dataset - Classification By Sepal Sizes')
   ...: plt.scatter(x,y, c=species)
   ...: plt.xlabel('Sepal length')
   ...: plt.ylabel('Sepal width')
   ...: plt.xlim(x_min, x_max)
   ...: plt.ylim(y_min, y_max)
   ...: plt.xticks(())
   ...: plt.yticks(())
   ...: plt.show()

```

结果，你得到了如图 [8-2](#Fig2) 所示的散点图。蓝色的是鸢尾，绿色的是杂色鸢尾，红色的是海滨鸢尾。

从图 [8-2](#Fig2) 中，你可以看到鸢尾的特征与其他两种不同，形成了一簇与其他不同的蓝点。

![../images/336498_2_En_8_Chapter/336498_2_En_8_Fig2_HTML.jpg](../images/336498_2_En_8_Chapter/336498_2_En_8_Fig2_HTML.jpg)

图 8-2

不同种类的鸢尾花以不同的颜色显示

尝试遵循相同的程序，但这一次使用其他两个变量，即花瓣的长度和宽度的措施。您可以使用相同的代码，只需更改几个值。

```py
In [ ]: import matplotlib.pyplot as plt
   ...: import matplotlib.patches as mpatches
   ...: from sklearn import datasets
   ...:
   ...: iris = datasets.load_iris()
   ...: x = iris.data[:,2]  #X-Axis - petal length
   ...: y = iris.data[:,3]  #Y-Axis - petal length
   ...: species = iris.target     #Species
   ...:
   ...: x_min, x_max = x.min() - .5,x.max() + .5
   ...: y_min, y_max = y.min() - .5,y.max() + .5
   ...: #SCATTERPLOT
   ...: plt.figure()
   ...: plt.title('Iris Dataset - Classification By Petal Sizes', size=14)
   ...: plt.scatter(x,y, c=species)
   ...: plt.xlabel('Petal length')
   ...: plt.ylabel('Petal width')
   ...: plt.xlim(x_min, x_max)
   ...: plt.ylim(y_min, y_max)
   ...: plt.xticks(())
   ...: plt.yticks(())

```

结果是如图 [8-3](#Fig3) 所示的散点图。在这种情况下，这三个物种之间的区别更加明显。如您所见，您有三个不同的集群。

![../images/336498_2_En_8_Chapter/336498_2_En_8_Fig3_HTML.jpg](../images/336498_2_En_8_Chapter/336498_2_En_8_Fig3_HTML.jpg)

图 8-3

不同种类的鸢尾花以不同的颜色显示

### 主成分分析分解

考虑到花瓣和萼片大小的四个测量值，你已经看到了这三个物种是如何被表征的。我们描绘了两个散点图，一个是花瓣，一个是萼片，但是你怎么能把整个事情统一起来呢？四维是一个即使散点图 3D 也无法解决的问题。

在这方面，开发了一种称为*主成分分析(PCA)* 的特殊技术。这种技术可以减少系统的维数，保留各点特征的所有信息，生成的新维数称为*主成分*。在我们的例子中，您可以将系统从四维缩减到三维，然后在三维散点图中绘制结果。这样，您可以使用萼片和花瓣的测量值来表征数据集中测试元素的各种鸢尾。

允许您进行维度缩减的`scikit-learn`函数是`fit_transform()`函数。它属于`PCA`的对象。为了使用它，首先你需要导入 PCA `sklearn.decomposition`模块。然后，您必须使用`PCA()`定义对象构造函数，并将新维度(主成分)的数量定义为`n_components`选项的值。对你来说，是 3。最后，您必须通过传递四维虹膜数据集作为参数来调用`fit_transform()`函数。

```py
from sklearn.decomposition import PCA
x_reduced = PCA(n_components=3).fit_transform(iris.data)

```

此外，为了可视化新值，您将使用 matplotlib 的`mpl_toolkits.mplot3d`模块使用散点图 3D。如果您不记得如何操作，请参见第 [7](07.html) 章中的散点图 3D 部分。

```py
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import datasets
from sklearn.decomposition import PCA

iris = datasets.load_iris()
species = iris.target     #Species
x_reduced = PCA(n_components=3).fit_transform(iris.data)

#SCATTERPLOT 3D
fig = plt.figure()
ax = Axes3D(fig)
ax.set_title('Iris Dataset by PCA', size=14)
ax.scatter(x_reduced[:,0],x_reduced[:,1],x_reduced[:,2], c=species)
ax.set_xlabel('First eigenvector')
ax.set_ylabel('Second eigenvector')
ax.set_zlabel('Third eigenvector')
ax.w_xaxis.set_ticklabels(())
ax.w_yaxis.set_ticklabels(())
ax.w_zaxis.set_ticklabels(())

```

结果将是如图 [8-4](#Fig4) 所示的散点图。这三种鸢尾彼此之间有很好的特征，可以形成一个类群。

![../images/336498_2_En_8_Chapter/336498_2_En_8_Fig4_HTML.jpg](../images/336498_2_En_8_Chapter/336498_2_En_8_Fig4_HTML.jpg)

图 8-4

代表每种鸢尾的三个集群的 3D 散点图

## k 近邻分类器

现在，您将执行一个*分类*，为了对`scikit-learn`库执行这个操作，您需要一个*分类器*。

给定一朵鸢尾花的新测量值，分类器的任务是找出它属于三个物种中的哪一个。最简单的可能分类器是*最近邻。*该算法将在训练集中搜索最接近新测试样本的观察值。

此时需要考虑的一个非常重要的事情是*训练集*和*测试集*的概念(已经在第 [1](01.html) 章中看到)。事实上，如果您只有一个数据集，不要在测试和训练中使用相同的数据是很重要的。在这方面，数据集的元素被分成两部分，一部分专用于训练算法，另一部分用于执行其验证。

因此，在进一步处理之前，你必须将虹膜数据集分成两部分。但是，明智的做法是随机混合数组元素，然后进行除法运算。事实上，通常数据可能是按照特定的顺序收集的，在您的情况下，Iris 数据集包含按物种排序的项目。因此，为了混合数据集的元素，您将使用一个名为`random.permutation()`的 NumPy 函数。混合数据集由 150 个不同的观测值组成；前 140 个将用作训练集，剩下的 10 个将用作测试集。

```py
import numpy as np
from sklearn import datasets
np.random.seed(0)
iris = datasets.load_iris()
x = iris.data
y = iris.target
i = np.random.permutation(len(iris.data))
x_train = x[i[:-10]]
y_train = y[i[:-10]]
x_test = x[i[-10:]]
y_test = y[i[-10:]]

```

现在，您可以应用 K-最近邻算法。导入`KNeighborsClassifier`，调用分类器的构造器，然后用`fit()`函数训练。

```py
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(x_train,y_train)
Out[86]:
KNeighborsClassifier(algorithm='auto', leaf_size=30, metric="minkowski",
           metric_params=None, n_neighbors=5, p=2, weights="uniform")

```

现在你已经有了一个由 140 次观察训练的`knn`分类器组成的预测模型，你将发现它是如何有效的。分类器应该正确地预测测试集的 10 个观察值的虹膜种类。为了获得预测，您必须使用`predict()`函数，该函数将直接应用于预测模型`knn`。最后，您将比较`y_test`中包含的预测结果和实际观察结果。

```py
knn.predict(x_test)
Out[100]: array([1, 2, 1, 0, 0, 0, 2, 1, 2, 0])
y_test
Out[101]: array([1, 1, 1, 0, 0, 0, 2, 1, 2, 0])

```

你可以看到你得到了 10%的误差。现在你可以用萼片的 2D 散点图所代表的空间中的决策边界来可视化这一切。

```py
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn import datasets
from sklearn.neighbors import KNeighborsClassifier
iris = datasets.load_iris()
x = iris.data[:,:2]      #X-Axis - sepal length-width
y = iris.target          #Y-Axis - species

x_min, x_max = x[:,0].min() - .5,x[:,0].max() + .5
y_min, y_max = x[:,1].min() - .5,x[:,1].max() + .5

#MESH
cmap_light = ListedColormap(['#AAAAFF','#AAFFAA','#FFAAAA'])
h = .02
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
knn = KNeighborsClassifier()
knn.fit(x,y)
Z = knn.predict(np.c_[xx.ravel(),yy.ravel()])
Z = Z.reshape(xx.shape)
plt.figure()
plt.pcolormesh(xx,yy,Z,cmap=cmap_light)

#Plot the training points
plt.scatter(x[:,0],x[:,1],c=y)
plt.xlim(xx.min(),xx.max())
plt.ylim(yy.min(),yy.max())

Out[120]: (1.5, 4.900000000000003)

```

你在决策边界中得到散点图的细分，如图 [8-5](#Fig5) 所示。

![../images/336498_2_En_8_Chapter/336498_2_En_8_Fig5_HTML.jpg](../images/336498_2_En_8_Chapter/336498_2_En_8_Fig5_HTML.jpg)

图 8-5

三个决策边界由三种不同的颜色表示

考虑到花瓣的大小，你也可以做同样的事情。

```py
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn import datasets
from sklearn.neighbors import KNeighborsClassifier
iris = datasets.load_iris()
x = iris.data[:,2:4]      #X-Axis - petals length-width
y = iris.target           #Y-Axis - species

x_min, x_max = x[:,0].min() - .5,x[:,0].max() + .5
y_min, y_max = x[:,1].min() - .5,x[:,1].max() + .5

#MESH
cmap_light = ListedColormap(['#AAAAFF','#AAFFAA','#FFAAAA'])
h = .02
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
knn = KNeighborsClassifier()
knn.fit(x,y)
Z = knn.predict(np.c_[xx.ravel(),yy.ravel()])
Z = Z.reshape(xx.shape)
plt.figure()
plt.pcolormesh(xx,yy,Z,cmap=cmap_light)

#Plot the training points
plt.scatter(x[:,0],x[:,1],c=y)
plt.xlim(xx.min(),xx.max())
plt.ylim(yy.min(),yy.max())

Out[126]: (-0.40000000000000002, 2.9800000000000031)

```

如图 [8-6](#Fig6) 所示，考虑到花瓣的大小，您将拥有关于鸢尾花特征的相应决策界限。

![../images/336498_2_En_8_Chapter/336498_2_En_8_Fig6_HTML.jpg](../images/336498_2_En_8_Chapter/336498_2_En_8_Fig6_HTML.jpg)

图 8-6

描述花瓣大小的 2D 散点图上的决策边界

## 糖尿病数据集

在`scikit-learn`库中可用的各种数据集当中，有一个是糖尿病数据集。这个数据集在 2004 年第一次被使用(*统计年鉴*，作者 Efron，Hastie，Johnston 和 Tibshirani)。从那时起，它已经成为一个广泛用于研究各种预测模型及其有效性的例子。

为了上传这个数据集中包含的数据，在你必须导入`scikit-learn`库的`datasets`模块之前，然后你调用`load_diabetes()`函数将数据集加载到一个名为`diabetes`的变量中。

```py
In [ ]: from sklearn import datasets
   ...: diabetes = datasets.load_diabetes()

```

该数据集包含从 442 名患者收集的生理数据，并作为一年后疾病进展的相应指标。生理数据占据前 10 列，其值分别表示以下内容:

*   年龄

*   性

*   体重指数

*   血压

*   S1、S2、S3、S4、S5 和 S6(六次血清测量)

这些度量可以通过调用`data`属性来获得。但是，如果要检查数据集中的值，您会发现这些值与您预期的非常不同。例如，我们查看第一个患者的 10 个值。

```py
diabetes.data[0]
Out[ ]:
array([ 0.03807591,  0.05068012,  0.06169621,  0.02187235, -0.0442235 ,
       -0.03482076, -0.04340085, -0.00259226,  0.01990842, -0.01764613])

```

这些值实际上是处理的结果。10 个值中的每一个都以平均值为中心，随后用标准偏差乘以样本数进行缩放。检查会发现每列的平方和等于 1。试着用年龄测量值做这个计算；你会得到一个非常接近 1 的值。

```py
np.sum(diabetes.data[:,0]**2)
Out[143]: 1.0000000000000746

```

尽管这些值是标准化的，因此难以读取，但它们继续表达 10 个生理特征，因此没有丢失它们的值或统计信息。

至于疾病进展的指标，也就是必须与你的预测结果相对应的值，这些都可以通过`target`属性获得。

```py
diabetes.target

Out[146]:
array([ 151.,   75.,  141.,  206.,  135.,   97.,  138.,   63.,  110.,
        310.,  101.,   69.,  179.,  185.,  118.,  171.,  166.,  144.,
         97.,  168.,   68.,   49.,   68.,  245.,  184.,  202.,  137
        . . .

```

您将获得一系列介于 25 和 346 之间的 442 个整数值。

## 线性回归:最小二乘回归

线性回归是一种使用训练集中包含的数据来构建线性模型的过程。最简单的是基于带有两个参数`a`和`b`的 a `rect`的方程来表征它。将计算这些参数，以使残差平方和尽可能小。

*y = a*x + c*

在这个表达式中，`x`是训练集，`y`是目标，`b`是斜率，`c`是模型所代表的`rect`的截距。在`scikit-learn`中，要使用线性回归的预测模型，您必须导入`linear_model`模块，然后使用制造商的`LinearRegression()`构造函数来创建预测模型，您称之为`linreg`。

```py
from sklearn import linear_model
linreg = linear_model.LinearRegression()

```

为了练习线性回归的例子，您可以使用前面描述的糖尿病数据集。首先，您需要将 442 名患者分为一个训练集(由前 422 名患者组成)和一个测试集(后 20 名患者)。

```py
from sklearn import datasets
diabetes = datasets.load_diabetes()
x_train = diabetes.data[:-20]
y_train = diabetes.target[:-20]
x_test = diabetes.data[-20:]
y_test = diabetes.target[-20:]

```

现在通过使用`fit()`函数将训练集应用于预测模型。

```py
linreg.fit(x_train,y_train)
Out[ ]: LinearRegression(copy_X=True, fit_intercept=True, normalize=False)

```

一旦模型训练完毕，您就可以使用预测模型的`coef_`属性获得为每个生理变量计算的 10 个`b`系数。

```py
linreg.coef_
Out[164]:
array([  3.03499549e-01,  -2.37639315e+02,   5.10530605e+02,
         3.27736980e+02,  -8.14131709e+02,   4.92814588e+02,
         1.02848452e+02,   1.84606489e+02,   7.43519617e+02,
         7.60951722e+01])

```

如果您将测试集应用于`linreg`预测模型，您将获得一系列要与实际观察值进行比较的目标。

```py
linreg.predict(x_test)

Out[ ]:
array([ 197.61846908,  155.43979328,  172.88665147,  111.53537279,
        164.80054784,  131.06954875,  259.12237761,  100.47935157,
        117.0601052 ,  124.30503555,  218.36632793,   61.19831284,
        132.25046751,  120.3332925 ,   52.54458691,  194.03798088,
        102.57139702,  123.56604987,  211.0346317 ,   52.60335674])

y_test
Out[ ]:
array([ 233.,   91.,  111.,  152.,  120.,   67.,  310.,   94.,  183.,
         66.,  173.,   72.,   49.,   64.,   48.,  178.,  104.,  132.,
        220.,   57.])

```

然而，什么样的预测应该是完美的一个很好的指标是*方差*。方差越接近 1，预测就越准确。

```py
linreg.score(x_test, y_test)
Out[ ]: 0.58507530226905713

```

现在你将从线性回归开始，考虑单一的生理因素，例如，你可以从年龄开始。

```py
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model
from sklearn import datasets
diabetes = datasets.load_diabetes()
x_train = diabetes.data[:-20]
y_train = diabetes.target[:-20]
x_test = diabetes.data[-20:]
y_test = diabetes.target[-20:]

x0_test = x_test[:,0]
x0_train = x_train[:,0]
x0_test = x0_test[:,np.newaxis]
x0_train = x0_train[:,np.newaxis]
linreg = linear_model.LinearRegression()
linreg.fit(x0_train,y_train)
y = linreg.predict(x0_test)
plt.scatter(x0_test,y_test,color='k')
plt.plot(x0_test,y,color='b',linewidth=3)

Out[230]: [<matplotlib.lines.Line2D at 0x380b1908>]

```

图 [8-7](#Fig7) 显示了代表患者年龄和疾病进展之间线性相关性的蓝线。

![../images/336498_2_En_8_Chapter/336498_2_En_8_Fig7_HTML.jpg](../images/336498_2_En_8_Chapter/336498_2_En_8_Fig7_HTML.jpg)

图 8-7

线性回归表示特征和目标之间的线性相关性

实际上，糖尿病数据集中有 10 个生理因素。因此，为了更全面地了解所有训练集，您可以对每个生理特征进行线性回归，创建 10 个模型，并通过线性图表查看每个模型的结果。

```py
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model
from sklearn import datasets
diabetes = datasets.load_diabetes()
x_train = diabetes.data[:-20]
y_train = diabetes.target[:-20]
x_test = diabetes.data[-20:]
y_test = diabetes.target[-20:]
plt.figure(figsize=(8,12))
for f in range(0,10):
   xi_test = x_test[:,f]
   xi_train = x_train[:,f]
   xi_test = xi_test[:,np.newaxis]
   xi_train = xi_train[:,np.newaxis]
   linreg.fit(xi_train,y_train)
   y = linreg.predict(xi_test)
   plt.subplot(5,2,f+1)
   plt.scatter(xi_test,y_test,color='k')
   plt.plot(xi_test,y,color='b',linewidth=3)

```

图 [8-8](#Fig8) 显示了 10 个线性图表，每个图表代表一个生理因素与糖尿病进展之间的相关性。

![../images/336498_2_En_8_Chapter/336498_2_En_8_Fig8_HTML.png](../images/336498_2_En_8_Chapter/336498_2_En_8_Fig8_HTML.png)

图 8-8

显示生理因素和糖尿病进展之间相关性的 10 个线性图表

## 支持向量机

*支持向量机*是一系列机器学习技术，最早于 20 世纪 90 年代初由 Vapnik 和他的同事在 AT & T 实验室开发。这类程序的基础实际上是一种叫做*支持向量*的算法，它是以前一种叫做广义肖像的算法的推广，也是由 Vapnik 于 1963 年在俄国开发的。

简而言之，SVM 分类器是二元或区分模型，处理两类区分。他们的主要任务基本上是区分两个类之间的新观测值。在学习阶段，这些分类器将观察结果投射到一个名为*决策空间*的多维空间中，并构建一个名为*决策边界*的分离面，将该空间划分为两个归属区域。在最简单的情况下，即线性情况下，决策边界将由平面(在 3D 中)或直线(在 2D 中)表示。在更复杂的情况下，分离面是弯曲的形状，具有越来越多的铰接形状。

SVM 既可用于使用 *SVR(支持向量回归)*的回归，也可用于使用 *SVC(支持向量分类)*的分类。

### 支持向量分类

如果你想更好地理解这个算法是如何工作的，你可以从参考最简单的情况开始，那就是线性 2D 情况，其中决策边界将是一条直线，将决策区域分成两部分。以一个简单的训练集为例，其中一些点被分配给两个不同的类。训练集将由 11 个点(观察值)组成，具有两个不同的属性，其值在 0 到 4 之间。这些值将包含在名为`x`的 NumPy 数组中。它们属于两类中的一类将由包含在另一个名为`y`的数组中的 0 或 1 值来定义。

用散点图可视化这些点在空间中的分布，然后将其定义为决策空间(见图 [8-9](#Fig9) )。

![../images/336498_2_En_8_Chapter/336498_2_En_8_Fig9_HTML.jpg](../images/336498_2_En_8_Chapter/336498_2_En_8_Fig9_HTML.jpg)

图 8-9

训练集的散点图显示决策空间

```py
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
x = np.array([[1,3],[1,2],[1,1.5],[1.5,2],[2,3],[2.5,1.5],
     [2,1],[3,1],[3,2],[3.5,1],[3.5,3]])
y = [0]*6 + [1]*5
plt.scatter(x[:,0],x[:,1],c=y,s=50,alpha=0.9)
Out[360]: <matplotlib.collections.PathCollection at 0x545634a8>

```

既然已经定义了训练集，就可以应用 SVC(支持向量分类)算法了。该算法将创建一条线(决策边界)以将决策区域分为两个部分(见图 [8-10](#Fig10) ),这条直线将被放置以最大化其包含在训练集中的最近观察的距离。这个条件应该产生两个不同的部分，其中应该包含同一类的所有点。

然后将 SVC 算法应用于训练集，为此，首先使用`SVC(` **)** 构造函数定义模型，将内核定义为线性。(内核是一类用于模式分析的算法。)然后，您将使用带有训练集的`fit()`函数作为参数。一旦模型被训练，你就可以用`decision_function()`函数绘制决策边界。然后绘制散点图，并为决策空间的两个部分提供不同的颜色。

```py
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
x = np.array([[1,3],[1,2],[1,1.5],[1.5,2],[2,3],[2.5,1.5],
     [2,1],[3,1],[3,2],[3.5,1],[3.5,3]])
y = [0]*6 + [1]*5
svc = svm.SVC(kernel='linear').fit(x,y)
X,Y = np.mgrid[0:4:200j,0:4:200j]
Z = svc.decision_function(np.c_[X.ravel(),Y.ravel()])
Z = Z.reshape(X.shape)
plt.contourf(X,Y,Z > 0,alpha=0.4)
plt.contour(X,Y,Z,colors=['k'], linestyles=['-'],levels=[0])
plt.scatter(x[:,0],x[:,1],c=y,s=50,alpha=0.9)

Out[363]: <matplotlib.collections.PathCollection at 0x54acae10>

```

在图 [8-10](#Fig10) 中，您可以看到包含两个类的两个部分。可以说除了红色部分有一个蓝点之外，划分是成功的。

![../images/336498_2_En_8_Chapter/336498_2_En_8_Fig10_HTML.jpg](../images/336498_2_En_8_Chapter/336498_2_En_8_Fig10_HTML.jpg)

图 8-10

决策区域被分成两部分

因此，一旦模型经过训练，就很容易理解预测是如何运作的。从图形上看，根据新观测所占据的位置，你会知道它在两个类之一中的相应成员。

相反，从更具编程性的角度来看，`predict()`函数将返回相应的所属类的编号(0 表示蓝色的类，1 表示红色的类)。

```py
svc.predict([[1.5,2.5]])
Out[56]: array([0])

svc.predict([[2.5,1]])
Out[57]: array([1])

```

与 SVC 算法相关的概念是*正则化*。它由参数`C`设置:C 值小意味着使用分离线周围的许多或所有观测值计算边距(正则化程度较高)，而 C 值大意味着边距是根据线分离附近的观测值计算的(正则化程度较低)。除非另有说明，否则 C 的默认值等于 1。您可以突出显示参与边距计算的点，通过`support_vectors`数组来标识它们。

```py
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
x = np.array([[1,3],[1,2],[1,1.5],[1.5,2],[2,3],[2.5,1.5],
     [2,1],[3,1],[3,2],[3.5,1],[3.5,3]])
y = [0]*6 + [1]*5
svc = svm.SVC(kernel='linear',C=1).fit(x,y)
X,Y = np.mgrid[0:4:200j,0:4:200j]
Z = svc.decision_function(np.c_[X.ravel(),Y.ravel()])
Z = Z.reshape(X.shape)
plt.contourf(X,Y,Z > 0,alpha=0.4)
plt.contour(X,Y,Z,colors=['k','k','k'], linestyles=['--','-','--'],levels=[-1,0,1])
plt.scatter(svc.support_vectors_[:,0],svc.support_vectors_[:,1],s=120,facecolors='r')
plt.scatter(x[:,0],x[:,1],c=y,s=50,alpha=0.9)

Out[23]: <matplotlib.collections.PathCollection at 0x177066a0>

```

这些点在散点图中用带边框的圆圈表示。此外，它们将位于分隔线附近的评估区域内(见图 [8-11](#Fig11) 中的虚线)。

![../images/336498_2_En_8_Chapter/336498_2_En_8_Fig11_HTML.jpg](../images/336498_2_En_8_Chapter/336498_2_En_8_Fig11_HTML.jpg)

图 8-11

计算中涉及的点数取决于 C 参数

要查看对决策边界的影响，可以将该值限制为 C = 0.1。看看会考虑多少分吧。

```py
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
x = np.array([[1,3],[1,2],[1,1.5],[1.5,2],[2,3],[2.5,1.5],
     [2,1],[3,1],[3,2],[3.5,1],[3.5,3]])
y = [0]*6 + [1]*5
svc = svm.SVC(kernel='linear',C=0.1).fit(x,y)
X,Y = np.mgrid[0:4:200j,0:4:200j]
Z = svc.decision_function(np.c_[X.ravel(),Y.ravel()])
Z = Z.reshape(X.shape)
plt.contourf(X,Y,Z > 0,alpha=0.4)
plt.contour(X,Y,Z,colors=['k','k','k'], linestyles=['--','-','--'],levels=[-1,0,1])
plt.scatter(svc.support_vectors_[:,0],svc.support_vectors_[:,1],s=120,facecolors='w')
plt.scatter(x[:,0],x[:,1],c=y,s=50,alpha=0.9)

Out[24]: <matplotlib.collections.PathCollection at 0x1a01ecc0>

```

考虑的点增加了，因此分离线(决策边界)改变了方向。但是现在有两个点处于错误的决策区域(见图 [8-12](#Fig12) )。

![../images/336498_2_En_8_Chapter/336498_2_En_8_Fig12_HTML.jpg](../images/336498_2_En_8_Chapter/336498_2_En_8_Fig12_HTML.jpg)

图 8-12

计算中涉及的点数随着 C 的减少而增加

### 非线性 SVC

到目前为止，您已经看到了 SVC 线性算法定义了一条旨在划分两个类的分隔线。还有更复杂的 SVC 算法，它们可以基于最大化最接近表面的点之间的距离的相同原理来建立曲线(2D)或曲面(3D)。让我们看看使用多项式核的系统。

顾名思义，您可以定义一条多项式曲线，将面积决策分为两部分。多项式的次数可由`degree`选项定义。即使在这种情况下，C 也是正则化系数。因此，请尝试应用具有三次多项式内核且 C 系数等于 1 的 SVC 算法。

```py
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
x = np.array([[1,3],[1,2],[1,1.5],[1.5,2],[2,3],[2.5,1.5],
     [2,1],[3,1],[3,2],[3.5,1],[3.5,3]])
y = [0]*6 + [1]*5
svc = svm.SVC(kernel='poly',C=1, degree=3).fit(x,y)
X,Y = np.mgrid[0:4:200j,0:4:200j]
Z = svc.decision_function(np.c_[X.ravel(),Y.ravel()])
Z = Z.reshape(X.shape)
plt.contourf(X,Y,Z > 0,alpha=0.4)
plt.contour(X,Y,Z,colors=['k','k','k'], linestyles=['--','-','--'],levels=[-1,0,1])
plt.scatter(svc.support_vectors_[:,0],svc.support_vectors_[:,1],s=120,facecolors='w')
plt.scatter(x[:,0],x[:,1],c=y,s=50,alpha=0.9)

Out[34]: <matplotlib.collections.PathCollection at 0x1b6a9198>

```

如你所见，你会得到如图 [8-13](#Fig13) 所示的情况。

![../images/336498_2_En_8_Chapter/336498_2_En_8_Fig13_HTML.jpg](../images/336498_2_En_8_Chapter/336498_2_En_8_Fig13_HTML.jpg)

图 8-13

使用具有多项式核的 SVC 的决策空间

另一种非线性核是*径向基函数(RBF)* 。在这种情况下，分离曲线倾向于相对于训练集的观察点径向地定义区域。

```py
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
x = np.array([[1,3],[1,2],[1,1.5],[1.5,2],[2,3],[2.5,1.5],
     [2,1],[3,1],[3,2],[3.5,1],[3.5,3]])
y = [0]*6 + [1]*5
svc = svm.SVC(kernel='rbf', C=1, gamma=3).fit(x,y)
X,Y = np.mgrid[0:4:200j,0:4:200j]
Z = svc.decision_function(np.c_[X.ravel(),Y.ravel()])
Z = Z.reshape(X.shape)
plt.contourf(X,Y,Z > 0,alpha=0.4)
plt.contour(X,Y,Z,colors=['k','k','k'], linestyles=['--','-','--'],levels=[-1,0,1])
plt.scatter(svc.support_vectors_[:,0],svc.support_vectors_[:,1],s=120,facecolors='w')
plt.scatter(x[:,0],x[:,1],c=y,s=50,alpha=0.9)

Out[43]: <matplotlib.collections.PathCollection at 0x1cb8d550>

```

在图 [8-14](#Fig14) 中，您可以看到决策的两个部分，训练集的所有点都被正确定位。

![../images/336498_2_En_8_Chapter/336498_2_En_8_Fig14_HTML.jpg](../images/336498_2_En_8_Chapter/336498_2_En_8_Fig14_HTML.jpg)

图 8-14

决策区域使用具有 RBF 核的 SVC

### 使用 Iris 数据集绘制不同的 SVM 分类器

你刚才看到的 SVM 的例子是基于一个非常简单的数据集。本节使用更复杂的数据集来解决 SVC 的分类问题。事实上，它使用了以前使用的数据集:虹膜数据集。

以前使用的 SVC 算法是从只包含两个类的训练集中学习的。在这种情况下，您将把案例扩展到三个分类，因为 Iris 数据集被分成三个类，对应于三种不同的花卉。

在这种情况下，决策边界彼此相交，将决策区域(在 2D 的情况下)或决策体积(3D)细分成几个部分。

两个线性模型都具有线性决策边界(相交超平面)，而具有非线性核(多项式或高斯 RBF)的模型具有非线性决策边界。对于依赖于内核类型及其参数的图形，这些边界更加灵活。

```py
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets

iris = datasets.load_iris()
x = iris.data[:,:2]
y = iris.target
h = .05
svc = svm.SVC(kernel='linear',C=1.0).fit(x,y)
x_min,x_max = x[:,0].min() - .5, x[:,0].max() + .5
y_min,y_max = x[:,1].min() - .5, x[:,1].max() + .5

h = .02
X, Y = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min,y_max,h))

Z = svc.predict(np.c_[X.ravel(),Y.ravel()])
Z = Z.reshape(X.shape)
plt.contourf(X,Y,Z,alpha=0.4)
plt.contour(X,Y,Z,colors='k')
plt.scatter(x[:,0],x[:,1],c=y)

Out[49]: <matplotlib.collections.PathCollection at 0x1f2bd828>

```

在图 [8-15](#Fig15) 中，决策空间被决策边界分成三个部分。

![../images/336498_2_En_8_Chapter/336498_2_En_8_Fig15_HTML.jpg](../images/336498_2_En_8_Chapter/336498_2_En_8_Fig15_HTML.jpg)

图 8-15

决策边界把决策区域分成三个不同的部分

现在是应用非线性核来生成非线性决策边界的时候了，比如多项式核。

```py
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets

iris = datasets.load_iris()
x = iris.data[:,:2]
y = iris.target
h = .05

svc = svm.SVC(kernel='poly',C=1.0,degree=3).fit(x,y)

x_min,x_max = x[:,0].min() - .5, x[:,0].max() + .5
y_min,y_max = x[:,1].min() - .5, x[:,1].max() + .5

h = .02
X, Y = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min,y_max,h))

Z = svc.predict(np.c_[X.ravel(),Y.ravel()])
Z = Z.reshape(X.shape)
plt.contourf(X,Y,Z,alpha=0.4)
plt.contour(X,Y,Z,colors='k')
plt.scatter(x[:,0],x[:,1],c=y)

Out[50]: <matplotlib.collections.PathCollection at 0x1f4cc4e0>

```

图 [8-16](#Fig16) 显示了与线性情况相比，多项式决策边界如何以非常不同的方式分割区域。

![../images/336498_2_En_8_Chapter/336498_2_En_8_Fig16_HTML.jpg](../images/336498_2_En_8_Chapter/336498_2_En_8_Fig16_HTML.jpg)

图 8-16

在多项式的情况下，蓝色部分不直接与红色部分相连

现在，您可以应用 RBF 核来查看区域分布的差异。

```py
svc = svm.SVC(kernel='rbf', gamma=3, C=1.0).fit(x,y)

```

图 [8-17](#Fig17) 显示了 RBF 内核如何生成径向区域。

![../images/336498_2_En_8_Chapter/336498_2_En_8_Fig17_HTML.jpg](../images/336498_2_En_8_Chapter/336498_2_En_8_Fig17_HTML.jpg)

图 8-17

核 RBF 定义了径向决策区域

### 支持向量回归机

SVC 方法甚至可以扩展到解决回归问题。这种方法叫做*支持向量回归*。

SVC 产生的模型实际上不依赖于完整的训练集，而是仅使用元素的子集，即最接近决策边界的元素。同样，SVR 生成的模型也只依赖于训练集的一个子集。

我们将演示 SVR 算法如何使用您在本章中已经看到的糖尿病数据集。举例来说，您将仅参考第三生理数据。您将执行三个不同的回归，一个线性和两个非线性(多项式)。线性情况将产生一条直线，因为线性预测模型非常类似于前面看到的线性回归，而多项式回归将建立在二次和三次的基础上。`SVR()`功能与之前看到的`SVC()function`几乎相同。

唯一要考虑的方面是测试数据集必须按升序排序。

```py
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
from sklearn import datasets
diabetes = datasets.load_diabetes()
x_train = diabetes.data[:-20]
y_train = diabetes.target[:-20]
x_test = diabetes.data[-20:]
y_test = diabetes.target[-20:]

x0_test = x_test[:,2]
x0_train = x_train[:,2]
x0_test = x0_test[:,np.newaxis]
x0_train = x0_train[:,np.newaxis]

x0_test.sort(axis=0)
x0_test = x0_test*100
x0_train = x0_train*100

svr = svm.SVR(kernel='linear',C=1000)
svr2 = svm.SVR(kernel='poly',C=1000,degree=2)
svr3 = svm.SVR(kernel='poly',C=1000,degree=3)
svr.fit(x0_train,y_train)
svr2.fit(x0_train,y_train)
svr3.fit(x0_train,y_train)
y = svr.predict(x0_test)
y2 = svr2.predict(x0_test)
y3 = svr3.predict(x0_test)
plt.scatter(x0_test,y_test,color='k')
plt.plot(x0_test,y,color='b')
plt.plot(x0_test,y2,c='r')
plt.plot(x0_test,y3,c='g')

Out[155]: [<matplotlib.lines.Line2D at 0x262e10b8>]

```

三条回归曲线将用三种颜色表示。线性回归将是蓝色的；二次多项式即抛物线，会是红色的；而三次多项式会是绿色的(见图 [8-18](#Fig18) )。

![../images/336498_2_En_8_Chapter/336498_2_En_8_Fig18_HTML.jpg](../images/336498_2_En_8_Chapter/336498_2_En_8_Fig18_HTML.jpg)

图 8-18

三条回归曲线从训练集开始产生非常不同的趋势

## 结论

在这一章中，你看到了使用`scikit-learn`库解决回归和分类问题的最简单的例子。通过一些实例，以实用的方式介绍了预测模型验证阶段的许多概念。

在下一章中，你将看到一个完整的案例，其中数据分析的所有步骤都是通过一个实际的例子来讨论的。一切都将在 IPython Notebook 上实现，这是一个交互式的创新环境，非常适合以交互式文档的形式共享数据分析的每一步，可用作报告或 web 演示。